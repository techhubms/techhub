{
  "Tags": [
    ".NET"
  ],
  "OutputDir": "_blogs",
  "FeedLevelAuthor": "Rick Strahl's Web Log",
  "ProcessedDate": "2025-08-05 14:25:42",
  "FeedUrl": "https://feeds.feedburner.com/rickstrahl",
  "Title": "Configuring Microsoft.AI.Extensions with multiple providers",
  "Description": "Microsoft.Extensions.AI is the new base library for creating AI clients in an abstracted way. While the library does a great job with the abstraction of the interfaces it works with, the provider interfaces that create the intial abstractions like IChatClient are a lot less user friendly with hard to discover syntax based on extension methods and different syntax for each provider. In this post I review three providers and how to get them instantiated along with a small streaming example to use them.",
  "Link": "https://weblog.west-wind.com/posts/2025/May/30/Configuring-MicrosoftAIExtension-with-multiple-providers",
  "FeedName": "Rick Strahl's Blog",
  "Author": "Rick Strahl",
  "EnhancedContent": "![](/images/HeroImages/RickHero11.jpg) ![](/images/rick175x175.jpg)\n\nhttps://twitter.com/rickstrahl https://feeds.feedburner.com/rickstrahl\n\n[Contact](https://west-wind.com/contact/) Â  â€¢ [Articles](https://west-wind.com/articles.aspx) Â  â€¢ [Products](https://store.west-wind.com) Â  â€¢ [Support](https://support.west-wind.com) Â  â€¢ [Advertise](https://weblog.west-wind.com/advertise)\n\nSponsored by:\n\n[!\\[\\](https://markdownmonster.west-wind.com/Images/MarkdownMonster_Icon_256.png) **Markdown Monster**](https://markdownmonster.west-wind.com?utm_campaign=westwind-weblog-sponsored)\n- The Markdown Editor for Windows\nhttps://markdownmonster.west-wind.com?utm_campaign=westwind-weblog-sponsored\n\n[advertise here](/advertise)\n\nShare on:\n\nhttps://twitter.com/intent/tweet?url=https://weblog.west-wind.com/posts/2025/May/30/Configuring-MicrosoftAIExtension-with-multiple-providers&amp;text=Configuring%20Microsoft.AI.Extensions%20with%20multiple%20providers&amp;via=RickStrahl https://www.facebook.com/sharer/sharer.php?u=https://weblog.west-wind.com/posts/2025/May/30/Configuring-MicrosoftAIExtension-with-multiple-providers&amp;t=Configuring%20Microsoft.AI.Extensions%20with%20multiple%20providers http://www.reddit.com/submit?url=https://weblog.west-wind.com/posts/2025/May/30/Configuring-MicrosoftAIExtension-with-multiple-providers&amp;title=Configuring%20Microsoft.AI.Extensions%20with%20multiple%20providers\n\nMay 30, 2025 â€¢ from Hood River, Oregon\n\n:P\n\nOn this page:\n\n![Post Banner](https://weblog.west-wind.com/images/2025/Configuring-Microsoft-AI-Extension-with-multiple-providers/PostBanner.jpg)\n\nI just went through a bit of a struggle to find the right ways to instantiate different AI providers for a streaming completions project that I'm working on, and so I thought I'd write a short post to show the required invocations to get various providers loaded.\n\nWhy now? Up until now I've been using my own [home brew Http based OpenAI interface](https://github.com/RickStrahl/Westwind.Ai) for making OpenAI calls. It's lightweight, has no heavy dependencies and it's allowed me to keep a simple API interface that has skirted nearly two years of API churn that the Microsoft interfaces have suffered. But the interface is very simple and it only works with transactional chat/completions and Dall-E image generation. The Microsoft APIs handle a lot more than that, and I knew at some point - once the API churn calms down a bit - I would eventually switch to the new libraries.\n\nThat time came today as I needed to work with streaming completions and so I'm off to try out `Microsoft.AI.Extensions` . In my use case I need to let end users choose their providers and keys so I need to be able to fairly generically use different AI providers - OpenAI, Azure and Ollama for the most part and the extension provide those interfaces.\n\nI expected this to be pretty easy to do, but it turns out it's not quite as simple as I thought. The `Microsoft.AI.Extensions` SDK is relatively new and has gone through a ton of changes, and... because the various provider implementations for OpenAI, Azure and Ollama are still in preview they are still changing and in the churn. Several problems with this: Much of the documentation online is wrong, and if you try to use LLMs for direction or examples, they are **highly** likely to show and build outdated code with outdated libraries.\n\nIn fact, that's exactly what happened to me. I had Claude build a small sample for me and it used an old version of the libraries. The code worked with a few minor tweaks and some missing dependencies - Great! However, when I tried to update the libraries to the latest stable release everything broke, and there was no obvious way to fix it. All the types and the myriad of extension methods had changed and I spent the next 2 hours hunting around for the right syntax with many bad starts and outdated information from 3 different LLMs and the Microsoft docs. Eventually I found the right place in the documentation, but even so the information was scattered in different places for each of the providers none of it in places where you'd expected it to be.\n\n> >\n> Got a message from Stephen Toub earlier, who pointed out - embarrasingly for me - that the ReadMe files of the NuGet packages have the information. That was one place I didn't look.ðŸ˜„\n> >\n\nRegardless, I thought I'd write down how to set up the different providers in one place in this post, if for nothing else that I can easily reference it later.\n\n[!\\[\\](/images/sponsors/banner-example.png?v=1.2)](https://markdownmonster.west-wind.com?ut=weblog)\n\n## Accessing ChatClients with Various Providers\n\nSo as of today end of May 2025, here's what's needed to support:\n\n- OpenAI\n- Azure OpenAI\n- Ollama\n- Any other OpenAI client that uses the standard OpenAI protocol\n\nwhich is essentially the same what I support in my home brew AI engine (with its limited functionality).\n\nIf you use all three providers (and you can remove the relevant dependencies for what you don't use obviously) you need a bunch of libraries:\n\n**NuGet packages:**\n\n```xml <ItemGroup> <PackageReference Include=\"Microsoft.Extensions.AI\" Version=\"9.5.0\" /> <PackageReference Include=\"Microsoft.Extensions.AI.OpenAI\" Version=\"9.5.0-preview.1.25265.7\" /> <PackageReference Include=\"Microsoft.Extensions.AI.Ollama\" Version=\"9.5.0-preview.1.25265.7\" /> <PackageReference Include=\"Microsoft.Extensions.AI.AzureAIInference\" Version=\"9.5.0-preview.1.25265.7\" /> <PackageReference Include=\"Azure.AI.OpenAI\" Version=\"2.2.0-beta.4\" /> <PackageReference Include=\"Azure.Identity\" Version=\"1.14.0\" />\n\n<PackageReference Include=\"Microsoft.Extensions.DependencyInjection\" Version=\"9.0.5\" />\n\n<!-- unrelated to the AI libraries but probably need this --> <PackageReference Include=\"Microsoft.Extensions.Configuration\" Version=\"9.0.5\" /> <PackageReference Include=\"Microsoft.Extensions.Configuration.Json\" Version=\"9.0.5\" /> <PackageReference Include=\"Microsoft.Extensions.Logging\" Version=\"9.0.5\" /> </ItemGroup>\n\n```\n\nThe biggest pisser in this list - as always - is the `Azure.Identity` library which adds shit-ton of dependencies which you are unlikely to even be using if you're using API keys.\n\n> >\n> Note I'm doing this as part of a desktop application, but this works for any application. ASP.NET has an additional package that has builder syntax to add AI libraries to the ASP.NET DI Service collection - that's not covered here.\n> >\n\n## Instantiating a Chat Client\n\nThe goal with this extension library is that you can use different providers and have a single interface for the actual AI operations like Chat, Completions, Images etc. and the library does a good job with that.\n\nHowever, the first step of setting up the providers - that is a pain in the ass, because each provider has a different setup procedure.\n\nAll the examples end up with an `IChatClient` instance which can then be used the same independently of the provider:\n\n```cs private readonly IChatClient _chatClient;\n\n```\n\nEach of the providers then has their own syntax:\n\n### OpenAI\n\nThis one was the trickiest because I didn't find the 'right' documentation. The OpenAI client in these extensions has gone through many preview syntax iterations. LLM generated code was using an old library and while that worked great it completely broke when using the latest release library.\n\n> >\n> The most up to date documentation is in the NuGet Readme file!\n> >\n\nThe following uses the stable `Microsoft.Extensions.AI` and a preview of `Microsoft.Extensions.AI.OpenAI` .\n\nOpenAI typically needs only an API key and model name, but you can also use this library for other providers and provide an endpoint Uri. Since many providers use OpenAI style APIs this driver works for most of them (Except Azure with its 'special' deployment requirements).\n\n```cs var apiKey = Environment.GetEnvironmentVariable(\"OPENAI_KEY\"); _chatClient = new OpenAIClient(apiKey) .GetChatClient(\"gpt-4o-mini\") .AsIChatClient();\n\n```\n\n### Azure OpenAI\n\nAzure has to make things more difficult of course, because of the way Azure OpenAI has to be self-hosted in your own Azure deployment. These models are hosted as an Azure resource where you basically run your own OpenAI server, rather than using a packaged service provided directly by Microsoft. You essentially set up a custom hosted model and then provide access to your own hosted Azure OpenAI deployment.\n\nUsing the Azure provider requires pulling in a whole bunch of extra Azure libraries and you need to specify the Azure site base url, and deployment name in addition to an API key or other Azure credentials.\n\nHere's the code to use set up the Azure provider with an API key:\n\n```csharp // site is something like https://youraisite.openai.azure.com/ var site = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_SITE\"); apiKey = Environment.GetEnvironmentVariable(\"AZURE_OPENAI_KEY\"); var deployment = \"gpt-4o-mini\"; // deployment name - name it for the model _chatClient = new AzureOpenAIClient( new Uri(site), new ApiKeyCredential(apiKey)) .GetChatClient(deployment) .AsIChatClient();\n\n```\n\n### Ollama\n\nFor accessing local models Ollama is the most common choice. Ollama lets you download and install models from Ollama or Hugging face repositories and then run them locally.\n\nThere are two ways you can do this:\n\n- Using the OpenAI Provider shown above\n- Using the custom Ollama Provider\n\n#### Using the OpenAI provider\n\nThis uses the standard OpenAI provider with a custom endpoint. Since Ollama uses the OpenAI protocol, this works fine. You have to specify the an API key but Ollama ignores it.\n\n```csharp _chatClient = new OpenAIClient(new ApiKeyCredential(\"ignored\"), new OpenAIClientOptions { Endpoint = new Uri(\"http://localhost:11434/v1\"), }) .GetChatClient(\"llama3.1\") .AsIChatClient();\n\n```\n\n### Using the Ollama Provider\n\nYou can also use the explicit Ollama provider. The main reason you might want to do this is to avoid accessing features that are not available for Ollama in the OpenAI provider, although that point is moot with the ChatClient. It's more important for things like the Image API which isn't supported by Ollama, but OpenAI does.\n\nThe Ollama provider is considerably simpler than the others providing an IChatClient directly:\n\n```cs _chatClient = new OllamaChatClient(\"http://localhost:11434\",\"llama3.1\");\n\n```\n\nYeah this is the way I would have expected the other provides to work too.\n\nAs much as I wanted to be excited about using Ollama and local models, in my experience local models are too slow to be useful for most things. Other than perhaps for local chat interfaces, anything in an application running locally takes too much horsepower to be useful and even then the online models - even with Http overhead - many times faster than local models.\n\nBecause you can access Ollama with the OpenAI provider, personally I would skip the Ollama library unless Ollama is the only provider you intend to include in your project.\n\n### Other Providers\n\nThere are a number of other providers available as well such as an Onyx provider for local file models that load into the application which might provide better performance than Ollama at the cost of higher resource usage by your application.\n\n## Streaming Completions\n\nThis is off the topic, but since I'm here:\n\nOne of the reasons I'm playing around with this today is because I'm in need of using the streaming API to capture input as it comes in rather than waiting for completion.\n\nIn this case text is streaming into a textbox in a WPF application as part of a proof of concept example:\n\n```csharp private async Task GetAutocompletionSuggestions(string inputText) { _currentRequestCts = new CancellationTokenSource();\n\ntry { Status.ShowProgress(\" Generating suggestions...\");\n\n// Clear previous suggestions SuggestionsTextBox.Text = \"\";\n\n// Prepare the prompt for autocompletion var messages = new List<ChatMessage> { new(ChatRole.System, \"You are an AI writing assistant. Complete the user's text naturally and helpfully. \" + \"Provide a continuation that flows well with their input. Keep suggestions concise and relevant. Keep it to 30 words or so.\"), new(ChatRole.User, $\"Please continue this text: \\\"{inputText}\\\"\") };\n\nvar options = new ChatOptions { MaxOutputTokens = 150, Temperature = 0.7f };\n\n// Stream the response var suggestionBuilder = new StringBuilder();\n\n//var responseText = (await _chatClient.GetResponseAsync(messages)).Text; await foreach (var update in _chatClient.GetStreamingResponseAsync(messages, options, _currentRequestCts.Token)) { if (_currentRequestCts.Token.IsCancellationRequested) break;\n\nvar content = update.Text; suggestionBuilder.Append(content);\n\nif (!string.IsNullOrEmpty(content)) { // Update UI on main thread Dispatcher.Invoke(() => { SuggestionsTextBox.Text = suggestionBuilder.ToString(); SuggestionsTextBox.ScrollToEnd(); }); } }\n\n// Update final status Dispatcher.Invoke(() => { SuggestionsTextBox.Text = suggestionBuilder.ToString();\n\nStatus.ShowSuccess(\" Suggestions complete\"); }); } catch (OperationCanceledException) { // Request was cancelled, this is expected Dispatcher.Invoke(() => { Status.ShowError(\" Cancelled\"); }); } catch (Exception ex) { Dispatcher.Invoke(() => { SuggestionsTextBox.Text = $\"Error: {ex.Message}\"; Status.ShowError(\"Error occurred\") }); } }\n\n```\n\nPretty sweet how simple that process is using the async enumeration implementation in the Extension API.\n\n[!\\[\\](/images/sponsors/BrokenMoney-Display.jpg)](https://amzn.to/3EaiBqi)\n\n## Summary\n\nThe AI space is changing so fast that it's hard to keep up with, and the libraries that are being build reflect that with a lot of churn of changing specifications and features. Hopefully the fact that `Microsoft.Extensions.AI` has now a stable release means that these `Abstractions` stay fixed. I suspect that this is the case at least for the base interfaces, and at the Client level these APIs really succeed of making AI tasks easy to integrate across providers.\n\nNow we just need to get the providers to get cleaned up and provide some consistency in how they create the shared interfaces...\n\n### Other Posts you might also like\n\n- [Adding minimal OWIN Identity Authentication to an Existing ASP.NET MVC Application](https://weblog.west-wind.com/posts/2015/Apr/29/Adding-minimal-OWIN-Identity-Authentication-to-an-Existing-ASPNET-MVC-Application)\n- [Keeping Content Out of the Publish Folder for WebDeploy](https://weblog.west-wind.com/posts/2022/Aug/24/Keeping-Content-Out-of-the-Publish-Folder-for-WebDeploy)\n- [Using SQL Server on Windows ARM](https://weblog.west-wind.com/posts/2024/Oct/24/Using-Sql-Server-on-Windows-ARM)\n- [Map Physical Paths with an HttpContext.MapPath() Extension Method in ASP.NET](https://weblog.west-wind.com/posts/2023/Aug/15/Map-Physical-Paths-with-an-HttpContextMapPath-Extension-Method-in-ASPNET)\n\nShare on:\n\nhttps://twitter.com/intent/tweet?url=https://weblog.west-wind.com/posts/2025/May/30/Configuring-MicrosoftAIExtension-with-multiple-providers&amp;text=Configuring%20Microsoft.AI.Extensions%20with%20multiple%20providers&amp;via=RickStrahl\n\nhttps://www.facebook.com/sharer/sharer.php?u=https://weblog.west-wind.com/posts/2025/May/30/Configuring-MicrosoftAIExtension-with-multiple-providers&amp;t=Configuring%20Microsoft.AI.Extensions%20with%20multiple%20providers http://www.reddit.com/submit?url=https://weblog.west-wind.com/posts/2025/May/30/Configuring-MicrosoftAIExtension-with-multiple-providers&amp;title=Configuring%20Microsoft.AI.Extensions%20with%20multiple%20providers\n\n![Make Donation](/images/donation.png \"Find this content useful? Consider making a small donation.\")\n\nIs this content useful to you? **Consider making a small donation** to show your support.\n\nPosted in **[.NET](/ShowPosts.aspx?Category=.NET)**",
  "PubDate": "2025-05-31T08:56:33+00:00"
}
