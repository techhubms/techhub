{
  "Author": "pratikpanda",
  "EnhancedContent": "## Design patterns for accepting natural language as input without losing control\n\n## TL;DR\n\nBuilding production natural language APIs requires separating **semantic parsing** from **execution**. Use LLMs to translate user text into canonical structured requests (via schemas), then execute those requests deterministically.\n\nKey patterns: schema completion for clarification, confidence gates to prevent silent failures, code-based ontologies for normalization, and an orchestration layer. This keeps language as input, not as your API contract.\n\n## Introduction\n\nAPIs that accept natural language as input are quickly becoming the norm in the age of agentic AI apps and LLMs. From search and recommendations to workflows and automation, users increasingly expect to \"just ask\" and get results.\n\nBut treating natural language as an API contract introduces serious risks in production systems:\n\n- Nondeterministic behavior\n- Prompt-driven business logic\n- Difficult debugging and replay\n- Silent failures that are hard to detect\n\nIn this post, I'll describe a **production-grade architecture for building safe, natural language-driven APIs**: one that embraces LLMs for intent discovery and entity extraction while preserving the determinism, observability, and reliability that backend systems require.\n\nThis approach is based on building real systems using **Azure OpenAI** and **LangGraph**, and on lessons learned the hard way.\n\n## The Core Problem with Natural Language APIs\n\nNatural language is an excellent interface for humans. It is a poor interface for systems.\n\nWhen APIs accept raw text directly and execute logic based on it, several problems emerge:\n\n- The API contract becomes implicit and unversioned\n- Small prompt changes cause behavioral changes\n- Business logic quietly migrates into prompts\n\nIn short: **language becomes the contract**, and that's fragile.\n\nThe solution is not to avoid natural language, but to **contain it**.\n\n## A Key Principle: Natural Language Is Input, Not a Contract\n\nSo how do we contain it? The answer lies in treating natural language fundamentally differently than we treat traditional API inputs.\n\nThe most important design decision we made was this:\n\n> >\n> **Natural language should be translated into structure, not executed directly.**\n> >\n\nThat single principle drives the entire architecture.\n\nInstead of building \"chatty APIs,\" we split responsibilities clearly:\n\n- Natural language is used for *intent discovery and entity extraction*\n- Structured data is used for *execution*\n\n## Two Explicit API Layers\n\nThis principle translates into a concrete architecture with two distinct API layers, each with a single, clear responsibility.\n\n### 1. Semantic Parse API (Natural Language → Structure)\n\nThis API:\n\n- Accepts user text\n- Extracts intent and entities using LLMs\n- Completes a predefined schema\n- Asks clarifying questions when required\n- Returns a **canonical, structured request**\n- **Does not execute business logic**\n\nThink of this as a **compiler**, not an engine.\n\n### 2. Structured Execution API (Structure → Action)\n\nThis API:\n\n- Accepts only structured input\n- Calls downstream systems to process the request and get results\n- Is deterministic and versioned\n- Contains no natural language handling\n- Is fully testable and replayable\n\nThis is where execution happens.\n\n## Why This Separation Matters\n\nSeparating these layers gives you:\n\n- A stable, versionable API contract\n- Freedom to improve NLP without breaking clients\n- Clear ownership boundaries\n- Deterministic execution paths\n\nMost importantly, it prevents LLM behavior from leaking into core business logic.\n\n## Canonical Schemas Are the Backbone\n\nNow that we've established the two-layer architecture, let's dive into what makes it work: canonical schemas.\n\nEach supported intent is defined by a **canonical schema** that lives in code.\n\nExample (simplified):\n\nThis schema is used when a user is looking for similar product recommendations. The entities capture which product to use as reference and how to bias the recommendations toward price or quality.\n\n``` { \"intent\": \"recommend_similar\", \"entities\": { \"reference_product_id\": \"string\", \"price_bias\": \"number (-1 to 1)\", \"quality_bias\": \"number (-1 to 1)\" } } ```\n\nSchemas define:\n\n- Required vs optional fields\n- Allowed ranges and types\n- Validation rules\n\nThey are the contract, not the prompt.\n\nWhen a user says *\"show me products like the blue backpack but cheaper\"*, the LLM extracts:\n\n- Intent: recommend\\_similar\n- reference\\_product\\_id: \"blue\\_backpack\\_123\"\n- price\\_bias: -0.8 (strongly prefer cheaper)\n- quality\\_bias: 0.0 (neutral)\n\nThe schema ensures that even if the user phrased it as *\"find alternatives to item 123 with better pricing\"* or *\"cheaper versions of that blue bag\"*, the **output is always the same structure**. The natural language variation is absorbed at the semantic layer. The execution layer receives a consistent, validated request every time.\n\nThis decoupling is what makes the system maintainable.\n\n## Schema Completion, Not Free-Form Chat\n\nBut what happens when the user's input doesn't contain all the information needed to complete the schema? This is where structured clarification comes in.\n\nA common misconception is that clarification means \"chatting until it feels right.\"\n\nIn production systems, clarification is **schema completion**.\n\nIf required fields are missing or ambiguous, the semantic API responds with:\n\n- What information is missing\n- A targeted clarification question\n- The current schema state\n\nExample response:\n\n``` { \"status\": \"needs_clarification\", \"missing_fields\": [\"reference_product_id\"], \"question\": \"Which product should I compare against?\", \"state\": { \"intent\": \"recommend_similar\", \"entities\": { \"reference_product_id\": null, \"price_bias\": -0.3, \"quality_bias\": 0.4 } } } ```\n\nThe **state object** is the memory. The API itself remains stateless.\n\n### A Complete Conversation Flow\n\nTo illustrate how schema completion works in practice, here's a full conversation flow where the user's initial request is missing required information:\n\n**Initial Request:**\n\n``` User: \"Show me cheaper alternatives with good quality\" ```\n\n**API Response (needs clarification):**\n\n``` { \"status\": \"needs_clarification\", \"missing_fields\": [\"reference_product_id\"], \"question\": \"Which product should I compare against?\", \"state\": { \"intent\": \"recommend_similar\", \"entities\": { \"reference_product_id\": null, \"price_bias\": -0.3, \"quality_bias\": 0.4 } } } ```\n\n**Follow-up Request:**\n\n``` User: \"The blue backpack\" ```\n\n**Client sends:**\n\n``` { \"user_input\": \"The blue backpack\", \"state\": { \"intent\": \"recommend_similar\", \"entities\": { \"reference_product_id\": null, \"price_bias\": -0.3, \"quality_bias\": 0.4 } } } ```\n\n**API Response (complete):**\n\n``` { \"status\": \"complete\", \"canonical_request\": { \"intent\": \"recommend_similar\", \"entities\": { \"reference_product_id\": \"blue_backpack_123\", \"price_bias\": -0.3, \"quality_bias\": 0.4 } } } ```\n\nThe client passes the state back with each clarification. The API remains stateless, while the client manages the conversation context. Once complete, the canonical\\_request can be sent directly to the execution API.\n\n## Why LangGraph Fits This Problem Perfectly\n\nWith schemas and clarification flows defined, we need a way to orchestrate the semantic parsing workflow reliably. This is where LangGraph becomes valuable.\n\nLangGraph allows semantic parsing to be modeled as a **structured, deterministic workflow** with explicit decision points:\n\n1. **Classify intent**: Determine what the user wants to do from a predefined set of supported actions\n2. **Extract candidate entities**: Pull out relevant parameters from the natural language input using the LLM\n3. **Merge into schema state**: Map the extracted values into the canonical schema structure\n4. **Validate required fields**: Check if all mandatory fields are present and values are within acceptable ranges\n5. **Either complete or request clarification**: Return the canonical request if complete, or ask a targeted question if information is missing\n\nEach node has a single responsibility. Validation and routing are done in code, not by the LLM.\n\nLangGraph provides:\n\n- Explicit state transitions\n- Deterministic routing\n- Observable execution\n- Safe retries\n\nUsed this way, it becomes a powerful orchestration tool, not a conversational agent.\n\n## Confidence Gates Prevent Silent Failures\n\nStructured workflows handle the process, but there's another critical safety mechanism we need: knowing when the LLM isn't confident about its extraction.\n\nEven when outputs are structurally valid, they may not be reliable.\n\nWe require the semantic layer to emit a confidence score. If confidence falls below a threshold, execution is blocked and clarification is requested.\n\nThis simple rule eliminates an entire class of silent misinterpretations that are otherwise very hard to detect.\n\n**Example:**\n\nWhen a user says *\"Show me items similar to the bag\"*, the LLM might extract:\n\n``` { \"intent\": \"recommend_similar\", \"confidence\": 0.55, \"entities\": { \"reference_product_id\": \"generic_bag_001\", \"confidence_scores\": { \"reference_product_id\": 0.4 } } } ```\n\nThe overall confidence is low (0.55), and the entity confidence for reference\\_product\\_id is very low (0.4) because \"the bag\" is ambiguous. There might be hundreds of bags in the catalog.\n\n**Instead of proceeding with a potentially wrong guess, the API responds:**\n\n``` { \"status\": \"needs_clarification\", \"reason\": \"low_confidence\", \"question\": \"I found multiple bags. Did you mean the blue backpack, the leather tote, or the travel duffel?\", \"confidence\": 0.55 } ```\n\nThis prevents the system from silently executing the wrong recommendation and provides a better user experience.\n\n## Lightweight Ontologies (Keep Them in Code)\n\nBeyond confidence scoring, we need a way to normalize the variety of terms users might use into consistent canonical values.\n\nWe also introduced lightweight, code-level ontologies:\n\n- Allowed intents\n- Required entities per intent\n- Synonym-to-canonical mappings\n- Cross-field validation rules\n\nThese live in code and configuration, not in prompts.\n\nLLMs propose values. Code enforces meaning.\n\n**Example:**\n\nConsider these user inputs that all mean the same thing:\n\n- *\"Show me cheaper options\"*\n- *\"Find budget-friendly alternatives\"*\n- *\"I want something more affordable\"*\n- *\"Give me lower-priced items\"*\n\nThe LLM might extract different values: \"cheaper\", \"budget-friendly\", \"affordable\", \"lower-priced\".\n\nThe ontology maps all of these to a canonical value:\n\n``` PRICE_BIAS_SYNONYMS = { \"cheaper\": -0.7, \"budget-friendly\": -0.7, \"affordable\": -0.7, \"lower-priced\": -0.7, \"expensive\": 0.7, \"premium\": 0.7, \"high-end\": 0.7 } ```\n\nWhen the LLM extracts \"budget-friendly\", the code normalizes it to -0.7 for the price\\_bias field.\n\nSimilarly, cross-field validation catches logical inconsistencies:\n\n``` if entities[\"price_bias\"] < -0.5 and entities[\"quality_bias\"] > 0.5: return clarification(\"You want cheaper items with higher quality. This might be difficult. Should I prioritize price or quality?\") ```\n\nThe LLM proposes. The ontology normalizes. The validation enforces business rules.\n\n## What About Latency?\n\nA common concern with multi-step semantic parsing is performance.\n\nIn practice, we observed:\n\n- Intent classification: ~40 ms\n- Entity extraction: ~200 ms\n- Validation and routing: ~1 ms\n\nTotal overhead: **~250–300 ms**.\n\nFor chat-driven user experiences, this is well within acceptable bounds and far cheaper than incorrect or inconsistent execution.\n\n## Key Takeaways\n\nLet's bring it all together.\n\nIf you're building APIs that accept natural language in production:\n\n1. **Do not make language your API contract**\n2. **Translate language into canonical structure**\n3. **Own schema completion server-side**\n4. **Use LLMs for discovery and extraction, not execution**\n5. **Treat safety and determinism as first-class requirements**\n\nNatural language is an input format. Structure is the contract.\n\n## Closing Thoughts\n\nLLMs make it easy to build impressive demos. Building **safe, reliable systems** with them requires discipline.\n\nBy separating semantic interpretation from execution, and by using tools like Azure OpenAI and LangGraph thoughtfully, you can build natural language-driven APIs that scale, evolve, and behave predictably in production.\n\nHopefully, this architecture saves you a few painful iterations.\n\nPublished Feb 03, 2026\n\nVersion 1.0\n\n[agents](/tag/agents?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai foundry](/tag/ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure](/tag/azure?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure ai foundry](/tag/azure%20ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[best practices](/tag/best%20practices?nodeId=board%3AAzureDevCommunityBlog)\n\n[developer](/tag/developer?nodeId=board%3AAzureDevCommunityBlog)\n\n[genai](/tag/genai?nodeId=board%3AAzureDevCommunityBlog)\n\n[llm](/tag/llm?nodeId=board%3AAzureDevCommunityBlog)\n\n[python](/tag/python?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[pratikpanda&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xMjc0MjQzLTMzOTI1MGk4NTAyREFGNDFEQzkxQTE2?image-dimensions=50x50)](/users/pratikpanda/1274243) [pratikpanda](/users/pratikpanda/1274243) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined January 12, 2022\n\n[View Profile](/users/pratikpanda/1274243)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/how-to-build-safe-natural-language-driven-apis/ba-p/4488509",
  "PubDate": "2026-02-03T08:00:00+00:00",
  "Tags": [],
  "Title": "How to Build Safe Natural Language-Driven APIs",
  "FeedName": "Microsoft Tech Community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Description": "## TL;DR\n\nBuilding production natural language APIs requires separating **semantic parsing** from **execution**. Use LLMs to translate user text into canonical structured requests (via schemas), then execute those requests deterministically.\n\nKey patterns: schema completion for clarification, confidence gates to prevent silent failures, code-based ontologies for normalization, and an orchestration layer. This keeps language as input, not as your API contract.\n\n## Introduction\n\nAPIs that accept natural language as input are quickly becoming the norm in the age of agentic AI apps and LLMs. From search and recommendations to workflows and automation, users increasingly expect to \"just ask\" and get results.\n\nBut treating natural language as an API contract introduces serious risks in production systems:\n\n- Nondeterministic behavior\n- Prompt-driven business logic\n- Difficult debugging and replay\n- Silent failures that are hard to detect\n\nIn this post, I'll describe a **production-grade architecture for building safe, natural language-driven APIs**: one that embraces LLMs for intent discovery and entity extraction while preserving the determinism, observability, and reliability that backend systems require.\n\nThis approach is based on building real systems using **Azure OpenAI** and **LangGraph**, and on lessons learned the hard way.\n\n## The Core Problem with Natural Language APIs\n\nNatural language is an excellent interface for humans. It is a poor interface for systems.\n\nWhen APIs accept raw text directly and execute logic based on it, several problems emerge:\n\n- The API contract becomes implicit and unversioned\n- Small prompt changes cause behavioral changes\n- Business logic quietly migrates into prompts\n\nIn short: **language becomes the contract**, and that's fragile.\n\nThe solution is not to avoid natural language, but to **contain it**.\n\n## A Key Principle: Natural Language Is Input, Not a Contract\n\nSo how do we contain it? The answer lies in treating natural language fundamentally differently than we treat traditional API inputs.\n\nThe most important design decision we made was this:\n\n> >\n> **Natural language should be translated into structure, not executed directly.**\n> >\n\nThat single principle drives the entire architecture.\n\nInstead of building \"chatty APIs,\" we split responsibilities clearly:\n\n- Natural language is used for *intent discovery and entity extraction*\n- Structured data is used for *execution*\n\n## Two Explicit API Layers\n\nThis principle translates into a concrete architecture with two distinct API layers, each with a single, clear responsibility.\n\n### 1. Semantic Parse API (Natural Language → Structure)\n\nThis API:\n\n- Accepts user text\n- Extracts intent and entities using LLMs\n- Completes a predefined schema\n- Asks clarifying questions when required\n- Returns a **canonical, structured request**\n- **Does not execute business logic**\n\nThink of this as a **compiler**, not an engine.\n\n### 2. Structured Execution API (Structure → Action)\n\nThis API:\n\n- Accepts only structured input\n- Calls downstream systems to process the request and get results\n- Is deterministic and versioned\n- Contains no natural language handling\n- Is fully testable and replayable\n\nThis is where execution happens.\n\n## Why This Separation Matters\n\nSeparating these layers gives you:\n\n- A stable, versionable API contract\n- Freedom to improve NLP without breaking clients\n- Clear ownership boundaries\n- Deterministic execution paths\n\nMost importantly, it prevents LLM behavior from leaking into core business logic.\n\n## Canonical Schemas Are the Backbone\n\nNow that we've established the two-layer architecture, let's dive into what makes it work: canonical schemas.\n\nEach supported intent is defined by a **canonical schema** that lives in code.\n\nExample (simplified):\n\nThis schema is used when a user is looking for similar product recommendations. The entities capture which product to use as reference and how to bias the recommendations toward price or quality.\n\n- {\n\"intent\": \"recommend\\_similar\", \"entities\": { \"reference\\_product\\_id\": \"string\", \"price\\_bias\": \"number (-1 to 1)\", \"quality\\_bias\": \"number (-1 to 1)\" } }\n\nSchemas define:\n\n- Required vs optional fields\n- Allowed ranges and types\n- Validation rules\n\nThey are the contract, not the prompt.\n\nWhen a user says *\"show me products like the blue backpack but cheaper\"*, the LLM extracts:\n\n- Intent: recommend\\_similar\n- reference\\_product\\_id: \"blue\\_backpack\\_123\"\n- price\\_bias: -0.8 (strongly prefer cheaper)\n- quality\\_bias: 0.0 (neutral)\n\nThe schema ensures that even if the user phrased it as *\"find alternatives to item 123 with better pricing\"* or *\"cheaper versions of that blue bag\"*, the **output is always the same structure**. The natural language variation is absorbed at the semantic layer. The execution layer receives a consistent, validated request every time.\n\nThis decoupling is what makes the system maintainable.\n\n## Schema Completion, Not Free-Form Chat\n\nBut what happens when the user's input doesn't contain all the information needed to complete the schema? This is where structured clarification comes in.\n\nA common misconception is that clarification means \"chatting until it feels right.\"\n\nIn production systems, clarification is **schema completion**.\n\nIf required fields are missing or ambiguous, the semantic API responds with:\n\n- What information is missing\n- A targeted clarification question\n- The current schema state\n\nExample response:\n- {\n\"status\": \"needs\\_clarification\", \"missing\\_fields\": [\"reference\\_product\\_id\"], \"question\": \"Which product should I compare against?\", \"state\": { \"intent\": \"recommend\\_similar\", \"entities\": { \"reference\\_product\\_id\": null, \"price\\_bias\": -0.3, \"quality\\_bias\": 0.4 } } }\n\nThe **state object** is the memory. The API itself remains stateless.\n\n### A Complete Conversation Flow\n\nTo illustrate how schema completion works in practice, here's a full conversation flow where the user's initial request is missing required information:\n\n**Initial Request:**\n- User: \"Show me cheaper alternatives with good quality\"\n\n**API Response (needs clarification):**\n- {\n\"status\": \"needs\\_clarification\", \"missing\\_fields\": [\"reference\\_product\\_id\"], \"question\": \"Which product should I compare against?\", \"state\": { \"intent\": \"recommend\\_similar\", \"entities\": { \"reference\\_product\\_id\": null, \"price\\_bias\": -0.3, \"quality\\_bias\": 0.4 } } }\n\n**Follow-up Request:**\n- User: \"The blue backpack\"\n\n**Client sends:**\n- {\n\"user\\_input\": \"The blue backpack\", \"state\": { \"intent\": \"recommend\\_similar\", \"entities\": { \"reference\\_product\\_id\": null, \"price\\_bias\": -0.3, \"quality\\_bias\": 0.4 } } }\n\n**API Response (complete):**\n- {\n\"status\": \"complete\", \"canonical\\_request\": { \"intent\": \"recommend\\_similar\", \"entities\": { \"reference\\_product\\_id\": \"blue\\_backpack\\_123\", \"price\\_bias\": -0.3, \"quality\\_bias\": 0.4 } } }\n\nThe client passes the state back with each clarification. The API remains stateless, while the client manages the conversation context. Once complete, the canonical\\_request can be sent directly to the execution API.\n\n## Why LangGraph Fits This Problem Perfectly\n\nWith schemas and clarification flows defined, we need a way to orchestrate the semantic parsing workflow reliably. This is where LangGraph becomes valuable.\n\nLangGraph allows semantic parsing to be modeled as a **structured, deterministic workflow** with explicit decision points:\n\n1. **Classify intent**: Determine what the user wants to do from a predefined set of supported actions\n2. **Extract candidate entities**: Pull out relevant parameters from the natural language input using the LLM\n3. **Merge into schema state**: Map the extracted values into the canonical schema structure\n4. **Validate required fields**: Check if all mandatory fields are present and values are within acceptable ranges\n5. **Either complete or request clarification**: Return the canonical request if complete, or ask a targeted question if information is missing\n\nEach node has a single responsibility. Validation and routing are done in code, not by the LLM.\n\nLangGraph provides:\n\n- Explicit state transitions\n- Deterministic routing\n- Observable execution\n- Safe retries\n\nUsed this way, it becomes a powerful orchestration tool, not a conversational agent.\n\n## Confidence Gates Prevent Silent Failures\n\nStructured workflows handle the process, but there's another critical safety mechanism we need: knowing when the LLM isn't confident about its extraction.\n\nEven when outputs are structurally valid, they may not be reliable.\n\nWe require the semantic layer to emit a confidence score. If confidence falls below a threshold, execution is blocked and clarification is requested.\n\nThis simple rule eliminates an entire class of silent misinterpretations that are otherwise very hard to detect.\n\n**Example:**\n\nWhen a user says *\"Show me items similar to the bag\"*, the LLM might extract:\n- {\n\"intent\": \"recommend\\_similar\", \"confidence\": 0.55, \"entities\": { \"reference\\_product\\_id\": \"generic\\_bag\\_001\", \"confidence\\_scores\": { \"reference\\_product\\_id\": 0.4 } } }\n\nThe overall confidence is low (0.55), and the entity confidence for reference\\_product\\_id is very low (0.4) because \"the bag\" is ambiguous. There might be hundreds of bags in the catalog.\n\n**Instead of proceeding with a potentially wrong guess, the API responds:**\n- {\n\"status\": \"needs\\_clarification\", \"reason\": \"low\\_confidence\", \"question\": \"I found multiple bags. Did you mean the blue backpack, the leather tote, or the travel duffel?\", \"confidence\": 0.55 }\n\nThis prevents the system from silently executing the wrong recommendation and provides a better user experience.\n\n## Lightweight Ontologies (Keep Them in Code)\n\nBeyond confidence scoring, we need a way to normalize the variety of terms users might use into consistent canonical values.\n\nWe also introduced lightweight, code-level ontologies:\n\n- Allowed intents\n- Required entities per intent\n- Synonym-to-canonical mappings\n- Cross-field validation rules\n\nThese live in code and configuration, not in prompts.\n\nLLMs propose values. Code enforces meaning.\n\n**Example:**\n\nConsider these user inputs that all mean the same thing:\n\n- *\"Show me cheaper options\"*\n- *\"Find budget-friendly alternatives\"*\n- *\"I want something more affordable\"*\n- *\"Give me lower-priced items\"*\n\nThe LLM might extract different values: \"cheaper\", \"budget-friendly\", \"affordable\", \"lower-priced\".\n\nThe ontology maps all of these to a canonical value:\n- PRICE\\_BIAS\\_SYNONYMS = {\n\"cheaper\": -0.7, \"budget-friendly\": -0.7, \"affordable\": -0.7, \"lower-priced\": -0.7, \"expensive\": 0.7, \"premium\": 0.7, \"high-end\": 0.7 }\n\nWhen the LLM extracts \"budget-friendly\", the code normalizes it to -0.7 for the price\\_bias field.\n\nSimilarly, cross-field validation catches logical inconsistencies:\n- if entities[\"price\\_bias\"] 0.5:\nreturn clarification(\"You want cheaper items with higher quality. This might be difficult. Should I prioritize price or quality?\")\n\nThe LLM proposes. The ontology normalizes. The validation enforces business rules.\n\n## What About Latency?\n\nA common concern with multi-step semantic parsing is performance.\n\nIn practice, we observed:\n\n- Intent classification: ~40 ms\n- Entity extraction: ~200 ms\n- Validation and routing: ~1 ms\n\nTotal overhead: **~250–300 ms**.\n\nFor chat-driven user experiences, this is well within acceptable bounds and far cheaper than incorrect or inconsistent execution.\n\n## Key Takeaways\n\nLet's bring it all together.\n\nIf you're building APIs that accept natural language in production:\n\n1. **Do not make language your API contract**\n2. **Translate language into canonical structure**\n3. **Own schema completion server-side**\n4. **Use LLMs for discovery and extraction, not execution**\n5. **Treat safety and determinism as first-class requirements**\n\nNatural language is an input format. Structure is the contract.\n\n## Closing Thoughts\n\nLLMs make it easy to build impressive demos. Building **safe, reliable systems** with them requires discipline.\n\nBy separating semantic interpretation from execution, and by using tools like Azure OpenAI and LangGraph thoughtfully, you can build natural language-driven APIs that scale, evolve, and behave predictably in production.\n\nHopefully, this architecture saves you a few painful iterations.",
  "OutputDir": "_community",
  "ProcessedDate": "2026-02-03 08:08:50"
}
