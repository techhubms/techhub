{
  "Description": "## Introduction\n\nSelecting the right AI model for your application requires more than reading benchmark leaderboards. Published benchmarks measure academic capabilities, question answering, reasoning, coding, but your application has specific requirements: latency budgets, hardware constraints, quality thresholds. How do you know if Phi-4 provides acceptable quality for your document summarization use case? Will Qwen2.5-0.5B meet your 100ms response time requirement? Does your edge device have sufficient memory for Phi-3.5 Mini?\n\nThe answer lies in empirical testing: running actual models on your hardware with your workload patterns. This article demonstrates building a comprehensive model benchmarking platform using [FLPerformance](https://github.com/leestott/FLPerformance), Node.js, React, and Microsoft Foundry Local. You'll learn how to implement scientific performance measurement, design meaningful benchmark suites, visualize multi-dimensional comparisons, and make data-driven model selection decisions.\n\nWhether you're evaluating models for production deployment, optimizing inference costs, or validating hardware specifications, this platform provides the tools for rigorous performance analysis.\n\n## Why Model Benchmarking Requires Purpose-Built Tools\n\nYou cannot assess model performance by running a few manual tests and noting the results. Scientific benchmarking demands controlled conditions, statistically significant sample sizes, multi-dimensional metrics, and reproducible methodology. Understand why purpose-built tooling is essential.\n\nPerformance is multi-dimensional. A model might excel at throughput (tokens per second) but suffer at latency (time to first token). Another might generate high-quality outputs slowly. Your application might prioritize consistency over average performance, a model with variable response times (high p95/p99 latency) creates poor user experiences even if averages look good. Measuring all dimensions simultaneously enables informed tradeoffs.\n\nHardware matters enormously. Benchmark results from NVIDIA A100 GPUs don't predict performance on consumer laptops. NPU acceleration changes the picture again. Memory constraints affect which models can even load. Test on your actual deployment hardware or comparable specifications to get actionable results.\n\nConcurrency reveals bottlenecks. A model handling one request excellently might struggle with ten concurrent requests. Real applications experience variable load, measuring only single-threaded performance misses critical scalability constraints. Controlled concurrency testing reveals these limits.\n\nStatistical rigor prevents false conclusions. Running a prompt once and noting the response time tells you nothing about performance distribution. Was this result typical? An outlier? You need dozens or hundreds of trials to establish p50/p95/p99 percentiles, understand variance, and detect stability issues.\n\nComparison requires controlled experiments. Different prompts, different times of day, different system loads, all introduce confounding variables. Scientific comparison runs identical workloads across models sequentially, controlling for external factors.\n\n## Architecture: Three-Layer Performance Testing Platform\n\nFLPerformance implements a clean separation between orchestration, measurement, and presentation:\n\nThe frontend React application provides model management, benchmark configuration, test execution, and results visualization. Users add models from the [Foundry Local catalog](https://foundrylocal.ai), configure benchmark parameters (iterations, concurrency, timeout values), launch test runs, and view real-time progress. The results dashboard displays comparison tables, latency distribution charts, throughput graphs, and \"best model for...\" recommendations.\n\nThe backend Node.js/Express server orchestrates tests and captures metrics. It manages the single Foundry Local service instance, loads/unloads models as needed, executes benchmark suites with controlled concurrency, measures comprehensive metrics (TTFT, TPOT, total latency, throughput, error rates), and persists results to JSON storage. WebSocket connections provide real-time progress updates during long benchmark runs.\n\nFoundry Local SDK integration uses the official `foundry-local-sdk` npm package. The SDK manages service lifecycle, starting, stopping, health checkin, and handles model operations, downloading, loading into memory, unloading. It provides OpenAI-compatible inference APIs for consistent request formatting across models.\n\nThe architecture supports simultaneous testing of multiple models by loading them one at a time, running identical benchmarks, and aggregating results for comparison:\n\n``` User Initiates Benchmark Run ↓ Backend receives {models: [...], suite: \"default\", iterations: 10} ↓ For each model:\n1. Load model into Foundry Local\n2. Execute benchmark suite\n- For each prompt in suite:\n* Run N iterations\n* Measure TTFT, TPOT, total time\n* Track errors and timeouts\n* Calculate tokens/second\n3. Aggregate statistics (mean, p50, p95, p99)\n4. Unload model\n↓ Store results with metadata ↓ Return comparison data to frontend ↓ Visualize performance metrics ```\n\n## Implementing Scientific Measurement Infrastructure\n\nAccurate performance measurement requires instrumentation that captures multiple dimensions without introducing measurement overhead:\n\n``` // src/server/benchmark.js import { performance } from 'perf_hooks';\n\nexport class BenchmarkExecutor { constructor(foundryClient, options = {}) { this.client = foundryClient; this.options = { iterations: options.iterations || 10, concurrency: options.concurrency || 1, timeout_ms: options.timeout_ms || 30000, warmup_iterations: options.warmup_iterations || 2 }; }\n\nasync runBenchmarkSuite(modelId, prompts) { const results = [];\n\n// Warmup phase (exclude from results) console.log(`Running ${this.options.warmup_iterations} warmup iterations...`); for (let i = 0; i 1 && firstTokenTime) { // TPOT = time after first token / (tokens - 1) const timeAfterFirstToken = endTime - firstTokenTime; measurement.tpot_ms = timeAfterFirstToken / (tokenCount - 1); measurement.tokens_per_second = 1000 / measurement.tpot_ms; }\n\nmeasurement.success = true;\n\n} catch (error) { measurement.error = error.message; measurement.success = false; }\n\nreturn measurement; }\n\ncalculateStatistics(measurements) { const successful = measurements.filter(m => m.success); const total = measurements.length;\n\nif (successful.length === 0) { return { success_rate: 0, error_rate: 1.0, sample_size: total }; }\n\nconst ttfts = successful.map(m => m.ttft_ms).sort((a, b) => a - b); const tpots = successful.map(m => m.tpot_ms).filter(v => v !== null).sort((a, b) => a - b); const totals = successful.map(m => m.total_ms).sort((a, b) => a - b); const throughputs = successful.map(m => m.tokens_per_second).filter(v => v > 0);\n\nreturn { success_rate: successful.length / total, error_rate: (total - successful.length) / total, sample_size: total,\n\nttft: { mean: mean(ttfts), median: percentile(ttfts, 50), p95: percentile(ttfts, 95), p99: percentile(ttfts, 99), min: Math.min(...ttfts), max: Math.max(...ttfts) },\n\ntpot: tpots.length > 0 ? { mean: mean(tpots), median: percentile(tpots, 50), p95: percentile(tpots, 95) } : null,\n\ntotal_latency: { mean: mean(totals), median: percentile(totals, 50), p95: percentile(totals, 95), p99: percentile(totals, 99) },\n\nthroughput: { mean_tps: mean(throughputs), median_tps: percentile(throughputs, 50) } }; } }\n\nfunction mean(arr) { return arr.reduce((sum, val) => sum + val, 0) / arr.length; }\n\nfunction percentile(sortedArr, p) { const index = Math.ceil((sortedArr.length * p) / 100) - 1; return sortedArr[Math.max(0, index)]; }\n\nfunction sleep(ms) { return new Promise(resolve => setTimeout(resolve, ms)); } ```\n\nThis measurement infrastructure captures:\n\n- **Time to First Token (TTFT)**: Critical for perceived responsiveness—users notice delays before output begins\n- **Time Per Output Token (TPOT)**: Determines generation speed after first token—affects throughput\n- **Total latency**: End-to-end time—matters for batch processing and high-volume scenarios\n- **Tokens per second**: Overall throughput metric—useful for capacity planning\n- **Statistical distributions**: Mean alone masks variability—p95/p99 reveal tail latencies that impact user experience\n- **Success/error rates**: Stability metrics—some models timeout or crash under load\n\n## Designing Meaningful Benchmark Suites\n\nBenchmark quality depends on prompt selection. Generic prompts don't reflect real application behavior. Design suites that mirror actual use cases:\n\n``` // benchmarks/suites/default.json { \"name\": \"default\", \"description\": \"General-purpose benchmark covering diverse scenarios\", \"prompts\": [ { \"id\": \"short-factual\", \"text\": \"What is the capital of France?\", \"category\": \"factual\", \"expected_tokens\": 5 }, { \"id\": \"medium-explanation\", \"text\": \"Explain how photosynthesis works in 3-4 sentences.\", \"category\": \"explanation\", \"expected_tokens\": 80 }, { \"id\": \"long-reasoning\", \"text\": \"Analyze the economic factors that led to the 2008 financial crisis. Discuss at least 5 major causes with supporting details.\", \"category\": \"reasoning\", \"expected_tokens\": 250 }, { \"id\": \"code-generation\", \"text\": \"Write a Python function that finds the longest palindrome in a string. Include docstring and example usage.\", \"category\": \"coding\", \"expected_tokens\": 150 }, { \"id\": \"creative-writing\", \"text\": \"Write a short story (3 paragraphs) about a robot learning to paint.\", \"category\": \"creative\", \"expected_tokens\": 200 } ] } ```\n\nThis suite covers multiple dimensions:\n\n- **Length variation**: Short (5 tokens), medium (80), long (250)—tests models across output ranges\n- **Task diversity**: Factual recall, explanation, reasoning, code, creative—reveals capability breadth\n- **Token predictability**: Expected token counts enable throughput calculations\n\nFor production applications, create custom suites matching your actual workload:\n\n``` { \"name\": \"customer-support\", \"description\": \"Simulates actual customer support queries\", \"prompts\": [ { \"id\": \"product-question\", \"text\": \"How do I reset my password for the customer portal?\" }, { \"id\": \"troubleshooting\", \"text\": \"I'm getting error code 503 when trying to upload files. What should I do?\" }, { \"id\": \"policy-inquiry\", \"text\": \"What is your refund policy for annual subscriptions?\" } ] } ```\n\n## Visualizing Multi-Dimensional Performance Comparisons\n\nRaw numbers don't reveal insights—visualization makes patterns obvious. The frontend implements several comparison views:\n\n**Comparison Table** shows side-by-side metrics:\n\n``` // frontend/src/components/ResultsTable.jsx export function ResultsTable({ results }) { return (\n\n{results.map(result => (\n\n))}\n\n```\n\n| Model | TTFT (ms) | TPOT (ms) | Throughput (tok/s) | P95 Latency | Error Rate | | --- | --- | --- | --- | --- | --- | | {result.model\\_id} | {result.stats.ttft.median.toFixed(0)} (p95: {result.stats.ttft.p95.toFixed(0)}) | {result.stats.tpot?.median.toFixed(1) || 'N/A'} | {result.stats.throughput.median\\_tps.toFixed(1)} | {result.stats.total\\_latency.p95.toFixed(0)} ms | 0.05 ? 'error' : 'success'}> {(result.stats.error\\_rate \\* 100).toFixed(1)}% |\n\n``` ); } ```\n\n**Latency Distribution Chart** reveals performance consistency:\n\n``` // Using Chart.js for visualization export function LatencyChart({ results }) { const data = { labels: results.map(r => r.model_id), datasets: [ { label: 'Median (p50)', data: results.map(r => r.stats.total_latency.median), backgroundColor: 'rgba(75, 192, 192, 0.5)' }, { label: 'p95', data: results.map(r => r.stats.total_latency.p95), backgroundColor: 'rgba(255, 206, 86, 0.5)' }, { label: 'p99', data: results.map(r => r.stats.total_latency.p99), backgroundColor: 'rgba(255, 99, 132, 0.5)' } ] };\n\nreturn (\n\n); } ```\n\n**Recommendations Engine** synthesizes multi-dimensional comparison:\n\n``` export function generateRecommendations(results) { const recommendations = [];\n\n// Find fastest TTFT (best perceived responsiveness) const fastestTTFT = results.reduce((best, r) => r.stats.ttft.median r.stats.throughput.median_tps > best.stats.throughput.median_tps ? r : best ); recommendations.push({ category: 'Best Throughput', model: highestThroughput.model_id, reason: `Highest tok/s: ${highestThroughput.stats.throughput.median_tps.toFixed(1)}` });\n\n// Find most consistent (lowest p95-p50 spread) const mostConsistent = results.reduce((best, r) => { const spread = r.stats.total_latency.p95 - r.stats.total_latency.median; const bestSpread = best.stats.total_latency.p95 - best.stats.total_latency.median; return spread Key Takeaways and Benchmarking Best Practices\n\nEffective model benchmarking requires scientific methodology, comprehensive metrics, and application-specific testing. FLPerformance demonstrates that rigorous performance measurement is accessible to any development team.\n\nCritical principles for model evaluation: Test on target hardware: Results from cloud GPUs don't predict laptop performance\n\nMeasure multiple dimensions: TTFT, TPOT, throughput, consistency all matter\n\nUse statistical rigor: Single runs mislead—capture distributions with adequate sample sizes\n\nDesign realistic workloads: Generic benchmarks don't predict your application's behavior\n\nInclude warmup iterations: Model loading and JIT compilation affect early measurements\n\nControl concurrency: Real applications handle multiple requests—test at realistic loads\n\nDocument methodology: Reproducible results require documented procedures and configurations\n\nThe complete benchmarking platform with model management, measurement infrastructure, visualization dashboards, and comprehensive documentation is available at github.com/leestott/FLPerformance. Clone the repository and run the startup script to begin evaluating models on your hardware.\n\nResources and Further Reading FLPerformance Repository - Complete benchmarking platform\n\nQuick Start Guide - Setup and first benchmark run\n\nMicrosoft Foundry Local Documentation - SDK reference and model catalog\n\nArchitecture Guide - System design and SDK integration\n\nBenchmarking Best Practices - Methodology and troubleshooting\n\n```",
  "FeedName": "Microsoft Tech Community",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Author": "Lee_Stott",
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/benchmarking-local-ai-models/ba-p/4490780",
  "OutputDir": "_community",
  "PubDate": "2026-02-02T08:00:00+00:00",
  "Tags": [],
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Title": "Benchmarking Local AI Models",
  "EnhancedContent": "## Scientific Performance Measurement with Foundry Local\n\n## Introduction\n\nSelecting the right AI model for your application requires more than reading benchmark leaderboards. Published benchmarks measure academic capabilities, question answering, reasoning, coding, but your application has specific requirements: latency budgets, hardware constraints, quality thresholds. How do you know if Phi-4 provides acceptable quality for your document summarization use case? Will Qwen2.5-0.5B meet your 100ms response time requirement? Does your edge device have sufficient memory for Phi-3.5 Mini?\n\nThe answer lies in empirical testing: running actual models on your hardware with your workload patterns. This article demonstrates building a comprehensive model benchmarking platform using [FLPerformance](https://github.com/leestott/FLPerformance), Node.js, React, and Microsoft Foundry Local. You'll learn how to implement scientific performance measurement, design meaningful benchmark suites, visualize multi-dimensional comparisons, and make data-driven model selection decisions.\n\nWhether you're evaluating models for production deployment, optimizing inference costs, or validating hardware specifications, this platform provides the tools for rigorous performance analysis.\n\n## Why Model Benchmarking Requires Purpose-Built Tools\n\nYou cannot assess model performance by running a few manual tests and noting the results. Scientific benchmarking demands controlled conditions, statistically significant sample sizes, multi-dimensional metrics, and reproducible methodology. Understand why purpose-built tooling is essential.\n\nPerformance is multi-dimensional. A model might excel at throughput (tokens per second) but suffer at latency (time to first token). Another might generate high-quality outputs slowly. Your application might prioritize consistency over average performance, a model with variable response times (high p95/p99 latency) creates poor user experiences even if averages look good. Measuring all dimensions simultaneously enables informed tradeoffs.\n\nHardware matters enormously. Benchmark results from NVIDIA A100 GPUs don't predict performance on consumer laptops. NPU acceleration changes the picture again. Memory constraints affect which models can even load. Test on your actual deployment hardware or comparable specifications to get actionable results.\n\nConcurrency reveals bottlenecks. A model handling one request excellently might struggle with ten concurrent requests. Real applications experience variable load, measuring only single-threaded performance misses critical scalability constraints. Controlled concurrency testing reveals these limits.\n\nStatistical rigor prevents false conclusions. Running a prompt once and noting the response time tells you nothing about performance distribution. Was this result typical? An outlier? You need dozens or hundreds of trials to establish p50/p95/p99 percentiles, understand variance, and detect stability issues.\n\nComparison requires controlled experiments. Different prompts, different times of day, different system loads, all introduce confounding variables. Scientific comparison runs identical workloads across models sequentially, controlling for external factors.\n\n## Architecture: Three-Layer Performance Testing Platform\n\nFLPerformance implements a clean separation between orchestration, measurement, and presentation:\n\nThe frontend React application provides model management, benchmark configuration, test execution, and results visualization. Users add models from the [Foundry Local catalog](https://foundrylocal.ai), configure benchmark parameters (iterations, concurrency, timeout values), launch test runs, and view real-time progress. The results dashboard displays comparison tables, latency distribution charts, throughput graphs, and \"best model for...\" recommendations.\n\nThe backend Node.js/Express server orchestrates tests and captures metrics. It manages the single Foundry Local service instance, loads/unloads models as needed, executes benchmark suites with controlled concurrency, measures comprehensive metrics (TTFT, TPOT, total latency, throughput, error rates), and persists results to JSON storage. WebSocket connections provide real-time progress updates during long benchmark runs.\n\nFoundry Local SDK integration uses the official `foundry-local-sdk` npm package. The SDK manages service lifecycle, starting, stopping, health checkin, and handles model operations, downloading, loading into memory, unloading. It provides OpenAI-compatible inference APIs for consistent request formatting across models.\n\nThe architecture supports simultaneous testing of multiple models by loading them one at a time, running identical benchmarks, and aggregating results for comparison:\n\n``` User Initiates Benchmark Run ↓ Backend receives {models: [...], suite: \"default\", iterations: 10} ↓ For each model:\n1. Load model into Foundry Local\n2. Execute benchmark suite\n- For each prompt in suite:\n* Run N iterations\n* Measure TTFT, TPOT, total time\n* Track errors and timeouts\n* Calculate tokens/second\n3. Aggregate statistics (mean, p50, p95, p99)\n4. Unload model\n↓ Store results with metadata ↓ Return comparison data to frontend ↓ Visualize performance metrics ```\n\n## Implementing Scientific Measurement Infrastructure\n\nAccurate performance measurement requires instrumentation that captures multiple dimensions without introducing measurement overhead:\n\n``` // src/server/benchmark.js import { performance } from 'perf_hooks';\n\nexport class BenchmarkExecutor { constructor(foundryClient, options = {}) { this.client = foundryClient; this.options = { iterations: options.iterations || 10, concurrency: options.concurrency || 1, timeout_ms: options.timeout_ms || 30000, warmup_iterations: options.warmup_iterations || 2 }; }\n\nasync runBenchmarkSuite(modelId, prompts) { const results = [];\n\n// Warmup phase (exclude from results) console.log(`Running ${this.options.warmup_iterations} warmup iterations...`); for (let i = 0; i < this.options.warmup_iterations; i++) { await this.executePrompt(modelId, prompts[0].text); }\n\n// Actual benchmark runs for (const prompt of prompts) { console.log(`Benchmarking prompt: ${prompt.id}`); const measurements = [];\n\nfor (let i = 0; i < this.options.iterations; i++) { const measurement = await this.executeMeasuredPrompt( modelId, prompt.text ); measurements.push(measurement);\n\n// Small delay between iterations to stabilize await sleep(100); }\n\nresults.push({ prompt_id: prompt.id, prompt_text: prompt.text, measurements, statistics: this.calculateStatistics(measurements) }); }\n\nreturn { model_id: modelId, timestamp: new Date().toISOString(), config: this.options, results }; }\n\nasync executeMeasuredPrompt(modelId, promptText) { const measurement = { success: false, error: null, ttft_ms: null, // Time to first token tpot_ms: null, // Time per output token total_ms: null, tokens_generated: 0, tokens_per_second: 0 };\n\ntry { const startTime = performance.now(); let firstTokenTime = null; let tokenCount = 0;\n\n// Streaming completion to measure TTFT const stream = await this.client.chat.completions.create({ model: modelId, messages: [{ role: 'user', content: promptText }], max_tokens: 200, temperature: 0.7, stream: true });\n\nfor await (const chunk of stream) { if (chunk.choices[0]?.delta?.content) { if (firstTokenTime === null) { firstTokenTime = performance.now(); measurement.ttft_ms = firstTokenTime - startTime; } tokenCount++; } }\n\nconst endTime = performance.now(); measurement.total_ms = endTime - startTime; measurement.tokens_generated = tokenCount;\n\nif (tokenCount > 1 && firstTokenTime) { // TPOT = time after first token / (tokens - 1) const timeAfterFirstToken = endTime - firstTokenTime; measurement.tpot_ms = timeAfterFirstToken / (tokenCount - 1); measurement.tokens_per_second = 1000 / measurement.tpot_ms; }\n\nmeasurement.success = true;\n\n} catch (error) { measurement.error = error.message; measurement.success = false; }\n\nreturn measurement; }\n\ncalculateStatistics(measurements) { const successful = measurements.filter(m => m.success); const total = measurements.length;\n\nif (successful.length === 0) { return { success_rate: 0, error_rate: 1.0, sample_size: total }; }\n\nconst ttfts = successful.map(m => m.ttft_ms).sort((a, b) => a - b); const tpots = successful.map(m => m.tpot_ms).filter(v => v !== null).sort((a, b) => a - b); const totals = successful.map(m => m.total_ms).sort((a, b) => a - b); const throughputs = successful.map(m => m.tokens_per_second).filter(v => v > 0);\n\nreturn { success_rate: successful.length / total, error_rate: (total - successful.length) / total, sample_size: total,\n\nttft: { mean: mean(ttfts), median: percentile(ttfts, 50), p95: percentile(ttfts, 95), p99: percentile(ttfts, 99), min: Math.min(...ttfts), max: Math.max(...ttfts) },\n\ntpot: tpots.length > 0 ? { mean: mean(tpots), median: percentile(tpots, 50), p95: percentile(tpots, 95) } : null,\n\ntotal_latency: { mean: mean(totals), median: percentile(totals, 50), p95: percentile(totals, 95), p99: percentile(totals, 99) },\n\nthroughput: { mean_tps: mean(throughputs), median_tps: percentile(throughputs, 50) } }; } }\n\nfunction mean(arr) { return arr.reduce((sum, val) => sum + val, 0) / arr.length; }\n\nfunction percentile(sortedArr, p) { const index = Math.ceil((sortedArr.length * p) / 100) - 1; return sortedArr[Math.max(0, index)]; }\n\nfunction sleep(ms) { return new Promise(resolve => setTimeout(resolve, ms)); } ```\n\nThis measurement infrastructure captures:\n\n- **Time to First Token (TTFT)**: Critical for perceived responsiveness—users notice delays before output begins\n- **Time Per Output Token (TPOT)**: Determines generation speed after first token—affects throughput\n- **Total latency**: End-to-end time—matters for batch processing and high-volume scenarios\n- **Tokens per second**: Overall throughput metric—useful for capacity planning\n- **Statistical distributions**: Mean alone masks variability—p95/p99 reveal tail latencies that impact user experience\n- **Success/error rates**: Stability metrics—some models timeout or crash under load\n\n## Designing Meaningful Benchmark Suites\n\nBenchmark quality depends on prompt selection. Generic prompts don't reflect real application behavior. Design suites that mirror actual use cases:\n\n``` // benchmarks/suites/default.json { \"name\": \"default\", \"description\": \"General-purpose benchmark covering diverse scenarios\", \"prompts\": [ { \"id\": \"short-factual\", \"text\": \"What is the capital of France?\", \"category\": \"factual\", \"expected_tokens\": 5 }, { \"id\": \"medium-explanation\", \"text\": \"Explain how photosynthesis works in 3-4 sentences.\", \"category\": \"explanation\", \"expected_tokens\": 80 }, { \"id\": \"long-reasoning\", \"text\": \"Analyze the economic factors that led to the 2008 financial crisis. Discuss at least 5 major causes with supporting details.\", \"category\": \"reasoning\", \"expected_tokens\": 250 }, { \"id\": \"code-generation\", \"text\": \"Write a Python function that finds the longest palindrome in a string. Include docstring and example usage.\", \"category\": \"coding\", \"expected_tokens\": 150 }, { \"id\": \"creative-writing\", \"text\": \"Write a short story (3 paragraphs) about a robot learning to paint.\", \"category\": \"creative\", \"expected_tokens\": 200 } ] } ```\n\nThis suite covers multiple dimensions:\n\n- **Length variation**: Short (5 tokens), medium (80), long (250)—tests models across output ranges\n- **Task diversity**: Factual recall, explanation, reasoning, code, creative—reveals capability breadth\n- **Token predictability**: Expected token counts enable throughput calculations\n\nFor production applications, create custom suites matching your actual workload:\n\n``` { \"name\": \"customer-support\", \"description\": \"Simulates actual customer support queries\", \"prompts\": [ { \"id\": \"product-question\", \"text\": \"How do I reset my password for the customer portal?\" }, { \"id\": \"troubleshooting\", \"text\": \"I'm getting error code 503 when trying to upload files. What should I do?\" }, { \"id\": \"policy-inquiry\", \"text\": \"What is your refund policy for annual subscriptions?\" } ] } ```\n\n## Visualizing Multi-Dimensional Performance Comparisons\n\nRaw numbers don't reveal insights—visualization makes patterns obvious. The frontend implements several comparison views:\n\n**Comparison Table** shows side-by-side metrics:\n\n``` // frontend/src/components/ResultsTable.jsx export function ResultsTable({ results }) { return (\n\n{results.map(result => (\n\n))}\n\n```\n\n| Model | TTFT (ms) | TPOT (ms) | Throughput (tok/s) | P95 Latency | Error Rate | | --- | --- | --- | --- | --- | --- | | {result.model\\_id} | {result.stats.ttft.median.toFixed(0)} (p95: {result.stats.ttft.p95.toFixed(0)}) | {result.stats.tpot?.median.toFixed(1) || 'N/A'} | {result.stats.throughput.median\\_tps.toFixed(1)} | {result.stats.total\\_latency.p95.toFixed(0)} ms | 0.05 ? 'error' : 'success'}&gt; {(result.stats.error\\_rate \\* 100).toFixed(1)}% |\n\n``` ); } ```\n\n**Latency Distribution Chart** reveals performance consistency:\n\n``` // Using Chart.js for visualization export function LatencyChart({ results }) { const data = { labels: results.map(r => r.model_id), datasets: [ { label: 'Median (p50)', data: results.map(r => r.stats.total_latency.median), backgroundColor: 'rgba(75, 192, 192, 0.5)' }, { label: 'p95', data: results.map(r => r.stats.total_latency.p95), backgroundColor: 'rgba(255, 206, 86, 0.5)' }, { label: 'p99', data: results.map(r => r.stats.total_latency.p99), backgroundColor: 'rgba(255, 99, 132, 0.5)' } ] };\n\nreturn (\n\n); } ```\n\n**Recommendations Engine** synthesizes multi-dimensional comparison:\n\n``` export function generateRecommendations(results) { const recommendations = [];\n\n// Find fastest TTFT (best perceived responsiveness) const fastestTTFT = results.reduce((best, r) => r.stats.ttft.median < best.stats.ttft.median ? r : best ); recommendations.push({ category: 'Fastest Response', model: fastestTTFT.model_id, reason: `Lowest median TTFT: ${fastestTTFT.stats.ttft.median.toFixed(0)}ms` });\n\n// Find highest throughput const highestThroughput = results.reduce((best, r) => r.stats.throughput.median_tps > best.stats.throughput.median_tps ? r : best ); recommendations.push({ category: 'Best Throughput', model: highestThroughput.model_id, reason: `Highest tok/s: ${highestThroughput.stats.throughput.median_tps.toFixed(1)}` });\n\n// Find most consistent (lowest p95-p50 spread) const mostConsistent = results.reduce((best, r) => { const spread = r.stats.total_latency.p95 - r.stats.total_latency.median; const bestSpread = best.stats.total_latency.p95 - best.stats.total_latency.median; return spread < bestSpread ? r : best; }); recommendations.push({ category: 'Most Consistent', model: mostConsistent.model_id, reason: 'Lowest latency variance (p95-p50 spread)' });\n\nreturn recommendations; } ```\n\n## Key Takeaways and Benchmarking Best Practices\n\nEffective model benchmarking requires scientific methodology, comprehensive metrics, and application-specific testing. FLPerformance demonstrates that rigorous performance measurement is accessible to any development team.\n\nCritical principles for model evaluation:\n\n- **Test on target hardware**: Results from cloud GPUs don't predict laptop performance\n- **Measure multiple dimensions**: TTFT, TPOT, throughput, consistency all matter\n- **Use statistical rigor**: Single runs mislead—capture distributions with adequate sample sizes\n- **Design realistic workloads**: Generic benchmarks don't predict your application's behavior\n- **Include warmup iterations**: Model loading and JIT compilation affect early measurements\n- **Control concurrency**: Real applications handle multiple requests—test at realistic loads\n- **Document methodology**: Reproducible results require documented procedures and configurations\n\nThe complete benchmarking platform with model management, measurement infrastructure, visualization dashboards, and comprehensive documentation is available at [github.com/leestott/FLPerformance](https://github.com/leestott/FLPerformance). Clone the repository and run the startup script to begin evaluating models on your hardware.\n\n## Resources and Further Reading\n\n- [FLPerformance Repository](https://github.com/leestott/FLPerformance) - Complete benchmarking platform\n- [Quick Start Guide](https://github.com/leestott/FLPerformance/blob/master/QUICK_START.md) - Setup and first benchmark run\n- [Microsoft Foundry Local Documentation](https://foundrylocal.ai) - SDK reference and model catalog\n- [Architecture Guide](https://github.com/leestott/FLPerformance/blob/master/docs/ARCHITECTURE.md) - System design and SDK integration\n- [Benchmarking Best Practices](https://github.com/leestott/FLPerformance/blob/master/docs/BENCHMARK_GUIDE.md) - Methodology and troubleshooting\n\nPublished Feb 02, 2026\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai foundry](/tag/ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[genai](/tag/genai?nodeId=board%3AAzureDevCommunityBlog)\n\n[get started](/tag/get%20started?nodeId=board%3AAzureDevCommunityBlog)\n\n[javascript](/tag/javascript?nodeId=board%3AAzureDevCommunityBlog)\n\n[learning](/tag/learning?nodeId=board%3AAzureDevCommunityBlog)\n\n[performance](/tag/performance?nodeId=board%3AAzureDevCommunityBlog)\n\n[slm](/tag/slm?nodeId=board%3AAzureDevCommunityBlog)\n\n[tips and tricks](/tag/tips%20and%20tricks?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[Lee_Stott&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yMTA1NDYtODM5MjVpMDI2ODNGQTMwMzAwNDFGQQ?image-dimensions=50x50)](/users/lee_stott/210546) [Lee_Stott](/users/lee_stott/210546) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined September 25, 2018\n\n[View Profile](/users/lee_stott/210546)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "ProcessedDate": "2026-02-02 08:09:45"
}
