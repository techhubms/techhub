{
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "PubDate": "2026-02-26T11:09:25+00:00",
  "Title": "Exploring Azure Face API: Facial Landmark Detection and Real-Time Analysis with C#",
  "FeedName": "Microsoft Tech Community",
  "Description": "In today’s world, applications that understand and respond to human facial cues are no longer science fiction—they’re becoming a reality in domains like security, driver monitoring, gaming, and AR/VR. With **Azure Face API**, developers can leverage powerful cloud-based facial recognition and analysis tools without building complex machine learning models from scratch.\n\nIn this blog, we’ll explore how to use **C#** to detect faces, identify key facial landmarks, estimate head pose, track eye and mouth movements, and process real-time video streams. Using **OpenCV** for visualization, we’ll show how to overlay landmarks, draw bounding boxes, and calculate metrics like **Eye Aspect Ratio (EAR)** and **Mouth Aspect Ratio (MAR)**—all in real time.\n\nYou'll learn to:\n\n- Set up Azure Face API\n- Detect 27 facial landmarks\n- Estimate head pose (yaw, pitch, roll)\n- Calculate eye aspect ratio (EAR) and mouth openness\n- Draw bounding boxes around features using OpenCV\n- Process real-time video\n\n**Prerequisites**\n\n- **.NET 8 SDK** installed\n- **Azure subscription** with Face API resource\n- **Visual Studio 2022** or later\n- **Webcam** for testing (optional)\n- Basic understanding of **C#** and **computer vision concepts**\n\n**Part 1: Azure Face API Setup**\n\n**1.1 Install Required NuGet Packages**\n\ndotnet add package Azure.AI.Vision.Face\n\ndotnet add package OpenCvSharp4\n\ndotnet add package OpenCvSharp4.runtime.win\n\n**1.2 Create Azure Face API Resource**\n\n1. Navigate to [Azure Portal](https://portal.azure.com)\n2. Search for **\"Face\"** and create a new Face API resource\n3. Choose your **pricing tier** (Free tier: 20 calls/min, 30K calls/month)\n4. Copy the **Endpoint URL** and **API Key**\n\n**1.3 Configure in .NET Application**\n\n**appsettings.json:**\n\n> >\n> {\n> > >\n> \"Azure\": {\n> > >\n> \"FaceApi\": {\n> > >\n> \"Endpoint\": \"https://your-resource.cognitiveservices.azure.com/\",\n> > >\n> \"ApiKey\": \"your-api-key-here\"\n> > >\n> }\n> > >\n> }\n> > >\n> }\n> >\n\n**Initialize Face Client:**\n\n> >\n> using Azure;\n> > >\n> using Azure.AI.Vision.Face;\n> > >\n> using Microsoft.Extensions.Configuration;\n> > >\n> public class FaceAnalysisService\n> > >\n> {\n> > >\n> private readonly FaceClient \\_faceClient;\n> > >\n> private readonly ILogger \\_logger;\n> > >\n> public FaceAnalysisService(ILogger logger, IConfiguration configuration)\n> > >\n> {\n> > >\n> \\_logger = logger;\n> > >\n> string endpoint = configuration[\"Azure:FaceApi:Endpoint\"];\n> > >\n> string apiKey = configuration[\"Azure:FaceApi:ApiKey\"];\n> > >\n> \\_faceClient = new FaceClient(new Uri(endpoint), new AzureKeyCredential(apiKey));\n> > >\n> \\_logger.LogInformation(\"FaceClient initialized with endpoint: {Endpoint}\", endpoint);\n> > >\n> }\n> > >\n> }\n> >\n\n**Part 2: Understanding Face Detection Models**\n\n**2.1 Basic Face Detection**\n\n> >\n> public async Task> DetectFacesAsync(byte[] imageBytes)\n> > >\n> {\n> > >\n> using var stream = new MemoryStream(imageBytes);\n> > >\n> var response = await \\_faceClient.DetectAsync(\n> > >\n> BinaryData.FromStream(stream),\n> > >\n> FaceDetectionModel.Detection03,\n> > >\n> FaceRecognitionModel.Recognition04,\n> > >\n> returnFaceId: false,\n> > >\n> returnFaceAttributes: new FaceAttributeType[] { FaceAttributeType.HeadPose },\n> > >\n> returnFaceLandmarks: true,\n> > >\n> returnRecognitionModel: false\n> > >\n> );\n> > >\n> \\_logger.LogInformation(\"Detected {Count} faces\", response.Value.Count);\n> > >\n> return response.Value.ToList();\n> > >\n> }\n> >\n\n**Part 3: Facial Landmarks - The 27 Key Points**\n\n**3.1 Understanding Facial Landmarks**\n\n![]()\n\n**3.2 Accessing Landmarks in Code**\n\n> >\n> public void PrintLandmarks(FaceDetectionResult face)\n> > >\n> {\n> > >\n> var landmarks = face.FaceLandmarks;\n> > >\n> if (landmarks == null)\n> > >\n> {\n> > >\n> \\_logger.LogWarning(\"No landmarks detected\");\n> > >\n> return;\n> > >\n> }\n> > >\n> // Eye landmarks\n> > >\n> Console.WriteLine($\"Left Eye Outer: ({landmarks.EyeLeftOuter.X}, {landmarks.EyeLeftOuter.Y})\");\n> > >\n> Console.WriteLine($\"Left Eye Inner: ({landmarks.EyeLeftInner.X}, {landmarks.EyeLeftInner.Y})\");\n> > >\n> Console.WriteLine($\"Left Eye Top: ({landmarks.EyeLeftTop.X}, {landmarks.EyeLeftTop.Y})\");\n> > >\n> Console.WriteLine($\"Left Eye Bottom: ({landmarks.EyeLeftBottom.X}, {landmarks.EyeLeftBottom.Y})\");\n> > >\n> // Mouth landmarks\n> > >\n> Console.WriteLine($\"Upper Lip Top: ({landmarks.UpperLipTop.X}, {landmarks.UpperLipTop.Y})\");\n> > >\n> Console.WriteLine($\"Under Lip Bottom: ({landmarks.UnderLipBottom.X}, {landmarks.UnderLipBottom.Y})\");\n> > >\n> // Nose landmarks\n> > >\n> Console.WriteLine($\"Nose Tip: ({landmarks.NoseTip.X}, {landmarks.NoseTip.Y})\");\n> > >\n> }\n> >\n\n**3.3 Visualizing All Landmarks**\n\n> >\n> public void DrawAllLandmarks(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> void DrawPoint(FaceLandmarkCoordinate point, Scalar color)\n> > >\n> {\n> > >\n> if (point != null)\n> > >\n> {\n> > >\n> Cv2.Circle(frame, new Point((int)point.X, (int)point.Y), radius: 3, color: color, thickness: -1);\n> > >\n> }\n> > >\n> }\n> > >\n> // Eyes (Green)\n> > >\n> DrawPoint(landmarks.EyeLeftOuter, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeLeftInner, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeLeftTop, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeLeftBottom, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightOuter, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightInner, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightTop, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightBottom, new Scalar(0, 255, 0));\n> > >\n> // Eyebrows (Cyan)\n> > >\n> DrawPoint(landmarks.EyebrowLeftOuter, new Scalar(255, 255, 0));\n> > >\n> DrawPoint(landmarks.EyebrowLeftInner, new Scalar(255, 255, 0));\n> > >\n> DrawPoint(landmarks.EyebrowRightOuter, new Scalar(255, 255, 0));\n> > >\n> DrawPoint(landmarks.EyebrowRightInner, new Scalar(255, 255, 0));\n> > >\n> // Nose (Yellow)\n> > >\n> DrawPoint(landmarks.NoseTip, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseRootLeft, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseRootRight, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseLeftAlarOutTip, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseRightAlarOutTip, new Scalar(0, 255, 255));\n> > >\n> // Mouth (Blue)\n> > >\n> DrawPoint(landmarks.UpperLipTop, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.UpperLipBottom, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.UnderLipTop, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.UnderLipBottom, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.MouthLeft, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.MouthRight, new Scalar(255, 0, 0));\n> > >\n> // Pupils (Red)\n> > >\n> DrawPoint(landmarks.PupilLeft, new Scalar(0, 0, 255));\n> > >\n> DrawPoint(landmarks.PupilRight, new Scalar(0, 0, 255));\n> > >\n> }\n> >\n\n**Part 4: Drawing Bounding Boxes Around Features**\n\n**4.1 Eye Bounding Boxes**\n\n> >\n> ///\n> > >\n> /// Draws rectangles around eyes using OpenCV.\n> > >\n> ///\n> > >\n> public void DrawEyeBoxes(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> int boxWidth = 60;\n> > >\n> int boxHeight = 35;\n> > >\n> // Calculate Rectangles\n> > >\n> var leftEyeRect = new Rect((int)landmarks.EyeLeftOuter.X - boxWidth / 2, (int)landmarks.EyeLeftOuter.Y - boxHeight / 2, boxWidth, boxHeight);\n> > >\n> var rightEyeRect = new Rect((int)landmarks.EyeRightOuter.X - boxWidth / 2, (int)landmarks.EyeRightOuter.Y - boxHeight / 2, boxWidth, boxHeight);\n> > >\n> // Draw Rectangles (Green in BGR)\n> > >\n> Cv2.Rectangle(frame, leftEyeRect, new Scalar(0, 255, 0), 2);\n> > >\n> Cv2.Rectangle(frame, rightEyeRect, new Scalar(0, 255, 0), 2);\n> > >\n> // Add Labels\n> > >\n> Cv2.PutText(frame, \"Left Eye\", new Point(leftEyeRect.X, leftEyeRect.Y - 5), HersheyFonts.HersheySimplex, 0.4, new Scalar(0, 255, 0), 1);\n> > >\n> Cv2.PutText(frame, \"Right Eye\", new Point(rightEyeRect.X, rightEyeRect.Y - 5), HersheyFonts.HersheySimplex, 0.4, new Scalar(0, 255, 0), 1);\n> > >\n> }\n> >\n\n**4.2 Mouth Bounding Box**\n\n> >\n> ///\n> > >\n> /// Draws rectangle around mouth region.\n> > >\n> ///\n> > >\n> public void DrawMouthBox(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> int boxWidth = 80;\n> > >\n> int boxHeight = 50;\n> > >\n> // Calculate center based on the vertical lip landmarks\n> > >\n> int centerX = (int)((landmarks.UpperLipTop.X + landmarks.UnderLipBottom.X) / 2);\n> > >\n> int centerY = (int)((landmarks.UpperLipTop.Y + landmarks.UnderLipBottom.Y) / 2);\n> > >\n> var mouthRect = new Rect(centerX - boxWidth / 2, centerY - boxHeight / 2, boxWidth, boxHeight);\n> > >\n> // Draw Mouth Box (Blue in BGR)\n> > >\n> Cv2.Rectangle(frame, mouthRect, new Scalar(255, 0, 0), 2);\n> > >\n> // Add Label\n> > >\n> Cv2.PutText(frame, \"Mouth\", new Point(mouthRect.X, mouthRect.Y - 5), HersheyFonts.HersheySimplex, 0.4, new Scalar(255, 0, 0), 1);\n> > >\n> }\n> >\n\n**4.3 Face Bounding Box**\n\n> >\n> ///\n> > >\n> /// Draws rectangle around entire face using the face rectangle from API.\n> > >\n> ///\n> > >\n> public void DrawFaceBox(FaceDetectionResult face, Mat frame)\n> > >\n> {\n> > >\n> var faceRect = face.FaceRectangle;\n> > >\n> if (faceRect == null)\n> > >\n> {\n> > >\n> return;\n> > >\n> }\n> > >\n> var rect = new Rect(\n> > >\n> faceRect.Left,\n> > >\n> faceRect.Top,\n> > >\n> faceRect.Width,\n> > >\n> faceRect.Height\n> > >\n> );\n> > >\n> // Draw Face Bounding Box (Red in BGR)\n> > >\n> Cv2.Rectangle(frame, rect, new Scalar(0, 0, 255), 2);\n> > >\n> // Add Label with dimensions\n> > >\n> Cv2.PutText(frame, $\"Face {faceRect.Width}x{faceRect.Height}\", new Point(rect.X, rect.Y - 10), HersheyFonts.HersheySimplex, 0.5,\n> > >\n> new Scalar(0, 0, 255), 2);\n> > >\n> }\n> >\n\n**4.4 Nose Bounding Box**\n\n> >\n> ///\n> > >\n> /// Draws bounding box around nose using nose landmarks.\n> > >\n> ///\n> > >\n> public void DrawNoseBox(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> // Calculate horizontal bounds from Alar tips\n> > >\n> int minX = (int)Math.Min(landmarks.NoseLeftAlarOutTip.X, landmarks.NoseRightAlarOutTip.X);\n> > >\n> int maxX = (int)Math.Max(landmarks.NoseLeftAlarOutTip.X, landmarks.NoseRightAlarOutTip.X);\n> > >\n> // Calculate vertical bounds from Root to Tip\n> > >\n> int minY = (int)Math.Min(landmarks.NoseRootLeft.Y, landmarks.NoseTip.Y);\n> > >\n> int maxY = (int)landmarks.NoseTip.Y;\n> > >\n> // Create Rect with a 10px padding buffer\n> > >\n> var noseRect = new Rect(\n> > >\n> minX - 10,\n> > >\n> minY - 10,\n> > >\n> (maxX - minX) + 20,\n> > >\n> (maxY - minY) + 20\n> > >\n> );\n> > >\n> // Draw Nose Box (Yellow in BGR)\n> > >\n> Cv2.Rectangle(frame, noseRect, new Scalar(0, 255, 255), 2);\n> > >\n> }\n> >\n\n**Part 5: Geometric Calculations with Landmarks**\n\n**5.1 Calculating Euclidean Distance**\n\n> >\n> ///\n> > >\n> /// Calculates distance between two landmark points.\n> > >\n> ///\n> > >\n> public static double CalculateDistance(dynamic point1, dynamic point2)\n> > >\n> {\n> > >\n> double dx = point1.X - point2.X;\n> > >\n> double dy = point1.Y - point2.Y;\n> > >\n> return Math.Sqrt(dx \\* dx + dy \\* dy);\n> > >\n> }\n> >\n\n**5.2 Eye Aspect Ratio (EAR) Formula**\n\n> >\n> ///\n> > >\n> /// Calculates the Eye Aspect Ratio (EAR) to detect eye closure.\n> > >\n> ///\n> > >\n> public double CalculateEAR(\n> > >\n> FaceLandmarkCoordinate top1,\n> > >\n> FaceLandmarkCoordinate top2,\n> > >\n> FaceLandmarkCoordinate bottom1,\n> > >\n> FaceLandmarkCoordinate bottom2,\n> > >\n> FaceLandmarkCoordinate inner,\n> > >\n> FaceLandmarkCoordinate outer)\n> > >\n> {\n> > >\n> // Vertical distances\n> > >\n> double v1 = CalculateDistance(top1, bottom1);\n> > >\n> double v2 = CalculateDistance(top2, bottom2);\n> > >\n> // Horizontal distance\n> > >\n> double h = CalculateDistance(inner, outer);\n> > >\n> // EAR formula: (||p2-p6|| + ||p3-p5||) / (2 \\* ||p1-p4||)\n> > >\n> return (v1 + v2) / (2.0 \\* h);\n> > >\n> }\n> >\n\n**Simplified Implementation:**\n\n> >\n> ///\n> > >\n> /// Calculates Eye Aspect Ratio (EAR) for a single eye.\n> > >\n> /// Reference: \"Real-Time Eye Blink Detection using Facial Landmarks\" (Soukupová & Čech, 2016)\n> > >\n> ///\n> > >\n> public double ComputeEAR(FaceLandmarks landmarks, bool isLeftEye)\n> > >\n> {\n> > >\n> var top = isLeftEye ? landmarks.EyeLeftTop : landmarks.EyeRightTop;\n> > >\n> var bottom = isLeftEye ? landmarks.EyeLeftBottom : landmarks.EyeRightBottom;\n> > >\n> var inner = isLeftEye ? landmarks.EyeLeftInner : landmarks.EyeRightInner;\n> > >\n> var outer = isLeftEye ? landmarks.EyeLeftOuter : landmarks.EyeRightOuter;\n> > >\n> if (top == null || bottom == null || inner == null || outer == null)\n> > >\n> {\n> > >\n> \\_logger.LogWarning(\"Missing eye landmarks\");\n> > >\n> return 1.0; // Return 1.0 (open) to prevent false positives for drowsiness\n> > >\n> }\n> > >\n> double verticalDist = CalculateDistance(top, bottom);\n> > >\n> double horizontalDist = CalculateDistance(inner, outer);\n> > >\n> // Simplified EAR for Azure 27-point model\n> > >\n> double ear = verticalDist / horizontalDist;\n> > >\n> \\_logger.LogDebug(\n> > >\n> \"EAR for {Eye}: {Value:F3}\",\n> > >\n> isLeftEye ? \"left\" : \"right\",\n> > >\n> ear\n> > >\n> );\n> > >\n> return ear;\n> > >\n> }\n> >\n\n**Usage Example:**\n\n> >\n> var leftEAR = ComputeEAR(landmarks, isLeftEye: true);\n> > >\n> var rightEAR = ComputeEAR(landmarks, isLeftEye: false);\n> > >\n> var avgEAR = (leftEAR + rightEAR) / 2.0;\n> > >\n> Console.WriteLine($\"Average EAR: {avgEAR:F3}\");\n> > >\n> // Open eyes: ~0.25-0.30\n> > >\n> // Closed eyes: ~0.10-0.15\n> >\n\n**5.3 Mouth Aspect Ratio (MAR)**\n\n> >\n> ///\n> > >\n> /// Calculates Mouth Aspect Ratio relative to face height.\n> > >\n> ///\n> > >\n> public double CalculateMouthAspectRatio(FaceLandmarks landmarks, FaceRectangle faceRect)\n> > >\n> {\n> > >\n> double mouthHeight = landmarks.UnderLipBottom.Y - landmarks.UpperLipTop.Y;\n> > >\n> double mouthWidth = CalculateDistance(landmarks.MouthLeft, landmarks.MouthRight);\n> > >\n> double mouthOpenRatio = mouthHeight / faceRect.Height;\n> > >\n> double mouthWidthRatio = mouthWidth / faceRect.Width;\n> > >\n> \\_logger.LogDebug(\n> > >\n> \"Mouth - Height ratio: {HeightRatio:F3}, Width ratio: {WidthRatio:F3}\",\n> > >\n> mouthOpenRatio,\n> > >\n> mouthWidthRatio\n> > >\n> );\n> > >\n> return mouthOpenRatio;\n> > >\n> }\n> >\n\n**5.4 Inter-Eye Distance**\n\n> >\n> ///\n> > >\n> /// Calculates the distance between pupils (inter-pupillary distance).\n> > >\n> ///\n> > >\n> public double CalculateInterEyeDistance(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> return CalculateDistance(landmarks.PupilLeft, landmarks.PupilRight);\n> > >\n> }\n> > >\n> ///\n> > >\n> /// Calculates distance between inner eye corners.\n> > >\n> ///\n> > >\n> public double CalculateInnerEyeDistance(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> return CalculateDistance(landmarks.EyeLeftInner, landmarks.EyeRightInner);\n> > >\n> }\n> >\n\n**5.5 Face Symmetry Analysis**\n\n> >\n> ///\n> > >\n> /// Analyzes facial symmetry by comparing left and right sides.\n> > >\n> ///\n> > >\n> public FaceSymmetryMetrics AnalyzeFaceSymmetry(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> double centerX = landmarks.NoseTip.X;\n> > >\n> double leftEyeDistance = CalculateDistance(landmarks.EyeLeftInner, new { X = centerX, Y = landmarks.EyeLeftInner.Y });\n> > >\n> double leftMouthDistance = CalculateDistance(landmarks.MouthLeft, new { X = centerX, Y = landmarks.MouthLeft.Y });\n> > >\n> double rightEyeDistance = CalculateDistance(landmarks.EyeRightInner, new { X = centerX, Y = landmarks.EyeRightInner.Y });\n> > >\n> double rightMouthDistance = CalculateDistance(landmarks.MouthRight, new { X = centerX, Y = landmarks.MouthRight.Y });\n> > >\n> return new FaceSymmetryMetrics\n> > >\n> {\n> > >\n> EyeSymmetryRatio = leftEyeDistance / rightEyeDistance,\n> > >\n> MouthSymmetryRatio = leftMouthDistance / rightMouthDistance,\n> > >\n> IsSymmetric = Math.Abs(leftEyeDistance - rightEyeDistance)\n>\n> };\n> > >\n> }\n> > >\n> public class FaceSymmetryMetrics\n> > >\n> {\n> > >\n> public double EyeSymmetryRatio { get; set; }\n> > >\n> public double MouthSymmetryRatio { get; set; }\n> > >\n> public bool IsSymmetric { get; set; }\n> > >\n> }\n> >\n\n**Part 6: Head Pose Estimation**\n\n**6.1 Understanding Head Pose Angles**\n\nAzure Face API provides three Euler angles for head orientation:\n\n**6.2 Accessing Head Pose Data**\n\n> >\n> public void AnalyzeHeadPose(FaceDetectionResult face)\n> > >\n> {\n> > >\n> var headPose = face.FaceAttributes?.HeadPose;\n> > >\n> if (headPose == null)\n> > >\n> {\n> > >\n> \\_logger.LogWarning(\"Head pose not available\");\n> > >\n> return;\n> > >\n> }\n> > >\n> double yaw = headPose.Yaw;\n> > >\n> double pitch = headPose.Pitch;\n> > >\n> double roll = headPose.Roll;\n> > >\n> Console.WriteLine(\"Head Pose:\");\n> > >\n> Console.WriteLine($\" Yaw: {yaw:F2}° (Left/Right)\");\n> > >\n> Console.WriteLine($\" Pitch: {pitch:F2}° (Up/Down)\");\n> > >\n> Console.WriteLine($\" Roll: {roll:F2}° (Tilt)\");\n> > >\n> InterpretHeadPose(yaw, pitch, roll);\n> > >\n> }\n> >\n\n**6.3 Interpreting Head Pose**\n\n> >\n> public string InterpretHeadPose(double yaw, double pitch, double roll) {\n> > >\n> var directions = new List();\n> > >\n> // Interpret Yaw (horizontal)\n> > >\n> if (Math.Abs(yaw)\n>\n> else if (yaw\n>\n> else if (yaw > 20) directions.Add($\"Turned Right ({yaw:F0}°)\");\n> > >\n> // Interpret Pitch (vertical)\n> > >\n> if (Math.Abs(pitch)\n>\n> else if (pitch\n>\n> else if (pitch > 15) directions.Add($\"Looking Up ({pitch:F0}°)\");\n> > >\n> // Interpret Roll (tilt)\n> > >\n> if (Math.Abs(roll) > 15) {\n> > >\n> string side = roll\n>\n> directions.Add($\"Tilted {side} ({Math.Abs(roll):F0}°)\");\n> > >\n> }\n> > >\n> return string.Join(\", \", directions);\n> > >\n> }\n> >\n\n**6.4 Visualizing Head Pose on Frame**\n\n> >\n> ///\n> > >\n> /// Draws head pose information with color-coded indicators.\n> > >\n> ///\n> > >\n> public void DrawHeadPoseInfo(Mat frame, HeadPose headPose, FaceRectangle faceRect)\n> > >\n> {\n> > >\n> double yaw = headPose.Yaw;\n> > >\n> double pitch = headPose.Pitch;\n> > >\n> double roll = headPose.Roll;\n> > >\n> int centerX = faceRect.Left + faceRect.Width / 2;\n> > >\n> int centerY = faceRect.Top + faceRect.Height / 2;\n> > >\n> string poseText = $\"Yaw: {yaw:F1}° Pitch: {pitch:F1}° Roll: {roll:F1}°\";\n> > >\n> Cv2.PutText(frame, poseText, new Point(faceRect.Left, faceRect.Top - 10), HersheyFonts.HersheySimplex, 0.5, new Scalar(255, 255, 255), 1);\n> > >\n> int arrowLength = 50;\n> > >\n> double yawRadians = yaw \\* Math.PI / 180.0;\n> > >\n> int arrowEndX = centerX + (int)(arrowLength \\* Math.Sin(yawRadians));\n> > >\n> Cv2.ArrowedLine(frame, new Point(centerX, centerY), new Point(arrowEndX, centerY), new Scalar(0, 255, 0), 2, tipLength: 0.3);\n> > >\n> double pitchRadians = -pitch \\* Math.PI / 180.0;\n> > >\n> int arrowPitchEndY = centerY + (int)(arrowLength \\* Math.Sin(pitchRadians));\n> > >\n> Cv2.ArrowedLine(frame, new Point(centerX, centerY), new Point(centerX, arrowPitchEndY), new Scalar(255, 0, 0), 2, tipLength: 0.3);\n> > >\n> }\n> >\n\n**6.5 Detecting Head Orientation States**\n\n> >\n> public enum HeadOrientation { Forward, Left, Right, Up, Down, TiltedLeft, TiltedRight, UpLeft, UpRight, DownLeft, DownRight }\n> > >\n> public List DetectHeadOrientation(HeadPose headPose)\n> > >\n> {\n> > >\n> const double THRESHOLD = 15.0;\n> > >\n> bool lookingUp = headPose.Pitch > THRESHOLD;\n> > >\n> bool lookingDown = headPose.Pitch\n>\n> bool lookingLeft = headPose.Yaw\n>\n> bool lookingRight = headPose.Yaw > THRESHOLD;\n> > >\n> var orientations = new List();\n> > >\n> if (!lookingUp && !lookingDown && !lookingLeft && !lookingRight) orientations.Add(HeadOrientation.Forward);\n> > >\n> if (lookingUp && !lookingLeft && !lookingRight) orientations.Add(HeadOrientation.Up);\n> > >\n> if (lookingDown && !lookingLeft && !lookingRight) orientations.Add(HeadOrientation.Down);\n> > >\n> if (lookingLeft && !lookingUp && !lookingDown) orientations.Add(HeadOrientation.Left);\n> > >\n> if (lookingRight && !lookingUp && !lookingDown) orientations.Add(HeadOrientation.Right);\n> > >\n> if (lookingUp && lookingLeft) orientations.Add(HeadOrientation.UpLeft);\n> > >\n> if (lookingUp && lookingRight) orientations.Add(HeadOrientation.UpRight);\n> > >\n> if (lookingDown && lookingLeft) orientations.Add(HeadOrientation.DownLeft);\n> > >\n> if (lookingDown && lookingRight) orientations.Add(HeadOrientation.DownRight);\n> > >\n> return orientations;\n> > >\n> }\n> >\n\n**Part 7: Real-Time Video Processing**\n\n**7.1 Setting Up Video Capture**\n\n> >\n> using OpenCvSharp;\n> > >\n> public class RealTimeFaceAnalyzer : IDisposable\n> > >\n> {\n> > >\n> private VideoCapture? \\_capture;\n> > >\n> private Mat? \\_frame;\n> > >\n> private readonly FaceClient \\_faceClient;\n> > >\n> private bool \\_isRunning;\n> > >\n> public async Task StartAsync()\n> > >\n> {\n> > >\n> \\_capture = new VideoCapture(0);\n> > >\n> \\_frame = new Mat();\n> > >\n> \\_isRunning = true;\n> > >\n> await Task.Run(() => ProcessVideoLoop());\n> > >\n> }\n> > >\n> private async Task ProcessVideoLoop()\n> > >\n> {\n> > >\n> while (\\_isRunning)\n> > >\n> {\n> > >\n> if (\\_capture == null || !\\_capture.IsOpened()) break;\n> > >\n> \\_capture.Read(\\_frame);\n> > >\n> if (\\_frame == null || \\_frame.Empty())\n> > >\n> {\n> > >\n> await Task.Delay(1); // Minimal delay to prevent CPU spiking\n> > >\n> continue;\n> > >\n> }\n> > >\n> Cv2.Resize(\\_frame, \\_frame, new Size(640, 480));\n> > >\n> // Ensure we don't await indefinitely in the rendering loop\n> > >\n> \\_ = ProcessFrameAsync(\\_frame.Clone());\n> > >\n> Cv2.ImShow(\"Face Analysis\", \\_frame);\n> > >\n> if (Cv2.WaitKey(30) == 'q') break;\n> > >\n> }\n> > >\n> Dispose();\n> > >\n> }\n> > >\n> private async Task ProcessFrameAsync(Mat frame)\n> > >\n> {\n> > >\n> // This is where your DrawFaceBox, DrawAllLandmarks, and EAR logic will sit.\n> > >\n> // Remember to use try-catch here to prevent API errors from crashing the loop.\n> > >\n> }\n> > >\n> public void Dispose()\n> > >\n> {\n> > >\n> \\_isRunning = false;\n> > >\n> \\_capture?.Dispose();\n> > >\n> \\_frame?.Dispose();\n> > >\n> Cv2.DestroyAllWindows();\n> > >\n> }\n> > >\n> }\n> >\n\n**7.2 Optimizing API Calls**\n\n**Problem:** Calling Azure Face API on every frame (30 fps) is expensive and slow.\n\n**Solution:** Call API once per second, cache results for 30 frames.\n\n> >\n> private List \\_cachedFaces = new();\n> > >\n> private DateTime \\_lastDetectionTime = DateTime.MinValue;\n> > >\n> private readonly object \\_cacheLock = new();\n> > >\n> private async Task ProcessFrameAsync(Mat frame)\n> > >\n> {\n> > >\n> if ((DateTime.Now - \\_lastDetectionTime).TotalSeconds >= 1.0)\n> > >\n> {\n> > >\n> \\_lastDetectionTime = DateTime.Now;\n> > >\n> byte[] imageBytes;\n> > >\n> Cv2.ImEncode(\".jpg\", frame, out imageBytes);\n> > >\n> var faces = await DetectFacesAsync(imageBytes);\n> > >\n> lock (\\_cacheLock)\n> > >\n> {\n> > >\n> \\_cachedFaces = faces;\n> > >\n> }\n> > >\n> }\n> > >\n> List facesToProcess;\n> > >\n> lock (\\_cacheLock)\n> > >\n> {\n> > >\n> facesToProcess = \\_cachedFaces.ToList();\n> > >\n> }\n> > >\n> foreach (var face in facesToProcess)\n> > >\n> {\n> > >\n> DrawFaceAnnotations(face, frame);\n> > >\n> }\n> > >\n> }\n> >\n\n**Performance Improvement:**\n\n- **30x fewer API calls** (1/sec instead of 30/sec)\n- **~$0.02/hour** instead of **~$0.60/hour**\n- **Smooth 30 fps** rendering\n- **for visual updates**\n\n**7.3 Drawing Complete Face Annotations**\n\n> >\n> private void DrawFaceAnnotations(FaceDetectionResult face, Mat frame)\n> > >\n> {\n> > >\n> DrawFaceBox(face, frame);\n> > >\n> if (face.FaceLandmarks != null)\n> > >\n> {\n> > >\n> DrawAllLandmarks(face.FaceLandmarks, frame);\n> > >\n> DrawEyeBoxes(face.FaceLandmarks, frame);\n> > >\n> DrawMouthBox(face.FaceLandmarks, frame);\n> > >\n> DrawNoseBox(face.FaceLandmarks, frame);\n> > >\n> double leftEAR = ComputeEAR(face.FaceLandmarks, isLeftEye: true);\n> > >\n> double rightEAR = ComputeEAR(face.FaceLandmarks, isLeftEye: false);\n> > >\n> double avgEAR = (leftEAR + rightEAR) / 2.0;\n> > >\n> Cv2.PutText(frame, $\"EAR: {avgEAR:F3}\", new Point(10, 30), HersheyFonts.HersheySimplex, 0.6, new Scalar(0, 255, 0), 2);\n> > >\n> }\n> > >\n> if (face.FaceAttributes?.HeadPose != null)\n> > >\n> {\n> > >\n> DrawHeadPoseInfo(frame, face.FaceAttributes.HeadPose, face.FaceRectangle);\n> > >\n> string orientation = InterpretHeadPose(face.FaceAttributes.HeadPose.Yaw, face.FaceAttributes.HeadPose.Pitch, face.FaceAttributes.HeadPose.Roll);\n> > >\n> Cv2.PutText(frame, orientation, new Point(10, 60), HersheyFonts.HersheySimplex, 0.6, new Scalar(255, 255, 0), 2);\n> > >\n> }\n> > >\n> }\n> >\n\n**Part 8: Advanced Features and Use Cases**\n\n**8.1 Face Tracking Across Frames**\n\n> >\n> public class FaceTracker\n> > >\n> {\n> > >\n> private class TrackedFace\n> > >\n> {\n> > >\n> public FaceRectangle Rectangle { get; set; }\n> > >\n> public DateTime LastSeen { get; set; }\n> > >\n> public int TrackId { get; set; }\n> > >\n> }\n> > >\n> private List \\_trackedFaces = new();\n> > >\n> private int \\_nextTrackId = 1;\n> > >\n> public int TrackFace(FaceRectangle newFace)\n> > >\n> {\n> > >\n> const int MATCH\\_THRESHOLD = 50;\n> > >\n> var match = \\_trackedFaces.FirstOrDefault(tf => {\n> > >\n> double distance = Math.Sqrt(Math.Pow(tf.Rectangle.Left - newFace.Left, 2) + Math.Pow(tf.Rectangle.Top - newFace.Top, 2));\n> > >\n> return distance\n>\n> });\n> > >\n> if (match != null)\n> > >\n> {\n> > >\n> match.Rectangle = newFace;\n> > >\n> match.LastSeen = DateTime.Now;\n> > >\n> return match.TrackId;\n> > >\n> }\n> > >\n> var newTrack = new TrackedFace { Rectangle = newFace, LastSeen = DateTime.Now, TrackId = \\_nextTrackId++ };\n> > >\n> \\_trackedFaces.Add(newTrack);\n> > >\n> return newTrack.TrackId;\n> > >\n> }\n> > >\n> public void RemoveOldTracks(TimeSpan maxAge)\n> > >\n> {\n> > >\n> \\_trackedFaces.RemoveAll(tf => DateTime.Now - tf.LastSeen > maxAge);\n> > >\n> }\n> > >\n> }\n> >\n\n**8.2 Multi-Face Detection and Analysis**\n\n> >\n> public async Task AnalyzeMultipleFacesAsync(byte[] imageBytes)\n> > >\n> {\n> > >\n> var faces = await DetectFacesAsync(imageBytes);\n> > >\n> var report = new FaceAnalysisReport { TotalFacesDetected = faces.Count, Timestamp = DateTime.Now, Faces = new List() };\n> > >\n> for (int i = 0; i\n>\n> {\n> > >\n> var face = faces[i];\n> > >\n> var analysis = new SingleFaceAnalysis { FaceIndex = i, FaceLocation = face.FaceRectangle, FaceSize = face.FaceRectangle.Width \\* face.FaceRectangle.Height };\n> > >\n> if (face.FaceLandmarks != null)\n> > >\n> {\n> > >\n> analysis.LeftEyeEAR = ComputeEAR(face.FaceLandmarks, true);\n> > >\n> analysis.RightEyeEAR = ComputeEAR(face.FaceLandmarks, false);\n> > >\n> analysis.InterPupillaryDistance = CalculateInterEyeDistance(face.FaceLandmarks);\n> > >\n> }\n> > >\n> if (face.FaceAttributes?.HeadPose != null)\n> > >\n> {\n> > >\n> analysis.HeadYaw = face.FaceAttributes.HeadPose.Yaw;\n> > >\n> analysis.HeadPitch = face.FaceAttributes.HeadPose.Pitch;\n> > >\n> analysis.HeadRoll = face.FaceAttributes.HeadPose.Roll;\n> > >\n> }\n> > >\n> report.Faces.Add(analysis);\n> > >\n> }\n> > >\n> report.Faces = report.Faces.OrderByDescending(f => f.FaceSize).ToList();\n> > >\n> return report;\n> > >\n> }\n> > >\n> public class FaceAnalysisReport\n> > >\n> {\n> > >\n> public int TotalFacesDetected { get; set; }\n> > >\n> public DateTime Timestamp { get; set; }\n> > >\n> public List Faces { get; set; }\n> > >\n> }\n> > >\n> public class SingleFaceAnalysis\n> > >\n> {\n> > >\n> public int FaceIndex { get; set; }\n> > >\n> public FaceRectangle FaceLocation { get; set; }\n> > >\n> public int FaceSize { get; set; }\n> > >\n> public double LeftEyeEAR { get; set; }\n> > >\n> public double RightEyeEAR { get; set; }\n> > >\n> public double InterPupillaryDistance { get; set; }\n> > >\n> public double HeadYaw { get; set; }\n> > >\n> public double HeadPitch { get; set; }\n> > >\n> public double HeadRoll { get; set; }\n> > >\n> }\n> >\n\n**8.3 Exporting Landmark Data to JSON**\n\n> >\n> using System.Text.Json;\n> > >\n> public string ExportLandmarksToJson(FaceDetectionResult face)\n> > >\n> {\n> > >\n> var landmarks = face.FaceLandmarks;\n> > >\n> var landmarkData = new\n> > >\n> {\n> > >\n> Face = new { Rectangle = new { face.FaceRectangle.Left, face.FaceRectangle.Top, face.FaceRectangle.Width, face.FaceRectangle.Height } },\n> > >\n> Eyes = new\n> > >\n> {\n> > >\n> Left = new { Outer = new { landmarks.EyeLeftOuter.X, landmarks.EyeLeftOuter.Y }, Inner = new { landmarks.EyeLeftInner.X, landmarks.EyeLeftInner.Y }, Top = new { landmarks.EyeLeftTop.X, landmarks.EyeLeftTop.Y }, Bottom = new { landmarks.EyeLeftBottom.X, landmarks.EyeLeftBottom.Y } },\n> > >\n> Right = new { Outer = new { landmarks.EyeRightOuter.X, landmarks.EyeRightOuter.Y }, Inner = new { landmarks.EyeRightInner.X, landmarks.EyeRightInner.Y }, Top = new { landmarks.EyeRightTop.X, landmarks.EyeRightTop.Y }, Bottom = new { landmarks.EyeRightBottom.X, landmarks.EyeRightBottom.Y } }\n> > >\n> },\n> > >\n> Mouth = new { UpperLipTop = new { landmarks.UpperLipTop.X, landmarks.UpperLipTop.Y }, UnderLipBottom = new { landmarks.UnderLipBottom.X, landmarks.UnderLipBottom.Y }, Left = new { landmarks.MouthLeft.X, landmarks.MouthLeft.Y }, Right = new { landmarks.MouthRight.X, landmarks.MouthRight.Y } },\n> > >\n> Nose = new { Tip = new { landmarks.NoseTip.X, landmarks.NoseTip.Y }, RootLeft = new { landmarks.NoseRootLeft.X, landmarks.NoseRootLeft.Y }, RootRight = new { landmarks.NoseRootRight.X, landmarks.NoseRootRight.Y } },\n> > >\n> HeadPose = face.FaceAttributes?.HeadPose != null ? new { face.FaceAttributes.HeadPose.Yaw, face.FaceAttributes.HeadPose.Pitch, face.FaceAttributes.HeadPose.Roll } : null\n> > >\n> };\n> > >\n> return JsonSerializer.Serialize(landmarkData, new JsonSerializerOptions { WriteIndented = true });\n> > >\n> }\n> >\n\n**Part 9: Practical Applications**\n\n**9.1 Gaze Direction Estimation**\n\n> >\n> public enum GazeDirection { Center, Left, Right, Up, Down, UpLeft, UpRight, DownLeft, DownRight }\n> > >\n> public GazeDirection EstimateGazeDirection(HeadPose headPose)\n> > >\n> {\n> > >\n> const double THRESHOLD = 15.0;\n> > >\n> bool lookingUp = headPose.Pitch > THRESHOLD;\n> > >\n> bool lookingDown = headPose.Pitch\n>\n> bool lookingLeft = headPose.Yaw\n>\n> bool lookingRight = headPose.Yaw > THRESHOLD;\n> > >\n> if (lookingUp && lookingLeft) return GazeDirection.UpLeft;\n> > >\n> if (lookingUp && lookingRight) return GazeDirection.UpRight;\n> > >\n> if (lookingDown && lookingLeft) return GazeDirection.DownLeft;\n> > >\n> if (lookingDown && lookingRight) return GazeDirection.DownRight;\n> > >\n> if (lookingUp) return GazeDirection.Up;\n> > >\n> if (lookingDown) return GazeDirection.Down;\n> > >\n> if (lookingLeft) return GazeDirection.Left;\n> > >\n> if (lookingRight) return GazeDirection.Right;\n> > >\n> return GazeDirection.Center;\n> > >\n> }\n> >\n\n**9.2 Expression Analysis Using Landmarks**\n\n> >\n> public class ExpressionAnalyzer\n> > >\n> {\n> > >\n> public bool IsSmiling(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> double mouthCenterY = (landmarks.UpperLipTop.Y + landmarks.UnderLipBottom.Y) / 2;\n> > >\n> double leftCornerY = landmarks.MouthLeft.Y;\n> > >\n> double rightCornerY = landmarks.MouthRight.Y;\n> > >\n> return leftCornerY\n>\n> }\n> > >\n> public bool IsMouthOpen(FaceLandmarks landmarks, FaceRectangle faceRect)\n> > >\n> {\n> > >\n> double mouthHeight = landmarks.UnderLipBottom.Y - landmarks.UpperLipTop.Y;\n> > >\n> double mouthOpenRatio = mouthHeight / faceRect.Height;\n> > >\n> return mouthOpenRatio > 0.08; // 8% of face height\n> > >\n> }\n> > >\n> public bool AreEyesClosed(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> double leftEAR = ComputeEAR(landmarks, isLeftEye: true);\n> > >\n> double rightEAR = ComputeEAR(landmarks, isLeftEye: false);\n> > >\n> double avgEAR = (leftEAR + rightEAR) / 2.0;\n> > >\n> return avgEAR\n>\n> }\n> > >\n> }\n> >\n\n**9.3 Face Orientation for AR/VR Applications**\n\n> >\n> public class FaceOrientationFor3D\n> > >\n> {\n> > >\n> public (Vector3 forward, Vector3 up, Vector3 right) GetFaceOrientation(HeadPose headPose)\n> > >\n> {\n> > >\n> double yawRad = headPose.Yaw \\* Math.PI / 180.0;\n> > >\n> double pitchRad = headPose.Pitch \\* Math.PI / 180.0;\n> > >\n> double rollRad = headPose.Roll \\* Math.PI / 180.0;\n> > >\n> var forward = new Vector3((float)(Math.Sin(yawRad) \\* Math.Cos(pitchRad)), (float)(-Math.Sin(pitchRad)), (float)(Math.Cos(yawRad) \\* Math.Cos(pitchRad)));\n> > >\n> var up = new Vector3((float)(Math.Sin(yawRad) \\* Math.Sin(pitchRad) \\* Math.Cos(rollRad) - Math.Cos(yawRad) \\* Math.Sin(rollRad)), (float)(Math.Cos(pitchRad) \\* Math.Cos(rollRad)), (float)(Math.Cos(yawRad) \\* Math.Sin(pitchRad) \\* Math.Cos(rollRad) + Math.Sin(yawRad) \\* Math.Sin(rollRad)));\n> > >\n> var right = Vector3.Cross(up, forward);\n> > >\n> return (forward, up, right);\n> > >\n> }\n> > >\n> }\n> > >\n> public struct Vector3\n> > >\n> {\n> > >\n> public float X, Y, Z;\n> > >\n> public Vector3(float x, float y, float z) { X = x; Y = y; Z = z; }\n> > >\n> public static Vector3 Cross(Vector3 a, Vector3 b) => new Vector3(a.Y \\* b.Z - a.Z \\* b.Y, a.Z \\* b.X - a.X \\* b.Z, a.X \\* b.Y - a.Y \\* b.X);\n> > >\n> }\n> >\n\n**Conclusion**\n\nThis technical guide has explored the capabilities of Azure Face API for facial analysis in C#. We've covered:\n\n**Key Capabilities Demonstrated**\n\n- Facial Landmark Detection - Accessing 27 precise points on the face\n- Head Pose Estimation - Tracking yaw, pitch, and roll angles\n- Geometric Calculations - Computing EAR, distances, and ratios\n- Visual Annotations - Drawing bounding boxes with OpenCV\n- Real-Time Processing - Optimized video stream analysis\n\n**Technical Achievements**\n\n**Computer Vision Math:**\n\n- Euclidean distance calculations\n- Eye Aspect Ratio (EAR) formula\n- Mouth aspect ratio measurements\n- Face symmetry analysis\n\n**OpenCV Integration:**\n\n- Drawing bounding boxes and landmarks\n- Color-coded feature highlighting\n- Real-time annotation overlays\n- Video capture and processing\n\n**Practical Applications**\n\nThis technology enables:\n\n- 👁️ **Gaze tracking** for UI/UX studies\n- 🎮 **Head-controlled** game interfaces\n- 📸 **Auto-focus** camera systems\n- 🎭 **Expression analysis** for feedback\n- 🥽 **AR/VR** avatar control\n- 📊 **Attention analytics** for presentations\n- ♿ **Accessibility features** for disabled users\n\n**Performance Metrics**\n\n- **Detection Accuracy**: 95%+ for frontal faces\n- **Landmark Precision**: ±2-3 pixels\n- **Processing Latency**: 200-500ms per API call\n- **Frame Rate**: 30 fps with caching\n\n**Further Exploration**\n\n**Advanced Topics to Explore:**\n\n1. **Face Recognition** - Identify individuals\n2. **Age/Gender Detection** - Demographic analysis\n3. **Emotion Detection** - Facial expression classification\n4. **Face Verification** - 1:1 identity confirmation\n5. **Similar Face Search** - 1:N face matching\n6. **Face Grouping** - Cluster similar faces\n\n## Call to Action\n\n📌 Explore these resources to get started:\n\n**Official Documentation**\n\n- [Azure Face API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-identity)\n- [Face API REST Reference](https://learn.microsoft.com/en-us/rest/api/face/)\n- [Azure Face SDK for .NET](https://www.nuget.org/packages/Azure.AI.Vision.Face)\n\n**Related Libraries**\n\n- [OpenCVSharp](https://github.com/shimat/opencvsharp) - OpenCV wrapper for .NET\n- [System.Drawing](https://docs.microsoft.com/en-us/dotnet/api/system.drawing) - .NET image processing\n\n**Source Code**\n\n- **GitHub Repository: [ravimodi_microsoft/SmartDriver](https://github.com/ravimodi_microsoft/SmartDriver)**\n- **Sample Code**: Included in this article",
  "Tags": [],
  "OutputDir": "_community",
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/exploring-azure-face-api-facial-landmark-detection-and-real-time/ba-p/4495335",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Author": "ravimodi",
  "ProcessedDate": "2026-02-26 11:14:58",
  "EnhancedContent": "## Azure Face API is a cloud-based vision service offering advanced facial analysis. This article explains how to use facial landmark detection, head pose estimation, and real-time video processing with C# and OpenCV.\n\nIn today’s world, applications that understand and respond to human facial cues are no longer science fiction—they’re becoming a reality in domains like security, driver monitoring, gaming, and AR/VR. With **Azure Face API**, developers can leverage powerful cloud-based facial recognition and analysis tools without building complex machine learning models from scratch.\n\nIn this blog, we’ll explore how to use **C#** to detect faces, identify key facial landmarks, estimate head pose, track eye and mouth movements, and process real-time video streams. Using **OpenCV** for visualization, we’ll show how to overlay landmarks, draw bounding boxes, and calculate metrics like **Eye Aspect Ratio (EAR)** and **Mouth Aspect Ratio (MAR)**—all in real time.\n\nYou'll learn to:\n\n- Set up Azure Face API\n- Detect 27 facial landmarks\n- Estimate head pose (yaw, pitch, roll)\n- Calculate eye aspect ratio (EAR) and mouth openness\n- Draw bounding boxes around features using OpenCV\n- Process real-time video\n\n**Prerequisites**\n\n- **.NET 8 SDK** installed\n- **Azure subscription** with Face API resource\n- **Visual Studio 2022** or later\n- **Webcam** for testing (optional)\n- Basic understanding of **C#** and **computer vision concepts**\n\n**Part 1: Azure Face API Setup**\n\n**1.1 Install Required NuGet Packages**\n\ndotnet add package Azure.AI.Vision.Face\n\ndotnet add package OpenCvSharp4\n\ndotnet add package OpenCvSharp4.runtime.win\n\n**1.2 Create Azure Face API Resource**\n\n1. Navigate to [Azure Portal](https://portal.azure.com)\n2. Search for **\"Face\"** and create a new Face API resource\n3. Choose your **pricing tier** (Free tier: 20 calls/min, 30K calls/month)\n4. Copy the **Endpoint URL** and **API Key**\n\n**1.3 Configure in .NET Application**\n\n**appsettings.json:**\n\n> >\n> {\n> > >\n> \"Azure\": {\n> > >\n> \"FaceApi\": {\n> > >\n> \"Endpoint\": \"https://your-resource.cognitiveservices.azure.com/\",\n> > >\n> \"ApiKey\": \"your-api-key-here\"\n> > >\n> }\n> > >\n> }\n> > >\n> }\n> >\n\n**Initialize Face Client:**\n\n> >\n> using Azure;\n> > >\n> using Azure.AI.Vision.Face;\n> > >\n> using Microsoft.Extensions.Configuration;\n> > >\n> public class FaceAnalysisService\n> > >\n> {\n> > >\n> private readonly FaceClient \\_faceClient;\n> > >\n> private readonly ILogger&lt;FaceAnalysisService&gt; \\_logger;\n> > >\n> public FaceAnalysisService(ILogger&lt;FaceAnalysisService&gt; logger, IConfiguration configuration)\n> > >\n> {\n> > >\n> \\_logger = logger;\n> > >\n> string endpoint = configuration[\"Azure:FaceApi:Endpoint\"];\n> > >\n> string apiKey = configuration[\"Azure:FaceApi:ApiKey\"];\n> > >\n> \\_faceClient = new FaceClient(new Uri(endpoint), new AzureKeyCredential(apiKey));\n> > >\n> \\_logger.LogInformation(\"FaceClient initialized with endpoint: {Endpoint}\", endpoint);\n> > >\n> }\n> > >\n> }\n> >\n\n**Part 2: Understanding Face Detection Models**\n\n**2.1 Basic Face Detection**\n\n> >\n> public async Task&lt;List&lt;FaceDetectionResult&gt;&gt; DetectFacesAsync(byte[] imageBytes)\n> > >\n> {\n> > >\n> using var stream = new MemoryStream(imageBytes);\n> > >\n> var response = await \\_faceClient.DetectAsync(\n> > >\n> BinaryData.FromStream(stream),\n> > >\n> FaceDetectionModel.Detection03,\n> > >\n> FaceRecognitionModel.Recognition04,\n> > >\n> returnFaceId: false,\n> > >\n> returnFaceAttributes: new FaceAttributeType[] { FaceAttributeType.HeadPose },\n> > >\n> returnFaceLandmarks: true,\n> > >\n> returnRecognitionModel: false\n> > >\n> );\n> > >\n> \\_logger.LogInformation(\"Detected {Count} faces\", response.Value.Count);\n> > >\n> return response.Value.ToList();\n> > >\n> }\n> >\n\n**Part 3: Facial Landmarks - The 27 Key Points**\n\n**3.1 Understanding Facial Landmarks**\n\n**3.2 Accessing Landmarks in Code**\n\n> >\n> public void PrintLandmarks(FaceDetectionResult face)\n> > >\n> {\n> > >\n> var landmarks = face.FaceLandmarks;\n> > >\n> if (landmarks == null)\n> > >\n> {\n> > >\n> \\_logger.LogWarning(\"No landmarks detected\");\n> > >\n> return;\n> > >\n> }\n> > >\n> // Eye landmarks\n> > >\n> Console.WriteLine($\"Left Eye Outer: ({landmarks.EyeLeftOuter.X}, {landmarks.EyeLeftOuter.Y})\");\n> > >\n> Console.WriteLine($\"Left Eye Inner: ({landmarks.EyeLeftInner.X}, {landmarks.EyeLeftInner.Y})\");\n> > >\n> Console.WriteLine($\"Left Eye Top: ({landmarks.EyeLeftTop.X}, {landmarks.EyeLeftTop.Y})\");\n> > >\n> Console.WriteLine($\"Left Eye Bottom: ({landmarks.EyeLeftBottom.X}, {landmarks.EyeLeftBottom.Y})\");\n> > >\n> // Mouth landmarks\n> > >\n> Console.WriteLine($\"Upper Lip Top: ({landmarks.UpperLipTop.X}, {landmarks.UpperLipTop.Y})\");\n> > >\n> Console.WriteLine($\"Under Lip Bottom: ({landmarks.UnderLipBottom.X}, {landmarks.UnderLipBottom.Y})\");\n> > >\n> // Nose landmarks\n> > >\n> Console.WriteLine($\"Nose Tip: ({landmarks.NoseTip.X}, {landmarks.NoseTip.Y})\");\n> > >\n> }\n> >\n\n**3.3 Visualizing All Landmarks**\n\n> >\n> public void DrawAllLandmarks(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> void DrawPoint(FaceLandmarkCoordinate point, Scalar color)\n> > >\n> {\n> > >\n> if (point != null)\n> > >\n> {\n> > >\n> Cv2.Circle(frame, new Point((int)point.X, (int)point.Y), radius: 3, color: color, thickness: -1);\n> > >\n> }\n> > >\n> }\n> > >\n> // Eyes (Green)\n> > >\n> DrawPoint(landmarks.EyeLeftOuter, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeLeftInner, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeLeftTop, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeLeftBottom, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightOuter, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightInner, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightTop, new Scalar(0, 255, 0));\n> > >\n> DrawPoint(landmarks.EyeRightBottom, new Scalar(0, 255, 0));\n> > >\n> // Eyebrows (Cyan)\n> > >\n> DrawPoint(landmarks.EyebrowLeftOuter, new Scalar(255, 255, 0));\n> > >\n> DrawPoint(landmarks.EyebrowLeftInner, new Scalar(255, 255, 0));\n> > >\n> DrawPoint(landmarks.EyebrowRightOuter, new Scalar(255, 255, 0));\n> > >\n> DrawPoint(landmarks.EyebrowRightInner, new Scalar(255, 255, 0));\n> > >\n> // Nose (Yellow)\n> > >\n> DrawPoint(landmarks.NoseTip, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseRootLeft, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseRootRight, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseLeftAlarOutTip, new Scalar(0, 255, 255));\n> > >\n> DrawPoint(landmarks.NoseRightAlarOutTip, new Scalar(0, 255, 255));\n> > >\n> // Mouth (Blue)\n> > >\n> DrawPoint(landmarks.UpperLipTop, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.UpperLipBottom, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.UnderLipTop, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.UnderLipBottom, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.MouthLeft, new Scalar(255, 0, 0));\n> > >\n> DrawPoint(landmarks.MouthRight, new Scalar(255, 0, 0));\n> > >\n> // Pupils (Red)\n> > >\n> DrawPoint(landmarks.PupilLeft, new Scalar(0, 0, 255));\n> > >\n> DrawPoint(landmarks.PupilRight, new Scalar(0, 0, 255));\n> > >\n> }\n> >\n\n**Part 4: Drawing Bounding Boxes Around Features**\n\n**4.1 Eye Bounding Boxes**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Draws rectangles around eyes using OpenCV.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public void DrawEyeBoxes(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> int boxWidth = 60;\n> > >\n> int boxHeight = 35;\n> > >\n> // Calculate Rectangles\n> > >\n> var leftEyeRect = new Rect((int)landmarks.EyeLeftOuter.X - boxWidth / 2, (int)landmarks.EyeLeftOuter.Y - boxHeight / 2, boxWidth, boxHeight);\n> > >\n> var rightEyeRect = new Rect((int)landmarks.EyeRightOuter.X - boxWidth / 2, (int)landmarks.EyeRightOuter.Y - boxHeight / 2, boxWidth, boxHeight);\n> > >\n> // Draw Rectangles (Green in BGR)\n> > >\n> Cv2.Rectangle(frame, leftEyeRect, new Scalar(0, 255, 0), 2);\n> > >\n> Cv2.Rectangle(frame, rightEyeRect, new Scalar(0, 255, 0), 2);\n> > >\n> // Add Labels\n> > >\n> Cv2.PutText(frame, \"Left Eye\", new Point(leftEyeRect.X, leftEyeRect.Y - 5), HersheyFonts.HersheySimplex, 0.4, new Scalar(0, 255, 0), 1);\n> > >\n> Cv2.PutText(frame, \"Right Eye\", new Point(rightEyeRect.X, rightEyeRect.Y - 5), HersheyFonts.HersheySimplex, 0.4, new Scalar(0, 255, 0), 1);\n> > >\n> }\n> >\n\n**4.2 Mouth Bounding Box**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Draws rectangle around mouth region.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public void DrawMouthBox(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> int boxWidth = 80;\n> > >\n> int boxHeight = 50;\n> > >\n> // Calculate center based on the vertical lip landmarks\n> > >\n> int centerX = (int)((landmarks.UpperLipTop.X + landmarks.UnderLipBottom.X) / 2);\n> > >\n> int centerY = (int)((landmarks.UpperLipTop.Y + landmarks.UnderLipBottom.Y) / 2);\n> > >\n> var mouthRect = new Rect(centerX - boxWidth / 2, centerY - boxHeight / 2, boxWidth, boxHeight);\n> > >\n> // Draw Mouth Box (Blue in BGR)\n> > >\n> Cv2.Rectangle(frame, mouthRect, new Scalar(255, 0, 0), 2);\n> > >\n> // Add Label\n> > >\n> Cv2.PutText(frame, \"Mouth\", new Point(mouthRect.X, mouthRect.Y - 5), HersheyFonts.HersheySimplex, 0.4, new Scalar(255, 0, 0), 1);\n> > >\n> }\n> >\n\n**4.3 Face Bounding Box**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Draws rectangle around entire face using the face rectangle from API.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public void DrawFaceBox(FaceDetectionResult face, Mat frame)\n> > >\n> {\n> > >\n> var faceRect = face.FaceRectangle;\n> > >\n> if (faceRect == null)\n> > >\n> {\n> > >\n> return;\n> > >\n> }\n> > >\n> var rect = new Rect(\n> > >\n> faceRect.Left,\n> > >\n> faceRect.Top,\n> > >\n> faceRect.Width,\n> > >\n> faceRect.Height\n> > >\n> );\n> > >\n> // Draw Face Bounding Box (Red in BGR)\n> > >\n> Cv2.Rectangle(frame, rect, new Scalar(0, 0, 255), 2);\n> > >\n> // Add Label with dimensions\n> > >\n> Cv2.PutText(frame, $\"Face {faceRect.Width}x{faceRect.Height}\", new Point(rect.X, rect.Y - 10), HersheyFonts.HersheySimplex, 0.5,\n> > >\n> new Scalar(0, 0, 255), 2);\n> > >\n> }\n> >\n\n**4.4 Nose Bounding Box**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Draws bounding box around nose using nose landmarks.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public void DrawNoseBox(FaceLandmarks landmarks, Mat frame)\n> > >\n> {\n> > >\n> // Calculate horizontal bounds from Alar tips\n> > >\n> int minX = (int)Math.Min(landmarks.NoseLeftAlarOutTip.X, landmarks.NoseRightAlarOutTip.X);\n> > >\n> int maxX = (int)Math.Max(landmarks.NoseLeftAlarOutTip.X, landmarks.NoseRightAlarOutTip.X);\n> > >\n> // Calculate vertical bounds from Root to Tip\n> > >\n> int minY = (int)Math.Min(landmarks.NoseRootLeft.Y, landmarks.NoseTip.Y);\n> > >\n> int maxY = (int)landmarks.NoseTip.Y;\n> > >\n> // Create Rect with a 10px padding buffer\n> > >\n> var noseRect = new Rect(\n> > >\n> minX - 10,\n> > >\n> minY - 10,\n> > >\n> (maxX - minX) + 20,\n> > >\n> (maxY - minY) + 20\n> > >\n> );\n> > >\n> // Draw Nose Box (Yellow in BGR)\n> > >\n> Cv2.Rectangle(frame, noseRect, new Scalar(0, 255, 255), 2);\n> > >\n> }\n> >\n\n**Part 5: Geometric Calculations with Landmarks**\n\n**5.1 Calculating Euclidean Distance**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Calculates distance between two landmark points.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public static double CalculateDistance(dynamic point1, dynamic point2)\n> > >\n> {\n> > >\n> double dx = point1.X - point2.X;\n> > >\n> double dy = point1.Y - point2.Y;\n> > >\n> return Math.Sqrt(dx \\* dx + dy \\* dy);\n> > >\n> }\n> >\n\n**5.2 Eye Aspect Ratio (EAR) Formula**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Calculates the Eye Aspect Ratio (EAR) to detect eye closure.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public double CalculateEAR(\n> > >\n> FaceLandmarkCoordinate top1,\n> > >\n> FaceLandmarkCoordinate top2,\n> > >\n> FaceLandmarkCoordinate bottom1,\n> > >\n> FaceLandmarkCoordinate bottom2,\n> > >\n> FaceLandmarkCoordinate inner,\n> > >\n> FaceLandmarkCoordinate outer)\n> > >\n> {\n> > >\n> // Vertical distances\n> > >\n> double v1 = CalculateDistance(top1, bottom1);\n> > >\n> double v2 = CalculateDistance(top2, bottom2);\n> > >\n> // Horizontal distance\n> > >\n> double h = CalculateDistance(inner, outer);\n> > >\n> // EAR formula: (||p2-p6|| + ||p3-p5||) / (2 \\* ||p1-p4||)\n> > >\n> return (v1 + v2) / (2.0 \\* h);\n> > >\n> }\n> >\n\n**Simplified Implementation:**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Calculates Eye Aspect Ratio (EAR) for a single eye.\n> > >\n> /// Reference: \"Real-Time Eye Blink Detection using Facial Landmarks\" (Soukupová & Čech, 2016)\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public double ComputeEAR(FaceLandmarks landmarks, bool isLeftEye)\n> > >\n> {\n> > >\n> var top = isLeftEye ? landmarks.EyeLeftTop : landmarks.EyeRightTop;\n> > >\n> var bottom = isLeftEye ? landmarks.EyeLeftBottom : landmarks.EyeRightBottom;\n> > >\n> var inner = isLeftEye ? landmarks.EyeLeftInner : landmarks.EyeRightInner;\n> > >\n> var outer = isLeftEye ? landmarks.EyeLeftOuter : landmarks.EyeRightOuter;\n> > >\n> if (top == null || bottom == null || inner == null || outer == null)\n> > >\n> {\n> > >\n> \\_logger.LogWarning(\"Missing eye landmarks\");\n> > >\n> return 1.0; // Return 1.0 (open) to prevent false positives for drowsiness\n> > >\n> }\n> > >\n> double verticalDist = CalculateDistance(top, bottom);\n> > >\n> double horizontalDist = CalculateDistance(inner, outer);\n> > >\n> // Simplified EAR for Azure 27-point model\n> > >\n> double ear = verticalDist / horizontalDist;\n> > >\n> \\_logger.LogDebug(\n> > >\n> \"EAR for {Eye}: {Value:F3}\",\n> > >\n> isLeftEye ? \"left\" : \"right\",\n> > >\n> ear\n> > >\n> );\n> > >\n> return ear;\n> > >\n> }\n> >\n\n**Usage Example:**\n\n> >\n> var leftEAR = ComputeEAR(landmarks, isLeftEye: true);\n> > >\n> var rightEAR = ComputeEAR(landmarks, isLeftEye: false);\n> > >\n> var avgEAR = (leftEAR + rightEAR) / 2.0;\n> > >\n> Console.WriteLine($\"Average EAR: {avgEAR:F3}\");\n> > >\n> // Open eyes: ~0.25-0.30\n> > >\n> // Closed eyes: ~0.10-0.15\n> >\n\n**5.3 Mouth Aspect Ratio (MAR)**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Calculates Mouth Aspect Ratio relative to face height.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public double CalculateMouthAspectRatio(FaceLandmarks landmarks, FaceRectangle faceRect)\n> > >\n> {\n> > >\n> double mouthHeight = landmarks.UnderLipBottom.Y - landmarks.UpperLipTop.Y;\n> > >\n> double mouthWidth = CalculateDistance(landmarks.MouthLeft, landmarks.MouthRight);\n> > >\n> double mouthOpenRatio = mouthHeight / faceRect.Height;\n> > >\n> double mouthWidthRatio = mouthWidth / faceRect.Width;\n> > >\n> \\_logger.LogDebug(\n> > >\n> \"Mouth - Height ratio: {HeightRatio:F3}, Width ratio: {WidthRatio:F3}\",\n> > >\n> mouthOpenRatio,\n> > >\n> mouthWidthRatio\n> > >\n> );\n> > >\n> return mouthOpenRatio;\n> > >\n> }\n> >\n\n**5.4 Inter-Eye Distance**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Calculates the distance between pupils (inter-pupillary distance).\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public double CalculateInterEyeDistance(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> return CalculateDistance(landmarks.PupilLeft, landmarks.PupilRight);\n> > >\n> }\n> > >\n> /// &lt;summary&gt;\n> > >\n> /// Calculates distance between inner eye corners.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public double CalculateInnerEyeDistance(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> return CalculateDistance(landmarks.EyeLeftInner, landmarks.EyeRightInner);\n> > >\n> }\n> >\n\n**5.5 Face Symmetry Analysis**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Analyzes facial symmetry by comparing left and right sides.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public FaceSymmetryMetrics AnalyzeFaceSymmetry(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> double centerX = landmarks.NoseTip.X;\n> > >\n> double leftEyeDistance = CalculateDistance(landmarks.EyeLeftInner, new { X = centerX, Y = landmarks.EyeLeftInner.Y });\n> > >\n> double leftMouthDistance = CalculateDistance(landmarks.MouthLeft, new { X = centerX, Y = landmarks.MouthLeft.Y });\n> > >\n> double rightEyeDistance = CalculateDistance(landmarks.EyeRightInner, new { X = centerX, Y = landmarks.EyeRightInner.Y });\n> > >\n> double rightMouthDistance = CalculateDistance(landmarks.MouthRight, new { X = centerX, Y = landmarks.MouthRight.Y });\n> > >\n> return new FaceSymmetryMetrics\n> > >\n> {\n> > >\n> EyeSymmetryRatio = leftEyeDistance / rightEyeDistance,\n> > >\n> MouthSymmetryRatio = leftMouthDistance / rightMouthDistance,\n> > >\n> IsSymmetric = Math.Abs(leftEyeDistance - rightEyeDistance) &lt; 5.0\n> > >\n> };\n> > >\n> }\n> > >\n> public class FaceSymmetryMetrics\n> > >\n> {\n> > >\n> public double EyeSymmetryRatio { get; set; }\n> > >\n> public double MouthSymmetryRatio { get; set; }\n> > >\n> public bool IsSymmetric { get; set; }\n> > >\n> }\n> >\n\n**Part 6: Head Pose Estimation**\n\n**6.1 Understanding Head Pose Angles**\n\nAzure Face API provides three Euler angles for head orientation:\n\n**6.2 Accessing Head Pose Data**\n\n> >\n> public void AnalyzeHeadPose(FaceDetectionResult face)\n> > >\n> {\n> > >\n> var headPose = face.FaceAttributes?.HeadPose;\n> > >\n> if (headPose == null)\n> > >\n> {\n> > >\n> \\_logger.LogWarning(\"Head pose not available\");\n> > >\n> return;\n> > >\n> }\n> > >\n> double yaw = headPose.Yaw;\n> > >\n> double pitch = headPose.Pitch;\n> > >\n> double roll = headPose.Roll;\n> > >\n> Console.WriteLine(\"Head Pose:\");\n> > >\n> Console.WriteLine($\" Yaw: {yaw:F2}° (Left/Right)\");\n> > >\n> Console.WriteLine($\" Pitch: {pitch:F2}° (Up/Down)\");\n> > >\n> Console.WriteLine($\" Roll: {roll:F2}° (Tilt)\");\n> > >\n> InterpretHeadPose(yaw, pitch, roll);\n> > >\n> }\n> >\n\n**6.3 Interpreting Head Pose**\n\n> >\n> public string InterpretHeadPose(double yaw, double pitch, double roll) {\n> > >\n> var directions = new List&lt;string&gt;();\n> > >\n> // Interpret Yaw (horizontal)\n> > >\n> if (Math.Abs(yaw) &lt; 10) directions.Add(\"Looking Forward\");\n> > >\n> else if (yaw &lt; -20) directions.Add($\"Turned Left ({Math.Abs(yaw):F0}°)\");\n> > >\n> else if (yaw &gt; 20) directions.Add($\"Turned Right ({yaw:F0}°)\");\n> > >\n> // Interpret Pitch (vertical)\n> > >\n> if (Math.Abs(pitch) &lt; 10) directions.Add(\"Level\");\n> > >\n> else if (pitch &lt; -15) directions.Add($\"Looking Down ({Math.Abs(pitch):F0}°)\");\n> > >\n> else if (pitch &gt; 15) directions.Add($\"Looking Up ({pitch:F0}°)\");\n> > >\n> // Interpret Roll (tilt)\n> > >\n> if (Math.Abs(roll) &gt; 15) {\n> > >\n> string side = roll &lt; 0 ? \"Left\" : \"Right\";\n> > >\n> directions.Add($\"Tilted {side} ({Math.Abs(roll):F0}°)\");\n> > >\n> }\n> > >\n> return string.Join(\", \", directions);\n> > >\n> }\n> >\n\n**6.4 Visualizing Head Pose on Frame**\n\n> >\n> /// &lt;summary&gt;\n> > >\n> /// Draws head pose information with color-coded indicators.\n> > >\n> /// &lt;/summary&gt;\n> > >\n> public void DrawHeadPoseInfo(Mat frame, HeadPose headPose, FaceRectangle faceRect)\n> > >\n> {\n> > >\n> double yaw = headPose.Yaw;\n> > >\n> double pitch = headPose.Pitch;\n> > >\n> double roll = headPose.Roll;\n> > >\n> int centerX = faceRect.Left + faceRect.Width / 2;\n> > >\n> int centerY = faceRect.Top + faceRect.Height / 2;\n> > >\n> string poseText = $\"Yaw: {yaw:F1}° Pitch: {pitch:F1}° Roll: {roll:F1}°\";\n> > >\n> Cv2.PutText(frame, poseText, new Point(faceRect.Left, faceRect.Top - 10), HersheyFonts.HersheySimplex, 0.5, new Scalar(255, 255, 255), 1);\n> > >\n> int arrowLength = 50;\n> > >\n> double yawRadians = yaw \\* Math.PI / 180.0;\n> > >\n> int arrowEndX = centerX + (int)(arrowLength \\* Math.Sin(yawRadians));\n> > >\n> Cv2.ArrowedLine(frame, new Point(centerX, centerY), new Point(arrowEndX, centerY), new Scalar(0, 255, 0), 2, tipLength: 0.3);\n> > >\n> double pitchRadians = -pitch \\* Math.PI / 180.0;\n> > >\n> int arrowPitchEndY = centerY + (int)(arrowLength \\* Math.Sin(pitchRadians));\n> > >\n> Cv2.ArrowedLine(frame, new Point(centerX, centerY), new Point(centerX, arrowPitchEndY), new Scalar(255, 0, 0), 2, tipLength: 0.3);\n> > >\n> }\n> >\n\n**6.5 Detecting Head Orientation States**\n\n> >\n> public enum HeadOrientation { Forward, Left, Right, Up, Down, TiltedLeft, TiltedRight, UpLeft, UpRight, DownLeft, DownRight }\n> > >\n> public List&lt;HeadOrientation&gt; DetectHeadOrientation(HeadPose headPose)\n> > >\n> {\n> > >\n> const double THRESHOLD = 15.0;\n> > >\n> bool lookingUp = headPose.Pitch &gt; THRESHOLD;\n> > >\n> bool lookingDown = headPose.Pitch &lt; -THRESHOLD;\n> > >\n> bool lookingLeft = headPose.Yaw &lt; -THRESHOLD;\n> > >\n> bool lookingRight = headPose.Yaw &gt; THRESHOLD;\n> > >\n> var orientations = new List&lt;HeadOrientation&gt;();\n> > >\n> if (!lookingUp && !lookingDown && !lookingLeft && !lookingRight) orientations.Add(HeadOrientation.Forward);\n> > >\n> if (lookingUp && !lookingLeft && !lookingRight) orientations.Add(HeadOrientation.Up);\n> > >\n> if (lookingDown && !lookingLeft && !lookingRight) orientations.Add(HeadOrientation.Down);\n> > >\n> if (lookingLeft && !lookingUp && !lookingDown) orientations.Add(HeadOrientation.Left);\n> > >\n> if (lookingRight && !lookingUp && !lookingDown) orientations.Add(HeadOrientation.Right);\n> > >\n> if (lookingUp && lookingLeft) orientations.Add(HeadOrientation.UpLeft);\n> > >\n> if (lookingUp && lookingRight) orientations.Add(HeadOrientation.UpRight);\n> > >\n> if (lookingDown && lookingLeft) orientations.Add(HeadOrientation.DownLeft);\n> > >\n> if (lookingDown && lookingRight) orientations.Add(HeadOrientation.DownRight);\n> > >\n> return orientations;\n> > >\n> }\n> >\n\n**Part 7: Real-Time Video Processing**\n\n**7.1 Setting Up Video Capture**\n\n> >\n> using OpenCvSharp;\n> > >\n> public class RealTimeFaceAnalyzer : IDisposable\n> > >\n> {\n> > >\n> private VideoCapture? \\_capture;\n> > >\n> private Mat? \\_frame;\n> > >\n> private readonly FaceClient \\_faceClient;\n> > >\n> private bool \\_isRunning;\n> > >\n> public async Task StartAsync()\n> > >\n> {\n> > >\n> \\_capture = new VideoCapture(0);\n> > >\n> \\_frame = new Mat();\n> > >\n> \\_isRunning = true;\n> > >\n> await Task.Run(() =&gt; ProcessVideoLoop());\n> > >\n> }\n> > >\n> private async Task ProcessVideoLoop()\n> > >\n> {\n> > >\n> while (\\_isRunning)\n> > >\n> {\n> > >\n> if (\\_capture == null || !\\_capture.IsOpened()) break;\n> > >\n> \\_capture.Read(\\_frame);\n> > >\n> if (\\_frame == null || \\_frame.Empty())\n> > >\n> {\n> > >\n> await Task.Delay(1); // Minimal delay to prevent CPU spiking\n> > >\n> continue;\n> > >\n> }\n> > >\n> Cv2.Resize(\\_frame, \\_frame, new Size(640, 480));\n> > >\n> // Ensure we don't await indefinitely in the rendering loop\n> > >\n> \\_ = ProcessFrameAsync(\\_frame.Clone());\n> > >\n> Cv2.ImShow(\"Face Analysis\", \\_frame);\n> > >\n> if (Cv2.WaitKey(30) == 'q') break;\n> > >\n> }\n> > >\n> Dispose();\n> > >\n> }\n> > >\n> private async Task ProcessFrameAsync(Mat frame)\n> > >\n> {\n> > >\n> // This is where your DrawFaceBox, DrawAllLandmarks, and EAR logic will sit.\n> > >\n> // Remember to use try-catch here to prevent API errors from crashing the loop.\n> > >\n> }\n> > >\n> public void Dispose()\n> > >\n> {\n> > >\n> \\_isRunning = false;\n> > >\n> \\_capture?.Dispose();\n> > >\n> \\_frame?.Dispose();\n> > >\n> Cv2.DestroyAllWindows();\n> > >\n> }\n> > >\n> }\n> >\n\n**7.2 Optimizing API Calls**\n\n**Problem:** Calling Azure Face API on every frame (30 fps) is expensive and slow.\n\n**Solution:** Call API once per second, cache results for 30 frames.\n\n> >\n> private List&lt;FaceDetectionResult&gt; \\_cachedFaces = new();\n> > >\n> private DateTime \\_lastDetectionTime = DateTime.MinValue;\n> > >\n> private readonly object \\_cacheLock = new();\n> > >\n> private async Task ProcessFrameAsync(Mat frame)\n> > >\n> {\n> > >\n> if ((DateTime.Now - \\_lastDetectionTime).TotalSeconds &gt;= 1.0)\n> > >\n> {\n> > >\n> \\_lastDetectionTime = DateTime.Now;\n> > >\n> byte[] imageBytes;\n> > >\n> Cv2.ImEncode(\".jpg\", frame, out imageBytes);\n> > >\n> var faces = await DetectFacesAsync(imageBytes);\n> > >\n> lock (\\_cacheLock)\n> > >\n> {\n> > >\n> \\_cachedFaces = faces;\n> > >\n> }\n> > >\n> }\n> > >\n> List&lt;FaceDetectionResult&gt; facesToProcess;\n> > >\n> lock (\\_cacheLock)\n> > >\n> {\n> > >\n> facesToProcess = \\_cachedFaces.ToList();\n> > >\n> }\n> > >\n> foreach (var face in facesToProcess)\n> > >\n> {\n> > >\n> DrawFaceAnnotations(face, frame);\n> > >\n> }\n> > >\n> }\n> >\n\n**Performance Improvement:**\n\n- **30x fewer API calls** (1/sec instead of 30/sec)\n- **~$0.02/hour** instead of **~$0.60/hour**\n- **Smooth 30 fps** rendering\n- **&lt; 100ms latency** for visual updates\n\n**7.3 Drawing Complete Face Annotations**\n\n> >\n> private void DrawFaceAnnotations(FaceDetectionResult face, Mat frame)\n> > >\n> {\n> > >\n> DrawFaceBox(face, frame);\n> > >\n> if (face.FaceLandmarks != null)\n> > >\n> {\n> > >\n> DrawAllLandmarks(face.FaceLandmarks, frame);\n> > >\n> DrawEyeBoxes(face.FaceLandmarks, frame);\n> > >\n> DrawMouthBox(face.FaceLandmarks, frame);\n> > >\n> DrawNoseBox(face.FaceLandmarks, frame);\n> > >\n> double leftEAR = ComputeEAR(face.FaceLandmarks, isLeftEye: true);\n> > >\n> double rightEAR = ComputeEAR(face.FaceLandmarks, isLeftEye: false);\n> > >\n> double avgEAR = (leftEAR + rightEAR) / 2.0;\n> > >\n> Cv2.PutText(frame, $\"EAR: {avgEAR:F3}\", new Point(10, 30), HersheyFonts.HersheySimplex, 0.6, new Scalar(0, 255, 0), 2);\n> > >\n> }\n> > >\n> if (face.FaceAttributes?.HeadPose != null)\n> > >\n> {\n> > >\n> DrawHeadPoseInfo(frame, face.FaceAttributes.HeadPose, face.FaceRectangle);\n> > >\n> string orientation = InterpretHeadPose(face.FaceAttributes.HeadPose.Yaw, face.FaceAttributes.HeadPose.Pitch, face.FaceAttributes.HeadPose.Roll);\n> > >\n> Cv2.PutText(frame, orientation, new Point(10, 60), HersheyFonts.HersheySimplex, 0.6, new Scalar(255, 255, 0), 2);\n> > >\n> }\n> > >\n> }\n> >\n\n**Part 8: Advanced Features and Use Cases**\n\n**8.1 Face Tracking Across Frames**\n\n> >\n> public class FaceTracker\n> > >\n> {\n> > >\n> private class TrackedFace\n> > >\n> {\n> > >\n> public FaceRectangle Rectangle { get; set; }\n> > >\n> public DateTime LastSeen { get; set; }\n> > >\n> public int TrackId { get; set; }\n> > >\n> }\n> > >\n> private List&lt;TrackedFace&gt; \\_trackedFaces = new();\n> > >\n> private int \\_nextTrackId = 1;\n> > >\n> public int TrackFace(FaceRectangle newFace)\n> > >\n> {\n> > >\n> const int MATCH\\_THRESHOLD = 50;\n> > >\n> var match = \\_trackedFaces.FirstOrDefault(tf =&gt; {\n> > >\n> double distance = Math.Sqrt(Math.Pow(tf.Rectangle.Left - newFace.Left, 2) + Math.Pow(tf.Rectangle.Top - newFace.Top, 2));\n> > >\n> return distance &lt; MATCH\\_THRESHOLD;\n> > >\n> });\n> > >\n> if (match != null)\n> > >\n> {\n> > >\n> match.Rectangle = newFace;\n> > >\n> match.LastSeen = DateTime.Now;\n> > >\n> return match.TrackId;\n> > >\n> }\n> > >\n> var newTrack = new TrackedFace { Rectangle = newFace, LastSeen = DateTime.Now, TrackId = \\_nextTrackId++ };\n> > >\n> \\_trackedFaces.Add(newTrack);\n> > >\n> return newTrack.TrackId;\n> > >\n> }\n> > >\n> public void RemoveOldTracks(TimeSpan maxAge)\n> > >\n> {\n> > >\n> \\_trackedFaces.RemoveAll(tf =&gt; DateTime.Now - tf.LastSeen &gt; maxAge);\n> > >\n> }\n> > >\n> }\n> >\n\n**8.2 Multi-Face Detection and Analysis**\n\n> >\n> public async Task&lt;FaceAnalysisReport&gt; AnalyzeMultipleFacesAsync(byte[] imageBytes)\n> > >\n> {\n> > >\n> var faces = await DetectFacesAsync(imageBytes);\n> > >\n> var report = new FaceAnalysisReport { TotalFacesDetected = faces.Count, Timestamp = DateTime.Now, Faces = new List&lt;SingleFaceAnalysis&gt;() };\n> > >\n> for (int i = 0; i &lt; faces.Count; i++)\n> > >\n> {\n> > >\n> var face = faces[i];\n> > >\n> var analysis = new SingleFaceAnalysis { FaceIndex = i, FaceLocation = face.FaceRectangle, FaceSize = face.FaceRectangle.Width \\* face.FaceRectangle.Height };\n> > >\n> if (face.FaceLandmarks != null)\n> > >\n> {\n> > >\n> analysis.LeftEyeEAR = ComputeEAR(face.FaceLandmarks, true);\n> > >\n> analysis.RightEyeEAR = ComputeEAR(face.FaceLandmarks, false);\n> > >\n> analysis.InterPupillaryDistance = CalculateInterEyeDistance(face.FaceLandmarks);\n> > >\n> }\n> > >\n> if (face.FaceAttributes?.HeadPose != null)\n> > >\n> {\n> > >\n> analysis.HeadYaw = face.FaceAttributes.HeadPose.Yaw;\n> > >\n> analysis.HeadPitch = face.FaceAttributes.HeadPose.Pitch;\n> > >\n> analysis.HeadRoll = face.FaceAttributes.HeadPose.Roll;\n> > >\n> }\n> > >\n> report.Faces.Add(analysis);\n> > >\n> }\n> > >\n> report.Faces = report.Faces.OrderByDescending(f =&gt; f.FaceSize).ToList();\n> > >\n> return report;\n> > >\n> }\n> > >\n> public class FaceAnalysisReport\n> > >\n> {\n> > >\n> public int TotalFacesDetected { get; set; }\n> > >\n> public DateTime Timestamp { get; set; }\n> > >\n> public List&lt;SingleFaceAnalysis&gt; Faces { get; set; }\n> > >\n> }\n> > >\n> public class SingleFaceAnalysis\n> > >\n> {\n> > >\n> public int FaceIndex { get; set; }\n> > >\n> public FaceRectangle FaceLocation { get; set; }\n> > >\n> public int FaceSize { get; set; }\n> > >\n> public double LeftEyeEAR { get; set; }\n> > >\n> public double RightEyeEAR { get; set; }\n> > >\n> public double InterPupillaryDistance { get; set; }\n> > >\n> public double HeadYaw { get; set; }\n> > >\n> public double HeadPitch { get; set; }\n> > >\n> public double HeadRoll { get; set; }\n> > >\n> }\n> >\n\n**8.3 Exporting Landmark Data to JSON**\n\n> >\n> using System.Text.Json;\n> > >\n> public string ExportLandmarksToJson(FaceDetectionResult face)\n> > >\n> {\n> > >\n> var landmarks = face.FaceLandmarks;\n> > >\n> var landmarkData = new\n> > >\n> {\n> > >\n> Face = new { Rectangle = new { face.FaceRectangle.Left, face.FaceRectangle.Top, face.FaceRectangle.Width, face.FaceRectangle.Height } },\n> > >\n> Eyes = new\n> > >\n> {\n> > >\n> Left = new { Outer = new { landmarks.EyeLeftOuter.X, landmarks.EyeLeftOuter.Y }, Inner = new { landmarks.EyeLeftInner.X, landmarks.EyeLeftInner.Y }, Top = new { landmarks.EyeLeftTop.X, landmarks.EyeLeftTop.Y }, Bottom = new { landmarks.EyeLeftBottom.X, landmarks.EyeLeftBottom.Y } },\n> > >\n> Right = new { Outer = new { landmarks.EyeRightOuter.X, landmarks.EyeRightOuter.Y }, Inner = new { landmarks.EyeRightInner.X, landmarks.EyeRightInner.Y }, Top = new { landmarks.EyeRightTop.X, landmarks.EyeRightTop.Y }, Bottom = new { landmarks.EyeRightBottom.X, landmarks.EyeRightBottom.Y } }\n> > >\n> },\n> > >\n> Mouth = new { UpperLipTop = new { landmarks.UpperLipTop.X, landmarks.UpperLipTop.Y }, UnderLipBottom = new { landmarks.UnderLipBottom.X, landmarks.UnderLipBottom.Y }, Left = new { landmarks.MouthLeft.X, landmarks.MouthLeft.Y }, Right = new { landmarks.MouthRight.X, landmarks.MouthRight.Y } },\n> > >\n> Nose = new { Tip = new { landmarks.NoseTip.X, landmarks.NoseTip.Y }, RootLeft = new { landmarks.NoseRootLeft.X, landmarks.NoseRootLeft.Y }, RootRight = new { landmarks.NoseRootRight.X, landmarks.NoseRootRight.Y } },\n> > >\n> HeadPose = face.FaceAttributes?.HeadPose != null ? new { face.FaceAttributes.HeadPose.Yaw, face.FaceAttributes.HeadPose.Pitch, face.FaceAttributes.HeadPose.Roll } : null\n> > >\n> };\n> > >\n> return JsonSerializer.Serialize(landmarkData, new JsonSerializerOptions { WriteIndented = true });\n> > >\n> }\n> >\n\n**Part 9: Practical Applications**\n\n**9.1 Gaze Direction Estimation**\n\n> >\n> public enum GazeDirection { Center, Left, Right, Up, Down, UpLeft, UpRight, DownLeft, DownRight }\n> > >\n> public GazeDirection EstimateGazeDirection(HeadPose headPose)\n> > >\n> {\n> > >\n> const double THRESHOLD = 15.0;\n> > >\n> bool lookingUp = headPose.Pitch &gt; THRESHOLD;\n> > >\n> bool lookingDown = headPose.Pitch &lt; -THRESHOLD;\n> > >\n> bool lookingLeft = headPose.Yaw &lt; -THRESHOLD;\n> > >\n> bool lookingRight = headPose.Yaw &gt; THRESHOLD;\n> > >\n> if (lookingUp && lookingLeft) return GazeDirection.UpLeft;\n> > >\n> if (lookingUp && lookingRight) return GazeDirection.UpRight;\n> > >\n> if (lookingDown && lookingLeft) return GazeDirection.DownLeft;\n> > >\n> if (lookingDown && lookingRight) return GazeDirection.DownRight;\n> > >\n> if (lookingUp) return GazeDirection.Up;\n> > >\n> if (lookingDown) return GazeDirection.Down;\n> > >\n> if (lookingLeft) return GazeDirection.Left;\n> > >\n> if (lookingRight) return GazeDirection.Right;\n> > >\n> return GazeDirection.Center;\n> > >\n> }\n> >\n\n**9.2 Expression Analysis Using Landmarks**\n\n> >\n> public class ExpressionAnalyzer\n> > >\n> {\n> > >\n> public bool IsSmiling(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> double mouthCenterY = (landmarks.UpperLipTop.Y + landmarks.UnderLipBottom.Y) / 2;\n> > >\n> double leftCornerY = landmarks.MouthLeft.Y;\n> > >\n> double rightCornerY = landmarks.MouthRight.Y;\n> > >\n> return leftCornerY &lt; mouthCenterY && rightCornerY &lt; mouthCenterY;\n> > >\n> }\n> > >\n> public bool IsMouthOpen(FaceLandmarks landmarks, FaceRectangle faceRect)\n> > >\n> {\n> > >\n> double mouthHeight = landmarks.UnderLipBottom.Y - landmarks.UpperLipTop.Y;\n> > >\n> double mouthOpenRatio = mouthHeight / faceRect.Height;\n> > >\n> return mouthOpenRatio &gt; 0.08; // 8% of face height\n> > >\n> }\n> > >\n> public bool AreEyesClosed(FaceLandmarks landmarks)\n> > >\n> {\n> > >\n> double leftEAR = ComputeEAR(landmarks, isLeftEye: true);\n> > >\n> double rightEAR = ComputeEAR(landmarks, isLeftEye: false);\n> > >\n> double avgEAR = (leftEAR + rightEAR) / 2.0;\n> > >\n> return avgEAR &lt; 0.18; // Threshold for closed eyes\n> > >\n> }\n> > >\n> }\n> >\n\n**9.3 Face Orientation for AR/VR Applications**\n\n> >\n> public class FaceOrientationFor3D\n> > >\n> {\n> > >\n> public (Vector3 forward, Vector3 up, Vector3 right) GetFaceOrientation(HeadPose headPose)\n> > >\n> {\n> > >\n> double yawRad = headPose.Yaw \\* Math.PI / 180.0;\n> > >\n> double pitchRad = headPose.Pitch \\* Math.PI / 180.0;\n> > >\n> double rollRad = headPose.Roll \\* Math.PI / 180.0;\n> > >\n> var forward = new Vector3((float)(Math.Sin(yawRad) \\* Math.Cos(pitchRad)), (float)(-Math.Sin(pitchRad)), (float)(Math.Cos(yawRad) \\* Math.Cos(pitchRad)));\n> > >\n> var up = new Vector3((float)(Math.Sin(yawRad) \\* Math.Sin(pitchRad) \\* Math.Cos(rollRad) - Math.Cos(yawRad) \\* Math.Sin(rollRad)), (float)(Math.Cos(pitchRad) \\* Math.Cos(rollRad)), (float)(Math.Cos(yawRad) \\* Math.Sin(pitchRad) \\* Math.Cos(rollRad) + Math.Sin(yawRad) \\* Math.Sin(rollRad)));\n> > >\n> var right = Vector3.Cross(up, forward);\n> > >\n> return (forward, up, right);\n> > >\n> }\n> > >\n> }\n> > >\n> public struct Vector3\n> > >\n> {\n> > >\n> public float X, Y, Z;\n> > >\n> public Vector3(float x, float y, float z) { X = x; Y = y; Z = z; }\n> > >\n> public static Vector3 Cross(Vector3 a, Vector3 b) =&gt; new Vector3(a.Y \\* b.Z - a.Z \\* b.Y, a.Z \\* b.X - a.X \\* b.Z, a.X \\* b.Y - a.Y \\* b.X);\n> > >\n> }\n> >\n\n**Conclusion**\n\nThis technical guide has explored the capabilities of Azure Face API for facial analysis in C#. We've covered:\n\n**Key Capabilities Demonstrated**\n\n- Facial Landmark Detection - Accessing 27 precise points on the face\n- Head Pose Estimation - Tracking yaw, pitch, and roll angles\n- Geometric Calculations - Computing EAR, distances, and ratios\n- Visual Annotations - Drawing bounding boxes with OpenCV\n- Real-Time Processing - Optimized video stream analysis\n\n**Technical Achievements**\n\n**Computer Vision Math:**\n\n- Euclidean distance calculations\n- Eye Aspect Ratio (EAR) formula\n- Mouth aspect ratio measurements\n- Face symmetry analysis\n\n**OpenCV Integration:**\n\n- Drawing bounding boxes and landmarks\n- Color-coded feature highlighting\n- Real-time annotation overlays\n- Video capture and processing\n\n**Practical Applications**\n\nThis technology enables:\n\n- 👁️ **Gaze tracking** for UI/UX studies\n- 🎮 **Head-controlled** game interfaces\n- 📸 **Auto-focus** camera systems\n- 🎭 **Expression analysis** for feedback\n- 🥽 **AR/VR** avatar control\n- 📊 **Attention analytics** for presentations\n- ♿ **Accessibility features** for disabled users\n\n**Performance Metrics**\n\n- **Detection Accuracy**: 95%+ for frontal faces\n- **Landmark Precision**: ±2-3 pixels\n- **Processing Latency**: 200-500ms per API call\n- **Frame Rate**: 30 fps with caching\n\n**Further Exploration**\n\n**Advanced Topics to Explore:**\n\n1. **Face Recognition** - Identify individuals\n2. **Age/Gender Detection** - Demographic analysis\n3. **Emotion Detection** - Facial expression classification\n4. **Face Verification** - 1:1 identity confirmation\n5. **Similar Face Search** - 1:N face matching\n6. **Face Grouping** - Cluster similar faces\n\n## Call to Action\n\n📌 Explore these resources to get started:\n\n**Official Documentation**\n\n- [Azure Face API Documentation](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-identity)\n- [Face API REST Reference](https://learn.microsoft.com/en-us/rest/api/face/)\n- [Azure Face SDK for .NET](https://www.nuget.org/packages/Azure.AI.Vision.Face)\n\n**Related Libraries**\n\n- [OpenCVSharp](https://github.com/shimat/opencvsharp) - OpenCV wrapper for .NET\n- [System.Drawing](https://docs.microsoft.com/en-us/dotnet/api/system.drawing) - .NET image processing\n\n**Source Code**\n\n- **GitHub Repository: [ravimodi_microsoft/SmartDriver](https://github.com/ravimodi_microsoft/SmartDriver)**\n- **Sample Code**: Included in this article\n\nUpdated Feb 16, 2026\n\nVersion 1.0\n\n[.net](/tag/.net?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai studio](/tag/ai%20studio?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure](/tag/azure?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure ai foundry](/tag/azure%20ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[developer](/tag/developer?nodeId=board%3AAzureDevCommunityBlog)\n\n[genai](/tag/genai?nodeId=board%3AAzureDevCommunityBlog)\n\n[tips and tricks](/tag/tips%20and%20tricks?nodeId=board%3AAzureDevCommunityBlog)\n\n[visual studio](/tag/visual%20studio?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[ravimodi&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-12.svg?image-dimensions=50x50)](/users/ravimodi/3255162) [ravimodi](/users/ravimodi/3255162) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined November 04, 2025\n\n[View Profile](/users/ravimodi/3255162)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity"
}
