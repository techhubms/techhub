{
  "Author": "pauledwards",
  "Description": "When running containerized workloads on HPC clusters, one of the first problems you hit is getting container images onto the nodes quickly and repeatably. A .sqsh is a Squashfs image (commonly used by container runtimes on HPC). In some environments you *can* run a Squashfs image directly from shared storage, but at scale that often turns the shared filesystem into a hot spot.\n\nCopying the image to local NVMe keeps startup time predictable and avoids hundreds of nodes hammering the same source during job launch.\n\nIn this post, I'll introduce [mpi-stage](https://github.com/edwardsp/mpi-stage), a lightweight tool that uses MPI broadcasts to distribute large files across cluster nodes at speeds that can saturate the backend network.\n\n## The Problem: Staging Files at Scale\n\nOn an [Azure CycleCloud Workspace for Slurm](https://github.com/Azure/ai-infrastructure-on-azure/tree/main/infrastructure_references/azure_cyclecloud_workspace_for_slurm) cluster with [GB300](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/nd-gb300-v6-series?tabs=sizebasic) GPU nodes, I needed to stage a large Squashfs container image from shared storage onto each node's local NVMe storage before launching training jobs.\n\nAt small scale you can often get away with ad-hoc copies, but once hundreds of nodes are all trying to read the same source file, the shared source filesystem quickly becomes the bottleneck.\n\nI tried several approaches:\n\n### Attempt 1: Slurm's sbcast\n\nSlurm's built-in [sbcast](https://slurm.schedmd.com/sbcast.html) seemed like the natural choice. In my quick testing it was slower than I wanted, and the overwrite/skip-existing behavior didn't match the \"fast no-op if already present\" workflow I was after. I didn't spend much time exploring all the configuration options before moving on.\n\n### Attempt 2: Shell Script Fan-Out\n\nI wrote a shell script using a tree-based fan-out approach: copy to N nodes, then each of those copies to N more, and so on. This worked and scaled reasonably, but had some drawbacks:\n\n1. **Multiple stages**: The script required orchestrating multiple rounds of copy commands, adding complexity\n2. **Source filesystem stress**: Even with fan-out, the initial copies still hit the source filesystem simultaneously — a fan-out of 4 meant 4 nodes competing for source bandwidth\n3. **Frontend network**: Copies went over the Ethernet network by default — I could have configured IPoIB, but that added more setup\n\n### The Solution: MPI Broadcasts\n\nThe key insight was that MPI's broadcast primitive (MPI\\_Bcast) is specifically optimized for one-to-many data distribution. Modern MPI implementations like HPC-X use tree-based algorithms that efficiently utilize the high-bandwidth, low-latency InfiniBand network.\n\nWith [mpi-stage](https://github.com/edwardsp/mpi-stage):\n\n- **Single source read**: Only one node reads from the source filesystem\n- **Backend network utilization**: Data flows over InfiniBand using optimized MPI collectives\n- **Intelligent skipping**: Nodes that already have the file (verified by size or checksum) skip the copy entirely\n\nCombined, this keeps the shared source (NFS, Lustre, blobfuse, etc.) from being hammered by many concurrent readers while still taking full advantage of the backend fabric.\n\n## How It Works\n\n[mpi-stage](https://github.com/edwardsp/mpi-stage) is designed around a simple workflow:\n\n![]()\n\nThe source node reads the file in chunks and streams each chunk via MPI\\_Bcast. Destination nodes write each chunk to local storage immediately upon receipt. This streaming approach means the entire file never needs to fit in memory — only a small buffer is required.\n\n### Key Features\n\n1. **Pre-copy Validation**\n\nBefore any data is transferred, each node checks if the destination file already exists and matches the source. You can choose between:\n\n- **Size check** (default): Fast comparison of file sizes—sufficient for most use cases\n- **Checksum**: Stronger validation, but requires reading the full file and is therefore slower\n\nIf all nodes already have the correct file, [mpi-stage](https://github.com/edwardsp/mpi-stage) completes in milliseconds with no data transfer.\n\n1. **Double-Buffered Transfers**\n\nThe implementation uses double-buffered, chunked transfers to overlap network communication with disk I/O. While one buffer is being broadcast, the next chunk is being read from the source.\n\n1. **Post-copy Validation**\n\nOptionally verify that all nodes received the file correctly after the copy completes.\n\n1. **Single-Writer Per Node**\n\nThe tool enforces one MPI rank per node to prevent filesystem contention and ensure predictable performance.\n\n## Real-World Performance\n\nIn one run using 156 GPU nodes, distributing a container image achieved approximately **3 GB/s effective distribution rate (file\\_size/time)**, completing in just over 5 seconds:\n\n- [0] Copy required: yes\n[0] Starting copy phase (source writes: yes) [0] Copy complete, Bandwidth: 3007.14 MB/s [0] Post-validation complete [0] Timings (s): Topology check: 5.22463 Source metadata: -0.00803746 Pre-validation: -0.0046786 Copy phase: -5.21189 Post-validation: -2.2944e-05 Total time: 5.2563\n\nBecause every node writes the file to its own local NVMe, the *cumulative* write rate across the cluster is roughly this number times the node count: ~3 GB/s × 156 ≈ **~468 GB/s of total local writes**.\n\n## Workflow: Container Image Distribution\n\nThe primary use case is distributing Squashfs images to local NVMe before launching containerized workloads. Run [mpi-stage](https://github.com/edwardsp/mpi-stage) as a job step before your main application:\n- #!/bin/bash\n#SBATCH --job-name=my-training-job #SBATCH --ntasks-per-node=1 #SBATCH --exclusive\n\n# Stage the container image\nsrun --mpi=pmix ./mpi\\_stage \\ --source /shared/images/pytorch.sqsh \\ --dest /nvme/images/pytorch.sqsh \\ --pre-validate size \\ --verbose\n\n# Run the actual job (from local NVMe - much faster!)\nsrun --container-image=/nvme/images/pytorch.sqsh ...\n\n[mpi-stage](https://github.com/edwardsp/mpi-stage) will create the destination directory if it doesn't exist.\n\nIf your container runtime supports running the image directly from shared storage, you may not strictly need this step—but staging to local NVMe tends to be faster and more predictable at large scale.\n\nBecause of the pre-validation, you can include this step in every job script without penalty—if the image is already present, it completes in milliseconds.\n\n## Getting Started\n- git clone https://github.com/edwardsp/mpi-stage.git\ncd mpi-stage make\n\nFor detailed usage and options, see the [README](https://github.com/edwardsp/mpi-stage).\n\n## Summary\n\n[mpi-stage](https://github.com/edwardsp/mpi-stage) started as a solution to a very specific problem—staging large container images efficiently across a large GPU cluster—but the same pattern may be useful in other scenarios where many nodes need the same large file.\n\nBy using MPI broadcasts, only a single node reads from the source filesystem, while data is distributed over the backend network using optimized collectives. In practice, this can significantly reduce load on shared filesystems and cloud-backed mounts, such as Azure Blob Storage accessed via [blobfuse2](https://github.com/Azure/azure-storage-fuse), where hundreds of concurrent readers can otherwise become a bottleneck.\n\nWhile container images were the initial focus, this approach could also be applied to staging training datasets, distributing model checkpoints or pretrained weights, or copying large binaries to local NVMe before a job starts. Anywhere that a “many nodes, same file” pattern exists is a potential fit.\n\nIf you're running large-scale containerized workloads on Azure HPC infrastructure, give it a try. If you use [mpi-stage](https://github.com/edwardsp/mpi-stage) in other workflows, I'd love to hear what worked (and what didn't). Feedback and contributions are welcome.\n\n*Have questions or feedback? Leave a comment below or open an issue on [GitHub](https://github.com/edwardsp/mpi-stage).*",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "OutputDir": "_community",
  "Title": "mpi-stage: High-Performance File Distribution for HPC Clusters",
  "FeedName": "Microsoft Tech Community",
  "EnhancedContent": "## Efficiently staging large files across hundreds of nodes using MPI broadcasts\n\nWhen running containerized workloads on HPC clusters, one of the first problems you hit is getting container images onto the nodes quickly and repeatably. A .sqsh is a Squashfs image (commonly used by container runtimes on HPC). In some environments you *can* run a Squashfs image directly from shared storage, but at scale that often turns the shared filesystem into a hot spot.\n\nCopying the image to local NVMe keeps startup time predictable and avoids hundreds of nodes hammering the same source during job launch.\n\nIn this post, I'll introduce [mpi-stage](https://github.com/edwardsp/mpi-stage), a lightweight tool that uses MPI broadcasts to distribute large files across cluster nodes at speeds that can saturate the backend network.\n\n## The Problem: Staging Files at Scale\n\nOn an [Azure CycleCloud Workspace for Slurm](https://github.com/Azure/ai-infrastructure-on-azure/tree/main/infrastructure_references/azure_cyclecloud_workspace_for_slurm) cluster with [GB300](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/nd-gb300-v6-series?tabs=sizebasic) GPU nodes, I needed to stage a large Squashfs container image from shared storage onto each node's local NVMe storage before launching training jobs.\n\nAt small scale you can often get away with ad-hoc copies, but once hundreds of nodes are all trying to read the same source file, the shared source filesystem quickly becomes the bottleneck.\n\nI tried several approaches:\n\n### Attempt 1: Slurm's sbcast\n\nSlurm's built-in [sbcast](https://slurm.schedmd.com/sbcast.html) seemed like the natural choice. In my quick testing it was slower than I wanted, and the overwrite/skip-existing behavior didn't match the \"fast no-op if already present\" workflow I was after. I didn't spend much time exploring all the configuration options before moving on.\n\n### Attempt 2: Shell Script Fan-Out\n\nI wrote a shell script using a tree-based fan-out approach: copy to N nodes, then each of those copies to N more, and so on. This worked and scaled reasonably, but had some drawbacks:\n\n1. **Multiple stages**: The script required orchestrating multiple rounds of copy commands, adding complexity\n2. **Source filesystem stress**: Even with fan-out, the initial copies still hit the source filesystem simultaneously — a fan-out of 4 meant 4 nodes competing for source bandwidth\n3. **Frontend network**: Copies went over the Ethernet network by default — I could have configured IPoIB, but that added more setup\n\n### The Solution: MPI Broadcasts\n\nThe key insight was that MPI's broadcast primitive (MPI\\_Bcast) is specifically optimized for one-to-many data distribution. Modern MPI implementations like HPC-X use tree-based algorithms that efficiently utilize the high-bandwidth, low-latency InfiniBand network.\n\nWith [mpi-stage](https://github.com/edwardsp/mpi-stage):\n\n- **Single source read**: Only one node reads from the source filesystem\n- **Backend network utilization**: Data flows over InfiniBand using optimized MPI collectives\n- **Intelligent skipping**: Nodes that already have the file (verified by size or checksum) skip the copy entirely\n\nCombined, this keeps the shared source (NFS, Lustre, blobfuse, etc.) from being hammered by many concurrent readers while still taking full advantage of the backend fabric.\n\n## How It Works\n\n[mpi-stage](https://github.com/edwardsp/mpi-stage) is designed around a simple workflow:\n\nThe source node reads the file in chunks and streams each chunk via MPI\\_Bcast. Destination nodes write each chunk to local storage immediately upon receipt. This streaming approach means the entire file never needs to fit in memory — only a small buffer is required.\n\n### Key Features\n\n1. **Pre-copy Validation**\n\nBefore any data is transferred, each node checks if the destination file already exists and matches the source. You can choose between:\n\n- **Size check** (default): Fast comparison of file sizes—sufficient for most use cases\n- **Checksum**: Stronger validation, but requires reading the full file and is therefore slower\n\nIf all nodes already have the correct file, [mpi-stage](https://github.com/edwardsp/mpi-stage) completes in milliseconds with no data transfer.\n\n1. **Double-Buffered Transfers**\n\nThe implementation uses double-buffered, chunked transfers to overlap network communication with disk I/O. While one buffer is being broadcast, the next chunk is being read from the source.\n\n1. **Post-copy Validation**\n\nOptionally verify that all nodes received the file correctly after the copy completes.\n\n1. **Single-Writer Per Node**\n\nThe tool enforces one MPI rank per node to prevent filesystem contention and ensure predictable performance.\n\n## Real-World Performance\n\nIn one run using 156 GPU nodes, distributing a container image achieved approximately **3 GB/s effective distribution rate (file\\_size/time)**, completing in just over 5 seconds:\n\n``` [0] Copy required: yes [0] Starting copy phase (source writes: yes) [0] Copy complete, Bandwidth: 3007.14 MB/s [0] Post-validation complete [0] Timings (s): Topology check: 5.22463 Source metadata: -0.00803746 Pre-validation: -0.0046786 Copy phase: -5.21189 Post-validation: -2.2944e-05 Total time: 5.2563 ```\n\nBecause every node writes the file to its own local NVMe, the *cumulative* write rate across the cluster is roughly this number times the node count: ~3 GB/s × 156 ≈ **~468 GB/s of total local writes**.\n\n## Workflow: Container Image Distribution\n\nThe primary use case is distributing Squashfs images to local NVMe before launching containerized workloads. Run [mpi-stage](https://github.com/edwardsp/mpi-stage) as a job step before your main application:\n\n``` #!/bin/bash #SBATCH --job-name=my-training-job #SBATCH --ntasks-per-node=1 #SBATCH --exclusive\n\n# Stage the container image\nsrun --mpi=pmix ./mpi_stage \\ --source /shared/images/pytorch.sqsh \\ --dest /nvme/images/pytorch.sqsh \\ --pre-validate size \\ --verbose\n\n# Run the actual job (from local NVMe - much faster!)\nsrun --container-image=/nvme/images/pytorch.sqsh ... ```\n\n[mpi-stage](https://github.com/edwardsp/mpi-stage) will create the destination directory if it doesn't exist.\n\nIf your container runtime supports running the image directly from shared storage, you may not strictly need this step—but staging to local NVMe tends to be faster and more predictable at large scale.\n\nBecause of the pre-validation, you can include this step in every job script without penalty—if the image is already present, it completes in milliseconds.\n\n## Getting Started\n\n``` git clone https://github.com/edwardsp/mpi-stage.git cd mpi-stage make ```\n\nFor detailed usage and options, see the [README](https://github.com/edwardsp/mpi-stage).\n\n## Summary\n\n[mpi-stage](https://github.com/edwardsp/mpi-stage) started as a solution to a very specific problem—staging large container images efficiently across a large GPU cluster—but the same pattern may be useful in other scenarios where many nodes need the same large file.\n\nBy using MPI broadcasts, only a single node reads from the source filesystem, while data is distributed over the backend network using optimized collectives. In practice, this can significantly reduce load on shared filesystems and cloud-backed mounts, such as Azure Blob Storage accessed via [blobfuse2](https://github.com/Azure/azure-storage-fuse), where hundreds of concurrent readers can otherwise become a bottleneck.\n\nWhile container images were the initial focus, this approach could also be applied to staging training datasets, distributing model checkpoints or pretrained weights, or copying large binaries to local NVMe before a job starts. Anywhere that a “many nodes, same file” pattern exists is a potential fit.\n\nIf you're running large-scale containerized workloads on Azure HPC infrastructure, give it a try. If you use [mpi-stage](https://github.com/edwardsp/mpi-stage) in other workflows, I'd love to hear what worked (and what didn't). Feedback and contributions are welcome.\n\n*Have questions or feedback? Leave a comment below or open an issue on [GitHub](https://github.com/edwardsp/mpi-stage).*\n\nPublished Jan 09, 2026\n\nVersion 1.0\n\n[ai infrastructure](/tag/ai%20infrastructure?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[benchmarking](/tag/benchmarking?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[hpc](/tag/hpc?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[storage](/tag/storage?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[!\\[pauledwards&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-4.svg?image-dimensions=50x50)](/users/pauledwards/363080) [pauledwards](/users/pauledwards/363080) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined June 19, 2019\n\n[View Profile](/users/pauledwards/363080)\n\n/category/azure/blog/azurehighperformancecomputingblog [Azure High Performance Computing (HPC) Blog](/category/azure/blog/azurehighperformancecomputingblog) Follow this blog board to get notified when there's new activity",
  "Link": "https://techcommunity.microsoft.com/t5/azure-high-performance-computing/mpi-stage-high-performance-file-distribution-for-hpc-clusters/ba-p/4484366",
  "ProcessedDate": "2026-01-09 10:03:01",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Tags": [],
  "PubDate": "2026-01-09T09:49:38+00:00"
}
