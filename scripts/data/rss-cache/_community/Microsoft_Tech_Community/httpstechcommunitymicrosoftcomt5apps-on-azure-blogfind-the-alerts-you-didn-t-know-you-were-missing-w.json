{
  "OutputDir": "_community",
  "EnhancedContent": "I had 6 alert rules. CPU. Memory. Pod restarts. Container errors. OOMKilled. Job failures. I thought I was covered.\n\nThen my app went down. I kept refreshing the Azure portal, waiting for an alert. Nothing.\n\nThat's when it hit me: **my alerts were working perfectly. They just weren't designed for this failure mode.**\n\nSound familiar?\n\n## The Problem Every Developer Knows\n\nIf you're a developer or DevOps engineer, you've been here: a customer reports an issue, you scramble to check your monitoring, and then you realize *you don't have the right alerts set up*. By the time you find out, it's already too late.\n\nYou set up what seems like reasonable alerting and assume you're covered. But real-world failures are sneaky. They slip through the cracks of your carefully planned thresholds.\n\n## My Setup: AKS with Redis\n\nI love to vibe code apps using **GitHub Copilot Agent mode with Claude Opus 4.5**. It's fast, it understands context, and it lets me focus on building rather than boilerplate.\n\nFor this project, I built a simple **journal entry app**:\n\n- **AKS cluster** hosting the web API\n- **Azure Cache for Redis** storing journal data\n- **Azure Monitor alerts** for CPU, memory, pod restarts, container errors, OOMKilled, and job failures\n\nSeemed solid. What could go wrong?\n\n## The Scenario: Redis Password Rotation\n\nHere's something that happens constantly in enterprise environments: **the security team rotates passwords**. It's best practice. It's in the compliance checklist. And it breaks things when apps don't pick up the new credentials.\n\nI simulated exactly this.\n\nThe pods came back up. But they couldn't connect to Redis (as expected). The readiness probes started failing. The LoadBalancer had no healthy backends. The endpoint timed out.\n\n**And not a single alert fired.**\n\n## Using SRE Agent to Find the Alert Gaps\n\nInstead of manually auditing every alert rule and trying to figure out what I missed, I turned to **Azure SRE Agent**. I asked it a simple question: \"*My endpoint is timing out. What alerts do I have, and why didn't any of them fire?\"*\n\nWithin minutes, it had diagnosed the problem. Here's what it found:\n\n| My Existing Alerts | Why They Didn't Fire | | --- | --- | | High CPU/Memory | No resource pressure,just auth failures | | Pod Restarts | Pods weren't restarting, just unhealthy | | Container Errors | App logs weren't being written | | OOMKilled | No memory issues | | Job Failures | No K8s jobs involved |\n\n**The gaps SRE Agent identified:**\n\n1. ❌ No synthetic URL availability test\n2. ❌ No readiness/liveness probe failure alerts\n3. ❌ No \"pods not ready\" alerts scoped to my namespace\n4. ❌ No Redis connection error detection\n5. ❌ No ingress 5xx/timeout spike alerts\n6. ❌ No per-pod resource alerts (only node-level)\n\nSRE Agent didn't just tell me what was wrong, it created a **GitHub issue** with :\n\n- KQL queries to detect each failure type\n- Bicep code snippets for new alert rules\n- Remediation suggestions for the app code\n- Exact file paths in my repo to update\n\nCheck it out: GitHub [Issue](https://github.com/dm-chelupati/aksjournalapp/issues/3)\n\n## How I Built It: Step by Step\n\nLet me walk you through exactly how I set this up inside SRE Agent.\n\n### Step 1: Create an SRE Agent\n\nI created a new SRE Agent in the [Azure portal](https://ms.portal.azure.com/#view/Microsoft_Azure_PaasServerless/SreAgentHome.ReactView). Since this workflow analyzes alerts across my subscription (not just one resource group), I didn't configure any specific resource groups. Instead, I gave the agent's managed identity **Reader** permissions on my entire subscription. This lets it discover resources, list alert rules, and query Log Analytics across all my resource groups.\n\n### Step 2: Connect GitHub to SRE Agent via MCP\n\nI added a GitHub MCP server to give the agent access to my source code repository.MCP (Model Context Protocol) lets you bring any API into the agent. If your tool has an API, you can connect it. I use GitHub for both source code and tracking dev tickets, but you can connect to wherever your code lives **(GitLab, Azure DevOps)** or your ticketing system **(Jira, ServiceNow, PagerDuty).**\n\n### Step 3: Create a Subagent inside SRE Agent for managing Azure Monitor Alerts\n\nI created a focused subagent with a specific job and only the tools it needs:\n\n**Azure Monitor Alerts Expert**\n\nPrompt:\n\n*\" You are expert in managing operations related to azure monitor alerts on azure resources including discovering alert rules configured on azure resources, creating new alert rules (with user approval and authorization only), processing the alerts fired on azure resources and identifying gaps in the alert rules. You can get the resource details from azure monitor alert if triggered via alert. If not, you need to ask user for the specific resource to perform analysis on.  You can use az cli tool to diagnose logs, check the app health metrics. You must use the app code and infra code (bicep files) files you have access to in the github repo &lt;insert your repo&gt; to further understand the possible diagnoses and suggest remediations. Once analysis is done, you must create a github issue with details of analysis and suggested remediation to the source code files in the same repo*.*\"*\n\nTools enabled:\n\n- **az cli** – List resources, alert rules, action groups\n- **Log Analytics workspace querying** – Run KQL queries for diagnostics\n- **GitHub MCP** – Search repositories, read file contents, create issues\n\n### Step 4: Ask the Subagent About Alert Gaps\n\nI gave the agent context and asked a simple question: *\"@AzureAlertExpert: My API endpoint [http://132.196.167.102/api/journals/john](http://132.196.167.102/api/journals/john) is timing out. What alerts do I have configured in rg-aks-journal, and why didn't any of them fire?*\n\nThe agent did the analysis autonomously and summarized findings with suggestions to add new alert rules in a GitHub issue.\n\n**Here's the agentic workflow to perform azure monitor alert operations**\n\n## Why This Matters\n\n**Faster response times.** Issues get diagnosed in minutes, not hours of manual investigation.\n\n**Consistent analysis.** No more \"I thought we had an alert for that\" moments. The agent systematically checks what's covered and what's not.\n\n**Proactive coverage.** You don't have to wait for an incident to find gaps. Ask the agent to review your alerts before something breaks.\n\n## The Bottom Line\n\nYour alerts have gaps. You just don't know it until something slips through.\n\nI had 6 alert rules and still missed a basic failure. My pods weren't restarting, they were just unhealthy. My CPU wasn't spiking, the app was just returning errors. None of my alerts were designed for this.\n\nYou don't need to audit every alert rule manually. Give SRE Agent your environment, describe the failure, and let it tell you what's missing.\n\nStop discovering alert gaps from customer complaints. Start finding them before they matter.\n\n## A Few Tips\n\n- Give the agent Reader access at subscription level so it can discover all resources\n- Use a focused subagent prompt, don't try to do everything in one agent\n- Test your MCP connections before running workflows\n\n## What Alert Gaps Have Burned You?\n\nWhat's the alert you wish you had set up before an incident? Credential rotation? Certificate expiry? DNS failures? Let us know in the comments.\n\nPublished Jan 06, 2026\n\nVersion 1.0\n\n[azure app service](/tag/azure%20app%20service?nodeId=board%3AAppsonAzureBlog)\n\n[azure cache for redis](/tag/azure%20cache%20for%20redis?nodeId=board%3AAppsonAzureBlog)\n\n[azure container apps](/tag/azure%20container%20apps?nodeId=board%3AAppsonAzureBlog)\n\n[azure kubernetes service](/tag/azure%20kubernetes%20service?nodeId=board%3AAppsonAzureBlog)\n\n[azure paas](/tag/azure%20paas?nodeId=board%3AAppsonAzureBlog)\n\n[azure sre agent](/tag/azure%20sre%20agent?nodeId=board%3AAppsonAzureBlog)\n\n[best practices](/tag/best%20practices?nodeId=board%3AAppsonAzureBlog)\n\n[cloud native](/tag/cloud%20native?nodeId=board%3AAppsonAzureBlog)\n\n[containers](/tag/containers?nodeId=board%3AAppsonAzureBlog)\n\n[devops](/tag/devops?nodeId=board%3AAppsonAzureBlog)\n\n[!\\[dchelupati&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-8.svg?image-dimensions=50x50)](/users/dchelupati/3031090) [dchelupati](/users/dchelupati/3031090) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined May 13, 2025\n\n[View Profile](/users/dchelupati/3031090)\n\n/category/azure/blog/appsonazureblog [Apps on Azure Blog](/category/azure/blog/appsonazureblog) Follow this blog board to get notified when there's new activity",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "ProcessedDate": "2026-01-06 18:03:08",
  "Link": "https://techcommunity.microsoft.com/t5/apps-on-azure-blog/find-the-alerts-you-didn-t-know-you-were-missing-with-azure-sre/ba-p/4483494",
  "FeedName": "Microsoft Tech Community",
  "Description": "I had 6 alert rules. CPU. Memory. Pod restarts. Container errors. OOMKilled. Job failures. I thought I was covered.\n\nThen my app went down. I kept refreshing the Azure portal, waiting for an alert. Nothing.\n\nThat's when it hit me: **my alerts were working perfectly. They just weren't designed for this failure mode.**\n\nSound familiar?\n\n## The Problem Every Developer Knows\n\nIf you're a developer or DevOps engineer, you've been here: a customer reports an issue, you scramble to check your monitoring, and then you realize *you don't have the right alerts set up*. By the time you find out, it's already too late.\n\nYou set up what seems like reasonable alerting and assume you're covered. But real-world failures are sneaky. They slip through the cracks of your carefully planned thresholds.\n\n## My Setup: AKS with Redis\n\nI love to vibe code apps using **GitHub Copilot Agent mode with Claude Opus 4.5**. It's fast, it understands context, and it lets me focus on building rather than boilerplate.\n\nFor this project, I built a simple **journal entry app**:\n\n- **AKS cluster** hosting the web API\n- **Azure Cache for Redis** storing journal data\n- **Azure Monitor alerts** for CPU, memory, pod restarts, container errors, OOMKilled, and job failures\n\nSeemed solid. What could go wrong?\n\n## The Scenario: Redis Password Rotation\n\nHere's something that happens constantly in enterprise environments: **the security team rotates passwords**. It's best practice. It's in the compliance checklist. And it breaks things when apps don't pick up the new credentials.\n\nI simulated exactly this.\n\nThe pods came back up. But they couldn't connect to Redis (as expected). The readiness probes started failing. The LoadBalancer had no healthy backends. The endpoint timed out.\n\n**And not a single alert fired.**\n\n## Using SRE Agent to Find the Alert Gaps\n\nInstead of manually auditing every alert rule and trying to figure out what I missed, I turned to **Azure SRE Agent**. I asked it a simple question: \"*My endpoint is timing out. What alerts do I have, and why didn't any of them fire?\"*\n\nWithin minutes, it had diagnosed the problem. Here's what it found:\n\n| My Existing Alerts | Why They Didn't Fire | | --- | --- | | High CPU/Memory | No resource pressure,just auth failures | | Pod Restarts | Pods weren't restarting, just unhealthy | | Container Errors | App logs weren't being written | | OOMKilled | No memory issues | | Job Failures | No K8s jobs involved |\n\n**The gaps SRE Agent identified:**\n\n1. ❌ No synthetic URL availability test\n2. ❌ No readiness/liveness probe failure alerts\n3. ❌ No \"pods not ready\" alerts scoped to my namespace\n4. ❌ No Redis connection error detection\n5. ❌ No ingress 5xx/timeout spike alerts\n6. ❌ No per-pod resource alerts (only node-level)\n\nSRE Agent didn't just tell me what was wrong, it created a **GitHub issue** with :\n\n- KQL queries to detect each failure type\n- Bicep code snippets for new alert rules\n- Remediation suggestions for the app code\n- Exact file paths in my repo to update\n\nCheck it out: GitHub [Issue](https://github.com/dm-chelupati/aksjournalapp/issues/3)\n\n## How I Built It: Step by Step\n\nLet me walk you through exactly how I set this up inside SRE Agent.\n\n### Step 1: Create an SRE Agent\n\nI created a new SRE Agent in the [Azure portal](https://ms.portal.azure.com/#view/Microsoft_Azure_PaasServerless/SreAgentHome.ReactView). Since this workflow analyzes alerts across my subscription (not just one resource group), I didn't configure any specific resource groups. Instead, I gave the agent's managed identity **Reader** permissions on my entire subscription. This lets it discover resources, list alert rules, and query Log Analytics across all my resource groups.\n\n### Step 2: Connect GitHub to SRE Agent via MCP\n\nI added a GitHub MCP server to give the agent access to my source code repository.MCP (Model Context Protocol) lets you bring any API into the agent. If your tool has an API, you can connect it. I use GitHub for both source code and tracking dev tickets, but you can connect to wherever your code lives **(GitLab, Azure DevOps)** or your ticketing system **(Jira, ServiceNow, PagerDuty).**\n\n### Step 3: Create a Subagent inside SRE Agent for managing Azure Monitor Alerts\n\nI created a focused subagent with a specific job and only the tools it needs:\n\n**Azure Monitor Alerts Expert**\n\nPrompt:\n\n*\" You are expert in managing operations related to azure monitor alerts on azure resources including discovering alert rules configured on azure resources, creating new alert rules (with user approval and authorization only), processing the alerts fired on azure resources and identifying gaps in the alert rules. You can get the resource details from azure monitor alert if triggered via alert. If not, you need to ask user for the specific resource to perform analysis on. You can use az cli tool to diagnose logs, check the app health metrics. You must use the app code and infra code (bicep files) files you have access to in the github repo to further understand the possible diagnoses and suggest remediations. Once analysis is done, you must create a github issue with details of analysis and suggested remediation to the source code files in the same repo*.*\"*\n\nTools enabled:\n\n- **az cli** – List resources, alert rules, action groups\n- **Log Analytics workspace querying** – Run KQL queries for diagnostics\n- **GitHub MCP** – Search repositories, read file contents, create issues\n\n### Step 4: Ask the Subagent About Alert Gaps\n\nI gave the agent context and asked a simple question: *\"@AzureAlertExpert: My API endpoint [http://132.196.167.102/api/journals/john](http://132.196.167.102/api/journals/john) is timing out. What alerts do I have configured in rg-aks-journal, and why didn't any of them fire?*\n\nThe agent did the analysis autonomously and summarized findings with suggestions to add new alert rules in a GitHub issue.\n\n**Here's the agentic workflow to perform azure monitor alert operations**\n\n![]()\n\n## Why This Matters\n\n**Faster response times.** Issues get diagnosed in minutes, not hours of manual investigation.\n\n**Consistent analysis.** No more \"I thought we had an alert for that\" moments. The agent systematically checks what's covered and what's not.\n\n**Proactive coverage.** You don't have to wait for an incident to find gaps. Ask the agent to review your alerts before something breaks.\n\n## The Bottom Line\n\nYour alerts have gaps. You just don't know it until something slips through.\n\nI had 6 alert rules and still missed a basic failure. My pods weren't restarting, they were just unhealthy. My CPU wasn't spiking, the app was just returning errors. None of my alerts were designed for this.\n\nYou don't need to audit every alert rule manually. Give SRE Agent your environment, describe the failure, and let it tell you what's missing.\n\nStop discovering alert gaps from customer complaints. Start finding them before they matter.\n\n## A Few Tips\n\n- Give the agent Reader access at subscription level so it can discover all resources\n- Use a focused subagent prompt, don't try to do everything in one agent\n- Test your MCP connections before running workflows\n\n## What Alert Gaps Have Burned You?\n\nWhat's the alert you wish you had set up before an incident? Credential rotation? Certificate expiry? DNS failures? Let us know in the comments.",
  "PubDate": "2026-01-06T17:39:38+00:00",
  "Title": "Find the Alerts You Didn't Know You Were Missing with Azure SRE Agent",
  "Tags": [],
  "Author": "dchelupati"
}
