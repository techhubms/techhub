{
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "EnhancedContent": "Imagine having several clusters across different environments (dev, test and prod) or planning a migration between PBS and Slurm or porting codes to a different system. They can all seem like daunting tasks.\n\nThis is where the combination of ReFrame HPC, a powerful and feature rich testing framework, and Azure Log Analytics can help improve confidence and assurance in the performance and accuracy of a system.\n\nHere we will look at how to configure ReFrame HPC specifically for Azure: Deploying the required Azure resources, running a test and capturing the results in Log Analytics for analysis.\n\n## Deploying the required Azure Resources\n\nFirstly, deploy the required resources in Azure by using this [bicep](https://github.com/JimPaine/reframe-azure-perflog-handler) from GitHub. The deployment includes the creation and configuration of everything required for ReFrame HPC. These resources include a [data collection endpoint](https://docs.azure.cn/en-us/azure-monitor/data-collection/data-collection-endpoint-overview?tabs=portal), a [data collection rule](https://docs.azure.cn/en-us/azure-monitor/data-collection/data-collection-rule-overview) and a [log analytics workspace](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview).\n\nAzure icons for a Data Collection Endpoint, Data Collection Rule with an arrow pointing from them to the icon for Log Analytics Workspace.\n\n‚ö†Ô∏èMake sure to capture the URL from the bicep output. The structure of the endpoint that is needed later is complex, but the¬†[bicep](https://github.com/JimPaine/reframe-azure-perflog-handler/blob/main/main.bicep) generates it and outputs it at the end so make sure to caputure it now.\n\n## Running ior via ReFrame HPC\n\nFor the purpose of demonstrating a running test and capturing the results in Azure from start to finish, here is a simple ior test which will run both a read and a write operation against the shared storage.\n\n``` import reframe as rfm import reframe.utility.sanity as sn @rfm.simple_test class SimplePerfTest(rfm.RunOnlyRegressionTest): valid_systems = [\"*\"] valid_prog_environs = [\"+ior\"] executable = 'ior' executable_opts = [ '-a POSIX -w -r -C -e -g -F -b 2M -t 2M -s 25600 -o /data/demo/test.bin -D 300' ] reference = { 'tst:hbv4': { 'write_bandwidth_mib': (500, -0.05, 0.1, 'MiB/s'), 'read_bandwidth_mib': (350, -0.05, 0.5, 'MiB/s'), } } @sanity_function def validate_run(self): return sn.assert_found(r'Summary of all tests:', self.stdout) @performance_function('MiB/s') def write_bandwidth_mib(self): return sn.extractsingle(r'^write\\s+([0-9]+\\.?[0-9]*)', self.stdout, 1, float) @performance_function('MiB/s') def read_bandwidth_mib(self): return sn.extractsingle(r'^read\\s+([0-9]+\\.?[0-9]*)', self.stdout, 1, float) ```\n\n### Test explanation\n\nSet the binary to be executed to ior, along with its arguments.\n\n``` executable = 'ior' executable_opts = [ '-a POSIX -w -r -C -e -g -F -b 2M -t 2M -s 25600 -o /data/demo/test.bin -D 300' ] ```\n\nSpecify which systems the test should run on. In this case, any system/cluster which is known to have ior available will be selected. Look at the ReFrame HPC [documentation](https://reframe-hpc.readthedocs.io/en/stable/tutorial.html#systems-and-environments)¬†to get a better understanding of the options available for use.\n\n``` valid_systems = [\"*\"] valid_prog_environs = [\"+ior\"] ```\n\nVerify the stdout of the job by searching for a specific value to assert that it ran successfully.\n\n``` @sanity_function def validate_run(self): return sn.assert_found(r'Summary of all tests:', self.stdout) ```\n\nIf the sanity function passed it will then extract the performance metrics from the stdout of the job. The naming of the methods is important, as they will be stored in the results later.\n\n``` @performance_function('MiB/s') def write_bandwidth_mib(self): return sn.extractsingle(r'^write\\s+([0-9]+\\.?[0-9]*)', self.stdout, 1, float) @performance_function('MiB/s') def read_bandwidth_mib(self): return sn.extractsingle(r'^read\\s+([0-9]+\\.?[0-9]*)', self.stdout, 1, float) ```\n\n[Performance references](https://reframe-hpc.readthedocs.io/en/stable/tutorial.html#adding-performance-references)¬† are used to determine if the current cluster has met the requirement or not. It also allows margins to be specified in either direction.\n\n``` reference = { 'tst:hbv4': { 'write_bandwidth_mib': (500, -0.05, 0.1, 'MiB/s'), 'read_bandwidth_mib': (350, -0.05, 0.5, 'MiB/s'), } } ```\n\n## ReFrame HPC Configuration\n\nThe ReFrame HPC configuration is key to determine how and where the test will run. It is also where the logic allowing Reframe HPC to use Azure for centralized logging will be defined. The full configuration file is vast and is covered in detail within the¬†[ReFrame HPC documentation](https://reframe-hpc.readthedocs.io/en/stable/tutorial.html#systems-and-environments). For the purpose of this test an example can be found [on GitHub.](https://github.com/JimPaine/reframe-azure-perflog-handler/blob/main/config.py) Below is a breakdown of the key parts that allow Reframe HPC to push its results into Azure Log Analytics.\n\n### Logging Handler\n\nThe most important part of this configuration is the logging section, without it ReFrame HPC will not attempt to log the results. A handler\\_perflog of type httpjson is added to enable the logs to be sent to a HTTP endpoint with specific values which our covered below.\n\n``` 'logging': [ { 'perflog_multiline': True, 'handlers_perflog': [ { 'type': 'httpjson', 'url': 'REDACTED', 'level': 'info', 'debug': False, 'extra_headers': {'Authorization': f'Bearer {_get_token()}'}, 'extras': { 'TimeGenerated': f'{datetime.now(timezone.utc).isoformat()}', 'facility': 'reframe', 'reframe_azure_data_version': '1.0', }, 'ignore_keys': ['check_perfvalues'], 'json_formatter': _format_record } ] } ```\n\n### Multiline Perflog\n\nTo ensure this works with Azure, enable [perflog_multiline](https://reframe-hpc.readthedocs.io/en/stable/config_reference.html#config.logging.perflog_multiline). This will ensure a single record per metric is sent to Log Analytics. This is the cleanest way to output the results. Having this set to False will move the metric names into column names, which means that the schema will be different for each test and will become hard to maintain.\n\n### Extra Headers\n\nA bearer token is required to authenticate the request. ReFrame HPC allows the adding of headers via the extra\\_headers property and a simple Python function, which obtains a scoped token that can be appended to the additional header.\n\n``` def _get_token(scope='https://monitor.azure.com/.default') -> str: credential = DefaultAzureCredential() token = credential.get_token(scope) return token.token ```\n\n### Url Structure\n\nThe url can be found in the output of the [bicep](https://github.com/JimPaine/reframe-azure-perflog-handler/blob/main/main.bicep)¬†which was run previously. It can also be obtained via the portal. Here is the structure of the url for reference.\n\n``` '${dce.properties.logsIngestion.endpoint}/dataCollectionRules/${dcr.properties.immutableId}/streams/Custom-${table.name}?api-version=2023-01-01' ```\n\n### json Formatter\n\nA small work around is needed as the Data Collection Rule expects an array of items and ReFrame HPC outputs a single record. To resolve this another Python function can be used which simply wraps the record up in an array. In this example it also tidys up and removes some items that are not required and would cause issues with the json serialization.\n\n``` def _format_record(record, extras, ignore_keys): data = {} for attr, val in record.__dict__.items(): if attr in ignore_keys or attr.startswith('_'): continue data[attr] = val data.update(extras) return json.dumps([data]) ```\n\n## Running the Test\n\nNow that the infrastructure has been deployed, the test has been defined and is correctly configured, we can run the test.\n\nStart by logging in. Here I am using the managed identity of the node, but User auth and User Assigned Managed Identities are also supported.\n\n``` $ az login --identity ```\n\nReFrame HPC can be installed via Spack or Python and, while I am using Spack for packages on the cluster, I find the simplest approach is to activate a Python environment and install ReFrame HPC along with test specfic Python dependencies.\n\n``` $ python3 -m venv .venv $ . .venv/bin/activate $ python -m pip install -U pip $ pip install -r requirements.txt ```\n\nNow using the ReFrame HPC cli, the test can be run using the configuration file and the test file.\n\n``` $ reframe -C config.py -c simple_perf.py --performance-report -r ```\n\nReFrame HPC will now run the test against the system/cluster defined in the configuration. For this example it is a Slurm cluster on a partition of HBv4 nodes and running squeue clarifys that.\n\n``` $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 955 hbv4 rfm_Simp jim.pain R 0:28 1 tst4-hbv4-97 ```\n\n### Results\n\nAnd there we have it, results are now appearing in Azure! From here we can use kql to query and filter the results. This is just a subset of the values available but the dataset is vast and includes a huge range of values that are extremely helpful.\n\n### Summary\n\nBy standardizing on the combination of ReFrame HPC and Azure Log Analytics for testing and reporting of performance data across our clusters, whether Slurm based, Azure CycleCloud or existing on-prem clusters, you can gain unprecendented visibility and confidence in the systems you manage and the codes you deploy that were previously hard to obtain. Enabling the potential for:\n\n- üîéFast cross-cluster comparisions\n- üìàTrend analysis over long running periods\n- üìäStandardized metrics regardless of scheduler or system\n- ‚òÅÔ∏èUnified monitoring and reporting across clusters\n\nReFrame HPC is suitable for a wide range of testing, so if testing is something you have been looking to implement, take a look at¬†[ReFrame HPC](https://reframe-hpc.readthedocs.io/en/stable/index.html)\n\nUpdated Feb 06, 2026\n\nVersion 1.0\n\n[benchmarking](/tag/benchmarking?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[hpc](/tag/hpc?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[!\\[jimpaine&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-10.svg?image-dimensions=50x50)](/users/jimpaine/335139) [jimpaine](/users/jimpaine/335139) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined May 06, 2019\n\n[View Profile](/users/jimpaine/335139)\n\n/category/azure/blog/azurehighperformancecomputingblog [Azure High Performance Computing (HPC) Blog](/category/azure/blog/azurehighperformancecomputingblog) Follow this blog board to get notified when there's new activity",
  "Author": "jimpaine",
  "Title": "Centralized cluster performance metrics with ReFrame HPC and Azure Log Analytics",
  "ProcessedDate": "2026-02-06 10:12:24",
  "Tags": [],
  "PubDate": "2026-02-06T09:37:24+00:00",
  "Description": "Imagine having several clusters across different environments (dev, test and prod) or planning a migration between PBS and Slurm or porting codes to a different system. They can all seem like daunting tasks.\n\nThis is where the combination of ReFrame HPC, a powerful and feature rich testing framework, and Azure Log Analytics can help improve confidence and assurance in the performance and accuracy of a system.\n\nHere we will look at how to configure ReFrame HPC specifically for Azure: Deploying the required Azure resources, running a test and capturing the results in Log Analytics for analysis.\n\n## Deploying the required Azure Resources\n\nFirstly, deploy the required resources in Azure by using this [bicep](https://github.com/JimPaine/reframe-azure-perflog-handler) from GitHub. The deployment includes the creation and configuration of everything required for ReFrame HPC. These resources include a [data collection endpoint](https://docs.azure.cn/en-us/azure-monitor/data-collection/data-collection-endpoint-overview?tabs=portal), a [data collection rule](https://docs.azure.cn/en-us/azure-monitor/data-collection/data-collection-rule-overview) and a [log analytics workspace](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview).\n\n![]()Azure icons for a Data Collection Endpoint, Data Collection Rule with an arrow pointing from them to the icon for Log Analytics Workspace.\n\n- The structure of the endpoint that is needed later is complex, but the [bicep](https://github.com/JimPaine/reframe-azure-perflog-handler/blob/main/main.bicep) generates it and outputs it at the end so make sure to caputure it now.\n\n## Running ior via ReFrame HPC\n\nFor the purpose of demonstrating a running test and capturing the results in Azure from start to finish, here is a simple ior test which will run both a read and a write operation against the shared storage.\n- import reframe as rfm import reframe.utility.sanity as sn @rfm.simple\\_test class SimplePerfTest(rfm.RunOnlyRegressionTest): valid\\_systems = [\"\\*\"] valid\\_prog\\_environs = [\"+ior\"] executable = 'ior' executable\\_opts = [ '-a POSIX -w -r -C -e -g -F -b 2M -t 2M -s 25600 -o /data/demo/test.bin -D 300' ] reference = { 'tst:hbv4': { 'write\\_bandwidth\\_mib': (500, -0.05, 0.1, 'MiB/s'), 'read\\_bandwidth\\_mib': (350, -0.05, 0.5, 'MiB/s'), } } @sanity\\_function def validate\\_run(self): return sn.assert\\_found(r'Summary of all tests:', self.stdout) @performance\\_function('MiB/s') def write\\_bandwidth\\_mib(self): return sn.extractsingle(r'^write\\s+([0-9]+\\.?[0-9]\\*)', self.stdout, 1, float) @performance\\_function('MiB/s') def read\\_bandwidth\\_mib(self): return sn.extractsingle(r'^read\\s+([0-9]+\\.?[0-9]\\*)', self.stdout, 1, float)\n\n### Test explanation\n\nSet the binary to be executed to ior, along with its arguments.\n- executable = 'ior' executable\\_opts = [ '-a POSIX -w -r -C -e -g -F -b 2M -t 2M -s 25600 -o /data/demo/test.bin -D 300' ]\n\nSpecify which systems the test should run on. In this case, any system/cluster which is known to have ior available will be selected. Look at the ReFrame HPC [documentation](https://reframe-hpc.readthedocs.io/en/stable/tutorial.html#systems-and-environments) to get a better understanding of the options available for use.\n- valid\\_systems = [\"\\*\"] valid\\_prog\\_environs = [\"+ior\"]\n\nVerify the stdout of the job by searching for a specific value to assert that it ran successfully.\n- @sanity\\_function def validate\\_run(self): return sn.assert\\_found(r'Summary of all tests:', self.stdout)\n\nIf the sanity function passed it will then extract the performance metrics from the stdout of the job. The naming of the methods is important, as they will be stored in the results later.\n- @performance\\_function('MiB/s') def write\\_bandwidth\\_mib(self): return sn.extractsingle(r'^write\\s+([0-9]+\\.?[0-9]\\*)', self.stdout, 1, float) @performance\\_function('MiB/s') def read\\_bandwidth\\_mib(self): return sn.extractsingle(r'^read\\s+([0-9]+\\.?[0-9]\\*)', self.stdout, 1, float)\n\n[Performance references](https://reframe-hpc.readthedocs.io/en/stable/tutorial.html#adding-performance-references) are used to determine if the current cluster has met the requirement or not. It also allows margins to be specified in either direction.\n- reference = { 'tst:hbv4': { 'write\\_bandwidth\\_mib': (500, -0.05, 0.1, 'MiB/s'), 'read\\_bandwidth\\_mib': (350, -0.05, 0.5, 'MiB/s'), } }\n\n## ReFrame HPC Configuration\n\nThe ReFrame HPC configuration is key to determine how and where the test will run. It is also where the logic allowing Reframe HPC to use Azure for centralized logging will be defined. The full configuration file is vast and is covered in detail within the [ReFrame HPC documentation](https://reframe-hpc.readthedocs.io/en/stable/tutorial.html#systems-and-environments). For the purpose of this test an example can be found [on GitHub.](https://github.com/JimPaine/reframe-azure-perflog-handler/blob/main/config.py) Below is a breakdown of the key parts that allow Reframe HPC to push its results into Azure Log Analytics.\n\n### Logging Handler\n\nThe most important part of this configuration is the logging section, without it ReFrame HPC will not attempt to log the results. A handler\\_perflog of type httpjson is added to enable the logs to be sent to a HTTP endpoint with specific values which our covered below.\n- 'logging': [ { 'perflog\\_multiline': True, 'handlers\\_perflog': [ { 'type': 'httpjson', 'url': 'REDACTED', 'level': 'info', 'debug': False, 'extra\\_headers': {'Authorization': f'Bearer {\\_get\\_token()}'}, 'extras': { 'TimeGenerated': f'{datetime.now(timezone.utc).isoformat()}', 'facility': 'reframe', 'reframe\\_azure\\_data\\_version': '1.0', }, 'ignore\\_keys': ['check\\_perfvalues'], 'json\\_formatter': \\_format\\_record } ] }\n\n### Multiline Perflog\n\nTo ensure this works with Azure, enable [perflog_multiline](https://reframe-hpc.readthedocs.io/en/stable/config_reference.html#config.logging.perflog_multiline). This will ensure a single record per metric is sent to Log Analytics. This is the cleanest way to output the results. Having this set to False will move the metric names into column names, which means that the schema will be different for each test and will become hard to maintain.\n\n### Extra Headers\n\nA bearer token is required to authenticate the request. ReFrame HPC allows the adding of headers via the extra\\_headers property and a simple Python function, which obtains a scoped token that can be appended to the additional header.\n- def \\_get\\_token(scope='https://monitor.azure.com/.default') -> str: credential = DefaultAzureCredential() token = credential.get\\_token(scope) return token.token\n\n### Url Structure\n\nThe url can be found in the output of the [bicep](https://github.com/JimPaine/reframe-azure-perflog-handler/blob/main/main.bicep) which was run previously. It can also be obtained via the portal. Here is the structure of the url for reference.\n- '${dce.properties.logsIngestion.endpoint}/dataCollectionRules/${dcr.properties.immutableId}/streams/Custom-${table.name}?api-version=2023-01-01'\n\n### json Formatter\n\nA small work around is needed as the Data Collection Rule expects an array of items and ReFrame HPC outputs a single record. To resolve this another Python function can be used which simply wraps the record up in an array. In this example it also tidys up and removes some items that are not required and would cause issues with the json serialization.\n- def \\_format\\_record(record, extras, ignore\\_keys): data = {} for attr, val in record.\\_\\_dict\\_\\_.items(): if attr in ignore\\_keys or attr.startswith('\\_'): continue data[attr] = val data.update(extras) return json.dumps([data])\n\n## Running the Test\n\nNow that the infrastructure has been deployed, the test has been defined and is correctly configured, we can run the test.\n\nStart by logging in. Here I am using the managed identity of the node, but User auth and User Assigned Managed Identities are also supported.\n- $ az login --identity\n\nReFrame HPC can be installed via Spack or Python and, while I am using Spack for packages on the cluster, I find the simplest approach is to activate a Python environment and install ReFrame HPC along with test specfic Python dependencies.\n- $ python3 -m venv .venv $ . .venv/bin/activate $ python -m pip install -U pip $ pip install -r requirements.txt\n\nNow using the ReFrame HPC cli, the test can be run using the configuration file and the test file.\n- $ reframe -C config.py -c simple\\_perf.py --performance-report -r\n\nReFrame HPC will now run the test against the system/cluster defined in the configuration. For this example it is a Slurm cluster on a partition of HBv4 nodes and running squeue clarifys that.\n- $ squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 955 hbv4 rfm\\_Simp jim.pain R 0:28 1 tst4-hbv4-97\n\n### Results\n\nAnd there we have it, results are now appearing in Azure! From here we can use kql to query and filter the results. This is just a subset of the values available but the dataset is vast and includes a huge range of values that are extremely helpful.\n\n![]()\n\n### Summary\n\nBy standardizing on the combination of ReFrame HPC and Azure Log Analytics for testing and reporting of performance data across our clusters, whether Slurm based, Azure CycleCloud or existing on-prem clusters, you can gain unprecendented visibility and confidence in the systems you manage and the codes you deploy that were previously hard to obtain. Enabling the potential for:\n\n- üîéFast cross-cluster comparisions\n- üìàTrend analysis over long running periods\n- üìäStandardized metrics regardless of scheduler or system\n- ‚òÅÔ∏èUnified monitoring and reporting across clusters\n\nReFrame HPC is suitable for a wide range of testing, so if testing is something you have been looking to implement, take a look at [ReFrame HPC](https://reframe-hpc.readthedocs.io/en/stable/index.html)",
  "FeedName": "Microsoft Tech Community",
  "Link": "https://techcommunity.microsoft.com/t5/azure-high-performance-computing/centralized-cluster-performance-metrics-with-reframe-hpc-and/ba-p/4488077",
  "OutputDir": "_community"
}
