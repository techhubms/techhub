{
  "FeedName": "Microsoft Tech Community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "EnhancedContent": "## Introduction\n\nBack in late 2023, I published a blog post outlining [how to provision multiple egress IP addresses in AKS](https://techcommunity.microsoft.com/blog/azurearchitectureblog/provisioning-multiple-egress-ip-addresses-in-aks/3982130), using multiple node pools running in multiple subnets to achieve this outcome.\n\nIn July 2024, the AKS product group released the open-source [kube-egress-gateway](https://github.com/Azure/kube-egress-gateway) project to help customers provide multiple egress paths from their AKS clusters, and in September 2025 AKS added preview support for Static Egress Gateway, making the whole process simpler.\n\nThe purpose of this blog post is to re-visit the problem statement from the original blog post, and then look at how we can achieve this using the Static Egress Gateway feature built into AKS, and see how much simpler the modern solution is.\n\n## Problem Statement\n\nTo recap the original problem statement: some applications in a single AKS Cluster need **different outbound paths**, so you need a simple way to apply custom egress routing per workload to achieve this.\n\n## How Static Egress Gateway Improves on the 2023 Approach\n\nStatic Egress Gateway removes the architectural complexity required in the 2023 design by eliminating the need for multiple node pools, user defined routes and custom routing logic. Instead of mapping workloads to different subnets and maintaining separate outbound paths, the gateway provides a simple AKS native way to allocate predictable egress IPs to selected workloads. It also removes operational overhead because the gateway node pool owns the public IP prefix and handles translation without relying on cluster scale events. This gives a cleaner separation between compute and egress and is easier to automate and reason about. The result is the same outcome as the original post but achieved with fewer moving parts and a far more supportable model.\n\n## Proposed Solution\n\nThis time, instead of brewing our own solution, we will be using the [Static Egress Gateway](https://learn.microsoft.com/en-us/azure/aks/configure-static-egress-gateway) feature in AKS, as it is much simpler to implement, and more flexible too! This feature allows us to define multiple outbound egress pathways in a Kubernetes-native manner, without juggling node pools, UDRs (User Defined Routing), or custom routing logic.\n\nBefore we start, please be aware of the [limitations and considerations](https://learn.microsoft.com/en-us/azure/aks/configure-static-egress-gateway#limitations-and-considerations) related to using Static Egress Gateway.\n\nAs always, we start by defining some environment variables and creating a resource group to deploy our Azure resources into:\n\n``` rg=egress-gw location=swedencentral vnet_name=vnet-egress-gw cluster=egress-gw vm_size=Standard_D4as_v6\n\naz group create -n $rg -l $location\n\n```\n\nWe then create an AKS cluster, with the Static Egress Gateway feature enabled:\n\n``` az aks create \\ --name $cluster \\ --resource-group $rg \\ --location $location \\ --enable-static-egress-gateway\n\naz aks get-credentials -n $cluster -g $rg --overwrite\n\n```\n\nIt's also possible to enable the Static Egress Gateway feature on an existing cluster, using the `az aks update` command, e.g.:\n\n``` az aks update \\ --name $cluster \\ --resource-group $rg \\ --enable-static-egress-gateway ```\n\nOnce the cluster is created (or the feature has been enabled), we need to create a dedicated gateway node pool, which will handle the egress traffic. The `--gateway-prefix-size` is the size of the public IP prefix to be applied to the gateway node pools (28-31), and will define or limit the number of gateway nodes you can scale out in the gateway node pool.\n\n``` az aks nodepool add \\ --cluster-name $cluster \\ --name gwpool1 \\ --resource-group $rg \\ --mode gateway \\ --node-count 2 \\ --gateway-prefix-size 30 ```\n\n***Note: Gateway node pools should not be used for general-purpose workloads, and should be reserved for egress traffic only.***\n\nWith the gateway node pool created, we can connect to our cluster and deploy a `StaticGatewayConfiguration` custom resource. The `StaticGatewayConfiguration` CRD tells AKS which gateway node pool to use and which IP prefix to assign to the outbound traffic. A `StaticGatewayConfiguration` custom resource looks something like this:\n\n``` apiVersion: egressgateway.kubernetes.azure.com/v1alpha1 kind: StaticGatewayConfiguration metadata: name: egress-gw-1 namespace: default spec: gatewayNodepoolName: gwpool1 excludeCidrs: # Optional\n- 10.0.0.0/8\n- 172.16.0.0/12\n- 169.254.169.254/32\n# publicIpPrefixId: /subscriptions/<subscription-id>/resourceGroups/<resource-group>/providers/Microsoft.Network/publicIPPrefixes/<prefix-name> # Optional\n```\n\n***Note: In this example I excluded the `publicIpPrefixId` argument, so the AKS cluster will create a Public IP Prefix Azure resource for me automatically. If you already have a Public IP Prefix you would like to use, uncomment the line and set the resource ID accordingly.***\n\nWe can view the Public IP Prefix that has been provisioned for us by describing the `StaticGatewayConfiguration` resource we deployed. The Public IP Prefix can take a while to deploy and be configured, so you may need to run the command below a few times to see similar output.\n\n``` $ kubectl describe StaticGatewayConfiguration egress-gw-1 -n default Name: egress-gw-1 Namespace: default Labels: Annotations: API Version: egressgateway.kubernetes.azure.com/v1alpha1 Kind: StaticGatewayConfiguration Metadata: Creation Timestamp: 2026-01-07T11:34:17Z Finalizers: static-gateway-configuration-controller.microsoft.com Generation: 2 Resource Version: 15543 UID: de7642b6-c2fe-47cc-b478-f7c71c5db402 Spec: Default Route: staticEgressGateway Exclude Cidrs: 10.0.0.0/8 172.16.0.0/12 169.254.169.254/32 Gateway Nodepool Name: gwpool1 Gateway Vmss Profile: Provision Public Ips: true Status: Egress Ip Prefix: 20.91.186.64/30 Gateway Server Profile: Ip: 10.224.0.9 Port: 6000 Private Key Secret Ref: API Version: v1 Kind: Secret Name: sgw-de7642b6-c2fe-47cc-b478-f7c71c5db402 Namespace: aks-static-egress-gateway Public Key: FgeNumtkWbWnIGebcY1C/Ul19AmI1mLGf5DSMze5KBE= Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 14m (x5 over 14m) staticGatewayConfiguration-controller StaticGatewayConfiguration provisioned with egress prefix Normal ReconcileGatewayLBConfigurationSuccess 13m (x4 over 14m) gatewayLBConfiguration-controller GatewayLBConfiguration reconciled Normal Reconciled 13m (x2 over 13m) staticGatewayConfiguration-controller StaticGatewayConfiguration provisioned with egress prefix 20.91.186.64/30 Normal ReconcileGatewayVMConfigurationSuccess 13m (x2 over 13m) gatewayVMConfiguration-controller GatewayVMConfiguration reconciled\n\n```\n\nWith all the relevant pre-requisites in place, we can now configure and deploy an app to make use of this Static Egress Gateway. As in the previous blog post, we will be deploying the API component of [YADA: Yet Another Demo App](https://github.com/Microsoft/YADA) with a public-facing `LoadBalancer` (inbound) service, so that we can access the YADA app, which will show us our outbound IP addresses.\n\nCopy the [sample manifest from the GitHub repository](https://github.com/microsoft/YADA/blob/main/deploy/k8s.md#api), and update it to look like my two examples below.\n\nYADA API app using the default cluster egress (`yada-api-default.yaml` ):\n\n``` kubectl apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: labels: run: api-default name: api-default spec: replicas: 1 selector: matchLabels: run: api-default template: metadata: labels: run: api-default spec: containers:\n- image: erjosito/yadaapi:1.0\nname: api-default ports:\n- containerPort: 8080\nprotocol: TCP restartPolicy: Always --- apiVersion: v1 kind: Service metadata: name: yada-default annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \"false\" spec: type: LoadBalancer ports:\n- port: 8080\ntargetPort: 8080 selector: run: api-default EOF ```\n\nYADA API app using the Static Egress Gateway (`yada-api-egressgw.yaml` ):\n\n``` kubectl apply -f - <<EOF apiVersion: apps/v1 kind: Deployment metadata: labels: run: api-egressgw name: api-egressgw spec: replicas: 1 selector: matchLabels: run: api-egressgw template: metadata: labels: run: api-egressgw\n# This annotation defines which StaticGatewayConfiguration the workload should use for egress\nannotations: kubernetes.azure.com/static-gateway-configuration: egress-gw-1 spec: containers:\n- image: erjosito/yadaapi:1.0\nname: api-egressgw ports:\n- containerPort: 8080\nprotocol: TCP restartPolicy: Always --- apiVersion: v1 kind: Service metadata: name: yada-egressgw annotations: service.beta.kubernetes.io/azure-load-balancer-internal: \"false\" spec: type: LoadBalancer ports:\n- port: 8080\ntargetPort: 8080 selector: run: api-egressgw EOF ```\n\nNow we deploy both apps into our AKS cluster:\n\n``` kubectl apply -f yada-api-default.yaml kubectl apply -f yada-api-egressgw.yaml\n\n```\n\nUnlike in our cluster from my [2023 blog post](https://techcommunity.microsoft.com/blog/azurearchitectureblog/provisioning-multiple-egress-ip-addresses-in-aks/3982130), if we view the pods we will find that they are running in the **same** node pool:\n\n``` $ kubectl get pods -o wide NAME READY STATUS IP NODE api-default-7f4d8c5ccc-vcpxz 1/1 Running 10.244.0.166 aks-nodepool1-31740631-vmss000000 api-egressgw-6f45f5d8cc-xrfsm 1/1 Running 10.244.2.234 aks-nodepool1-31740631-vmss000002\n\n```\n\nAnd now we can use determine the **inbound** IP address for each `yada` service, and then use cURL to query the YADA API to view the **outbound** IP address for each `yada` pod we have deployed:\n\n``` echo \"default: svc IP=$(kubectl get svc yada-default -o jsonpath='{.status.loadBalancer.ingress[0].ip}'), egress IP=$(curl -s http://$(kubectl get svc yada-default -o jsonpath='{.status.loadBalancer.ingress[0].ip}'):8080/api/ip | jq -r '.my_public_ip')\" echo \"egressgw: svc IP=$(kubectl get svc yada-egressgw -o jsonpath='{.status.loadBalancer.ingress[0].ip}'), egress IP=$(curl -s http://$(kubectl get svc yada-egressgw -o jsonpath='{.status.loadBalancer.ingress[0].ip}'):8080/api/ip | jq -r '.my_public_ip')\" ```\n\nYou should see some output like this, demonstrating that each `yada` service has a unique **inbound** IP address (as expected with `LoadBalancer` services), as well as unique **outbound** IP addresses too:\n\n``` default: svc IP=135.116.221.253, egress IP=4.165.56.119 egressgw: svc IP=135.116.255.200, egress IP=20.91.186.64\n\n```\n\nIf we were to add more egress gateways, and configure additional workloads to use these gateways, we would see each Static Egress Gateway providing a new, unique set of outbound IP addresses.\n\n### Keeping Egress IPs Private\n\nIn some scenarios you may want or need to egress traffic on private IP addresses. You can do this by enabling private IP support on the gateway node pool. To do this, specify the `--vm-set-type VirtualMachines` parameter when creating the node pool, e.g.:\n\n``` az aks nodepool add \\ --cluster-name $cluster \\ --name privgwpool1 \\ --resource-group $rg \\ --mode gateway \\ --node-count 2 \\ --vm-set-type VirtualMachines \\ --gateway-prefix-size 30\n\n```\n\n***Note: At the time of writing, the private gateway deployment path is not functioning correctly. This should be resolved by the end of January once a fix has been rolled out.***\n\nWith this configuration, the `provisionPublicIps=false` setting keeps the private IPs allocated to the gateway nodes for the lifetime of the `StaticGatewayConfiguration` .\n\n### BYO Subnets\n\nAs you'll have noticed, so far we have used the default resources and configurations created automatically by AKS. For many customers this is fine. For others, the solution must integrate with their existing network architecture, and they will need to opt for Bring Your Own Networking. The Static Egress Gateway feature works well with BYO network configurations, but there are a few requirements to be aware of when the cluster runs inside a hub-and-spoke or centrally-managed IP space.\n\n#### Network Requirements\n\nWhen using BYO subnets there are some important constraints:\n\n- The gateway node pool must be deployed into a subnet with enough free IPs to host the Public IP Prefix assigned to the gateway.\n- The gateway subnet must not contain UDRs that redirect egress‑gateway traffic to a firewall before translation occurs.\n- Pod CIDRs, Service CIDRs, and the Public IP Prefix used by the gateway must not overlap with any internal ranges.\n\n#### Deploying the Resources\n\nHere is a minimal example using AZ CLI to create a BYO network layout suitable for Static Egress Gateway. Start by creating the Vnet and the subnets:\n\n``` az network vnet create \\ --name $vnet_name \\ --resource-group $rg \\ --address-prefixes 10.240.0.0/16 \\ --subnet-name aks-system \\ --subnet-prefixes 10.240.0.0/22 \\ --location $location \\ --output none\n\naz network vnet subnet create \\ --vnet-name $vnet_name \\ --resource-group $rg \\ --address-prefix 10.240.4.0/22 \\ --name aks-user \\ --output none\n\naz network vnet subnet create \\ --vnet-name $vnet_name \\ --resource-group $rg \\ --address-prefix 10.240.8.0/24 \\ --name aks-egress \\ --output none\n\n```\n\nDeploy an AKS cluster into the BYO Subnets:\n\n``` az aks create \\ --resource-group $rg \\ --name $cluster \\ --location $location \\ --enable-static-egress-gateway \\ --vnet-subnet-id $(az network vnet subnet show -g $rg --vnet-name $vnet_name -n aks-system --query id -o tsv) \\ --nodepool-name systempool1 \\ --node-count 2 \\ --node-vm-size $vm_size \\ --network-plugin azure \\ --network-dataplane=azure \\ --output none\n\naz aks get-credentials -n $cluster -g $rg --overwrite\n\n```\n\nAdd a node pool for user workloads:\n\n``` az aks nodepool add \\ --cluster-name $cluster \\ --name userpool1 \\ --resource-group $rg \\ --node-count 2 \\ --vnet-subnet-id $(az network vnet subnet show -g $rg --vnet-name $vnet_name -n aks-user --query id -o tsv)\n\n```\n\nUpdate the system-managed AKS identity with the \"Network Contributor\" role so it can configure the customer-managed subnets:\n\n``` clientId=$(az aks show --name $cluster --resource-group $rg --query identity.principalId --output tsv) rg_id=$(az group show --name $rg --query id -o tsv) mc_rg_id=$(az group show --name MC_${rg}_${cluster}_${location} --query id -o tsv) az role assignment create --assignee $clientId --role \"Network Contributor\" --scope $rg_id az role assignment create --assignee $clientId --role \"Network Contributor\" --scope $mc_rg_id\n\n```\n\nCreate the Gateway node pool:\n\n``` az aks nodepool add \\ --cluster-name $cluster \\ --name gwpool1 \\ --resource-group $rg \\ --mode gateway \\ --node-count 2 \\ --vnet-subnet-id $(az network vnet subnet show -g $rg --vnet-name $vnet_name -n aks-egress --query id -o tsv) \\ --gateway-prefix-size 30\n\n```\n\nOnce created, you can apply the `StaticGatewayConfiguration` as shown earlier in the post.\n\n## Conclusion\n\nAs you will have seen, the Static Egress Gateway feature removes almost all of the complexity from the design in the original blog post while giving you predictable outbound IPs that scale with your cluster and workloads. It provides the same outcome as before but with fewer moving parts and a model that is simpler to operate and automate.\n\nUpdated Jan 13, 2026\n\nVersion 1.0\n\n[!\\[pjlewis&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xOTk4MTU0LTUwMTc1NGlGNDMyREQwRDA2OTM5MDc0?image-dimensions=50x50)](/users/pjlewis/1998154) [pjlewis](/users/pjlewis/1998154) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined August 25, 2023\n\n[View Profile](/users/pjlewis/1998154)\n\n/category/azure/blog/azurearchitectureblog [Azure Architecture Blog](/category/azure/blog/azurearchitectureblog) Follow this blog board to get notified when there's new activity",
  "Tags": [],
  "Title": "Static Egress Gateway in AKS: The Native Way to Control Multiple Outbound IPs",
  "Author": "pjlewis",
  "OutputDir": "_community",
  "ProcessedDate": "2026-01-13 01:31:23",
  "Link": "https://techcommunity.microsoft.com/t5/azure-architecture-blog/static-egress-gateway-in-aks-the-native-way-to-control-multiple/ba-p/4484179",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Description": "## Introduction\n\nBack in late 2023, I published a blog post outlining [how to provision multiple egress IP addresses in AKS](https://techcommunity.microsoft.com/blog/azurearchitectureblog/provisioning-multiple-egress-ip-addresses-in-aks/3982130), using multiple node pools running in multiple subnets to achieve this outcome.\n\nIn July 2024, the AKS product group released the open-source [kube-egress-gateway](https://github.com/Azure/kube-egress-gateway) project to help customers provide multiple egress paths from their AKS clusters, and in September 2025 AKS added preview support for Static Egress Gateway, making the whole process simpler.\n\nThe purpose of this blog post is to re-visit the problem statement from the original blog post, and then look at how we can achieve this using the Static Egress Gateway feature built into AKS, and see how much simpler the modern solution is.\n\n## Problem Statement\n\nTo recap the original problem statement: some applications in a single AKS Cluster need **different outbound paths**, so you need a simple way to apply custom egress routing per workload to achieve this.\n\n## How Static Egress Gateway Improves on the 2023 Approach\n\nStatic Egress Gateway removes the architectural complexity required in the 2023 design by eliminating the need for multiple node pools, user defined routes and custom routing logic. Instead of mapping workloads to different subnets and maintaining separate outbound paths, the gateway provides a simple AKS native way to allocate predictable egress IPs to selected workloads. It also removes operational overhead because the gateway node pool owns the public IP prefix and handles translation without relying on cluster scale events. This gives a cleaner separation between compute and egress and is easier to automate and reason about. The result is the same outcome as the original post but achieved with fewer moving parts and a far more supportable model.\n\n## Proposed Solution\n\nThis time, instead of brewing our own solution, we will be using the [Static Egress Gateway](https://learn.microsoft.com/en-us/azure/aks/configure-static-egress-gateway) feature in AKS, as it is much simpler to implement, and more flexible too! This feature allows us to define multiple outbound egress pathways in a Kubernetes-native manner, without juggling node pools, UDRs (User Defined Routing), or custom routing logic.\n\nBefore we start, please be aware of the [limitations and considerations](https://learn.microsoft.com/en-us/azure/aks/configure-static-egress-gateway#limitations-and-considerations) related to using Static Egress Gateway.\n\nAs always, we start by defining some environment variables and creating a resource group to deploy our Azure resources into:\n\n- rg=egress-gw\nlocation=swedencentral vnet\\_name=vnet-egress-gw cluster=egress-gw vm\\_size=Standard\\_D4as\\_v6\n\naz group create -n $rg -l $location\n\nWe then create an AKS cluster, with the Static Egress Gateway feature enabled:\n- az aks create \\\n--name $cluster \\ --resource-group $rg \\ --location $location \\ --enable-static-egress-gateway\n\naz aks get-credentials -n $cluster -g $rg --overwrite\n\nIt's also possible to enable the Static Egress Gateway feature on an existing cluster, using the `az aks update` command, e.g.:\n- az aks update \\\n--name $cluster \\ --resource-group $rg \\ --enable-static-egress-gateway\n\nOnce the cluster is created (or the feature has been enabled), we need to create a dedicated gateway node pool, which will handle the egress traffic. The `--gateway-prefix-size` is the size of the public IP prefix to be applied to the gateway node pools (28-31), and will define or limit the number of gateway nodes you can scale out in the gateway node pool.\n- az aks nodepool add \\\n--cluster-name $cluster \\ --name gwpool1 \\ --resource-group $rg \\ --mode gateway \\ --node-count 2 \\ --gateway-prefix-size 30\n\n***Note: Gateway node pools should not be used for general-purpose workloads, and should be reserved for egress traffic only.***\n\nWith the gateway node pool created, we can connect to our cluster and deploy a `StaticGatewayConfiguration` custom resource. The `StaticGatewayConfiguration` CRD tells AKS which gateway node pool to use and which IP prefix to assign to the outbound traffic. A `StaticGatewayConfiguration` custom resource looks something like this:\n- apiVersion: egressgateway.kubernetes.azure.com/v1alpha1\nkind: StaticGatewayConfiguration metadata: name: egress-gw-1 namespace: default spec: gatewayNodepoolName: gwpool1 excludeCidrs: # Optional\n- 10.0.0.0/8\n- 172.16.0.0/12\n- 169.254.169.254/32\n# publicIpPrefixId: /subscriptions//resourceGroups//providers/Microsoft.Network/publicIPPrefixes/ # Optional\n\n***Note: In this example I excluded the `publicIpPrefixId` argument, so the AKS cluster will create a Public IP Prefix Azure resource for me automatically. If you already have a Public IP Prefix you would like to use, uncomment the line and set the resource ID accordingly.***\n\nWe can view the Public IP Prefix that has been provisioned for us by describing the `StaticGatewayConfiguration` resource we deployed. The Public IP Prefix can take a while to deploy and be configured, so you may need to run the command below a few times to see similar output.\n- $ kubectl describe StaticGatewayConfiguration egress-gw-1 -n default\nName: egress-gw-1 Namespace: default Labels: Annotations: API Version: egressgateway.kubernetes.azure.com/v1alpha1 Kind: StaticGatewayConfiguration Metadata: Creation Timestamp: 2026-01-07T11:34:17Z Finalizers: static-gateway-configuration-controller.microsoft.com Generation: 2 Resource Version: 15543 UID: de7642b6-c2fe-47cc-b478-f7c71c5db402 Spec: Default Route: staticEgressGateway Exclude Cidrs: 10.0.0.0/8 172.16.0.0/12 169.254.169.254/32 Gateway Nodepool Name: gwpool1 Gateway Vmss Profile: Provision Public Ips: true Status: Egress Ip Prefix: 20.91.186.64/30 Gateway Server Profile: Ip: 10.224.0.9 Port: 6000 Private Key Secret Ref: API Version: v1 Kind: Secret Name: sgw-de7642b6-c2fe-47cc-b478-f7c71c5db402 Namespace: aks-static-egress-gateway Public Key: FgeNumtkWbWnIGebcY1C/Ul19AmI1mLGf5DSMze5KBE= Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Reconciling 14m (x5 over 14m) staticGatewayConfiguration-controller StaticGatewayConfiguration provisioned with egress prefix Normal ReconcileGatewayLBConfigurationSuccess 13m (x4 over 14m) gatewayLBConfiguration-controller GatewayLBConfiguration reconciled Normal Reconciled 13m (x2 over 13m) staticGatewayConfiguration-controller StaticGatewayConfiguration provisioned with egress prefix 20.91.186.64/30 Normal ReconcileGatewayVMConfigurationSuccess 13m (x2 over 13m) gatewayVMConfiguration-controller GatewayVMConfiguration reconciled\n\nWith all the relevant pre-requisites in place, we can now configure and deploy an app to make use of this Static Egress Gateway. As in the previous blog post, we will be deploying the API component of [YADA: Yet Another Demo App](https://github.com/Microsoft/YADA) with a public-facing `LoadBalancer` (inbound) service, so that we can access the YADA app, which will show us our outbound IP addresses.\n\nCopy the [sample manifest from the GitHub repository](https://github.com/microsoft/YADA/blob/main/deploy/k8s.md#api), and update it to look like my two examples below.\n\nYADA API app using the default cluster egress (`yada-api-default.yaml` ):\n- kubectl apply -f -\n\nYADA API app using the Static Egress Gateway (`yada-api-egressgw.yaml` ):\n- kubectl apply -f -\n\nNow we deploy both apps into our AKS cluster:\n- kubectl apply -f yada-api-default.yaml\nkubectl apply -f yada-api-egressgw.yaml\n\nUnlike in our cluster from my [2023 blog post](https://techcommunity.microsoft.com/blog/azurearchitectureblog/provisioning-multiple-egress-ip-addresses-in-aks/3982130), if we view the pods we will find that they are running in the **same** node pool:\n- $ kubectl get pods -o wide\nNAME READY STATUS IP NODE api-default-7f4d8c5ccc-vcpxz 1/1 Running 10.244.0.166 aks-nodepool1-31740631-vmss000000 api-egressgw-6f45f5d8cc-xrfsm 1/1 Running 10.244.2.234 aks-nodepool1-31740631-vmss000002\n\nAnd now we can use determine the **inbound** IP address for each `yada` service, and then use cURL to query the YADA API to view the **outbound** IP address for each `yada` pod we have deployed:\n- echo \"default: svc IP=$(kubectl get svc yada-default -o jsonpath='{.status.loadBalancer.ingress[0].ip}'), egress IP=$(curl -s http://$(kubectl get svc yada-default -o jsonpath='{.status.loadBalancer.ingress[0].ip}'):8080/api/ip | jq -r '.my\\_public\\_ip')\"\necho \"egressgw: svc IP=$(kubectl get svc yada-egressgw -o jsonpath='{.status.loadBalancer.ingress[0].ip}'), egress IP=$(curl -s http://$(kubectl get svc yada-egressgw -o jsonpath='{.status.loadBalancer.ingress[0].ip}'):8080/api/ip | jq -r '.my\\_public\\_ip')\"\n\nYou should see some output like this, demonstrating that each `yada` service has a unique **inbound** IP address (as expected with `LoadBalancer` services), as well as unique **outbound** IP addresses too:\n- default: svc IP=135.116.221.253, egress IP=4.165.56.119\negressgw: svc IP=135.116.255.200, egress IP=20.91.186.64\n\nIf we were to add more egress gateways, and configure additional workloads to use these gateways, we would see each Static Egress Gateway providing a new, unique set of outbound IP addresses.\n\n### Keeping Egress IPs Private\n\nIn some scenarios you may want or need to egress traffic on private IP addresses. You can do this by enabling private IP support on the gateway node pool. To do this, specify the `--vm-set-type VirtualMachines` parameter when creating the node pool, e.g.:\n- az aks nodepool add \\\n--cluster-name $cluster \\ --name privgwpool1 \\ --resource-group $rg \\ --mode gateway \\ --node-count 2 \\ --vm-set-type VirtualMachines \\ --gateway-prefix-size 30\n\n***Note: At the time of writing, the private gateway deployment path is not functioning correctly. This should be resolved by the end of January once a fix has been rolled out.***\n\nWith this configuration, the `provisionPublicIps=false` setting keeps the private IPs allocated to the gateway nodes for the lifetime of the `StaticGatewayConfiguration` .\n\n### BYO Subnets\n\nAs you'll have noticed, so far we have used the default resources and configurations created automatically by AKS. For many customers this is fine. For others, the solution must integrate with their existing network architecture, and they will need to opt for Bring Your Own Networking. The Static Egress Gateway feature works well with BYO network configurations, but there are a few requirements to be aware of when the cluster runs inside a hub-and-spoke or centrally-managed IP space.\n\n#### Network Requirements\n\nWhen using BYO subnets there are some important constraints:\n\n- The gateway node pool must be deployed into a subnet with enough free IPs to host the Public IP Prefix assigned to the gateway.\n- The gateway subnet must not contain UDRs that redirect egress‑gateway traffic to a firewall before translation occurs.\n- Pod CIDRs, Service CIDRs, and the Public IP Prefix used by the gateway must not overlap with any internal ranges.\n\n#### Deploying the Resources\n\nHere is a minimal example using AZ CLI to create a BYO network layout suitable for Static Egress Gateway. Start by creating the Vnet and the subnets:\n- az network vnet create \\\n--name $vnet\\_name \\ --resource-group $rg \\ --address-prefixes 10.240.0.0/16 \\ --subnet-name aks-system \\ --subnet-prefixes 10.240.0.0/22 \\ --location $location \\ --output none\n\naz network vnet subnet create \\ --vnet-name $vnet\\_name \\ --resource-group $rg \\ --address-prefix 10.240.4.0/22 \\ --name aks-user \\ --output none\n\naz network vnet subnet create \\ --vnet-name $vnet\\_name \\ --resource-group $rg \\ --address-prefix 10.240.8.0/24 \\ --name aks-egress \\ --output none\n\nDeploy an AKS cluster into the BYO Subnets:\n- az aks create \\\n--resource-group $rg \\ --name $cluster \\ --location $location \\ --enable-static-egress-gateway \\ --vnet-subnet-id $(az network vnet subnet show -g $rg --vnet-name $vnet\\_name -n aks-system --query id -o tsv) \\ --nodepool-name systempool1 \\ --node-count 2 \\ --node-vm-size $vm\\_size \\ --network-plugin azure \\ --network-dataplane=azure \\ --output none\n\naz aks get-credentials -n $cluster -g $rg --overwrite\n\nAdd a node pool for user workloads:\n- az aks nodepool add \\\n--cluster-name $cluster \\ --name userpool1 \\ --resource-group $rg \\ --node-count 2 \\ --vnet-subnet-id $(az network vnet subnet show -g $rg --vnet-name $vnet\\_name -n aks-user --query id -o tsv)\n\nUpdate the system-managed AKS identity with the \"Network Contributor\" role so it can configure the customer-managed subnets:\n- clientId=$(az aks show --name $cluster --resource-group $rg --query identity.principalId --output tsv)\nrg\\_id=$(az group show --name $rg --query id -o tsv) mc\\_rg\\_id=$(az group show --name MC\\_${rg}\\_${cluster}\\_${location} --query id -o tsv) az role assignment create --assignee $clientId --role \"Network Contributor\" --scope $rg\\_id az role assignment create --assignee $clientId --role \"Network Contributor\" --scope $mc\\_rg\\_id\n\nCreate the Gateway node pool:\n- az aks nodepool add \\\n--cluster-name $cluster \\ --name gwpool1 \\ --resource-group $rg \\ --mode gateway \\ --node-count 2 \\ --vnet-subnet-id $(az network vnet subnet show -g $rg --vnet-name $vnet\\_name -n aks-egress --query id -o tsv) \\ --gateway-prefix-size 30\n\nOnce created, you can apply the `StaticGatewayConfiguration` as shown earlier in the post.\n\n## Conclusion\n\nAs you will have seen, the Static Egress Gateway feature removes almost all of the complexity from the design in the original blog post while giving you predictable outbound IPs that scale with your cluster and workloads. It provides the same outcome as before but with fewer moving parts and a model that is simpler to operate and automate.",
  "PubDate": "2026-01-13T00:22:15+00:00"
}
