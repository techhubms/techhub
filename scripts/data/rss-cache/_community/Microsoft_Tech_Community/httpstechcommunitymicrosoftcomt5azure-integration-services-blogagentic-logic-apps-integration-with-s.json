{
  "PubDate": "2026-02-17T19:10:38+00:00",
  "FeedName": "Microsoft Tech Community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "EnhancedContent": "## This two-part series presents an end-to-end Azure Logic Apps implementation that integrates the SAP built-in connector with an AI-assisted validation and analysis pipeline. The intent is twofold: (1) complement existing SAP connector documentation by showing the SAP configuration and contracts in real integration context, and (2) move beyond basic connectivity by using AI to produce more actionable results from the same data flow.\nIn the scenario, a Logic App sends CSV documents to an SAP system and receives back a structured analysis document (market trends, predictions, and recommendations) computed only from data that passes pre-defined business rules. Data is exchanged between SAP and Logic Apps using SAP RFCs, with results and failures returned to SAP in a consistent shape and summarized to end users via email. Part 1 focuses on the SAP ↔ Logic Apps integration mechanics—how data moves and how errors propagate. Part 2 covers the AI layer—how validation is performed using an agent loop and how insights are generated through an OpenAI-backed API connection. The approach is intentionally generic and reusable and serves as a starter pattern for new implementations or migrations from platforms such as BizTalk.\n\n### **1. Introduction**\n\nWhen you integrate Azure Logic Apps with SAP, the “hello world” part is usually easy. The part that bites you later is data quality. In SAP-heavy flows, validation isn’t a nice-to-have — it’s what makes the downstream results meaningful. If invalid data slips through, it can get expensive fast: you may create incorrect business documents, trigger follow-up processes**,** and end up in a cleanup path that’s harder (and more manual) than building validation upfront. And in “all-or-nothing” transactional patterns, things get even more interesting: one bad record can force a rollback strategy, compensating actions, or a whole replay/reconciliation story you didn’t want to own. See for instance [Handling Errors in SAP BAPI Transactions | Microsoft Community Hub](https://techcommunity.microsoft.com/blog/integrationsonazureblog/handling-errors-in-sap-bapi-transactions/1909185) to get an idea of the complexity in a BizTalk context.\n\nThat’s the motivation for this post: a practical starter pattern that you can adapt to many data shapes and domains for validating data in a Logic Apps + SAP integration.\n\n| **Note: For the full set of assets used here, see the [companion GitHub repository](https://github.com/Azure/logicapps/tree/master/agentic-sap-workflows) (workflows, schemas, SAP ABAP code, and sample files).** | | --- |\n\n##### **Scenario overview**\n\nThe scenario is intentionally simple, but it mirrors what shows up in real systems:\n\n1. A Logic App workflow sends CSV documents to an SAP endpoint.\n2. SAP forwards the payload to a second Logic App workflow that performs:\n- **rule-based validation** (based on pre-defined rules)\n- **analysis/enrichment** (market trends, predictions, recommendations)\n3. The workflow either:\n- returns validated results (or validation errors) to the initiating workflow, or\n- persists outputs for later use\n\nFor illustration, I’m using fictitious retail data. The content is made up, but the mechanics are generic: the same approach works for orders, inventory, pricing, master data feeds, or any “file in → decision out” integration. You’ll see sample inputs and outputs below to keep the transformations concrete.\n\n**Figure: Input CSV data and corresponding rules**\n\n**Figure: Outputs - Analysis of trends and predictions, validation summary and IDoc information.**\n\n##### **What this post covers**\n\nThis walkthrough focuses on the integration building blocks that tend to matter in production:\n\n- Calling SAP RFCs from Logic App workflows, and invoking Logic App workflows from SAP function modules\n- Using the Logic Apps SAP built-in trigger\n- Receiving and processing IDocs\n- Returning responses and exceptions back to SAP in a structured, actionable way\n- Data manipulation patterns in Logic Apps, including:\n- parsing and formatting\n- inline scripts\n- XPath (where it fits, and where it becomes painful).\n\n##### **Overall Implementation**\n\nA high-level view of the implementation is shown below. The source workflow handles end-to-end ingestion—file intake, transformation, SAP integration, error handling, and notifications—using Azure Logic Apps. The destination workflows focus on validation and downstream processing, including AI-assisted analysis and reporting, with robust exception handling across multiple technologies. I’ll cover the AI portion in a follow-up post.\n\n**Figure: Overall implementation.**\n\n| **Note on AI-assisted development**<br><br><br><br>Most of the workflow “glue” in this post—XPath, JavaScript snippets, and Logic Apps expressions—was built with help from Copilot and the AI assistant in the designer (see [Get AI-assisted help for Standard workflows - Azure Logic Apps | Microsoft Learn](https://learn.microsoft.com/en-us/azure/logic-apps/workflow-assistant-standard)). In my experience, this is exactly where AI assistance pays off: generating correct scaffolding quickly, then iterating based on runtime behavior.<br><br><br><br>I’ve also included SAP ABAP snippets for the SAP-side counterpart. You don’t need advanced ABAP knowledge to follow along; the snippets are deliberately narrow and integration-focused. I include them because it’s hard to design robust integrations if you only understand one side of the contract. When you understand how SAP expects to receive data, how it signals errors, and where transactional boundaries actually are, you end up with cleaner workflows and fewer surprises. | | --- |\n\n### **2. Source Workflow**\n\nThis workflow is a small, end‑to‑end “sender” pipeline: it reads a CSV file from Azure Blob Storage, converts the rows into the SAP table‑of‑lines XML shape expected by an RFC, calls `Z_GET_ORDERS_ANALYSIS` via the SAP connector, then extracts analysis or error details from the RFC response and emails a single consolidated result.\n\nAt a high level:\n\n- **Input**: an HTTP request (used to kick off the run) + a blob name.\n- **Processing**: CSV → array of rows → XML (…) → RFC call\n- **Output**: one email containing either:\n- the analysis (success path), or\n- a composed error summary (failure path).\n\nThe diagram below summarizes the sender pipeline: HTTP trigger → Blob CSV (header included) → rows → SAP RFC → parse response → email.\n\n**Figure: End‑to‑end sender pipeline**\n\nTwo design choices are doing most of the work here. First, the workflow keeps the CSV transport contract stable by sending the file as a verbatim list of lines—including the header—wrapped into `… ` elements under `IT_CSV` . Second, it treats the RFC response as the source of truth: `EXCEPTIONMSG` and `RETURN/MESSAGE` drive a single `Has errors` gate, which determines whether the email contains the analysis or a consolidated failure summary.\n\n##### **Step-by-step description**\n\n| **Phase 0 — Trigger**<br><ol><br><li><strong>Trigger </strong>— <code>When_an_HTTP_request_is_received</code><br><br>The workflow is invoked via an HTTP request trigger (stateful workflow).</li><br><br></ol> | | --- | | **Phase 1 — Load and split the CSV**<br><ol><br><li><strong>Read file</strong> — <code>Read_CSV_orders_from_blob</code><br><br>Reads the CSV from container onlinestoreorders using the blob name from @parameters('DataFileName').</li><br><br><li><strong>Split into rows </strong>— <code>Extract_rows</code><br><br>Splits the blob content on \\r\\n, producing an array of CSV lines.</li><br><br></ol><br><br>Design note: Keeping the header row is useful when downstream validation or analysis wants column names, and it avoids implicit assumptions in the sender workflow. | | **Phase 2 — Shape the RFC payload**<br><ol><br><li><strong>Convert CSV rows to SAP XML </strong>— <code>Transform_CSV_to_XML</code><br><br>Uses JavaScript to wrap each CSV row (including the header line)&nbsp;into the SAP line structure and XML‑escape special characters. The output is an XML fragment representing a table of <code>ZTY_CSV_LINE</code><br> rows.</li><br><br></ol> | | **Phase 3 — Call SAP and extract response fields**<br><ol><br><li><strong>Call the RFC </strong>— [RFC]_<code>Z_GET_ORDERS_ANALYSIS</code><br><br>Invokes <code>Z_GET_ORDERS_ANALYSIS</code><br> with an XML body containing … built from the transformed rows.</li><br><br><li><strong>Extract error/status </strong>— <code>Save_EXCEPTION_message</code><br> and <code>Save_RETURN_message</code><br><br>Uses XPath to pull:<ul><br><li><code>EXCEPTIONMSG</code><br> from the RFC response, and</li><br><br><li>the structured <code>RETURN</code><br>/<code>MESSAGE</code><br> field.</li><br><br></ul><br></li><br><br></ol> | | **Phase 4 — Decide success vs failure and notify**<br><ol><br><li><strong>Initialize output buffer </strong>— <code>Initialize_email_body</code><br><br>Creates the EmailBody variable used by both success and failure cases.</li><br><br><li><strong>Gate </strong>— <code>Has_errors</code><br><br>Determines whether to treat the run as failed based on:<ul><br><li><code>EXCEPTIONMSG</code><br> being different from \"ok\", <strong>or</strong></li><br><br><li><code>RETURN</code><br>/<code>MESSAGE</code><br> being non‑empty.</li><br><br></ul><br></li><br><br><li><strong>Send result — Send_an_email_(V2)</strong><br>Emails either:<ul><br><li>the extracted <code>ANALYSIS</code><br> (success), or</li><br><br><li>a concatenated error summary including <code>RETURN</code><br>/<code>MESSAGE</code><br> plus message details (<code>MESSAGE_V1</code><br>…<code>MESSAGE_V4</code><br>) and <code>EXCEPTIONMSG</code><br>.</li><br><br></ul><br></li><br><br></ol> |\n\n**Note**: Because the header row is included in `IT_CSV` , the SAP-side parsing/validation treats the first line as column titles (or simply ignores it). The sender workflow stays “schema-agnostic” by design.\n\n##### **Useful snippets**\n\n**Snippet 1 — Split the CSV into rows**\n\n``` split(string(body('Read_CSV_orders_from_blob')?['content']), '\\r\\n')\n\nTip: If your CSV has a header row you don’t want to send to SAP, switch back to: @skip(split(string(body('Read_CSV_orders_from_blob')?['content']), '\\r\\n'), 1) ```\n\n**Snippet 2 — JavaScript transform: “rows → SAP table‑of‑lines XML”**\n\n``` const lines = workflowContext.actions.Extract_rows.outputs;\n\nfunction xmlEscape(value) { return String(value) .replace(/&/g, \"&\") .replace(//g, \">\") .replace(/\"/g, \"\"\") .replace(/'/g, \"'\"); }\n\n// NOTE: we don't want to keep empty lines (which can be produced by reading the blobs) // the reason being that if the recipient uses a schema to validate the xml, // it may reject it if it does not allow empty nodes. const xml = lines .filter(line => line && line.trim() !== '') // keep only non-empty lines .map(line => `<zty_csv_line><line>${xmlEscape(line)}</line></zty_csv_line>`) .join('');\n\nreturn { xml }; ```\n\n**Snippet 3 — XPath extraction of response fields (namespace-robust)**\n\n``` EXCEPTIONMSG: @xpath(body('[RFC]_Call_Z_GET_ORDERS_ANALYSIS')?['content'], 'string( /*[local-name()=\"Z_GET_ORDERS_ANALYSISResponse\"] /*[local-name()=\"EXCEPTIONMSG\"])')\n\nRETURN/MESSAGE: @xpath(body('[RFC]_Call_Z_GET_ORDERS_ANALYSIS')?['content'], 'string( /*[local-name()=\"Z_GET_ORDERS_ANALYSISResponse\"] /*[local-name()=\"RETURN\"] /*[local-name()=\"MESSAGE\"])') ```\n\n**Snippet 4 — Failure email body composition**\n\n``` concat( 'Error message: ', outputs('Save_RETURN_message'), ', details: ', xpath(body('[RFC]_Call_Z_GET_ORDERS_ANALYSIS')?['content'], 'string(//*[local-name()=\\\"MESSAGE_V1\\\"])'), xpath(body('[RFC]_Call_Z_GET_ORDERS_ANALYSIS')?['content'], 'string(//*[local-name()=\\\"MESSAGE_V2\\\"])'), xpath(body('[RFC]_Call_Z_GET_ORDERS_ANALYSIS')?['content'], 'string(//*[local-name()=\\\"MESSAGE_V3\\\"])'), xpath(body('[RFC]_Call_Z_GET_ORDERS_ANALYSIS')?['content'], 'string(//*[local-name()=\\\"MESSAGE_V4\\\"])'), '; ', 'Exception message: ', outputs('Save_EXCEPTION_message'), '.') ```\n\n### **3. SAP Support**\n\nTo make the SAP/Logic Apps boundary simple, I model the incoming CSV as a table of “raw lines” on the SAP side. The function module `Z_GET_ORDERS_ANALYSIS` exposes a single table parameter, `IT_CSV` , typed using a custom line structure.\n\nFigure: `IT_CSV` is a table of CSV lines (`ZTY_CSV_LINE` ), with a single `LINE` field (`CHAR2048` ).\n\n`IT_CSV` uses the custom structure `ZTY_CSV_LINE` , which contains a single component `LINE` (`CHAR2048` ). This keeps the SAP interface stable: the workflow can send CSV lines without SAP having to know the schema up front, and the parsing/validation logic can evolve independently.\n\nThe diagram below shows the plumbing that connects SAP to Azure Logic Apps in two common patterns: SAP sending IDocs to a workflow and SAP calling a remote-enabled endpoint via an RFC destination. I’m showing all three pieces together—the ABAP call site, the `SM59` RFC destination, and the Logic Apps SAP built-in trigger—because most “it doesn’t work” problems come down to a small set of mismatched configuration values rather than workflow logic.\n\n**Figure: SAP-to-Logic Apps wiring for inbound IDocs and RFC callbacks**\n\n**The key takeaway** is that both patterns hinge on the same contract: Program ID plus the SAP Gateway host/service. In SAP, those live in `SM59` (TCP/IP destination, registered server program). In Logic Apps, the SAP built-in trigger listens using the same Program ID and gateway settings, while the trigger configuration (for example, IDoc format and degree of parallelism) controls how messages are interpreted and processed. Once these values line up, the rest of the implementation becomes “normal workflow engineering”: validation, predictable error propagation, and response shaping.\n\nBefore diving into workflow internals, I make the SAP-side contract explicit. The function module interface below shows the integration boundary: CSV lines come in as `IT_CSV` , results come back as `ANALYSIS` , and status/error information is surfaced both as a human-readable `EXCEPTIONMSG` and as a structured `RETURN` (`BAPIRET2` ). I also use a dedicated exception (`SENDEXCEPTIONTOSAPSERVER` ) to signal workflow-raised failures cleanly.\n\nContract (what goes over RFC):\n\n- **Input:** `IT_CSV`\n(CSV lines)\n- **Outputs:** `ANALYSIS`\n(analysis payload), `EXCEPTIONMSG` (human-readable status)\n- **Return structure:** `RETURN`\n(`BAPIRET2` ) for structured SAP-style success/error\n- **Custom exception:** `SENDEXCEPTIONTOSAPSERVER`\nfor workflow-raised failures\n\nHere is the ABAP wrapper that calls the remote implementation and normalizes the result.\n\n``` FUNCTION z_get_orders_analysis. *\"---------------------------------------------------------------------- *\" This module acts as a caller wrapper. *\" Important: the remote execution is determined by DESTINATION. *\" Even though the function name is the same, this is not recursion: *\" the call runs in the remote RFC server registered under DESTINATION \"DEST\". *\"---------------------------------------------------------------------- *\" Contract: *\" TABLES it_csv \"CSV lines *\" IMPORTING analysis \"Result payload *\" EXPORTING exceptionmsg \"Human-readable status / error *\" CHANGING return \"BAPIRET2 return structure *\" EXCEPTIONS sendexceptiontosapserver *\"----------------------------------------------------------------------\n\nCALL FUNCTION 'Z_GET_ORDERS_ANALYSIS' DESTINATION dest IMPORTING analysis = analysis TABLES it_csv = it_csv CHANGING return = return EXCEPTIONS sendexceptiontosapserver = 1 system_failure = 2 MESSAGE exceptionmsg communication_failure = 3 MESSAGE exceptionmsg OTHERS = 4.\n\nCASE sy-subrc. WHEN 0. exceptionmsg = 'ok'.\n\n\"Optional: normalize success into RETURN for callers that ignore EXCEPTIONMSG IF return-type IS INITIAL. return-type = 'S'. return-message = 'OK'. ENDIF.\n\nWHEN 1. exceptionmsg = |Exception from workflow: SENDEXCEPTIONTOSAPSERVER { sy-msgv1 }{ sy-msgv2 }{ sy-msgv3 }{ sy-msgv4 }|.\n\nreturn-type = 'E'. return-message = exceptionmsg.\n\nWHEN 2 OR 3. \"system_failure / communication_failure usually already populate exceptionmsg IF exceptionmsg IS INITIAL. exceptionmsg = |RFC system/communication failure.|. ENDIF.\n\nreturn-type = 'E'. return-message = exceptionmsg.\n\nWHEN OTHERS. exceptionmsg = |Error in workflow: { sy-msgv1 }{ sy-msgv2 }{ sy-msgv3 }{ sy-msgv4 }|.\n\nreturn-type = 'E'. return-message = exceptionmsg. ENDCASE.\n\nENDFUNCTION. ```\n\nThe wrapper is intentionally small: it forwards the payload to the remote implementation via the RFC destination and then normalizes the outcome into a predictable shape. The point isn’t fancy ABAP — it’s reliability. With a stable contract (`IT_CSV, ANALYSIS, RETURN, EXCEPTIONMSG` ) the Logic Apps side can evolve independently while SAP callers still get consistent success/error semantics.\n\n| **Important**: in 'CALL FUNCTION 'Z\\_GET\\_ORDERS\\_ANALYSIS' DESTINATION dest' the name of the called function should be the same as the name of the ABAP wrapper function module, the reason being that the SAP built-in trigger in the logic app uses the function module signature as the contract (i.e. metadata). | | --- |\n\n**Figure: ABAP interface and error propagation for `Z_GET_ORDERS_ANALYSIS`.**\n\nTo sum up, the integration is intentionally shaped around three outputs: the raw input table (`IT_CSV` ), a standardized SAP return structure (`RETURN` / `BAPIRET2` ), and a readable status string (`EXCEPTIONMSG` ). The custom exception (`SENDEXCEPTIONTOSAPSERVER` ) gives me a clean way to surface workflow failures back into SAP without burying them inside connector-specific error payloads. This is depicted in the figure below.\n\n### **4. Destination Workflow**\n\nThe diagram below shows the destination workflow at a high level. I designed it as a staged pipeline: guard early, normalize input, validate, and then split the workload into two paths—operational handling of invalid records (notifications and optional IDoc remediation) and analysis of the validated dataset. Importantly, the SAP response is intentionally narrow: SAP receives only the final analysis (or a structured error), while validation details are delivered out-of-band via email.\n\n| How to read this diagram<br><ol><br><li><strong>Guardrail:</strong> <strong>Validate requested action</strong> ensures the workflow only handles the expected request.</li><br><br><li><strong>Normalize:</strong> <strong>Create CSV payload</strong> converts the inbound content into a consistent CSV representation.</li><br><br><li><strong>Validate:</strong> <strong>Data Validation Agent</strong> identifies invalid records (and produces a summary).</li><br><br><li><strong>Operational handling (invalid data):</strong> invalid rows are <strong>reported by email</strong> and may optionally be turned into <strong>IDocs</strong> (right-hand block).</li><br><br><li><strong>Analyze (valid data):</strong> <strong>Analyze data</strong> runs only on the validated dataset (invalid IDs excluded).</li><br><br><li><strong>Outputs:</strong> users receive <strong>Email analysis</strong>, while <strong>SAP receives only the analysis (or a structured error)</strong> via <strong>Respond to SAP server</strong>.</li><br><br></ol> | | --- |\n\nFigure: Destination workflow with staged validation, optional IDoc remediation, and an SAP response .\n\nReading the workflow top-to-bottom, the main design choice is separation of concerns. Validation is used to filter and operationalize bad records (notify humans, optionally create IDocs), while the SAP-facing response stays clean and predictable: SAP receives the final analysis for the validated dataset, or an error if the run can’t complete. This keeps the SAP contract stable even as validation rules and reporting details evolve.\n\n##### **Step‑by‑step walkthrough**\n\n| **Phase 0 — Entry and routing**<br><ol><br><li><strong>Trigger </strong>— When a message is received<br>The workflow starts when an inbound SAP message is delivered to the Logic Apps SAP built‑in trigger.</li><br><br><li><strong>Guardrail </strong>— Validate requested action (2 cases)<br>The workflow immediately checks whether the inbound request is the operation it expects (for example, the function/action name equals <code>Z_GET_ORDERS_ANALYSIS</code>).<ul><br><li>If the action does not match: the workflow sends an exception back to SAP describing the unexpected action and terminates early (fail fast).</li><br><br><li>If the action matches: processing continues.</li><br><br></ul><br></li><br><br></ol> | | --- | | **Phase 1 — Normalize input into a workflow‑friendly payload**<br><ol><br><li><strong>Prepare input</strong> — Create CSV payload<br>The workflow extracts CSV lines from the inbound (XML) SAP payload and normalizes them into a consistent CSV text payload that downstream steps can process reliably.</li><br><br><li><strong>Initialize validation state </strong>— Initialize array of invalid order ids<br>The workflow creates an empty array variable to capture order IDs that fail validation. This becomes the “validation output channel” used later for reporting, filtering, and optional remediation.</li><br><br></ol> | | **Phase 2 — Validate the dataset (AI agent loop)**<br><ol><br><li><strong>Validate </strong>— Data Validation Agent (3 cases)<br>This stage performs rule‑based validation using an agent pattern (backed by Azure OpenAI). Conceptually, it does three things (as shown in the diagram’s expanded block):<ul><br><li>Get validation rules: retrieves business rules from a SharePoint‑hosted validation document.</li><br><br><li>Get CSV payload: loads the normalized CSV created earlier.</li><br><br><li>Summarize CSV payload review: evaluates the CSV against the rules and produces structured validation outputs.</li><br><br></ul>Outputs produced by validation:<ul><br><li>A list of invalid order IDs</li><br><br><li>The corresponding invalid CSV rows</li><br><br><li>A human‑readable validation summary</li><br><br></ul><br></li><br><br></ol><br><br>**Note**: The detailed AI prompt/agent mechanics are covered in **Part 2**. In **Part 1**, the focus is on the integration flow and how data moves. | | **Phase 3 — Operational handling of invalid records (email + optional SAP remediation)**<br><br>After validation, the workflow treats invalid records as an operational concern: they are reported to humans and can optionally be routed into an SAP remediation path. This is shown in the right‑hand “Create IDocs” block.<br><ol><br><li><strong>Notify </strong>— Send verification summary<br>The workflow sends an email report (Office 365) to configured recipients containing:<ul><br><li>the validation summary</li><br><br><li>the invalid order IDs</li><br><br><li>the invalid CSV payload (or the subset of invalid rows)</li><br><br></ul><br></li><br><br><li><strong>Transform </strong>— Transform CSV to XML<br>The workflow converts the invalid CSV lines into an XML format that is suitable for SAP processing.</li><br><br><li><strong>Optional remediation </strong>— [RFC] Create all IDocs (conditional)<br>If the workflow parameter (for example, CreateIDocs) is enabled, the workflow calls an SAP RFC (e.g., <code>Z_CREATE_ONLINEORDER_IDOC</code><br>) to create IDocs from the transformed invalid data.</li><br><br></ol><br><br>**Why this matters**: Validation results are made visible (email) and optionally actionable (IDocs), without polluting the primary analysis response that SAP receives. | | **Phase 4 — Analyze only the validated dataset (AI analysis)**<br><br><br><br>The workflow runs AI analysis on the validated dataset, explicitly excluding invalid order IDs discovered during the validation phase. The analysis prompt instructs the model to produce outputs such as trends, predictions, and recommendations.<br><br><br><br>**Note**: The AI analysis prompt design and output shaping are covered in **Part 2**. | | **Phase 5 — Post‑process the AI response and publish outputs**<br><ol><br><li><strong>Package results </strong>— Process analysis results (Scope)<br>The workflow converts the AI response into a format suitable for email and for SAP consumption:<ul><br><li>Parse the OpenAI JSON response</li><br><br><li>Extract the analysis content</li><br><br><li>Convert markdown → <code>HTML</code><br> using custom JavaScript formatting</li><br><br></ul><br></li><br><br><li><strong>Outputs</strong><ul><br><li>Email analysis: sends the formatted analysis to recipients.</li><br><br><li>Respond to SAP server: returns only the analysis (and errors) to SAP.</li><br><br></ul><br></li><br><br></ol><br><br>**Key design choice**: SAP receives a clean, stable contract—analysis on success, structured error on failure. Validation details are handled out‑of‑band via email (and optionally via IDoc creation).<br><br><br><br>**Note**: the analysis email sent by the destination workflow is there for testing purposes, to verify that the html content remains the same as it is sent back to the source workflow. |\n\n##### **Useful snippets**\n\n**Snippet 1 - Join each CSV line in the XML to make a CSV table:**\n\n``` join( xpath( xml(triggerBody()?['content']), '/*[local-name()=\\\"Z_GET_ORDERS_ANALYSIS\\\"] /*[local-name()=\\\"IT_CSV\\\"] /*[local-name()=\\\"ZTY_CSV_LINE\\\"] /*[local-name()=\\\"LINE\\\"]/text()' ), '\\r\\n')\n\n```\n\n**Note**: For the sake of simplicity, XPath is used here and throughout all places where XML is parsed. In the general case however, the **Parse XML with schema** action is the better and recommended way to strictly enforce the data contract between senders and receivers. More information about Parse XML with schema is provided inAppendix 1.\n\n**Snippet 2 - Format markdown to html (simplified):**\n\n``` const raw = workflowContext.actions.Extract_analysis.outputs;\n\n// Basic HTML escaping for safety (keeps <code> blocks clean) const escapeHtml = s => s.replace(/[&<>\"]/g, c => ({'&':'&','<':'<','>':'>','\"':'\"'}[c]));\n\n// Normalize line endings let md = raw; // raw.replace(/\\r\\n/g, '\\n').trim();\n\n// Convert code blocks (``` ... ```) md = md.replace(/```([\\s\\S]*?)```/g, (m, p1) => `<pre><code>${escapeHtml(p1)}</code></pre>`);\n\n// Horizontal rules --- or *** md = md.replace(/(?:^|\\n)---+(?:\\n|$)/g, '<hr/>');\n\n// Headings ###### to # for (let i = 6; i >= 1; i--) { const re = new RegExp(`(?:^|\\\\n)${'#'.repeat(i)}\\\\s+(.+?)\\\\s*(?=\\\\n|$)`, 'g'); md = md.replace(re, (m, p1) => `<h${i}>${p1.trim()}</h${i}>`); }\n\n// Bold and italic md = md.replace(/\\*\\*([^*]+)\\*\\*/g, '<strong>$1</strong>'); md = md.replace(/\\*([^*]+)\\*/g, '<em>$1</em>');\n\n// Unordered lists (lines starting with -, *, +) md = md.replace(/(?:^|\\n)([-*+]\\s.+(?:\\n[-*+]\\s.+)*)/g, (m) => { const items = m.trim().split(/\\n/).map(l => l.replace(/^[-*+]\\s+/, '').trim()); return '\\n<ul>' + items.map(i => `<li>${i}</li>`).join('\\n') + '</ul>'; });\n\n// Paragraphs: wrap remaining text blocks in <p>...</p> const blocks = md.split(/\\n{2,}/).map(b => { if (/^<h\\d>|^<ul>|^<pre>|^<hr\\/>/.test(b.trim())) return b; return `<p>${b.replace(/\\n/g, '<br/>')}</p>`; }); const html = blocks.join('');\n\nreturn { html }; ```\n\n### **5. Exception Handling**\n\nTo illustrate exception handling, we supposed that multiple workflows may listen to the same program id (by design or unexpectedly) and could therefore receive messages that were meant for others. So the first thing that happens is validate that the function name is as expected. It is shown below.\n\nIn this section I show three practical ways to surface workflow failures back to SAP using the Logic Apps action “Send exception to SAP server”, and the corresponding ABAP patterns used to handle them. The core idea is the same in all three: Logic Apps raises an exception on the SAP side, SAP receives it as an RFC exception, and your ABAP wrapper converts that into something predictable (for example, a readable `EXCEPTIONMSG` , a populated `RETURN` , or both). The differences are in how much control you want over the exception identity and whether you want to leverage SAP message classes for consistent, localized messages.\n\n#### **5.1 Default exception**\n\nThis first example shows the default behavior of Send exception to SAP server. When the action runs without a custom exception name configuration, the connector raises a pre-defined exception that can be handled explicitly in ABAP.\n\nOn the Logic Apps side, the action card “Send exception to SAP server” sends an Exception Error Message (for example, “Unexpected action in request: …”). On the ABAP side, the RFC call lists `SENDEXCEPTIONTOSAPSERVER` = 1 under `EXCEPTIONS` , and the code uses `CASE` `sy-subrc` to map that exception to a readable message.\n\n**Figure: Default exception**\n\n**The key takeaway** is that you get a reliable “out-of-the-box” exception path: ABAP can treat `sy-subrc` = 1 as the workflow‑raised failure and generate a consistent `EXCEPTIONMSG` . This is the simplest option and works well when you don’t need multiple exception names—just one clear “workflow failed” signal.\n\n#### **5.2 Message exception**\n\nIf you want more control than the default, you can configure the action to raise a named exception declared in your ABAP function module interface. This makes it easier to route different failure types without parsing free-form text.\n\nThe picture shows Advanced parameters under the Logic Apps action, including “Exception Name” with helper text indicating it must match an exception declared in the ABAP function module definition.\n\n**Figure: Message exception**\n\nThis option is useful when you want to distinguish workflow error categories (e.g., validation vs. routing vs. downstream failures) using exception identity, not just message text. The contract stays explicit: Logic Apps raises a named exception, and ABAP can branch on that name (or on `sy-subrc` mapping) with minimal ambiguity.\n\n#### **5.3 Message class exception**\n\nThe third approach uses SAP’s built-in message class mechanism so that the exception raised by the workflow can map cleanly into SAP’s message catalog (`T100` ). This is helpful when you want consistent formatting and localization aligned with standard SAP patterns.\n\nOn the Logic Apps side, the action shows advanced fields including Message Class, Message Number, and an Is ABAP Message toggle, with helper text stating the message class can come from message maintenance (`SE91` ) or be custom. On the ABAP side, the code highlights an error-handling block that calls using `sy-msgid` , `sy-msgno` , and variables `sy-msgv1` …`sy-msgv4` , then stores the resulting text in `EXCEPTIONMSG` .\n\n**Figure: Message class exception**\n\nThis pattern is ideal when you want workflow exceptions to look and behave like “native” SAP messages. Instead of hard-coding strings, you rely on the message catalog and let ABAP produce a consistent final message via `FORMAT_MESSAGE`. The result is easier to standardize across teams and environments—especially if you already manage message classes as part of your SAP development process.\n\nRefer to Appendix 2 for further information on `FORMAT_MESSAGE`.\n\n#### **5.4 Choosing an exception strategy that SAP can act on**\n\nAcross these examples, the goal is consistent: treat workflow failures as first‑class outcomes in SAP, not as connector noise buried in run history. The Logic Apps action Send exception to SAP server gives you three increasingly structured ways to do that, and the “right” choice depends on how much semantics you want SAP to understand.\n\n- Default exception (lowest ceremony): Use this when you just need a reliable “workflow failed” signal. The connector raises a pre-defined exception name (for example, `SENDEXCEPTIONTOSAPSERVER`\n), and ABAP can handle it with a simple `EXCEPTIONS` … = 1 mapping and a `sy-subrc` check. This is the fastest way to make failures visible and deterministic.\n- Named exception(s) (more routing control): Use this when you want SAP to distinguish failure *types* without parsing message text. By raising an exception name declared in the ABAP function module interface, you can branch cleanly in ABAP (or map to different return handling) and keep the contract explicit and maintainable.\n- Message class + number (most SAP-native): Use this when you want errors to look and behave like standard SAP messages—consistent wording, centralized maintenance, and better alignment with SAP operational practices. In this mode, ABAP can render the final localized string using `FORMAT_MESSAGE`\nand return it as `EXCEPTIONMSG` (and optionally `BAPIRET2` -`MESSAGE` ), which makes the failure both human-friendly and SAP-friendly.\n\nA practical rule of thumb: start with the default exception while you stabilize the integration, move to named exceptions when you need clearer routing semantics, and adopt message classes when you want SAP-native error governance (standardization, maintainability, and localization). Regardless of the option, the key is to end with a predictable SAP-side contract: a clear success path, and a failure path that produces a structured return and a readable message.\n\n### **6. Response Handling**\n\nThis section shows how the destination workflow returns either a successful analysis response or a workflow exception back to SAP, and how the source (caller) workflow interprets the RFC response structure to produce a single, human‑readable outcome (an email body). The key idea is to keep the SAP-facing contract stable: SAP always returns a `Z_GET_ORDERS_ANALYSISResponse` envelope, and the caller workflow decides between success and error using just two fields: `EXCEPTIONMSG` and `RETURN` /`MESSAGE` . To summarize the steps:\n\n1. **Destination workflow**either:\n- sends a normal response via **Respond to SAP server**, or\n- raises an exception via **Send exception to SAP server** (with an error message).\n2. **SAP server**exposes those outcomes through the RFC wrapper:\n- `sy-subrc`\n= 0 → success (`EXCEPTIONMSG` = 'ok')\n- `sy-subrc`\n= 1 → workflow exception (`SENDEXCEPTIONTOSAPSERVER` )\n- `sy-subrc`\n= 2/3 → system/communication failures\n3. **Source workflow**calls the RFC, extracts:\n- `EXCEPTIONMSG`\n- `RETURN`\n/`MESSAGE` and uses an **Has errors** gate to choose between a success email body (analysis) or a failure email body (error summary).\n\nThe figure below shows the full return path for results and failures. On the right, the destination workflow either responds normally (Respond to SAP server) or raises a workflow exception (Send exception to SAP server). SAP then maps that into the RFC outcome (`sy-subrc` and message fields). On the left, the source workflow parses the RFC response structure and populates a single EmailBody variable using two cases: failure (error details) or success (analysis text).\n\n**Figure: Response/exception flow**\n\nTwo things make this pattern easy to operationalize. First, the caller workflow does not need to understand every SAP field—only `EXCEPTIONMSG` and `RETURN` /`MESSAGE` are required to decide success vs failure. Second, the failure path intentionally aggregates details (`MESSAGE_V1` …`MESSAGE_V4` plus the exception text) into a single readable string so errors don’t get trapped in run history.\n\n**Callout**: The caller workflow deliberately treats `EXCEPTIONMSG` != \"ok\" or `RETURN` /`MESSAGE` present as the single source of truth for failure, which keeps the decision logic stable even if the response schema grows.\n\n##### **Detailed description**\n\n| **Phase 1 — Destination workflow: choose “response” vs “exception”**<br><ul><br><li><br><p><strong>Respond to SAP server</strong> returns the normal response payload back to SAP.</p><br><br></li><br><br><li><br><p><strong>Send exception to SAP server</strong> raises a workflow failure with an Exception Error Message (the screenshot shows an example beginning with “Unexpected action in request:” and a token for Function Name).</p><br><br></li><br><br></ul><br><br>**Outcome**: SAP receives either a normal response or a raised exception for the RFC call. | | --- | | **Phase 2 — SAP server: map workflow outcomes to RFC results**<br><br><br><br>The SAP-side wrapper code shown in the figure calls:<br><ul><br><li><code>CALL</code><br> <code>FUNCTION</code><br> '<code>Z_GET_ORDERS_ANALYSIS</code><br>' <code>DESTINATION</code><br> <code>DEST</code><br> ...</li><br><br><li>It declares exception mappings including:<ul><br><li><code>SENDEXCEPTIONTOSAPSERVER</code><br> = 1</li><br><br><li><code>system_failure</code><br> = 2 <code>MESSAGE</code><br> <code>EXCEPTIONMSG</code><br></li><br><br><li><code>communication_failure</code><br> = 3 <code>MESSAGE</code><br> <code>EXCEPTIONMSG</code><br></li><br><br><li><code>OTHERS</code><br> = 4</li><br><br></ul><br></li><br><br></ul><br><br>Then it uses `CASE`<br> `sy-subrc`<br>. to normalize outcomes (the figure shows `WHEN`<br> 0. setting `EXCEPTIONMSG`<br> = 'ok'., and `WHEN`<br> 1. building a readable message for the workflow exception).<br><br><br><br>**Outcome**: regardless of why it failed, SAP can provide a consistent set of fields back to the caller: a return structure and an exception/status message. | | **Phase 3 — Source workflow: parse response and build one “email body”**<br><br><br><br>After the RFC action ([RFC] Call Z GET `ORDERS`<br> `ANALYSIS`<br>) the source workflow performs:<br><ol><br><li><strong>Save <code>EXCEPTION</code><br> message</strong><br>Extracts <code>EXCEPTIONMSG</code><br> from the response XML using XPath.</li><br><br><li><strong>Save <code>RETURN</code><br> message</strong><br>Extracts <code>RETURN</code><br>/<code>MESSAGE</code><br> from the response XML using XPath.</li><br><br><li><strong>Initialize email body</strong><br>Creates EmailBody once, then sets it in exactly one of two cases.</li><br><br><li><strong>Has errors</strong> (two cases)<br>The condition treats the run as “error” if either:<ul><br><li><code>EXCEPTIONMSG</code><br> is not equal to \"ok\", <strong>or</strong></li><br><br><li><code>RETURN</code><br>/<code>MESSAGE</code><br> is not empty.</li><br><br></ul><br></li><br><br><li><strong>Set email body (failure)</strong> / <strong>Set email body (success)</strong><ul><br><li><strong>Failure:</strong> builds a consolidated string containing <code>RETURN</code><br>/<code>MESSAGE</code><br>, message details (<code>MESSAGE_V1</code><br>..V4), and <code>EXCEPTIONMSG</code><br>.</li><br><br><li><strong>Success:</strong> sets EmailBody to the <code>ANALYSIS</code><br> field extracted from the response.</li><br><br></ul><br></li><br><br></ol><br><br>**Outcome**: the caller produces a single artifact (EmailBody) that is readable and actionable, without requiring anyone to inspect the raw RFC response. |\n\n### **7. Destination Workflow #2: Persisting failed rows as custom IDocs**\n\nIn this section I zoom in on the optional “IDoc persistence” branch at the end of the destination workflow. After the workflow identifies invalid rows (via the Data Validation Agent) and emails a verification summary, it can optionally call a second SAP RFC to save the failed rows as IDocs for later processing.\n\nThis is mainly included to showcase another common SAP integration scenario—creating/handling IDocs—and to highlight that you can combine “AI-driven validation” with traditional enterprise workflows. The deeper motivation for invoking this as part of the agent tooling is covered in **Part 2**; here, the goal is to show the connector pattern and the custom RFC used to create IDocs from CSV input.\n\nThe figure below shows the destination workflow at two levels: a high-level overview at the top, and a zoomed view of the post-validation remediation steps at the bottom. The zoom starts from Data Validation Agent → Summarize CSV payload review and then expands the sequence that runs after Send verification summary: Transform CSV to XML followed by an SAP RFC call that creates IDocs from the failed data.\n\n**Figure: Zoomed remediation branch**\n\nThe key point is that this branch is not the main “analysis response” path. It’s a practical remediation option: once invalid rows are identified and reported, the workflow can persist them into SAP using a dedicated RFC (`Z_CREATE_ONLINEORDER_IDOC` ) and a simple `IT_CSV` payload. This keeps the end-to-end flow modular: analysis can remain focused on validated data, while failed records can be routed to SAP for follow-up processing on their own timeline.\n\n**Callout**: This branch exists to showcase an IDoc-oriented connector scenario. The “why this is invoked from the agent tooling” context is covered in **Part 2**; here the focus is the mechanics of calling `Z_CREATE_ONLINEORDER_IDOC` with `IT_CSV` and receiving `ET_RETURN` / `ET_DOCNUMS` .\n\nThe screenshot shows an XML body with the RFC root element and an SAP namespace:\n\n``` <z_create_onlineorder_idoc xmlns=\"http://Microsoft.LobServices.Sap/2007/03/Rfc/\"> <iv_direction>...</iv_direction> <iv_sndptr>...</iv_sndptr> <iv_sndprn>...</iv_sndprn> <iv_rcvptr>...</iv_rcvptr> <iv_rcvprn>...</iv_rcvprn> <it_csv> @{ ...Outputs... } </it_csv> <et_return></et_return> <et_docnums></et_docnums> </z_create_onlineorder_idoc> ```\n\n***What to notice:***\n\n- the workflow passes invalid CSV rows in `IT_CSV`\n, and SAP returns a status table (`ET_RETURN` ) and created document numbers (`ET_DOCNUMS` ) for traceability.\n- The payload includes standard-looking control fields (`IV_DIRECTION`\n, `IV_SNDPTR` , `IV_SNDPRN` , `IV_RCVPTR` , `IV_RCVPRN` ) *and* the actual failed-row payload as `IT_CSV` .\n- `IT_CSV`\nis populated via a Logic Apps expression (shown as @{ ...Outputs... } in the screenshot), which is the bridge between the prior transform step and the RFC call.\n- The response side indicates table-like outputs: `ET_RETURN`\nand `ET_DOCNUMS` .\n\n#### **7.1 From CSV to IDocs**\n\nI’ll cover the details of Destination workflow #2 in **Part 2**. In this post (**Part 1**), I focus on the contract and the end-to-end mechanics: what the RFC expects, what it returns, and how the created IDocs show up in the receiving workflow.\n\nBefore looking at the RFC itself, it helps to understand the payload we’re building inside the IDoc. The screenshot below shows the custom segment definition used by the custom IDoc type. This segment is intentionally shaped to mirror the columns of the CSV input so the mapping stays direct and easy to reason about.\n\nFigure: Custom segment `ZONLINEORDER000` (segment type `ZONLINEORDER` )\n\nThis segment definition is the contract anchor: it makes the CSV-to-IDoc mapping explicit and stable. Each CSV record becomes one segment instance with the same 14 business fields. That keeps the integration “boringly predictable,” which is exactly what you want when you’re persisting rejected records for later processing.\n\nThe figure below shows the full loop for persisting failed rows as IDocs. The source workflow calls the custom RFC and sends the invalid CSV rows as XML. SAP converts each row into the custom segment and creates outbound IDocs. Those outbound IDocs are then received by Destination workflow #2, which processes them asynchronously (one workflow instance per IDoc) and appends results into shared storage for reporting.\n\n**Figure: Persisting rejected rows as IDocs**\n\nThis pattern deliberately separates concerns:\n\n- the *first* destination workflow identifies invalid rows and decides whether to persist them,\n- SAP encapsulates the mechanics of **IDoc creation** behind a stable RFC interface, and\n- a *second* destination workflow processes those IDocs asynchronously (one per IDoc), which is closer to how IDoc-driven integrations typically operate in production.\n\nDestination workflow #2 is included here to show the end-to-end contract and the “receipt” side of the connector scenario:\n\n- Triggered by the SAP built-in trigger and checks FunctionName = `IDOC_INBOUND_ASYNCHRONOUS`\n- extracts `DOCNUM`\nfrom the IDoc control record (`EDI_DC40` /`DOCNUM` )\n- reconstructs a CSV payload from the IDoc data segment (the fields shown match the segment definition)\n- appends a “verification info” line to shared storage for reporting\n\nThe implementation details of that workflow (including why it is invoked from the agent tooling) are covered in **Part 2**.\n\n#### **`7.2 Z_CREATE_ONLINEORDER_IDOC` - Contract overview**\n\nThe full source code for `Z_CREATE_ONLINEORDER_IDOC` is included in the supporting material. It’s too long to reproduce inline, so this post focuses on the contract—the part you need to call the RFC correctly and interpret its results.\n\n*A quick note on authorship:* most of the implementation was generated with Copilot, with manual review and fixes to resolve build errors and align the behavior with the intended integration pattern. The contract is deliberately generic because the goal was to produce an RFC that’s reusable across more than one scenario, rather than tightly coupled to a single workflow.\n\nAt a high level, the RFC is designed to support:\n\n- **Both inbound and outbound IDoc creation**\nIt can either write IDocs to the SAP database (inbound-style persistence) or create/distribute IDocs outbound.\n- **Multiple IDoc/message/segment combinations**\nIDoc type (`IDOCTYP` ), message type (`MESTYP` ), and segment type (`SEGTP` ) are configurable so the same RFC can be reused.\n- **Explicit partner/port routing control**\nOptional sender/receiver partner/port fields can be supplied when routing matters.\n- **Traceability of created artifacts**\nThe RFC returns created **IDoc numbers** so the caller can correlate “these failed rows” to “these IDocs.”\n\n###### **Contract:**\n\n| **Inputs (import parameters)**<br><ul><br><li><code>IV_DIRECTION</code><br> <em>(default: 'O')</em> — 'I' for inbound write-to-db, 'O' for outbound distribute/dispatch</li><br><br><li><code>IV_IDOCTYP</code><br> <em>(default: <code>ZONLINEORDERIDOC</code><br>)</em></li><br><br><li><code>IV_MESTYP</code><br> <em>(default: <code>ZONLINEORDER</code><br>)</em></li><br><br><li><code>IV_SEGTP</code><br> <em>(default: <code>ZONLINEORDER</code><br>)</em></li><br><br><li>Optional partner/port routing fields: <code>IV_SNDPRT</code><br>, <code>IV_SNDPRN</code><br>, <code>IV_RCVPRT</code><br>, <code>IV_RCVPRN</code><br>, <code>IV_RCVPOR</code><br></li><br><br></ul> | | --- | | **Tables**<br><ul><br><li><code>IT_CSV</code><br> <em>(structure <code>ZTY_CSV_LINE</code><br>)</em> — each row is one CSV line (the “table-of-lines” pattern)</li><br><br><li><code>ET_RETURN</code><br> <em>(structure <code>BAPIRET2</code><br>)</em> — success/warning/error messages (per-row and/or aggregate)</li><br><br><li><code>ET_DOCNUMS</code><br> <em>(type <code>ZTY_DOCNUM_TT</code><br>)</em> — list of created IDoc numbers for correlation/traceability</li><br><br></ul> | | **Outputs**<br><ul><br><li><code>EV_DOCNUM</code><br> — a convenience “primary / last created” <code>DOCNUM</code><br> value returned by the RFC</li><br><br></ul> |\n\n### **8. Concluding Remarks**\n\n**Part 1** established a stable SAP ↔ Logic Apps integration baseline: CSV moves end‑to‑end using explicit contracts, and failures are surfaced predictably. The source workflow reads CSV from Blob, wraps rows into the `IT_CSV` table‑of‑lines payload, calls `Z_GET_ORDERS_ANALYSIS` , and builds one outcome using two fields from the RFC response: `EXCEPTIONMSG` and `RETURN` /`MESSAGE` . The destination workflow gates requests, validates input, and returns only analysis (or errors) back to SAP while handling invalid rows operationally (notification + optional persistence).\n\nOn the error path, we covered three concrete patterns to raise workflow failures back into SAP: the default connector exception (`SENDEXCEPTIONTOSAPSERVER` ), named exceptions (explicit ABAP contract), and message‑class‑based errors (SAP‑native formatting via `FORMAT_MESSAGE` ). On the remediation side, we added a realistic enterprise pattern: persist rejected rows as custom IDocs via `Z_CREATE_ONLINEORDER_IDOC` (`IT_CSV` in, `ET_RETURN`\n+ `ET_DOCNUMS`\nout), using the custom segment `ZONLINEORDER000` as the schema anchor and enabling downstream receipt in Destination workflow #2 (one run per IDoc, correlated via `DOCNUM` ).\n\n**Part 2** is separate because it tackles a different problem: the AI layer. With contracts and error semantics now fixed, **Part 2** can focus on the agent/tooling details that tend to iterate—rule retrieval, structured validation outputs, prompt constraints, token/history controls, and how the analysis output is generated and shaped—without muddying the transport story.\n\n### **Appendix 1:** Parse XML with schema\n\nIn this section I consider the CSV payload creation as an example, but parsing XML with schema applies in every place where we get an XML input to process, such as when receiving SAP responses, exceptions, or request/responses from other RFCs.\n\n##### **Strong contract**\n\nThe `Create_CSV_payload` step in the shown implementation uses an `xpath() + join()` expression to extract LINE values from the incoming XML:\n\n``` join( xpath( xml(triggerBody()?['content']), '/*[local-name()=\"Z_GET_ORDERS_ANALYSIS\"] /*[local-name()=\"IT_CSV\"] /*[local-name()=\"ZTY_CSV_LINE\"] /*[local-name()=\"LINE\"]/text()' ), '\\r\\n' ) ```\n\nThat approach works, but it’s essentially a **“weak contract”**: it assumes the message shape stays stable and that your XPath continues to match. By contrast, the [**Parse XML with schema**](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-xml-parse) action turns the XML payload into structured data based on an XSD, which gives you a **“strong contract”** and enables downstream steps to bind to known fields instead of re-parsing XML strings.\n\nThe figure below compares two equivalent ways to build the CSV payload from the RFC input. On the left is the direct xpath() compose (labeled “weak contract”). On the right is the schema-based approach (labeled “strong contract”), where the workflow parses the request first and then builds the CSV payload by iterating over typed rows.\n\n**Figure: comparison Compose/XPath vs. Parse XML with schema.**\n\nWhat’s visible in the diagram is the key tradeoff:\n\n- XPath compose path (left): the workflow creates the CSV payload directly using `join(xpath(...), '\\r\\n')`\n, with the XPath written using local-name() selectors. This is fast to prototype, but the contract is implicit—your workflow “trusts” the XML shape and your XPath accuracy.\n- Parse XML with schema path (right): the workflow inserts a Parse XML with schema step (“`Parse Z GET ORDERS ANALYSIS request`\n”), initializes variables, loops For each CSV row, and Appends to CSV payload, then performs `join(variables('CSVPayload'), '\\r\\n')` . Here, the contract is explicit—your XSD defines what IT\\_CSV and LINE mean, and downstream steps bind to those fields rather than re-parsing XML.\n\nA good rule of thumb is: XPath is great for lightweight extraction, while Parse XML with schema is better when you want contract enforcement and long-term maintainability, especially in enterprise integration / BizTalk migration scenarios where schemas are already part of the integration culture.\n\n##### **Implementation details**\n\nThe next figure shows the concrete configuration for Parse XML with schema and how its outputs flow into the “For each CSV row” loop. This is the “strong contract” version of the earlier XPath compose.\n\n**Figure: Parse XML with schema - details.**\n\nThis screenshot highlights three practical implementation details:\n\n1. **The Parse action is schema-backed.**\nIn the Parameters pane, the action uses:\n- **Content:** the incoming XML Response\n- **Schema source:** LogicApp\n- **Schema name:** Z\\_GET\\_ORDERS\\_ANALYSIS\nThe code view snippet shows the same idea: type: \"XmlParse\" with content: \"`@triggerBody()?['content']` \" and schema: { source: \"LogicApp\", name: \"Z\\_GET\\_ORDERS\\_ANALYSIS.xsd\" }.\n2. **The parsed output becomes typed “dynamic content.”**\nThe loop input is shown as “`JSON Schema for element 'Z_GET_ORDERS_ANALYSIS: IT_CSV'` ”. This is the key benefit: you are no longer scraping strings—you are iterating over a structured collection that was produced by schema-based parsing.\n3. **The `LINE` extraction becomes trivial and readable.**\nThe “Append to CSV payload” step appends `@item()?['LINE']` to the CSVpayload variable (as shown in the code snippet). Then the final Create CSV payload becomes a simple `join(variables('CSVPayload'), '\\r\\n')` . This is exactly the kind of “workflow readability” benefit you get once XML parsing is schema-backed.\n\n##### **Schema generation**\n\nThe Parse action requires XSD schemas, which can be stored in the Logic App (or via a linked Integration Account). The final figure shows a few practical ways to obtain and manage those XSDs:\n\n- **Generate Schema (SAP connector):** a “Generate Schema” action with Operation Type = RFC and an **** RFC Name field, which is a practical way to bootstrap schema artifacts when you already know the RFC you’re calling.\n- **Run Diagnostics / Fetch RFC Metadata:** a “Run Diagnostics” action showing **** Operation type = Fetch RFC Metadata and RFC Name, which is useful to confirm the shape of the RFC interface and reconcile it with your XSD/contract.\n\n**Figure: schema generation.**\n\nIf you don’t want to rely solely on connector-side schema generation, there are also classic “developer tools” approaches:\n\n- **Infer XSD from a sample XML** using .NET’s XmlSchemaInference (good for quick starting points).\n- **Generate XSD from an XML instance** using xsd.exe (handy when you already have representative sample payloads) or by asking your favorite AI prompt.\n\n##### **When to choose XPath vs Parse XML with schema (practical guidance)**\n\n*Generally speaking, choose **XPath** when…*\n\n- You need a quick extraction and you’re comfortable maintaining a single XPath.\n- You don’t want to manage schema artifacts yet (early prototypes).\n\n*Choose **Parse XML with schema** when…*\n\n- You want a **stronger, explicit contract** (XSD defines what the payload is).\n- You want the designer to expose **structured outputs** (“JSON Schema for element …”) so downstream steps are readable and less brittle.\n- You expect the message shape to evolve over time and prefer schema-driven changes over XPath surgery.\n\n### **Appendix 2:** Using FORMAT_MESSAGE to produce SAP‑native error text\n\nWhen propagating failures from Logic Apps back into SAP (for example via Send exception to SAP server), I want the SAP side to produce a predictable, human‑readable message without forcing callers to parse connector‑specific payloads. ABAP’s `FORMAT_MESSAGE` is ideal for this because it converts SAP’s message context—message class, message number, and up to four variables—into the final message text that SAP would normally display, but without raising a UI message.\n\n##### **What FORMAT_MESSAGE does**\n\n`FORMAT_MESSAGE` formats a message defined in SAP’s message catalog (`T100` / maintained via `SE91` ) using the values in `sy-msgid` , `sy-msgno` , and `sy-msgv1` …`sy-msgv4` . Conceptually, it answers the question:\n\n“Given message class + number + variables, what is the rendered message string?”\n\nThis is particularly useful after an RFC call fails, where ABAP may have message context available even if the exception itself is not a clean string.\n\n##### **Why this matters in an RFC wrapper**\n\nIn the message class–based exception configuration, the workflow can provide message metadata (class/number/type) so that SAP can behave “natively”: ABAP receives a failure (`sy-subrc` &lt;&gt; 0), formats the message using `FORMAT_MESSAGE` , and returns the final text in a field like `EXCEPTIONMSG` (and/or in `BAPIRET2` -`MESSAGE` ). The result is:\n\n- consistent wording across systems and environments\n- easier localization (SAP selects language-dependent text)\n- separation of concerns: code supplies variables; message content lives in message maintenance\n\n##### **A robust pattern**\n\nAfter the RFC call, I use this order of precedence:\n\n1. Use any explicit text already provided (for example via `system_failure`\n… `MESSAGE` exceptionmsg), because it’s already formatted.\n2. If that’s empty but SAP message context exists (`sy-msgid`\n/ `sy-msgno` ), call `FORMAT_MESSAGE` to produce the final string.\n3. If neither is available, fall back to a generic message that includes `sy-subrc`\n.\n\nHere is a compact version of that pattern:\n\n``` DATA: lv_text TYPE string.\n\nCALL FUNCTION 'Z_GET_ORDERS_ANALYSIS' DESTINATION dest IMPORTING analysis = analysis TABLES it_csv = it_csv CHANGING return = return EXCEPTIONS sendexceptiontosapserver = 1 system_failure = 2 MESSAGE exceptionmsg communication_failure = 3 MESSAGE exceptionmsg OTHERS = 4.\n\nIF sy-subrc <> 0.\n\n\"Prefer explicit message text if it already exists IF exceptionmsg IS INITIAL.\n\n\"Otherwise format SAP message context into a string IF sy-msgid IS NOT INITIAL AND sy-msgno IS NOT INITIAL. CALL FUNCTION 'FORMAT_MESSAGE' EXPORTING id = sy-msgid no = sy-msgno v1 = sy-msgv1 v2 = sy-msgv2 v3 = sy-msgv3 v4 = sy-msgv4 IMPORTING msg = lv_text.\n\nexceptionmsg = lv_text. ELSE. exceptionmsg = |RFC failed (sy-subrc={ sy-subrc }).|. ENDIF.\n\nENDIF.\n\n\"Optionally normalize into BAPIRET2 for structured consumption return-type = 'E'. return-message = exceptionmsg.\n\nENDIF. ```\n\n##### **Common gotchas**\n\n- `FORMAT_MESSAGE`\nonly helps if `sy-msgid` and `sy-msgno` are set. If the failure did not originate from an SAP message (or message mapping is disabled), these fields may be empty—so keep a fallback.\n- Message numbers are typically 3-digit strings (e.g., 001, 012), matching how messages are stored in the catalog.\n- `FORMAT_MESSAGE`\nformats text; it does not raise or display a message. That makes it safe to use in RFC wrappers and background processing.\n\n**Bottom line:** `FORMAT_MESSAGE` is a simple tool that helps workflow‑originated failures “land” in SAP as clean, SAP‑native messages—especially when using message classes to standardize and localize error text.\n\n### **References**\n\n[Agentic Logic Apps Integration with SAP - Part 2: AI Agents](https://techcommunity.microsoft.com/blog/integrationsonazureblog/agentic-logic-apps-integration-with-sap---part-2-ai-agents/4492362)\n\n[Handling Errors in SAP BAPI Transactions | Microsoft Community Hub](https://techcommunity.microsoft.com/blog/integrationsonazureblog/handling-errors-in-sap-bapi-transactions/1909185)\n\n[Access SAP from workflows | Microsoft Learn](https://learn.microsoft.com/en-us/azure/logic-apps/connectors/sap?tabs=standard)\n\n[Create common SAP workflows | Microsoft Learn](https://learn.microsoft.com/en-us/azure/logic-apps/connectors/sap-create-example-scenario-workflows?tabs=standard)\n\n[Generate Schemas for SAP Artifacts via Workflows | Microsoft Learn](https://learn.microsoft.com/en-us/azure/logic-apps/connectors/sap-generate-schemas-for-artifacts?tabs=standard)\n\n[Parse XML using Schemas in Standard workflows - Azure Logic Apps | Microsoft Learn](https://learn.microsoft.com/en-us/azure/logic-apps/logic-apps-enterprise-integration-xml-parse)\n\n[Announcing XML Parse and Compose for Azure Logic Apps GA](https://www.youtube.com/watch?v=5axAw_-HLU4)\n\n[Exception Handling | ABAP Keyword Documentation](https://help.sap.com/doc/abapdocu_cp_index_htm/CLOUD/en-US/abenabap_exceptions.html)\n\n[Handling and Propagating Exceptions - ABAP Keyword Documentation](https://help.sap.com/doc/abapdocu_751_index_htm/7.51/en-us/abenhandl_prop_except_guidl.htm)\n\n[SAP .NET Connector 3.1 Overview](https://support.sap.com/content/dam/support/en_us/library/ssp/products/connectors/msnet/NCo31_Overview.pdf)\n\n[SAP .NET Connector 3.1 Programming Guide](https://support.sap.com/content/dam/support/en_us/library/ssp/products/connectors/msnet/NCo31_ProgrammingGuide.pdf)\n\n**All supporting content for this post may be found in the [companion GitHub repository.](https://github.com/Azure/logicapps/tree/master/agentic-sap-workflows)**\n\nUpdated Feb 17, 2026\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AIntegrationsonAzureBlog)\n\n[biztalk migration](/tag/biztalk%20migration?nodeId=board%3AIntegrationsonAzureBlog)\n\n[biztalk server](/tag/biztalk%20server?nodeId=board%3AIntegrationsonAzureBlog)\n\n[connectors](/tag/connectors?nodeId=board%3AIntegrationsonAzureBlog)\n\n[logic apps](/tag/logic%20apps?nodeId=board%3AIntegrationsonAzureBlog)\n\n[logic apps standard](/tag/logic%20apps%20standard?nodeId=board%3AIntegrationsonAzureBlog)\n\n[openai](/tag/openai?nodeId=board%3AIntegrationsonAzureBlog)\n\n[rules engine](/tag/rules%20engine?nodeId=board%3AIntegrationsonAzureBlog)\n\n[sap](/tag/sap?nodeId=board%3AIntegrationsonAzureBlog)\n\n[!\\[Emmanuel_Abram_Profeta&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS01NjM0NTgtQUk3S2dQ?image-coordinates=106%2C0%2C2782%2C2676&amp;image-dimensions=50x50)](/users/emmanuel_abram_profeta/563458) [Emmanuel_Abram_Profeta](/users/emmanuel_abram_profeta/563458) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined February 19, 2020\n\n[View Profile](/users/emmanuel_abram_profeta/563458)\n\n/category/azure/blog/integrationsonazureblog [Azure Integration Services Blog](/category/azure/blog/integrationsonazureblog) Follow this blog board to get notified when there's new activity",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Tags": [],
  "ProcessedDate": "2026-02-17 19:21:54",
  "Title": "Agentic Logic Apps Integration with SAP - Part 1: Infrastructure",
  "Link": "https://techcommunity.microsoft.com/t5/azure-integration-services-blog/agentic-logic-apps-integration-with-sap-part-1-infrastructure/ba-p/4491906",
  "OutputDir": "_community",
  "Description": "### **1. Introduction**\n\nWhen you integrate Azure Logic Apps with SAP, the “hello world” part is usually easy. The part that bites you later is data quality. In SAP-heavy flows, validation isn’t a nice-to-have — it’s what makes the downstream results meaningful. If invalid data slips through, it can get expensive fast: you may create incorrect business documents, trigger follow-up processes**,** and end up in a cleanup path that’s harder (and more manual) than building validation upfront. And in “all-or-nothing” transactional patterns, things get even more interesting: one bad record can force a rollback strategy, compensating actions, or a whole replay/reconciliation story you didn’t want to own. See for instance [Handling Errors in SAP BAPI Transactions | Microsoft Community Hub](https://techcommunity.microsoft.com/blog/integrationsonazureblog/handling-errors-in-sap-bapi-transactions/1909185) to get an idea of the complexity in a BizTalk context.\n\nThat’s the motivation for this post: a practical starter pattern that you can adapt to many data shapes and domains for validating data in a Logic Apps + SAP integration.\n\n| **Note: For the full set of assets used here, see the [companion GitHub repository](https://github.com/Azure/logicapps/tree/master/agentic-sap-workflows) (workflows, schemas, SAP ABAP code, and sample files).** | | --- |\n\n##### **Scenario overview**\n\nThe scenario is intentionally simple, but it mirrors what shows up in real systems:\n\n1. A Logic App workflow sends CSV documents to an SAP endpoint.\n2. SAP forwards the payload to a second Logic App workflow that performs:\n- **rule-based validation** (based on pre-defined rules)\n- **analysis/enrichment** (market trends, predictions, recommendations)\n3. The workflow either:\n- returns validated results (or validation errors) to the initiating workflow, or\n- persists outputs for later use\n\nFor illustration, I’m using fictitious retail data. The content is made up, but the mechanics are generic: the same approach works for orders, inventory, pricing, master data feeds, or any “file in → decision out” integration. You’ll see sample inputs and outputs below to keep the transformations concrete.\n\n![]()**Figure: Input CSV data and corresponding rules**\n\n![]()**Figure: Outputs - Analysis of trends and predictions, validation summary and IDoc information.**\n\n##### **What this post covers**\n\nThis walkthrough focuses on the integration building blocks that tend to matter in production:\n\n- Calling SAP RFCs from Logic App workflows, and invoking Logic App workflows from SAP function modules\n- Using the Logic Apps SAP built-in trigger\n- Receiving and processing IDocs\n- Returning responses and exceptions back to SAP in a structured, actionable way\n- Data manipulation patterns in Logic Apps, including:\n- parsing and formatting\n- inline scripts\n- XPath (where it fits, and where it becomes painful).\n\n##### **Overall Implementation**\n\nA high-level view of the implementation is shown below. The source workflow handles end-to-end ingestion—file intake, transformation, SAP integration, error handling, and notifications—using Azure Logic Apps. The destination workflows focus on validation and downstream processing, including AI-assisted analysis and reporting, with robust exception handling across multiple technologies. I’ll cover the AI portion in a follow-up post.\n\n![]()**Figure: Overall implementation.**\n\n| **Note on AI-assisted development**<br><br><br><br>Most of the workflow “glue” in this post—XPath, JavaScript snippets, and Logic Apps expressions—was built with help from Copilot and the AI assistant in the designer (see [Get AI-assisted help for Standard workflows - Azure Logic Apps | Microsoft Learn](https://learn.microsoft.com/en-us/azure/logic-apps/workflow-assistant-standard)). In my experience, this is exactly where AI assistance pays off: generating correct scaffolding quickly, then iterating based on runtime behavior.<br><br><br><br>I’ve also included SAP ABAP snippets for the SAP-side counterpart. You don’t need advanced ABAP knowledge to follow along; the snippets are deliberately narrow and integration-focused. I include them because it’s hard to design robust integrations if you only understand one side of the contract. When you understand how SAP expects to receive data, how it signals errors, and where transactional boundaries actually are, you end up with cleaner workflows and fewer surprises. | | --- |\n\n### **2. Source Workflow**\n\nThis workflow is a small, end‑to‑end “sender” pipeline: it reads a CSV file from Azure Blob Storage, converts the rows into the SAP table‑of‑lines XML shape expected by an RFC, calls `Z_GET_ORDERS_ANALYSIS` via the SAP connector, then extracts analysis or error details from the RFC response and emails a single consolidated result.\n\nAt a high level:\n\n- **Input**: an HTTP request (used to kick off the run) + a blob name.\n- **Processing**: CSV → array of rows → XML (…) → RFC call\n- **Output**: one email containing either:\n- the analysis (success path), or\n- a composed error summary (failure path).\n\nThe diagram below summarizes the sender pipeline: HTTP trigger → Blob CSV (header included) → rows → SAP RFC → parse response → email.\n\n![]()**Figure: End‑to‑end sender pipeline**\n\nTwo design choices are doing most of the work here. First, the workflow keeps the CSV transport contract stable by sending the file as a verbatim list of lines—including the header—wrapped into `… ` elements under `IT_CSV` . Second, it treats the RFC response as the source of truth: `EXCEPTIONMSG` and `RETURN/MESSAGE` drive a single `Has errors` gate, which determines whether the email contains the analysis or a consolidated failure summary.\n\n##### **Step-by-step description**\n\n| **Phase 0 — Trigger**<br><ol><br><li><strong>Trigger </strong>— <code>When_an_HTTP_request_is_received</code><br><br>The workflow is invoked via an HTTP request trigger (stateful workflow).</li><br><br></ol> | | --- | | **Phase 1 — Load and split the CSV**<br><ol><br><li><strong>Read file</strong> — <code>Read_CSV_orders_from_blob</code><br><br>Reads the CSV from container onlinestoreorders using the blob name from @parameters('DataFileName').</li><br><br><li><strong>Split into rows </strong>— <code>Extract_rows</code><br><br>Splits the blob content on \\r\\n, producing an array of CSV lines.</li><br><br></ol><br><br>Design note: Keeping the header row is useful when downstream validation or analysis wants column names, and it avoids implicit assumptions in the sender workflow. | | **Phase 2 — Shape the RFC payload**<br><ol><br><li><strong>Convert CSV rows to SAP XML </strong>— <code>Transform_CSV_to_XML</code><br><br>Uses JavaScript to wrap each CSV row (including the header line) into the SAP line structure and XML‑escape special characters. The output is an XML fragment representing a table of <code>ZTY_CSV_LINE</code><br> rows.</li><br><br></ol> | | **Phase 3 — Call SAP and extract response fields**<br><ol><br><li><strong>Call the RFC </strong>— [RFC]_<code>Z_GET_ORDERS_ANALYSIS</code><br><br>Invokes <code>Z_GET_ORDERS_ANALYSIS</code><br> with an XML body containing … built from the transformed rows.</li><br><br><li><strong>Extract error/status </strong>— <code>Save_EXCEPTION_message</code><br> and <code>Save_RETURN_message</code><br><br>Uses XPath to pull:<ul><br><li><code>EXCEPTIONMSG</code><br> from the RFC response, and</li><br><br><li>the structured <code>RETURN</code><br>/<code>MESSAGE</code><br> field.</li><br><br></ul><br></li><br><br></ol> | | **Phase 4 — Decide success vs failure and notify**<br><ol><br><li><strong>Initialize output buffer </strong>— <code>Initialize_email_body</code><br><br>Creates the EmailBody variable used by both success and failure cases.</li><br><br><li><strong>Gate </strong>— <code>Has_errors</code><br><br>Determines whether to treat the run as failed based on:<ul><br><li><code>EXCEPTIONMSG</code><br> being different from \"ok\", <strong>or</strong></li><br><br><li><code>RETURN</code><br>/<code>MESSAGE</code><br> being non‑empty.</li><br><br></ul><br></li><br><br><li><strong>Send result — Send_an_email_(V2)</strong><br>Emails either:<ul><br><li>the extracted <code>ANALYSIS</code><br> (success), or</li><br><br><li>a concatenated error summary including <code>RETURN</code><br>/<code>MESSAGE</code><br> plus message details (<code>MESSAGE_V1</code><br>…<code>MESSAGE_V4</code><br>) and <code>EXCEPTIONMSG</code><br>.</li><br><br></ul><br></li><br><br></ol> |\n\n**Note**: Because the header row is included in `IT_CSV` , the SAP-side parsing/validation treats the first line as column titles (or simply ignores it). The sender workflow stays “schema-agnostic” by design.\n\n##### **Useful snippets**\n\n**Snippet 1 — Split the CSV into rows**\n\n- split(string(body('Read\\_CSV\\_orders\\_from\\_blob')?['content']), '\\r\\n')\n\nTip: If your CSV has a header row you don’t want to send to SAP, switch back to: @skip(split(string(body('Read\\_CSV\\_orders\\_from\\_blob')?['content']), '\\r\\n'), 1)\n\n**Snippet 2 — JavaScript transform: “rows → SAP table‑of‑lines XML”**\n- const lines = workflowContext.actions.Extract\\_rows.outputs;\n\nfunction xmlEscape(value) { return String(value) .replace(/&/g, \"&\") .replace(//g, \">\") .replace(/\"/g, \"\"\") .replace(/'/g, \"'\"); }\n\n// NOTE: we don't want to keep empty lines (which can be produced by reading the blobs) // the reason being that if the recipient uses a schema to validate the xml, // it may reject it if it does not allow empty nodes. const xml = lines .filter(line => line && line.trim() !== '') // keep only non-empty lines .map(line => `${xmlEscape(line)}`) .join('');\n\nreturn { xml };\n\n**Snippet 3 — XPath extraction of response fields (namespace-robust)**\n- EXCEPTIONMSG:\n@xpath(body('[RFC]\\_Call\\_Z\\_GET\\_ORDERS\\_ANALYSIS')?['content'], 'string( /\\*[local-name()=\"Z\\_GET\\_ORDERS\\_ANALYSISResponse\"] /\\*[local-name()=\"EXCEPTIONMSG\"])')\n\nRETURN/MESSAGE: @xpath(body('[RFC]\\_Call\\_Z\\_GET\\_ORDERS\\_ANALYSIS')?['content'], 'string( /\\*[local-name()=\"Z\\_GET\\_ORDERS\\_ANALYSISResponse\"] /\\*[local-name()=\"RETURN\"] /\\*[local-name()=\"MESSAGE\"])')\n\n**Snippet 4 — Failure email body composition**\n- concat(\n'Error message: ', outputs('Save\\_RETURN\\_message'), ', details: ', xpath(body('[RFC]\\_Call\\_Z\\_GET\\_ORDERS\\_ANALYSIS')?['content'], 'string(//\\*[local-name()=\\\"MESSAGE\\_V1\\\"])'), xpath(body('[RFC]\\_Call\\_Z\\_GET\\_ORDERS\\_ANALYSIS')?['content'], 'string(//\\*[local-name()=\\\"MESSAGE\\_V2\\\"])'), xpath(body('[RFC]\\_Call\\_Z\\_GET\\_ORDERS\\_ANALYSIS')?['content'], 'string(//\\*[local-name()=\\\"MESSAGE\\_V3\\\"])'), xpath(body('[RFC]\\_Call\\_Z\\_GET\\_ORDERS\\_ANALYSIS')?['content'], 'string(//\\*[local-name()=\\\"MESSAGE\\_V4\\\"])'), '; ', 'Exception message: ', outputs('Save\\_EXCEPTION\\_message'), '.')\n\n### **3. SAP Support**\n\nTo make the SAP/Logic Apps boundary simple, I model the incoming CSV as a table of “raw lines” on the SAP side. The function module `Z_GET_ORDERS_ANALYSIS` exposes a single table parameter, `IT_CSV` , typed using a custom line structure.\n\n![]()\n\nFigure: `IT_CSV` is a table of CSV lines (`ZTY_CSV_LINE` ), with a single `LINE` field (`CHAR2048` ).\n\n`IT_CSV` uses the custom structure `ZTY_CSV_LINE` , which contains a single component `LINE` (`CHAR2048` ). This keeps the SAP interface stable: the workflow can send CSV lines without SAP having to know the schema up front, and the parsing/validation logic can evolve independently.\n\nThe diagram below shows the plumbing that connects SAP to Azure Logic Apps in two common patterns: SAP sending IDocs to a workflow and SAP calling a remote-enabled endpoint via an RFC destination. I’m showing all three pieces together—the ABAP call site, the `SM59` RFC destination, and the Logic Apps SAP built-in trigger—because most “it doesn’t work” problems come down to a small set of mismatched configuration values rather than workflow logic.\n\n![]()**Figure: SAP-to-Logic Apps wiring for inbound IDocs and RFC callbacks**\n\n**The key takeaway** is that both patterns hinge on the same contract: Program ID plus the SAP Gateway host/service. In SAP, those live in `SM59` (TCP/IP destination, registered server program). In Logic Apps, the SAP built-in trigger listens using the same Program ID and gateway settings, while the trigger configuration (for example, IDoc format and degree of parallelism) controls how messages are interpreted and processed. Once these values line up, the rest of the implementation becomes “normal workflow engineering”: validation, predictable error propagation, and response shaping.\n\nBefore diving into workflow internals, I make the SAP-side contract explicit. The function module interface below shows the integration boundary: CSV lines come in as `IT_CSV` , results come back as `ANALYSIS` , and status/error information is surfaced both as a human-readable `EXCEPTIONMSG` and as a structured `RETURN` (`BAPIRET2` ). I also use a dedicated exception (`SENDEXCEPTIONTOSAPSERVER` ) to signal workflow-raised failures cleanly.\n\nContract (what goes over RFC):\n\n- **Input:** `IT_CSV`\n(CSV lines)\n- **Outputs:** `ANALYSIS`\n(analysis payload), `EXCEPTIONMSG` (human-readable status)\n- **Return structure:** `RETURN`\n(`BAPIRET2` ) for structured SAP-style success/error\n- **Custom exception:** `SENDEXCEPTIONTOSAPSERVER`\nfor workflow-raised failures\n\nHere is the ABAP wrapper that calls the remote implementation and normalizes the result.\n- FUNCTION z\\_get\\_orders\\_analysis.\n\\*\"---------------------------------------------------------------------- \\*\" This module acts as a caller wrapper. \\*\" Important: the remote execution is determined by DESTINATION. \\*\" Even though the function name is the same, this is not recursion: \\*\" the call runs in the remote RFC server registered under DESTINATION \"DEST\". \\*\"---------------------------------------------------------------------- \\*\" Contract: \\*\" TABLES it\\_csv \"CSV lines \\*\" IMPORTING analysis \"Result payload \\*\" EXPORTING exceptionmsg \"Human-readable status / error \\*\" CHANGING return \"BAPIRET2 return structure \\*\" EXCEPTIONS sendexceptiontosapserver \\*\"----------------------------------------------------------------------\n\nCALL FUNCTION 'Z\\_GET\\_ORDERS\\_ANALYSIS' DESTINATION dest IMPORTING analysis = analysis TABLES it\\_csv = it\\_csv CHANGING return = return EXCEPTIONS sendexceptiontosapserver = 1 system\\_failure = 2 MESSAGE exceptionmsg communication\\_failure = 3 MESSAGE exceptionmsg OTHERS = 4.\n\nCASE sy-subrc. WHEN 0. exceptionmsg = 'ok'.\n\n\"Optional: normalize success into RETURN for callers that ignore EXCEPTIONMSG IF return-type IS INITIAL. return-type = 'S'. return-message = 'OK'. ENDIF.\n\nWHEN 1. exceptionmsg = |Exception from workflow: SENDEXCEPTIONTOSAPSERVER { sy-msgv1 }{ sy-msgv2 }{ sy-msgv3 }{ sy-msgv4 }|.\n\nreturn-type = 'E'. return-message = exceptionmsg.\n\nWHEN 2 OR 3. \"system\\_failure / communication\\_failure usually already populate exceptionmsg IF exceptionmsg IS INITIAL. exceptionmsg = |RFC system/communication failure.|. ENDIF.\n\nreturn-type = 'E'. return-message = exceptionmsg.\n\nWHEN OTHERS. exceptionmsg = |Error in workflow: { sy-msgv1 }{ sy-msgv2 }{ sy-msgv3 }{ sy-msgv4 }|.\n\nreturn-type = 'E'. return-message = exceptionmsg. ENDCASE.\n\nENDFUNCTION.\n\nThe wrapper is intentionally small: it forwards the payload to the remote implementation via the RFC destination and then normalizes the outcome into a predictable shape. The point isn’t fancy ABAP — it’s reliability. With a stable contract (`IT_CSV, ANALYSIS, RETURN, EXCEPTIONMSG` ) the Logic Apps side can evolve independently while SAP callers still get consistent success/error semantics.\n\n| **Important**: in 'CALL FUNCTION 'Z\\_GET\\_ORDERS\\_ANALYSIS' DESTINATION dest' the name of the called function should be the same as the name of the ABAP wrapper function module, the reason being that the SAP built-in trigger in the logic app uses the function module signature as the contract (i.e. metadata). | | --- |\n\n![]()**Figure: ABAP interface and error propagation for `Z_GET_ORDERS_ANALYSIS`.**\n\nTo sum up, the integration is intentionally shaped around three outputs: the raw input table (`IT_CSV` ), a standardized SAP return structure (`RETURN` / `BAPIRET2` ), and a readable status string (`EXCEPTIONMSG` ). The custom exception (`SENDEXCEPTIONTOSAPSERVER` ) gives me a clean way to surface workflow failures back into SAP without burying them inside connector-specific error payloads. This is depicted in the figure below.\n\n### **4. Destination Workflow**\n\nThe diagram below shows the destination workflow at a high level. I designed it as a staged pipeline: guard early, normalize input, validate, and then split the workload into two paths—operational handling of invalid records (notifications and optional IDoc remediation) and analysis of the validated dataset. Importantly, the SAP response is intentionally narrow: SAP receives only the final analysis (or a structured error), while validation details are delivered out-of-band via email.\n\n| How to read this diagram<br><ol><br><li><strong>Guardrail:</strong> <strong>Validate requested action</strong> ensures the workflow only handles the expected request.</li><br><br><li><strong>Normalize:</strong> <strong>Create CSV payload</strong> converts the inbound content into a consistent CSV representation.</li><br><br><li><strong>Validate:</strong> <strong>Data Validation Agent</strong> identifies invalid records (and produces a summary).</li><br><br><li><strong>Operational handling (invalid data):</strong> invalid rows are <strong>reported by email</strong> and may optionally be turned into <strong>IDocs</strong> (right-hand block).</li><br><br><li><strong>Analyze (valid data):</strong> <strong>Analyze data</strong> runs only on the validated dataset (invalid IDs excluded).</li><br><br><li><strong>Outputs:</strong> users receive <strong>Email analysis</strong>, while <strong>SAP receives only the analysis (or a structured error)</strong> via <strong>Respond to SAP server</strong>.</li><br><br></ol> | | --- |\n\n![]()\n\nFigure: Destination workflow with staged validation, optional IDoc remediation, and an SAP response .\n\nReading the workflow top-to-bottom, the main design choice is separation of concerns. Validation is used to filter and operationalize bad records (notify humans, optionally create IDocs), while the SAP-facing response stays clean and predictable: SAP receives the final analysis for the validated dataset, or an error if the run can’t complete. This keeps the SAP contract stable even as validation rules and reporting details evolve.\n\n##### **Step‑by‑step walkthrough**\n\n| **Phase 0 — Entry and routing**<br><ol><br><li><strong>Trigger </strong>— When a message is received<br>The workflow starts when an inbound SAP message is delivered to the Logic Apps SAP built‑in trigger.</li><br><br><li><strong>Guardrail </strong>— Validate requested action (2 cases)<br>The workflow immediately checks whether the inbound request is the operation it expects (for example, the function/action name equals <code>Z_GET_ORDERS_ANALYSIS</code>).<ul><br><li>If the action does not match: the workflow sends an exception back to SAP describing the unexpected action and terminates early (fail fast).</li><br><br><li>If the action matches: processing continues.</li><br><br></ul><br></li><br><br></ol> | | --- | | **Phase 1 — Normalize input into a workflow‑friendly payload**<br><ol><br><li><strong>Prepare input</strong> — Create CSV payload<br>The workflow extracts CSV lines from the inbound (XML) SAP payload and normalizes them into a consistent CSV text payload that downstream steps can process reliably.</li><br><br><li><strong>Initialize validation state </strong>— Initialize array of invalid order ids<br>The workflow creates an empty array variable to capture order IDs that fail validation. This becomes the “validation output channel” used later for reporting, filtering, and optional remediation.</li><br><br></ol> | | **Phase 2 — Validate the dataset (AI agent loop)**<br><ol><br><li><strong>Validate </strong>— Data Validation Agent (3 cases)<br>This stage performs rule‑based validation using an agent pattern (backed by Azure OpenAI). Conceptually, it does three things (as shown in the diagram’s expanded block):<ul><br><li>Get validation rules: retrieves business rules from a SharePoint‑hosted validation document.</li><br><br><li>Get CSV payload: loads the normalized CSV created earlier.</li><br><br><li>Summarize CSV payload review: evaluates the CSV against the rules and produces structured validation outputs.</li><br><br></ul>Outputs produced by validation:<ul><br><li>A list of invalid order IDs</li><br><br><li>The corresponding invalid CSV rows</li><br><br><li>A human‑readable validation summary</li><br><br></ul><br></li><br><br></ol><br><br>**Note**: The detailed AI prompt/agent mechanics are covered in **Part 2**. In **Part 1**, the focus is on the integration flow and how data moves. | | **Phase 3 — Operational handling of invalid records (email + optional SAP remediation)**<br><br>After validation, the workflow treats invalid records as an operational concern: they are reported to humans and can optionally be routed into an SAP remediation path. This is shown in the right‑hand “Create IDocs” block.<br><ol><br><li><strong>Notify </strong>— Send verification summary<br>The workflow sends an email report (Office 365) to configured recipients containing:<ul><br><li>the validation summary</li><br><br><li>the invalid order IDs</li><br><br><li>the invalid CSV payload (or the subset of invalid rows)</li><br><br></ul><br></li><br><br><li><strong>Transform </strong>— Transform CSV to XML<br>The workflow converts the invalid CSV lines into an XML format that is suitable for SAP processing.</li><br><br><li><strong>Optional remediation </strong>— [RFC] Create all IDocs (conditional)<br>If the workflow parameter (for example, CreateIDocs) is enabled, the workflow calls an SAP RFC (e.g., <code>Z_CREATE_ONLINEORDER_IDOC</code><br>) to create IDocs from the transformed invalid data.</li><br><br></ol><br><br>**Why this matters**: Validation results are made visible (email) and optionally actionable (IDocs), without polluting the primary analysis response that SAP receives. | | **Phase 4 — Analyze only the validated dataset (AI analysis)**<br><br><br><br>The workflow runs AI analysis on the validated dataset, explicitly excluding invalid order IDs discovered during the validation phase. The analysis prompt instructs the model to produce outputs such as trends, predictions, and recommendations.<br><br><br><br>**Note**: The AI analysis prompt design and output shaping are covered in **Part 2**. | | **Phase 5 — Post‑process the AI response and publish outputs**<br><ol><br><li><strong>Package results </strong>— Process analysis results (Scope)<br>The workflow converts the AI response into a format suitable for email and for SAP consumption:<ul><br><li>Parse the OpenAI JSON response</li><br><br><li>Extract the analysis content</li><br><br><li>Convert markdown → <code>HTML</code><br> using custom JavaScript formatting</li><br><br></ul><br></li><br><br><li><strong>Outputs</strong><ul><br><li>Email analysis: sends the formatted analysis to recipients.</li><br><br><li>Respond to SAP server: returns only the analysis (and errors) to SAP.</li><br><br></ul><br></li><br><br></ol><br><br>**Key design choice**: SAP receives a clean, stable contract—analysis on success, structured error on failure. Validation details are handled out‑of‑band via email (and optionally via IDoc creation).<br><br><br><br>**Note**: the analysis email sent by the destination workflow is there for testing purposes, to verify that the html content remains the same as it is sent back to the source workflow. |\n\n##### **Useful snippets**\n\n**Snippet 1 - Join each CSV line in the XML to make a CSV table:**\n- join(\nxpath( xml(triggerBody()?['content']), '/\\*[local-name()=\\\"Z\\_GET\\_ORDERS\\_ANALYSIS\\\"] /\\*[local-name()=\\\"IT\\_CSV\\\"] /\\*[local-name()=\\\"ZTY\\_CSV\\_LINE\\\"] /\\*[local-name()=\\\"LINE\\\"]/text()' ), '\\r\\n')\n\n**Note**: For the sake of simplicity, XPath is used here and throughout all places where XML is parsed. In the general case however, the **Parse XML with schema** action is the better and recommended way to strictly enforce the data contract between senders and receivers. More information about Parse XML with schema is provided inAppendix 1.\n\n**Snippet 2 - Format markdown to html (simplified):**\n- const raw = workflowContext.actions.Extract\\_analysis.outputs;\n\n// Basic HTML escaping for safety (keeps ` blocks clean) const escapeHtml = s => s.replace(/[&\"]/g, c => ({'&':'&','':'>','\"':'\"'}[c]));\n\n// Normalize line endings let md = raw; // raw.replace(/\\r\\n/g, '\\n').trim();\n\n// Convert code blocks (``` ... ```) md = md.replace(/```([\\s\\S]*?)```/g, (m, p1) => `${escapeHtml(p1)}\n\n`);\n\n// Horizontal rules --- or *** md = md.replace(/(?:^|\\n)---+(?:\\n|$)/g, '');\n\n// Headings ###### to # for (let i = 6; i >= 1; i--) { const re = new RegExp(`(?:^|\\\\n)${'#'.repeat(i)}\\\\s+(.+?)\\\\s*(?=\\\\n|$)`, 'g'); md = md.replace(re, (m, p1) => `${p1.trim()}`); }\n\n// Bold and italic md = md.replace(/\\*\\*([^*]+)\\*\\*/g, '$1'); md = md.replace(/\\*([^*]+)\\*/g, '$1');\n\n// Unordered lists (lines starting with -, *, +) md = md.replace(/(?:^|\\n)([-*+]\\s.+(?:\\n[-*+]\\s.+)*)/g, (m) => { const items = m.trim().split(/\\n/).map(l => l.replace(/^[-*+]\\s+/, '').trim()); return '\\n' + items.map(i => `${i} `).join('\\n') + '\n\n'; });\n\n// Paragraphs: wrap remaining text blocks in ...const blocks = md.split(/\\n{2,}/).map(b => { if (/^|^|^|^/.test(b.trim())) return b; return `${b.replace(/\\n/g, '')} `; }); const html = blocks.join('');\n\nreturn { html };\n5. Exception Handling\n\nTo illustrate exception handling, we supposed that multiple workflows may listen to the same program id (by design or unexpectedly) and could therefore receive messages that were meant for others. So the first thing that happens is validate that the function name is as expected. It is shown below.\n\nIn this section I show three practical ways to surface workflow failures back to SAP using the Logic Apps action “Send exception to SAP server”, and the corresponding ABAP patterns used to handle them. The core idea is the same in all three: Logic Apps raises an exception on the SAP side, SAP receives it as an RFC exception, and your ABAP wrapper converts that into something predictable (for example, a readable EXCEPTIONMSG , a populated RETURN , or both). The differences are in how much control you want over the exception identity and whether you want to leverage SAP message classes for consistent, localized messages.\n\n5.1 Default exception\n\nThis first example shows the default behavior of Send exception to SAP server. When the action runs without a custom exception name configuration, the connector raises a pre-defined exception that can be handled explicitly in ABAP.\n\nOn the Logic Apps side, the action card “Send exception to SAP server” sends an Exception Error Message (for example, “Unexpected action in request: …”). On the ABAP side, the RFC call lists SENDEXCEPTIONTOSAPSERVER = 1 under EXCEPTIONS , and the code uses CASE sy-subrc to map that exception to a readable message.\n\nFigure: Default exception The key takeaway is that you get a reliable “out-of-the-box” exception path: ABAP can treat sy-subrc = 1 as the workflow‑raised failure and generate a consistent EXCEPTIONMSG . This is the simplest option and works well when you don’t need multiple exception names—just one clear “workflow failed” signal.\n\n5.2 Message exception\n\nIf you want more control than the default, you can configure the action to raise a named exception declared in your ABAP function module interface. This makes it easier to route different failure types without parsing free-form text.\n\nThe picture shows Advanced parameters under the Logic Apps action, including “Exception Name” with helper text indicating it must match an exception declared in the ABAP function module definition.\n\nFigure: Message exception This option is useful when you want to distinguish workflow error categories (e.g., validation vs. routing vs. downstream failures) using exception identity, not just message text. The contract stays explicit: Logic Apps raises a named exception, and ABAP can branch on that name (or on sy-subrc mapping) with minimal ambiguity.\n\n5.3 Message class exception\n\nThe third approach uses SAP’s built-in message class mechanism so that the exception raised by the workflow can map cleanly into SAP’s message catalog (T100 ). This is helpful when you want consistent formatting and localization aligned with standard SAP patterns.\n\nOn the Logic Apps side, the action shows advanced fields including Message Class, Message Number, and an Is ABAP Message toggle, with helper text stating the message class can come from message maintenance (SE91 ) or be custom. On the ABAP side, the code highlights an error-handling block that calls using sy-msgid , sy-msgno , and variables sy-msgv1 …sy-msgv4 , then stores the resulting text in EXCEPTIONMSG .\n\nFigure: Message class exception This pattern is ideal when you want workflow exceptions to look and behave like “native” SAP messages. Instead of hard-coding strings, you rely on the message catalog and let ABAP produce a consistent final message via FORMAT_MESSAGE . The result is easier to standardize across teams and environments—especially if you already manage message classes as part of your SAP development process.\n\nRefer to Appendix 2 for further information on FORMAT_MESSAGE .\n\n5.4 Choosing an exception strategy that SAP can act on\n\nAcross these examples, the goal is consistent: treat workflow failures as first‑class outcomes in SAP, not as connector noise buried in run history. The Logic Apps action Send exception to SAP server gives you three increasingly structured ways to do that, and the “right” choice depends on how much semantics you want SAP to understand. Default exception (lowest ceremony): Use this when you just need a reliable “workflow failed” signal. The connector raises a pre-defined exception name (for example, SENDEXCEPTIONTOSAPSERVER ), and ABAP can handle it with a simple EXCEPTIONS … = 1 mapping and a sy-subrc check. This is the fastest way to make failures visible and deterministic.\n\nNamed exception(s) (more routing control): Use this when you want SAP to distinguish failure types without parsing message text. By raising an exception name declared in the ABAP function module interface, you can branch cleanly in ABAP (or map to different return handling) and keep the contract explicit and maintainable.\n\nMessage class + number (most SAP-native): Use this when you want errors to look and behave like standard SAP messages—consistent wording, centralized maintenance, and better alignment with SAP operational practices. In this mode, ABAP can render the final localized string using FORMAT_MESSAGE and return it as EXCEPTIONMSG (and optionally BAPIRET2 -MESSAGE ), which makes the failure both human-friendly and SAP-friendly.\n\nA practical rule of thumb: start with the default exception while you stabilize the integration, move to named exceptions when you need clearer routing semantics, and adopt message classes when you want SAP-native error governance (standardization, maintainability, and localization). Regardless of the option, the key is to end with a predictable SAP-side contract: a clear success path, and a failure path that produces a structured return and a readable message.\n\n6. Response Handling\n\nThis section shows how the destination workflow returns either a successful analysis response or a workflow exception back to SAP, and how the source (caller) workflow interprets the RFC response structure to produce a single, human‑readable outcome (an email body). The key idea is to keep the SAP-facing contract stable: SAP always returns a Z_GET_ORDERS_ANALYSISResponse envelope, and the caller workflow decides between success and error using just two fields: EXCEPTIONMSG and RETURN /MESSAGE . To summarize the steps: Destination workfloweither: sends a normal response via Respond to SAP server, or\n\nraises an exception via Send exception to SAP server (with an error message).\n\nSAP serverexposes those outcomes through the RFC wrapper: sy-subrc = 0 → success (EXCEPTIONMSG = 'ok')\n\nsy-subrc = 1 → workflow exception (SENDEXCEPTIONTOSAPSERVER )\n\nsy-subrc = 2/3 → system/communication failures\n\nSource workflowcalls the RFC, extracts: EXCEPTIONMSG\n\nRETURN /MESSAGE and uses an Has errors gate to choose between a success email body (analysis) or a failure email body (error summary).\n\nThe figure below shows the full return path for results and failures. On the right, the destination workflow either responds normally (Respond to SAP server) or raises a workflow exception (Send exception to SAP server). SAP then maps that into the RFC outcome (sy-subrc and message fields). On the left, the source workflow parses the RFC response structure and populates a single EmailBody variable using two cases: failure (error details) or success (analysis text).\n\nFigure: Response/exception flow\n\nTwo things make this pattern easy to operationalize. First, the caller workflow does not need to understand every SAP field—only EXCEPTIONMSG and RETURN /MESSAGE are required to decide success vs failure. Second, the failure path intentionally aggregates details (MESSAGE_V1 …MESSAGE_V4 plus the exception text) into a single readable string so errors don’t get trapped in run history.\n\nCallout: The caller workflow deliberately treats EXCEPTIONMSG != \"ok\" or RETURN /MESSAGE present as the single source of truth for failure, which keeps the decision logic stable even if the response schema grows.\n\nDetailed description\n\nPhase 1 — Destination workflow: choose “response” vs “exception”\n\nRespond to SAP server returns the normal response payload back to SAP.\n\nSend exception to SAP server raises a workflow failure with an Exception Error Message (the screenshot shows an example beginning with “Unexpected action in request:” and a token for Function Name).\n\nOutcome: SAP receives either a normal response or a raised exception for the RFC call.\n\nPhase 2 — SAP server: map workflow outcomes to RFC results\n\nThe SAP-side wrapper code shown in the figure calls: CALL FUNCTION 'Z_GET_ORDERS_ANALYSIS ' DESTINATION DEST ...\n\nIt declares exception mappings including: SENDEXCEPTIONTOSAPSERVER = 1\n\nsystem_failure = 2 MESSAGE EXCEPTIONMSG\n\ncommunication_failure = 3 MESSAGE EXCEPTIONMSG\n\nOTHERS = 4\n\nThen it uses CASE sy-subrc . to normalize outcomes (the figure shows WHEN\n0. setting EXCEPTIONMSG\n= 'ok'., and WHEN\n1. building a readable message for the workflow exception).\n\nOutcome: regardless of why it failed, SAP can provide a consistent set of fields back to the caller: a return structure and an exception/status message.\n\nPhase 3 — Source workflow: parse response and build one “email body”\n\nAfter the RFC action ([RFC] Call Z GET ORDERS ANALYSIS ) the source workflow performs: Save EXCEPTION messageExtracts EXCEPTIONMSG from the response XML using XPath.\n\nSave RETURN messageExtracts RETURN /MESSAGE from the response XML using XPath.\n\nInitialize email bodyCreates EmailBody once, then sets it in exactly one of two cases.\n\nHas errors (two cases)The condition treats the run as “error” if either: EXCEPTIONMSG is not equal to \"ok\", or\n\nRETURN /MESSAGE is not empty.\n\nSet email body (failure) / Set email body (success) Failure: builds a consolidated string containing RETURN /MESSAGE , message details (MESSAGE_V1 ..V4), and EXCEPTIONMSG .\n\nSuccess: sets EmailBody to the ANALYSIS field extracted from the response.\n\nOutcome: the caller produces a single artifact (EmailBody) that is readable and actionable, without requiring anyone to inspect the raw RFC response.\n\n7. Destination Workflow #2: Persisting failed rows as custom IDocs\n\nIn this section I zoom in on the optional “IDoc persistence” branch at the end of the destination workflow. After the workflow identifies invalid rows (via the Data Validation Agent) and emails a verification summary, it can optionally call a second SAP RFC to save the failed rows as IDocs for later processing.\n\nThis is mainly included to showcase another common SAP integration scenario—creating/handling IDocs—and to highlight that you can combine “AI-driven validation” with traditional enterprise workflows. The deeper motivation for invoking this as part of the agent tooling is covered in Part 2; here, the goal is to show the connector pattern and the custom RFC used to create IDocs from CSV input.\n\nThe figure below shows the destination workflow at two levels: a high-level overview at the top, and a zoomed view of the post-validation remediation steps at the bottom. The zoom starts from Data Validation Agent → Summarize CSV payload review and then expands the sequence that runs after Send verification summary: Transform CSV to XML followed by an SAP RFC call that creates IDocs from the failed data.\n\nFigure: Zoomed remediation branch The key point is that this branch is not the main “analysis response” path. It’s a practical remediation option: once invalid rows are identified and reported, the workflow can persist them into SAP using a dedicated RFC (Z_CREATE_ONLINEORDER_IDOC ) and a simple IT_CSV payload. This keeps the end-to-end flow modular: analysis can remain focused on validated data, while failed records can be routed to SAP for follow-up processing on their own timeline.\n\nCallout: This branch exists to showcase an IDoc-oriented connector scenario. The “why this is invoked from the agent tooling” context is covered in Part 2; here the focus is the mechanics of calling Z_CREATE_ONLINEORDER_IDOC with IT_CSV and receiving ET_RETURN / ET_DOCNUMS .\n\nThe screenshot shows an XML body with the RFC root element and an SAP namespace:\n\n... ... ... ... ...\n\n@{ ...Outputs... }\n\nWhat to notice: the workflow passes invalid CSV rows in IT_CSV , and SAP returns a status table (ET_RETURN ) and created document numbers (ET_DOCNUMS ) for traceability.\n\nThe payload includes standard-looking control fields (IV_DIRECTION , IV_SNDPTR , IV_SNDPRN , IV_RCVPTR , IV_RCVPRN ) and the actual failed-row payload as IT_CSV .\n\nIT_CSV is populated via a Logic Apps expression (shown as @{ ...Outputs... } in the screenshot), which is the bridge between the prior transform step and the RFC call.\n\nThe response side indicates table-like outputs: ET_RETURN and ET_DOCNUMS .\n\n7.1 From CSV to IDocs\n\nI’ll cover the details of Destination workflow #2 in Part 2. In this post (Part 1), I focus on the contract and the end-to-end mechanics: what the RFC expects, what it returns, and how the created IDocs show up in the receiving workflow.\n\nBefore looking at the RFC itself, it helps to understand the payload we’re building inside the IDoc. The screenshot below shows the custom segment definition used by the custom IDoc type. This segment is intentionally shaped to mirror the columns of the CSV input so the mapping stays direct and easy to reason about.\n\nFigure: Custom segment ZONLINEORDER000 (segment type ZONLINEORDER )\n\nThis segment definition is the contract anchor: it makes the CSV-to-IDoc mapping explicit and stable. Each CSV record becomes one segment instance with the same 14 business fields. That keeps the integration “boringly predictable,” which is exactly what you want when you’re persisting rejected records for later processing.\n\nThe figure below shows the full loop for persisting failed rows as IDocs. The source workflow calls the custom RFC and sends the invalid CSV rows as XML. SAP converts each row into the custom segment and creates outbound IDocs. Those outbound IDocs are then received by Destination workflow #2, which processes them asynchronously (one workflow instance per IDoc) and appends results into shared storage for reporting.\n\nFigure: Persisting rejected rows as IDocs This pattern deliberately separates concerns: the first destination workflow identifies invalid rows and decides whether to persist them,\n\nSAP encapsulates the mechanics of IDoc creation behind a stable RFC interface, and\n\na second destination workflow processes those IDocs asynchronously (one per IDoc), which is closer to how IDoc-driven integrations typically operate in production.\n\nDestination workflow #2 is included here to show the end-to-end contract and the “receipt” side of the connector scenario: Triggered by the SAP built-in trigger and checks FunctionName = IDOC_INBOUND_ASYNCHRONOUS\n\nextracts DOCNUM from the IDoc control record (EDI_DC40 /DOCNUM )\n\nreconstructs a CSV payload from the IDoc data segment (the fields shown match the segment definition)\n\nappends a “verification info” line to shared storage for reporting\n\nThe implementation details of that workflow (including why it is invoked from the agent tooling) are covered in Part 2.\n\n7.2 Z_CREATE_ONLINEORDER_IDOC\n- Contract overview\n\nThe full source code for Z_CREATE_ONLINEORDER_IDOC is included in the supporting material. It’s too long to reproduce inline, so this post focuses on the contract—the part you need to call the RFC correctly and interpret its results.\n\nA quick note on authorship: most of the implementation was generated with Copilot, with manual review and fixes to resolve build errors and align the behavior with the intended integration pattern. The contract is deliberately generic because the goal was to produce an RFC that’s reusable across more than one scenario, rather than tightly coupled to a single workflow.\n\nAt a high level, the RFC is designed to support: Both inbound and outbound IDoc creationIt can either write IDocs to the SAP database (inbound-style persistence) or create/distribute IDocs outbound.\n\nMultiple IDoc/message/segment combinationsIDoc type (IDOCTYP ), message type (MESTYP ), and segment type (SEGTP ) are configurable so the same RFC can be reused.\n\nExplicit partner/port routing controlOptional sender/receiver partner/port fields can be supplied when routing matters.\n\nTraceability of created artifactsThe RFC returns created IDoc numbers so the caller can correlate “these failed rows” to “these IDocs.”\n\nContract:\n\nInputs (import parameters) IV_DIRECTION (default: 'O') — 'I' for inbound write-to-db, 'O' for outbound distribute/dispatch\n\nIV_IDOCTYP (default: ZONLINEORDERIDOC )\n\nIV_MESTYP (default: ZONLINEORDER )\n\nIV_SEGTP (default: ZONLINEORDER )\n\nOptional partner/port routing fields: IV_SNDPRT , IV_SNDPRN , IV_RCVPRT , IV_RCVPRN , IV_RCVPOR\n\nTables IT_CSV (structure ZTY_CSV_LINE ) — each row is one CSV line (the “table-of-lines” pattern)\n\nET_RETURN (structure BAPIRET2 ) — success/warning/error messages (per-row and/or aggregate)\n\nET_DOCNUMS (type ZTY_DOCNUM_TT ) — list of created IDoc numbers for correlation/traceability\n\nOutputs EV_DOCNUM — a convenience “primary / last created” DOCNUM value returned by the RFC\n\n8. Concluding Remarks\n\nPart 1 established a stable SAP ↔ Logic Apps integration baseline: CSV moves end‑to‑end using explicit contracts, and failures are surfaced predictably. The source workflow reads CSV from Blob, wraps rows into the IT_CSV table‑of‑lines payload, calls Z_GET_ORDERS_ANALYSIS , and builds one outcome using two fields from the RFC response: EXCEPTIONMSG and RETURN /MESSAGE . The destination workflow gates requests, validates input, and returns only analysis (or errors) back to SAP while handling invalid rows operationally (notification + optional persistence).\n\nOn the error path, we covered three concrete patterns to raise workflow failures back into SAP: the default connector exception (SENDEXCEPTIONTOSAPSERVER ), named exceptions (explicit ABAP contract), and message‑class‑based errors (SAP‑native formatting via FORMAT_MESSAGE ). On the remediation side, we added a realistic enterprise pattern: persist rejected rows as custom IDocs via Z_CREATE_ONLINEORDER_IDOC (IT_CSV in, ET_RETURN\n+ ET_DOCNUMS\nout), using the custom segment ZONLINEORDER000 as the schema anchor and enabling downstream receipt in Destination workflow #2 (one run per IDoc, correlated via DOCNUM ).\n\nPart 2 is separate because it tackles a different problem: the AI layer. With contracts and error semantics now fixed, Part 2 can focus on the agent/tooling details that tend to iterate—rule retrieval, structured validation outputs, prompt constraints, token/history controls, and how the analysis output is generated and shaped—without muddying the transport story.\n\nAppendix 1: Parse XML with schema\n\nIn this section I consider the CSV payload creation as an example, but parsing XML with schema applies in every place where we get an XML input to process, such as when receiving SAP responses, exceptions, or request/responses from other RFCs.\n\nStrong contract\n\nThe Create_CSV_payload step in the shown implementation uses an xpath() + join() expression to extract LINE values from the incoming XML:\n\njoin( xpath( xml(triggerBody()?['content']), '/*[local-name()=\"Z_GET_ORDERS_ANALYSIS\"] /*[local-name()=\"IT_CSV\"] /*[local-name()=\"ZTY_CSV_LINE\"] /*[local-name()=\"LINE\"]/text()' ), '\\r\\n' ) That approach works, but it’s essentially a “weak contract”: it assumes the message shape stays stable and that your XPath continues to match. By contrast, the Parse XML with schema action turns the XML payload into structured data based on an XSD, which gives you a “strong contract” and enables downstream steps to bind to known fields instead of re-parsing XML strings.\n\nThe figure below compares two equivalent ways to build the CSV payload from the RFC input. On the left is the direct xpath() compose (labeled “weak contract”). On the right is the schema-based approach (labeled “strong contract”), where the workflow parses the request first and then builds the CSV payload by iterating over typed rows.\n\nFigure: comparison Compose/XPath vs. Parse XML with schema. What’s visible in the diagram is the key tradeoff: XPath compose path (left): the workflow creates the CSV payload directly using join(xpath(...), '\\r\\n') , with the XPath written using local-name() selectors. This is fast to prototype, but the contract is implicit—your workflow “trusts” the XML shape and your XPath accuracy.\n\nParse XML with schema path (right): the workflow inserts a Parse XML with schema step (“Parse Z GET ORDERS ANALYSIS request ”), initializes variables, loops For each CSV row, and Appends to CSV payload, then performs join(variables('CSVPayload'), '\\r\\n') . Here, the contract is explicit—your XSD defines what IT_CSV and LINE mean, and downstream steps bind to those fields rather than re-parsing XML.\n\nA good rule of thumb is: XPath is great for lightweight extraction, while Parse XML with schema is better when you want contract enforcement and long-term maintainability, especially in enterprise integration / BizTalk migration scenarios where schemas are already part of the integration culture.\n\nImplementation details\n\nThe next figure shows the concrete configuration for Parse XML with schema and how its outputs flow into the “For each CSV row” loop. This is the “strong contract” version of the earlier XPath compose.\n\nFigure: Parse XML with schema - details. This screenshot highlights three practical implementation details: The Parse action is schema-backed.In the Parameters pane, the action uses: Content: the incoming XML Response\n\nSchema source: LogicApp\n\nSchema name: Z_GET_ORDERS_ANALYSISThe code view snippet shows the same idea: type: \"XmlParse\" with content: \"@triggerBody()?['content'] \" and schema: { source: \"LogicApp\", name: \"Z_GET_ORDERS_ANALYSIS.xsd\" }.\n\nThe parsed output becomes typed “dynamic content.”The loop input is shown as “JSON Schema for element 'Z_GET_ORDERS_ANALYSIS: IT_CSV' ”. This is the key benefit: you are no longer scraping strings—you are iterating over a structured collection that was produced by schema-based parsing.\n\nThe LINE extraction becomes trivial and readable.The “Append to CSV payload” step appends @item()?['LINE'] to the CSVpayload variable (as shown in the code snippet). Then the final Create CSV payload becomes a simple join(variables('CSVPayload'), '\\r\\n') . This is exactly the kind of “workflow readability” benefit you get once XML parsing is schema-backed.\n\nSchema generation\n\nThe Parse action requires XSD schemas, which can be stored in the Logic App (or via a linked Integration Account). The final figure shows a few practical ways to obtain and manage those XSDs: Generate Schema (SAP connector): a “Generate Schema” action with Operation Type = RFC and an RFC Name field, which is a practical way to bootstrap schema artifacts when you already know the RFC you’re calling.\n\nRun Diagnostics / Fetch RFC Metadata: a “Run Diagnostics” action showing Operation type = Fetch RFC Metadata and RFC Name, which is useful to confirm the shape of the RFC interface and reconcile it with your XSD/contract.\n\nFigure: schema generation. If you don’t want to rely solely on connector-side schema generation, there are also classic “developer tools” approaches: Infer XSD from a sample XML using .NET’s XmlSchemaInference (good for quick starting points).\n\nGenerate XSD from an XML instance using xsd.exe (handy when you already have representative sample payloads) or by asking your favorite AI prompt.\n\nWhen to choose XPath vs Parse XML with schema (practical guidance)\n\nGenerally speaking, choose XPath when… You need a quick extraction and you’re comfortable maintaining a single XPath.\n\nYou don’t want to manage schema artifacts yet (early prototypes).\n\nChoose Parse XML with schema when… You want a stronger, explicit contract (XSD defines what the payload is).\n\nYou want the designer to expose structured outputs (“JSON Schema for element …”) so downstream steps are readable and less brittle.\n\nYou expect the message shape to evolve over time and prefer schema-driven changes over XPath surgery.\n\nAppendix 2: Using FORMAT_MESSAGE to produce SAP‑native error text\n\nWhen propagating failures from Logic Apps back into SAP (for example via Send exception to SAP server), I want the SAP side to produce a predictable, human‑readable message without forcing callers to parse connector‑specific payloads. ABAP’s FORMAT_MESSAGE is ideal for this because it converts SAP’s message context—message class, message number, and up to four variables—into the final message text that SAP would normally display, but without raising a UI message.\n\nWhat FORMAT_MESSAGE does\n\nFORMAT_MESSAGE formats a message defined in SAP’s message catalog (T100 / maintained via SE91 ) using the values in sy-msgid , sy-msgno , and sy-msgv1 …sy-msgv4 . Conceptually, it answers the question:\n\n“Given message class + number + variables, what is the rendered message string?”\n\nThis is particularly useful after an RFC call fails, where ABAP may have message context available even if the exception itself is not a clean string.\n\nWhy this matters in an RFC wrapper\n\nIn the message class–based exception configuration, the workflow can provide message metadata (class/number/type) so that SAP can behave “natively”: ABAP receives a failure (sy-subrc 0), formats the message using FORMAT_MESSAGE , and returns the final text in a field like EXCEPTIONMSG (and/or in BAPIRET2 -MESSAGE ). The result is: consistent wording across systems and environments\n\neasier localization (SAP selects language-dependent text)\n\nseparation of concerns: code supplies variables; message content lives in message maintenance\n\nA robust pattern\n\nAfter the RFC call, I use this order of precedence: Use any explicit text already provided (for example via system_failure … MESSAGE exceptionmsg), because it’s already formatted.\n\nIf that’s empty but SAP message context exists (sy-msgid / sy-msgno ), call FORMAT_MESSAGE to produce the final string.\n\nIf neither is available, fall back to a generic message that includes sy-subrc .\n\nHere is a compact version of that pattern:\n\nDATA: lv_text TYPE string.\n\nCALL FUNCTION 'Z_GET_ORDERS_ANALYSIS' DESTINATION dest IMPORTING analysis = analysis TABLES it_csv = it_csv CHANGING return = return EXCEPTIONS sendexceptiontosapserver = 1 system_failure = 2 MESSAGE exceptionmsg communication_failure = 3 MESSAGE exceptionmsg OTHERS = 4.\n\nIF sy-subrc 0.\n\n\"Prefer explicit message text if it already exists IF exceptionmsg IS INITIAL.\n\n\"Otherwise format SAP message context into a string IF sy-msgid IS NOT INITIAL AND sy-msgno IS NOT INITIAL. CALL FUNCTION 'FORMAT_MESSAGE' EXPORTING id = sy-msgid no = sy-msgno v1 = sy-msgv1 v2 = sy-msgv2 v3 = sy-msgv3 v4 = sy-msgv4 IMPORTING msg = lv_text.\n\nexceptionmsg = lv_text. ELSE. exceptionmsg = |RFC failed (sy-subrc={ sy-subrc }).|. ENDIF.\n\nENDIF.\n\n\"Optionally normalize into BAPIRET2 for structured consumption return-type = 'E'. return-message = exceptionmsg.\n\nENDIF. Common gotchas FORMAT_MESSAGE only helps if sy-msgid and sy-msgno are set. If the failure did not originate from an SAP message (or message mapping is disabled), these fields may be empty—so keep a fallback.\n\nMessage numbers are typically 3-digit strings (e.g., 001, 012), matching how messages are stored in the catalog.\n\nFORMAT_MESSAGE formats text; it does not raise or display a message. That makes it safe to use in RFC wrappers and background processing.\n\nBottom line: FORMAT_MESSAGE is a simple tool that helps workflow‑originated failures “land” in SAP as clean, SAP‑native messages—especially when using message classes to standardize and localize error text.\n\nReferences\n\nAgentic Logic Apps Integration with SAP - Part 2: AI Agents\n\nHandling Errors in SAP BAPI Transactions | Microsoft Community Hub\n\nAccess SAP from workflows | Microsoft Learn\n\nCreate common SAP workflows | Microsoft Learn\n\nGenerate Schemas for SAP Artifacts via Workflows | Microsoft Learn\n\nParse XML using Schemas in Standard workflows - Azure Logic Apps | Microsoft Learn\n\nAnnouncing XML Parse and Compose for Azure Logic Apps GA\n\nException Handling | ABAP Keyword Documentation\n\nHandling and Propagating Exceptions - ABAP Keyword Documentation\n\nSAP .NET Connector 3.1 Overview\n\nSAP .NET Connector 3.1 Programming Guide\n\nAll supporting content for this post may be found in the companion GitHub repository.`",
  "Author": "Emmanuel_Abram_Profeta"
}
