{
  "OutputDir": "_community",
  "Author": "Lee_Stott",
  "Tags": [],
  "Title": "Optimising AI Costs with Microsoft Foundry Model Router",
  "PubDate": "2026-02-27T08:00:00+00:00",
  "ProcessedDate": "2026-02-27 09:12:43",
  "EnhancedContent": "## A hands-on demo of intelligent model routing with real benchmark data\n\nMicrosoft Foundry Model Router analyses each prompt in real-time and forwards it to the most appropriate LLM from a pool of underlying models. Simple requests go to fast, cheap models; complex requests go to premium ones, all automatically.\n\nI built an [interactive demo app](https://github.com/leestott/router-demo-app/) so you can see the routing decisions, measure latencies, and compare costs yourself. This post walks through how it works, what we measured, and when it makes sense to use.\n\n## The Problem: One Model for Everything Is Wasteful\n\nTraditional deployments force a single choice:\n\n| Strategy | Upside | Downside | | --- | --- | --- | | Use a small model | Fast, cheap | Struggles with complex tasks | | Use a large model | Handles everything | Overpay for simple tasks | | Build your own router | Full control | Maintenance burden; hard to optimise |\n\nMost production workloads are **mixed-complexity**. Classification, FAQ look-ups, and data extraction sit alongside code analysis, multi-constraint planning, and long-document summarisation. Paying premium-model prices for the simple 40% is money left on the table.\n\n## The Solution: Model Router\n\nModel Router is a **trained language model** deployed as a single Azure endpoint. For each incoming request it:\n\n1. **Analyses the prompt** — complexity, task type, context length\n2. **Selects an underlying model** from the routing pool\n3. **Forwards the request** and returns the response\n4. **Exposes the choice** via the `response.model`\nfield\n\nYou interact with one deployment. No if/else routing logic in your code.\n\n### Routing Modes\n\n| Mode | Goal | Trade-off | | --- | --- | --- | | **Balanced** (default) | Best cost-quality ratio | General-purpose | | **Cost** | Minimise spend | May use smaller models more aggressively | | **Quality** | Maximise accuracy | Higher cost for complex tasks |\n\nModes are configured in the Foundry Portal, no code change needed to switch.\n\n## Building the Demo\n\nTo make routing decisions tangible, we built a React + TypeScript app that sends the **same prompt** through both Model Router and a fixed standard deployment (e.g. GPT-5-nano), then compares:\n\n- **Which model** the router selected\n- **Latency** (ms)\n- **Token usage** (prompt + completion)\n- **Estimated cost** (based on per-model pricing)\n\nSelect a prompt, choose a routing mode, and hit Run Both to compare side-by-side\n\n### What You Can Do\n\n- **10 pre-built prompts** spanning simple classification to complex multi-constraint planning\n- **Custom prompt input** enter any text and benchmarks run automatically\n- **Three routing modes** switch and re-run to see how distribution changes\n- **Batch mode** run all 10 prompts in one click to gather aggregate stats\n\n### API Integration\n\nThe integration is a standard Azure OpenAI chat completion call. The only difference is the deployment name (`model-router` instead of a specific model):\n\n```\n\n```\n\n``` const response = await fetch( `${endpoint}/openai/deployments/model-router/chat/completions?api-version=2024-10-21`, { method: 'POST', headers: { 'Content-Type': 'application/json', 'api-key': apiKey, }, body: JSON.stringify({ messages: [{ role: 'user', content: prompt }], max_completion_tokens: 1024, }), } );\n\nconst data = await response.json();\n\n// The key insight: response.model reveals the underlying model const selectedModel = data.model; // e.g. \"gpt-5-nano-2025-08-07\" ```\n\n```\n\n```\n\nThat `data.model` field is what makes cost tracking and distribution analysis possible.\n\n## Results: What the Data Shows\n\nWe ran all 10 prompts through both Model Router (Balanced mode) and a fixed standard deployment.\n\n> >\n> **Note**: Results vary by run, region, model versions, and Azure load. These numbers are from a representative sample run.\n> >\n\nSide-by-side comparison across all 10 prompts in Balanced mode\n\n### Summary\n\n| Metric | Router (Balanced) | Standard (GPT-5-nano) | | --- | --- | --- | | Avg Latency | ~7,800 ms | ~7,700 ms | | Total Cost (10 prompts) | ~$0.029 | ~$0.030 | | **Cost Savings** | **~4.5%** | — | | Models Used | 4 | 1 |\n\n### Model Distribution\n\nThe router used **4 different models** across 10 prompts:\n\n| Model | Requests | Share | Typical Use | | --- | --- | --- | --- | | `gpt-5-nano` | 5 | 50% | Classification, summarisation, planning | | `gpt-5-mini` | 2 | 20% | FAQ answers, data extraction | | `gpt-oss-120b` | 2 | 20% | Long-context analysis, creative tasks | | `gpt-4.1-mini` | 1 | 10% | Complex debugging & reasoning |\n\nRouting distribution chart — the router favours efficient models for simple prompts\n\n### Across All Three Modes\n\n| Metric | Balanced | Cost-Optimised | Quality-Optimised | | --- | --- | --- | --- | | Cost Savings | ~4.5% | ~4.7% | ~14.2% | | Avg Latency (Router) | ~7,800 ms | ~7,800 ms | ~6,800 ms | | Avg Latency (Standard) | ~7,700 ms | ~7,300 ms | ~8,300 ms | | Primary Goal | Balance cost + quality | Minimise spend | Maximise accuracy | | Model Selection | Mixed (4 models) | Prefers cheaper | Prefers premium |\n\nCost-optimised mode — routes more aggressively to nano/mini models\n\nQuality-optimised mode — routes to larger models for complex tasks\n\n## Analysis\n\n### What Worked Well\n\n**Intelligent distribution**  The router didn't just default to one model. It used 4 different models and mapped prompt complexity to model capability: simple classification → nano, FAQ answers → mini, long-context documents → oss-120b, complex debugging → 4.1-mini.\n\n**Measurable cost savings across all modes**  4.5% in Balanced, 4.7% in Cost, and 14.2% in Quality mode. Quality mode was the surprise winner by choosing faster, cheaper models for simple prompts, it actually saved the most while still routing complex requests to capable models.\n\n**Zero routing logic in application code** One endpoint, one deployment name. The complexity lives in Azure's infrastructure, not yours.\n\n**Operational flexibility** Switch between Balanced, Cost, and Quality modes in the Foundry Portal without redeploying your app. Need to cut costs for a high-traffic period? Switch to Cost mode. Need accuracy for a compliance run? Switch to Quality.\n\n**Future-proofing**  As Azure adds new models to the routing pool, your deployment benefits automatically. No code changes needed.\n\n### Trade-offs to Consider\n\n**Latency is comparable, not always faster**  In Balanced mode, Router averaged ~7,800 ms vs Standard's ~7,700 ms  nearly identical. In Quality mode, the Router was actually *faster* (~6,800 ms vs ~8,300 ms) because it chose more efficient models for simple prompts. The delta depends on which models the router selects.\n\n**Savings scale with workload diversity** Our 10-prompt test set showed 4.5–14.2% savings. Production workloads with a wider spread of simple vs complex prompts should see larger savings, since the router has more opportunity to route simple requests to cheaper models.\n\n**Opaque routing decisions**  You can see *which* model was picked via `response.model` , but you can't see *why*. For most applications this is fine; for debugging edge cases you may want to test specific prompts in the demo first.\n\n## Custom Prompt Testing\n\nOne of the most practical features of the demo is testing **your own prompts** before committing to Model Router in production.\n\nEnter any prompt  `the quantum computing example is a medium-complexity educational prompt`\n\nBenchmarks execute automatically, showing the selected model, latency, tokens, and cost\n\n**Workflow:**\n\n1. Click **✏️ Custom** in the prompt selector\n2. Enter your production-representative prompt\n3. Click **✓ Use This Prompt** — Router and Standard run automatically\n4. Compare results — repeat with different routing modes\n5. Use the data to inform your deployment strategy\n\nThis lets you **predict costs and validate routing behaviour** with your actual workload before going to production.\n\n## When to Use Model Router\n\n### Great Fit\n\n- **Mixed-complexity workloads** — chatbots, customer service, content pipelines\n- **Cost-sensitive deployments** — where even single-digit percentage savings matter at scale\n- **Teams wanting simplicity** — one endpoint beats managing multi-model routing logic\n- **Rapid experimentation** — try new models without changing application code\n\n### Consider Carefully\n\n- **Ultra-low-latency requirements** — if you need sub-second responses, the routing overhead matters\n- **Single-task, single-model workloads** — if one model is clearly optimal for 100% of your traffic, a router adds complexity without benefit\n- **Full control over model selection** — if you need deterministic model choice per request\n\n### Mode Selection Guide\n\nIs accuracy critical (compliance, legal, medical)?\n\n``` Is accuracy critical (compliance, legal, medical)? └─ YES → Quality-Optimised └─ NO → Strict budget constraints? └─ YES → Cost-Optimised └─ NO → Balanced (recommended) ```\n\n## Best Practices\n\n1. **Start with Balanced mode** — measure actual results, then optimise\n2. **Test with your real prompts** — use the Custom Prompt feature to validate routing before production\n3. **Monitor model distribution** — track which models handle your traffic over time\n4. **Compare against a baseline** — always keep a standard deployment to measure savings\n5. **Review regularly** — as new models enter the routing pool, distributions shift\n\n## Technical Stack\n\n| Technology | Purpose | | --- | --- | | React 19 + TypeScript 5.9 | UI and type safety | | Vite 7 | Dev server and build tool | | Tailwind CSS 4 | Styling | | Recharts 3 | Distribution and comparison charts | | Azure OpenAI API (2024-10-21) | Model Router and standard completions |\n\nSecurity measures include an `ErrorBoundary` for crash resilience, sanitised API error messages, `AbortController` request timeouts, input length validation, and restrictive security headers. API keys are loaded from environment variables and gitignored. Source:[leestott/router-demo-app: An interactive web application demonstrating the power of Microsoft Foundry Model Router - an intelligent routing system that automatically selects the optimal language model for each request based on complexity, reasoning requirements, and task type.](https://github.com/leestott/router-demo-app/)\n\n⚠️ **This demo calls Azure OpenAI directly from the browser.** This is fine for local development. For production, proxy through a backend and use [Managed Identity](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity).\n\n## [Try It Yourself](https://github.com/leestott/router-demo-app/)\nQuick Start\n\n``` git clone https://github.com/leestott/router-demo-app/ ```\n\n```\n\ncd router-demo-app\n\n# Option A: Use the setup script (recommended)\n# Windows:\n.\\setup.ps1 -StartDev\n# macOS/Linux:\nchmod +x setup.sh && ./setup.sh --start-dev\n\n# Option B: Manual\nnpm install cp .env.example .env.local\n# Edit .env.local with your Azure credentials\nnpm run dev ```\n\nOpen `http://localhost:5173` , select a prompt, and click **⚡ Run Both**.\n\n### Get Your Credentials\n\n1. Go to [ai.azure.com](https://ai.azure.com) → open your project\n2. Copy the **Project connection string** (endpoint URL)\n3. Navigate to **Deployments** → confirm `model-router`\nis deployed\n4. Get your **API key** from **Project Settings → Keys**\n\n### Configuration\n\nEdit `.env.local` :\n\n``` VITE_ROUTER_ENDPOINT=https://your-resource.cognitiveservices.azure.com VITE_ROUTER_API_KEY=your-api-key VITE_ROUTER_DEPLOYMENT=model-router\n\nVITE_STANDARD_ENDPOINT=https://your-resource.cognitiveservices.azure.com VITE_STANDARD_API_KEY=your-api-key VITE_STANDARD_DEPLOYMENT=gpt-5-nano ```\n\n## Ideas for Enhancement\n\n- **Historical analysis** — persist results to track routing trends over time\n- **Cost projections** — estimate monthly spend based on prompt patterns and volume\n- **A/B testing framework** — compare modes with statistical significance\n- **Streaming support** — show model selection for streaming responses\n- **Export reports** — download benchmark data as CSV/JSON for further analysis\n\n## Conclusion\n\nModel Router addresses a real problem: most AI workloads have mixed complexity, but most deployments use a single model. By routing each request to the right model automatically, you get:\n\n- **Cost savings** (~4.5–14.2% measured across modes, scaling with volume)\n- **Intelligent distribution** (4 models used, zero routing code)\n- **Operational simplicity** (one endpoint, mode changes via portal)\n- **Future-proofing** (new models added to the pool automatically)\n\nThe latency trade-off is minimal — in Quality mode, the Router was actually *faster* than the standard deployment. The real value is **flexibility**: tune for cost, quality, or balance without touching your code.\n\n**Ready to try it?** Clone the demo repository, plug in your Azure credentials, and test with your own prompts.\n\n## Resources\n\n- [Model Router Benchmark Sample](https://github.com/leestott/router-demo-app/)  Sample App\n- [Model Router Concepts](https://learn.microsoft.com/azure/ai-foundry/openai/concepts/model-router)  Official documentation\n- [Model Router How-To](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/model-router)  Deployment guide\n- [Microsoft Foundry Portal](https://ai.azure.com)  Deploy and manage\n- [Model Router in the Catalog](https://ai.azure.com/catalog/models/model-router) Model listing\n- [Azure OpenAI Managed Identity](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity) Production auth\n\nUpdated Feb 13, 2026\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai foundry](/tag/ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure](/tag/azure?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure ai foundry](/tag/azure%20ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[best practices](/tag/best%20practices?nodeId=board%3AAzureDevCommunityBlog)\n\n[llm](/tag/llm?nodeId=board%3AAzureDevCommunityBlog)\n\n[tips and tricks](/tag/tips%20and%20tricks?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[Lee_Stott&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yMTA1NDYtODM5MjVpMDI2ODNGQTMwMzAwNDFGQQ?image-dimensions=50x50)](/users/lee_stott/210546) [Lee_Stott](/users/lee_stott/210546) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined September 25, 2018\n\n[View Profile](/users/lee_stott/210546)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "FeedName": "Microsoft Tech Community",
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/optimising-ai-costs-with-microsoft-foundry-model-router/ba-p/4494776",
  "Description": "Microsoft Foundry Model Router analyses each prompt in real-time and forwards it to the most appropriate LLM from a pool of underlying models. Simple requests go to fast, cheap models; complex requests go to premium ones, all automatically.\n\nI built an [interactive demo app](https://github.com/leestott/router-demo-app/) so you can see the routing decisions, measure latencies, and compare costs yourself. This post walks through how it works, what we measured, and when it makes sense to use.\n\n## The Problem: One Model for Everything Is Wasteful\n\nTraditional deployments force a single choice:\n\n| Strategy | Upside | Downside | | --- | --- | --- | | Use a small model | Fast, cheap | Struggles with complex tasks | | Use a large model | Handles everything | Overpay for simple tasks | | Build your own router | Full control | Maintenance burden; hard to optimise |\n\nMost production workloads are **mixed-complexity**. Classification, FAQ look-ups, and data extraction sit alongside code analysis, multi-constraint planning, and long-document summarisation. Paying premium-model prices for the simple 40% is money left on the table.\n\n## The Solution: Model Router\n\nModel Router is a **trained language model** deployed as a single Azure endpoint. For each incoming request it:\n\n1. **Analyses the prompt** — complexity, task type, context length\n2. **Selects an underlying model** from the routing pool\n3. **Forwards the request** and returns the response\n4. **Exposes the choice** via the `response.model`\nfield\n\nYou interact with one deployment. No if/else routing logic in your code.\n\n### Routing Modes\n\n| Mode | Goal | Trade-off | | --- | --- | --- | | **Balanced** (default) | Best cost-quality ratio | General-purpose | | **Cost** | Minimise spend | May use smaller models more aggressively | | **Quality** | Maximise accuracy | Higher cost for complex tasks |\n\nModes are configured in the Foundry Portal, no code change needed to switch.\n\n## Building the Demo\n\nTo make routing decisions tangible, we built a React + TypeScript app that sends the **same prompt** through both Model Router and a fixed standard deployment (e.g. GPT-5-nano), then compares:\n\n- **Which model** the router selected\n- **Latency** (ms)\n- **Token usage** (prompt + completion)\n- **Estimated cost** (based on per-model pricing)\n\nSelect a prompt, choose a routing mode, and hit Run Both to compare side-by-side\n\n![]()\n\n### What You Can Do\n\n- **10 pre-built prompts** spanning simple classification to complex multi-constraint planning\n- **Custom prompt input** enter any text and benchmarks run automatically\n- **Three routing modes** switch and re-run to see how distribution changes\n- **Batch mode** run all 10 prompts in one click to gather aggregate stats\n\n### API Integration\n\nThe integration is a standard Azure OpenAI chat completion call. The only difference is the deployment name (`model-router` instead of a specific model):\n\n```\n\n```\n\n- const response = await fetch(\n`${endpoint}/openai/deployments/model-router/chat/completions?api-version=2024-10-21`, { method: 'POST', headers: { 'Content-Type': 'application/json', 'api-key': apiKey, }, body: JSON.stringify({ messages: [{ role: 'user', content: prompt }], max\\_completion\\_tokens: 1024, }), } );\n\nconst data = await response.json();\n\n// The key insight: response.model reveals the underlying model const selectedModel = data.model; // e.g. \"gpt-5-nano-2025-08-07\"\n\n```\n\n```\n\nThat `data.model` field is what makes cost tracking and distribution analysis possible.\n\n## Results: What the Data Shows\n\nWe ran all 10 prompts through both Model Router (Balanced mode) and a fixed standard deployment.\n\n> >\n> **Note**: Results vary by run, region, model versions, and Azure load. These numbers are from a representative sample run.\n> >\n\nSide-by-side comparison across all 10 prompts in Balanced mode\n\n![]()\n\n### Summary\n\n| Metric | Router (Balanced) | Standard (GPT-5-nano) | | --- | --- | --- | | Avg Latency | ~7,800 ms | ~7,700 ms | | Total Cost (10 prompts) | ~$0.029 | ~$0.030 | | **Cost Savings** | **~4.5%** | — | | Models Used | 4 | 1 |\n\n### Model Distribution\n\nThe router used **4 different models** across 10 prompts:\n\n| Model | Requests | Share | Typical Use | | --- | --- | --- | --- | | `gpt-5-nano` | 5 | 50% | Classification, summarisation, planning | | `gpt-5-mini` | 2 | 20% | FAQ answers, data extraction | | `gpt-oss-120b` | 2 | 20% | Long-context analysis, creative tasks | | `gpt-4.1-mini` | 1 | 10% | Complex debugging & reasoning |\n\nRouting distribution chart — the router favours efficient models for simple prompts\n\n![]()\n\n### Across All Three Modes\n\n| Metric | Balanced | Cost-Optimised | Quality-Optimised | | --- | --- | --- | --- | | Cost Savings | ~4.5% | ~4.7% | ~14.2% | | Avg Latency (Router) | ~7,800 ms | ~7,800 ms | ~6,800 ms | | Avg Latency (Standard) | ~7,700 ms | ~7,300 ms | ~8,300 ms | | Primary Goal | Balance cost + quality | Minimise spend | Maximise accuracy | | Model Selection | Mixed (4 models) | Prefers cheaper | Prefers premium |\n\nCost-optimised mode — routes more aggressively to nano/mini models\n\n![]()\n\nQuality-optimised mode — routes to larger models for complex tasks\n\n![]()\n\n## Analysis\n\n### What Worked Well\n\n**Intelligent distribution** The router didn't just default to one model. It used 4 different models and mapped prompt complexity to model capability: simple classification → nano, FAQ answers → mini, long-context documents → oss-120b, complex debugging → 4.1-mini.\n\n**Measurable cost savings across all modes** 4.5% in Balanced, 4.7% in Cost, and 14.2% in Quality mode. Quality mode was the surprise winner by choosing faster, cheaper models for simple prompts, it actually saved the most while still routing complex requests to capable models.\n\n**Zero routing logic in application code** One endpoint, one deployment name. The complexity lives in Azure's infrastructure, not yours.\n\n**Operational flexibility** Switch between Balanced, Cost, and Quality modes in the Foundry Portal without redeploying your app. Need to cut costs for a high-traffic period? Switch to Cost mode. Need accuracy for a compliance run? Switch to Quality.\n\n**Future-proofing** As Azure adds new models to the routing pool, your deployment benefits automatically. No code changes needed.\n\n### Trade-offs to Consider\n\n**Latency is comparable, not always faster** In Balanced mode, Router averaged ~7,800 ms vs Standard's ~7,700 ms nearly identical. In Quality mode, the Router was actually *faster* (~6,800 ms vs ~8,300 ms) because it chose more efficient models for simple prompts. The delta depends on which models the router selects.\n\n**Savings scale with workload diversity** Our 10-prompt test set showed 4.5–14.2% savings. Production workloads with a wider spread of simple vs complex prompts should see larger savings, since the router has more opportunity to route simple requests to cheaper models.\n\n**Opaque routing decisions** You can see *which* model was picked via `response.model` , but you can't see *why*. For most applications this is fine; for debugging edge cases you may want to test specific prompts in the demo first.\n\n## Custom Prompt Testing\n\nOne of the most practical features of the demo is testing **your own prompts** before committing to Model Router in production.\n\nEnter any prompt `the quantum computing example is a medium-complexity educational prompt` ![]() Benchmarks execute automatically, showing the selected model, latency, tokens, and cost\n\n![]()\n\n**Workflow:**\n\n1. Click **✏️ Custom** in the prompt selector\n2. Enter your production-representative prompt\n3. Click **✓ Use This Prompt** — Router and Standard run automatically\n4. Compare results — repeat with different routing modes\n5. Use the data to inform your deployment strategy\n\nThis lets you **predict costs and validate routing behaviour** with your actual workload before going to production.\n\n## When to Use Model Router\n\n### Great Fit\n\n- **Mixed-complexity workloads** — chatbots, customer service, content pipelines\n- **Cost-sensitive deployments** — where even single-digit percentage savings matter at scale\n- **Teams wanting simplicity** — one endpoint beats managing multi-model routing logic\n- **Rapid experimentation** — try new models without changing application code\n\n### Consider Carefully\n\n- **Ultra-low-latency requirements** — if you need sub-second responses, the routing overhead matters\n- **Single-task, single-model workloads** — if one model is clearly optimal for 100% of your traffic, a router adds complexity without benefit\n- **Full control over model selection** — if you need deterministic model choice per request\n\n### Mode Selection Guide\n\nIs accuracy critical (compliance, legal, medical)?\n\n- Is accuracy critical (compliance, legal, medical)?\n└─ YES → Quality-Optimised └─ NO → Strict budget constraints? └─ YES → Cost-Optimised └─ NO → Balanced (recommended)\n\n## Best Practices\n\n1. **Start with Balanced mode** — measure actual results, then optimise\n2. **Test with your real prompts** — use the Custom Prompt feature to validate routing before production\n3. **Monitor model distribution** — track which models handle your traffic over time\n4. **Compare against a baseline** — always keep a standard deployment to measure savings\n5. **Review regularly** — as new models enter the routing pool, distributions shift\n\n## Technical Stack\n\n| Technology | Purpose | | --- | --- | | React 19 + TypeScript 5.9 | UI and type safety | | Vite 7 | Dev server and build tool | | Tailwind CSS 4 | Styling | | Recharts 3 | Distribution and comparison charts | | Azure OpenAI API (2024-10-21) | Model Router and standard completions |\n\nSecurity measures include an `ErrorBoundary` for crash resilience, sanitised API error messages, `AbortController` request timeouts, input length validation, and restrictive security headers. API keys are loaded from environment variables and gitignored. Source:[leestott/router-demo-app: An interactive web application demonstrating the power of Microsoft Foundry Model Router - an intelligent routing system that automatically selects the optimal language model for each request based on complexity, reasoning requirements, and task type.](https://github.com/leestott/router-demo-app/)\n\n⚠️ **This demo calls Azure OpenAI directly from the browser.** This is fine for local development. For production, proxy through a backend and use [Managed Identity](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity).\n\n## [Try It Yourself](https://github.com/leestott/router-demo-app/)\nQuick Start\n\n``` git clone https://github.com/leestott/router-demo-app/ ```\n\n```\n\ncd router-demo-app\n\n# Option A: Use the setup script (recommended)\n# Windows:\n.\\setup.ps1 -StartDev\n# macOS/Linux:\nchmod +x setup.sh && ./setup.sh --start-dev\n\n# Option B: Manual\nnpm install cp .env.example .env.local\n# Edit .env.local with your Azure credentials\nnpm run dev ```\n\nOpen `http://localhost:5173` , select a prompt, and click **⚡ Run Both**.\n\n### Get Your Credentials\n\n1. Go to [ai.azure.com](https://ai.azure.com) → open your project\n2. Copy the **Project connection string** (endpoint URL)\n3. Navigate to **Deployments** → confirm `model-router`\nis deployed\n4. Get your **API key** from **Project Settings → Keys**\n\n### Configuration\n\nEdit `.env.local` :\n\n``` VITE_ROUTER_ENDPOINT=https://your-resource.cognitiveservices.azure.com VITE_ROUTER_API_KEY=your-api-key VITE_ROUTER_DEPLOYMENT=model-router\n\nVITE_STANDARD_ENDPOINT=https://your-resource.cognitiveservices.azure.com VITE_STANDARD_API_KEY=your-api-key VITE_STANDARD_DEPLOYMENT=gpt-5-nano ```\n\n## Ideas for Enhancement\n\n- **Historical analysis** — persist results to track routing trends over time\n- **Cost projections** — estimate monthly spend based on prompt patterns and volume\n- **A/B testing framework** — compare modes with statistical significance\n- **Streaming support** — show model selection for streaming responses\n- **Export reports** — download benchmark data as CSV/JSON for further analysis\n\n## Conclusion\n\nModel Router addresses a real problem: most AI workloads have mixed complexity, but most deployments use a single model. By routing each request to the right model automatically, you get:\n\n- **Cost savings** (~4.5–14.2% measured across modes, scaling with volume)\n- **Intelligent distribution** (4 models used, zero routing code)\n- **Operational simplicity** (one endpoint, mode changes via portal)\n- **Future-proofing** (new models added to the pool automatically)\n\nThe latency trade-off is minimal — in Quality mode, the Router was actually *faster* than the standard deployment. The real value is **flexibility**: tune for cost, quality, or balance without touching your code.\n\n**Ready to try it?** Clone the demo repository, plug in your Azure credentials, and test with your own prompts.\n\n## Resources\n\n- [Model Router Benchmark Sample](https://github.com/leestott/router-demo-app/) Sample App\n- [Model Router Concepts](https://learn.microsoft.com/azure/ai-foundry/openai/concepts/model-router) Official documentation\n- [Model Router How-To](https://learn.microsoft.com/azure/ai-foundry/openai/how-to/model-router) Deployment guide\n- [Microsoft Foundry Portal](https://ai.azure.com) Deploy and manage\n- [Model Router in the Catalog](https://ai.azure.com/catalog/models/model-router) Model listing\n- [Azure OpenAI Managed Identity](https://learn.microsoft.com/azure/ai-services/openai/how-to/managed-identity) Production auth",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure"
}
