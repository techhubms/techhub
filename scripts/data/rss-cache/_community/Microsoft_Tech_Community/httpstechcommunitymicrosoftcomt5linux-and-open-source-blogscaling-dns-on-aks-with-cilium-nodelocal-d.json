{
  "OutputDir": "_community",
  "Title": "Scaling DNS on AKS with Cilium: NodeLocal DNSCache, LRP, and FQDN Policies",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Tags": [],
  "Author": "Simone_Rodigari",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "FeedName": "Microsoft Tech Community",
  "PubDate": "2026-01-23T17:28:47+00:00",
  "EnhancedContent": "## Standard Kubernetes DNS forces every pod to traverse the network fabric to a centralized CoreDNS service, a design that becomes a scaling and latency bottleneck at cluster scale.\nBy default, pods send DNS queries to the kube-dns Service IP, which kube-proxy translates to CoreDNS endpoints via iptables rules. NodeLocal DNSCache removes this network hop by resolving queries locally on each node.h node.\n\n## Why Adopt NodeLocal DNSCache?\n\nThe primary drivers for adoption are usually:\n\n1. **Eliminating Conntrack Pressure:** In high-QPS UDP DNS scenarios, conntrack contention and UDP tracking can cause intermittent DNS response loss and retries; depending on resolver retry/timeouts, this can appear as multi-second lookup delays and sometimes much longer tails.\n2. **Reducing Latency**: By placing a cache on every node, you remove the network hop to the CoreDNS service. Responses are practically instantaneous for cached records.\n3. **Offloading CoreDNS**: A DaemonSet architecture effectively shards the DNS query load across the entire cluster, preventing the central CoreDNS deployment from becoming a single point of congestion during bursty scaling events.\n\n#### Who needs this?\n\nYou should prioritize this architecture if you run:\n\n- **Large-scale clusters** large clusters (hundreds of nodes or thousands of pods), where CoreDNS scaling becomes difficult to manage.\n- **High-churn endpoints**, such as spot instances or frequent auto-scaling jobs that trigger massive waves of DNS queries.\n- **Real-time applications** where multi-second (and occasionally longer) DNS lookup delays are unacceptable.\n\n## The Challenge with Cilium\n\nDeploying NodeLocal DNSCache on a cluster managed by **Cilium** (CNI) requires a specific approach. Standard NodeLocal DNSCache relies on node-level *interface*/*iptables* setup. In Cilium environments, you can instead implement the interception via **Cilium Local Redirect Policy (LRP)**, which redirects traffic destined to the *kube-dns* ClusterIP service to a node-local backend pod.\n\nThis post details a production-ready deployment strategy aligned with Cilium’s Local Redirect Policy model. It covers necessary configuration tweaks to avoid conflicts and explains how to maintain security filtering.\n\n## Architecture Overview\n\nIn a standard Kubernetes deployment, NodeLocal DNSCache creates a dummy network interface and uses extensive iptables rules to hijack traffic destined for the Cluster DNS IP.\n\nWhen using Cilium, we can achieve this more elegantly and efficiently using **Local Redirect Policies**.\n\n1. **DaemonSet**: Runs *node-local-dns* on every node.\n2. **Configuration**: Configured to skip interface creation and iptables manipulation.\n3. **Redirection**: Cilium LRP intercepts traffic to the *kube-dns* Service IP and redirects it to the local pod on the same node.\n\n### 1. The NodeLocal DNSCache DaemonSet\n\nThe critical difference in this manifest is the arguments passed to the *node-local-dns* binary. We must explicitly disable its networking setup functions to let Cilium handle the traffic.\n\nThe NodeLocal DNSCache deployment also requires the *node-local-dns ConfigMap* and the *kube-dns-upstream Service* (plus *RBAC/ServiceAccount*). For brevity, the snippet below shows only the DaemonSet arguments that differ in the Cilium/LRP approach. The *node-cache* reads the template *Corefile* (*/etc/coredns/Corefile.base*) and generates the active *Corefile* (*/etc/Corefile*). The *-conf* flag points CoreDNS at the active *Corefile* it should load.\n\nThe node-cache binary accepts *-localip* as an IP list; *0.0.0.0* is a valid value and makes it listen on all interfaces, appropriate for the LRP-based redirection model.\n\n``` apiVersion: apps/v1 kind: DaemonSet metadata: name: node-local-dns namespace: kube-system labels: k8s-app: node-local-dns spec: selector: matchLabels: k8s-app: node-local-dns template: metadata: labels: k8s-app: node-local-dns annotations:\n# Optional: policy.cilium.io/no-track-port can be used to bypass conntrack for DNS.\n# Validate the impact on your Cilium version and your observability/troubleshooting needs.\npolicy.cilium.io/no-track-port: \"53\" spec:\n# IMPORTANT for the \"LRP + listen broadly\" approach:\n# keep hostNetwork off so you don't hijack node-wide :53\nhostNetwork: false dnsPolicy: ClusterFirst containers:\n- name: node-cache\nimage: registry.k8s.io/dns/k8s-dns-node-cache:1.15.16 args:\n- \"-localip\"\n# Use a bind-all approach. Ensure server blocks bind broadly in your Corefile.\n- \"0.0.0.0\"\n- \"-conf\"\n- \"/etc/Corefile\"\n- \"-upstreamsvc\"\n- \"kube-dns-upstream\"\n# CRITICAL: Disable internal setup\n- \"-skipteardown=true\"\n- \"-setupinterface=false\"\n- \"-setupiptables=false\"\nports:\n- containerPort: 53\nname: dns protocol: UDP\n- containerPort: 53\nname: dns-tcp protocol: TCP\n# Ensure your Corefile includes health :8080 so the liveness probe works\nlivenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 volumeMounts:\n- name: config-volume\nmountPath: /etc/coredns\n- name: kube-dns-config\nmountPath: /etc/kube-dns volumes:\n- name: kube-dns-config\nconfigMap: name: kube-dns optional: true\n- name: config-volume\nconfigMap: name: node-local-dns items:\n- key: Corefile\npath: Corefile.base ```\n\n### 2. The Cilium Local Redirect Policy (LRP)\n\nInstead of iptables, we define a CRD that tells Cilium: \"When you see traffic for `kube-dns`, send it to the `node-local-dns` pod on this same node.\"\n\n``` apiVersion: \"cilium.io/v2\" kind: CiliumLocalRedirectPolicy metadata: name: \"nodelocaldns\" namespace: kube-system spec: redirectFrontend:\n# ServiceMatcher mode is for ClusterIP services\nserviceMatcher: serviceName: kube-dns namespace: kube-system redirectBackend:\n# The backend pods selected by localEndpointSelector must be in the same namespace as the LRP\nlocalEndpointSelector: matchLabels: k8s-app: node-local-dns toPorts:\n- port: \"53\"\nname: dns protocol: UDP\n- port: \"53\"\nname: dns-tcp protocol: TCP ```\n\nThis is an **LRP-based NodeLocal DNSCache deployment**: we disable node-cache’s *iptables*/*interface* setup and let **Cilium LRP** handle local redirection. This differs from the upstream NodeLocal DNSCache manifest, which uses *hostNetwork* + *dummy interface* + *iptables*.\n\n> >\n> LRP must be enabled in Cilium (e.g., *localRedirectPolicies.enabled=true*) before applying the CRD. [Official Cilium LRP doc](https://docs.cilium.io/en/stable/network/kubernetes/local-redirect-policy/#prerequisites)\n> >\n\n## The Network Policy \"Gotcha\"\n\nIf you use **CiliumNetworkPolicy** to restrict egress traffic, specifically for **FQDN filtering,** you typically allow access to CoreDNS like this:\n\n```\n- toEndpoints:\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: kube-dns toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY ```\n\n**This will break with local redirection.**\n\nWhy? Because LRP redirects the DNS request to the **node-local-dns backend endpoint**; strict egress policies must therefore allow both *kube-dns* (upstream) **and** *node-local-dns* (the redirected destination).\n\n### The Repro Setup\n\nTo demonstrate this failure, the cluster is configured with:\n\n1. **NodeLocal DNSCache**: Deployed as a DaemonSet (*node-local-dns*) to cache DNS requests locally on every node.\n2. **Local Redirect Policy (LRP)**: An active LRP intercepts traffic destined for the *kube-dns* Service IP and redirects it to the local *node-local-dns* pod.\n3. **Incomplete Network Policy**: A strict *CiliumNetworkPolicy* (CNP) is enforced on the client pod. While it explicitly allows egress to *kube-dns*, it **misses** the corresponding rule for *node-local-dns*.\n\n#### Reveal the issue using Hubble:\n\nIn this scenario, the client pod *dns-client* is attempting to resolve the external domain *github.com*.\n\nWhen inspecting the traffic flows, you will see *EGRESS DENIED* verdicts. Crucially, notice the destination pod in the logs below: *kube-system/node-local-dns*, not *kube-dns*.\n\nAlthough the application originally sent the packet to the Cluster IP of CoreDNS, Cilium's Local Redirect Policy modified the destination to the local node cache. Since strictly defined Network Policies assume traffic is going to the *kube-dns* identity, this redirected traffic falls outside the allowed rules and is dropped by the default deny stance.\n\n### The Fix: You must allow egress to both labels.\n\n```\n- toEndpoints:\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: kube-dns\n# Add this selector for the local cache\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: node-local-dns toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY ```\n\nWithout this addition, pods protected by strict egress policies will timeout resolving DNS, even though the cache is running.\n\n#### Use Hubble to observe the network flows:\n\nAfter adding *matchLabels: k8s:k8s-app: node-local-dns*, the traffic is now allowed. Hubble confirms a policy verdict of *EGRESS ALLOWED* for UDP traffic on port 53. Because DNS resolution now succeeds, the response populates the Cilium FQDN cache, subsequently allowing the TCP traffic to *github.com* on port 443 as intended.\n\n### Real-World Example: Restricting Egress with FQDN Policies\n\nHere is a complete *CiliumNetworkPolicy* that locks down a workload to only *access api.example.com.* Note how the DNS rule explicitly allows traffic to both *kube-dns* (for upstream) and *node-local-dns* (for the local cache).\n\n``` apiVersion: \"cilium.io/v2\" kind: CiliumNetworkPolicy metadata: name: secure-workload-policy spec: endpointSelector: matchLabels: app: critical-workload egress:\n# 1. Allow DNS Resolution (REQUIRED for FQDN policies)\n- toEndpoints:\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: kube-dns\n# Allow traffic to the local cache redirection target\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: node-local-dns toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY rules: dns:\n- matchPattern: \"*\"\n\n# 2. Allow specific FQDN traffic (populated via DNS lookups)\n- toFQDNs:\n- matchName: \"api.example.com\"\ntoPorts:\n- ports:\n- port: \"443\"\nprotocol: TCP ```\n\n## Configuration & Upstream Loops\n\nWhen configuring the *ConfigMap* for *node-local-dns*, use the standard placeholders provided by the image. The binary replaces them at runtime:\n\n- \\_\\_PILLAR\\_\\_CLUSTER\\_\\_DNS\\_\\_: The Upstream Service IP (*kube-dns-upstream*).\n- \\_\\_PILLAR\\_\\_UPSTREAM\\_\\_SERVERS\\_\\_: The system resolvers (usually */etc/resolv.conf*).\n\nEnsure *kube-dns-upstream* exists as a Service selecting the CoreDNS pods so cache misses are forwarded to the actual CoreDNS backends.\n\n## Alternative: AKS LocalDNS\n\n**LocalDNS** is an Azure Kubernetes Services (AKS)-managed node-local DNS proxy/cache.\n\n#### Pros:\n\n- Managed lifecycle at the node pool level.\n- Support for custom configuration via *localdnsconfig.json* (e.g., custom server blocks, cache tuning).\n- No manual DaemonSet management required.\n\n#### Cons & Limitations:\n\n- **Incompatibility with FQDN Policies**: As noted in the [official documentation](https://learn.microsoft.com/en-us/azure/aks/localdns-custom), LocalDNS isn’t compatible with applied FQDN filter policies in ACNS/Cilium; if you rely on FQDN enforcement, prefer a DNS path that preserves FQDN learning/enforcement.\n- Updating configuration requires reimaging the node pool.\n\nFor environments heavily relying on strict Cilium Network Policies and FQDN filtering, the manual deployment method described above (using LRP) can be more reliable and transparent.\n\n> >\n> AKS recommends not enabling both upstream NodeLocal DNSCache and LocalDNS in the same node pool, as DNS traffic is routed through LocalDNS and results may be unexpected.\n> >\n\n## References\n\n1. [Kubernetes Documentation: NodeLocal DNSCache](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/)\n2. [Cilium Documentation: Local Redirect Policy](https://docs.cilium.io/en/stable/network/kubernetes/local-redirect-policy/)\n3. [AKS Documentation: Configure LocalDNS](https://learn.microsoft.com/en-us/azure/aks/localdns-custom)\n\nUpdated Jan 22, 2026\n\nVersion 1.0\n\n[ebpf](/tag/ebpf?nodeId=board%3ALinuxandOpenSourceBlog)\n\n[!\\[Simone_Rodigari&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yODc2ODgwLWJpa25QeA?image-coordinates=0%2C0%2C400%2C400&amp;image-dimensions=50x50)](/users/simone_rodigari/2876880) [Simone_Rodigari](/users/simone_rodigari/2876880) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined January 23, 2025\n\n[View Profile](/users/simone_rodigari/2876880)\n\n/category/azure/blog/linuxandopensourceblog [Linux and Open Source Blog](/category/azure/blog/linuxandopensourceblog) Follow this blog board to get notified when there's new activity",
  "ProcessedDate": "2026-01-23 18:03:00",
  "Description": "## Why Adopt NodeLocal DNSCache?\n\nThe primary drivers for adoption are usually:\n\n1. **Eliminating Conntrack Pressure:** In high-QPS UDP DNS scenarios, conntrack contention and UDP tracking can cause intermittent DNS response loss and retries; depending on resolver retry/timeouts, this can appear as multi-second lookup delays and sometimes much longer tails.\n2. **Reducing Latency**: By placing a cache on every node, you remove the network hop to the CoreDNS service. Responses are practically instantaneous for cached records.\n3. **Offloading CoreDNS**: A DaemonSet architecture effectively shards the DNS query load across the entire cluster, preventing the central CoreDNS deployment from becoming a single point of congestion during bursty scaling events.\n\n#### Who needs this?\n\nYou should prioritize this architecture if you run:\n\n- **Large-scale clusters** large clusters (hundreds of nodes or thousands of pods), where CoreDNS scaling becomes difficult to manage.\n- **High-churn endpoints**, such as spot instances or frequent auto-scaling jobs that trigger massive waves of DNS queries.\n- **Real-time applications** where multi-second (and occasionally longer) DNS lookup delays are unacceptable.\n\n## The Challenge with Cilium\n\nDeploying NodeLocal DNSCache on a cluster managed by **Cilium** (CNI) requires a specific approach. Standard NodeLocal DNSCache relies on node-level *interface*/*iptables* setup. In Cilium environments, you can instead implement the interception via **Cilium Local Redirect Policy (LRP)**, which redirects traffic destined to the *kube-dns* ClusterIP service to a node-local backend pod.\n\nThis post details a production-ready deployment strategy aligned with Cilium’s Local Redirect Policy model. It covers necessary configuration tweaks to avoid conflicts and explains how to maintain security filtering.\n\n## Architecture Overview\n\nIn a standard Kubernetes deployment, NodeLocal DNSCache creates a dummy network interface and uses extensive iptables rules to hijack traffic destined for the Cluster DNS IP.\n\nWhen using Cilium, we can achieve this more elegantly and efficiently using **Local Redirect Policies**.\n\n1. **DaemonSet**: Runs *node-local-dns* on every node.\n2. **Configuration**: Configured to skip interface creation and iptables manipulation.\n3. **Redirection**: Cilium LRP intercepts traffic to the *kube-dns* Service IP and redirects it to the local pod on the same node.\n\n### 1. The NodeLocal DNSCache DaemonSet\n\nThe critical difference in this manifest is the arguments passed to the *node-local-dns* binary. We must explicitly disable its networking setup functions to let Cilium handle the traffic.\n\nThe NodeLocal DNSCache deployment also requires the *node-local-dns ConfigMap* and the *kube-dns-upstream Service* (plus *RBAC/ServiceAccount*). For brevity, the snippet below shows only the DaemonSet arguments that differ in the Cilium/LRP approach. The *node-cache* reads the template *Corefile* (*/etc/coredns/Corefile.base*) and generates the active *Corefile* (*/etc/Corefile*). The *-conf* flag points CoreDNS at the active *Corefile* it should load.\n\nThe node-cache binary accepts *-localip* as an IP list; *0.0.0.0* is a valid value and makes it listen on all interfaces, appropriate for the LRP-based redirection model.\n\n- apiVersion: apps/v1\nkind: DaemonSet metadata: name: node-local-dns namespace: kube-system labels: k8s-app: node-local-dns spec: selector: matchLabels: k8s-app: node-local-dns template: metadata: labels: k8s-app: node-local-dns annotations:\n# Optional: policy.cilium.io/no-track-port can be used to bypass conntrack for DNS.\n# Validate the impact on your Cilium version and your observability/troubleshooting needs.\npolicy.cilium.io/no-track-port: \"53\" spec:\n# IMPORTANT for the \"LRP + listen broadly\" approach:\n# keep hostNetwork off so you don't hijack node-wide :53\nhostNetwork: false dnsPolicy: ClusterFirst containers:\n- name: node-cache\nimage: registry.k8s.io/dns/k8s-dns-node-cache:1.15.16 args:\n- \"-localip\"\n# Use a bind-all approach. Ensure server blocks bind broadly in your Corefile.\n- \"0.0.0.0\"\n- \"-conf\"\n- \"/etc/Corefile\"\n- \"-upstreamsvc\"\n- \"kube-dns-upstream\"\n# CRITICAL: Disable internal setup\n- \"-skipteardown=true\"\n- \"-setupinterface=false\"\n- \"-setupiptables=false\"\nports:\n- containerPort: 53\nname: dns protocol: UDP\n- containerPort: 53\nname: dns-tcp protocol: TCP\n# Ensure your Corefile includes health :8080 so the liveness probe works\nlivenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 volumeMounts:\n- name: config-volume\nmountPath: /etc/coredns\n- name: kube-dns-config\nmountPath: /etc/kube-dns volumes:\n- name: kube-dns-config\nconfigMap: name: kube-dns optional: true\n- name: config-volume\nconfigMap: name: node-local-dns items:\n- key: Corefile\npath: Corefile.base\n\n### 2. The Cilium Local Redirect Policy (LRP)\n\nInstead of iptables, we define a CRD that tells Cilium: \"When you see traffic for `kube-dns`, send it to the `node-local-dns` pod on this same node.\"\n- apiVersion: \"cilium.io/v2\"\nkind: CiliumLocalRedirectPolicy metadata: name: \"nodelocaldns\" namespace: kube-system spec: redirectFrontend:\n# ServiceMatcher mode is for ClusterIP services\nserviceMatcher: serviceName: kube-dns namespace: kube-system redirectBackend:\n# The backend pods selected by localEndpointSelector must be in the same namespace as the LRP\nlocalEndpointSelector: matchLabels: k8s-app: node-local-dns toPorts:\n- port: \"53\"\nname: dns protocol: UDP\n- port: \"53\"\nname: dns-tcp protocol: TCP\n\nThis is an **LRP-based NodeLocal DNSCache deployment**: we disable node-cache’s *iptables*/*interface* setup and let **Cilium LRP** handle local redirection. This differs from the upstream NodeLocal DNSCache manifest, which uses *hostNetwork* + *dummy interface* + *iptables*.\n\n> >\n> LRP must be enabled in Cilium (e.g., *localRedirectPolicies.enabled=true*) before applying the CRD. [Official Cilium LRP doc](https://docs.cilium.io/en/stable/network/kubernetes/local-redirect-policy/#prerequisites)\n> >\n\n## The Network Policy \"Gotcha\"\n\nIf you use **CiliumNetworkPolicy** to restrict egress traffic, specifically for **FQDN filtering,** you typically allow access to CoreDNS like this:\n- - toEndpoints:\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: kube-dns toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\n\n**This will break with local redirection.**\n\nWhy? Because LRP redirects the DNS request to the **node-local-dns backend endpoint**; strict egress policies must therefore allow both *kube-dns* (upstream) **and** *node-local-dns* (the redirected destination).\n\n### The Repro Setup\n\nTo demonstrate this failure, the cluster is configured with:\n\n1. **NodeLocal DNSCache**: Deployed as a DaemonSet (*node-local-dns*) to cache DNS requests locally on every node.\n2. **Local Redirect Policy (LRP)**: An active LRP intercepts traffic destined for the *kube-dns* Service IP and redirects it to the local *node-local-dns* pod.\n3. **Incomplete Network Policy**: A strict *CiliumNetworkPolicy* (CNP) is enforced on the client pod. While it explicitly allows egress to *kube-dns*, it **misses** the corresponding rule for *node-local-dns*.\n\n#### Reveal the issue using Hubble:\n\nIn this scenario, the client pod *dns-client* is attempting to resolve the external domain *github.com*.\n\nWhen inspecting the traffic flows, you will see *EGRESS DENIED* verdicts. Crucially, notice the destination pod in the logs below: *kube-system/node-local-dns*, not *kube-dns*.\n\nAlthough the application originally sent the packet to the Cluster IP of CoreDNS, Cilium's Local Redirect Policy modified the destination to the local node cache. Since strictly defined Network Policies assume traffic is going to the *kube-dns* identity, this redirected traffic falls outside the allowed rules and is dropped by the default deny stance.\n\n![]()\n\n### The Fix: You must allow egress to both labels.\n- - toEndpoints:\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: kube-dns\n# Add this selector for the local cache\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: node-local-dns toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY\n\nWithout this addition, pods protected by strict egress policies will timeout resolving DNS, even though the cache is running.\n\n#### Use Hubble to observe the network flows:\n\nAfter adding *matchLabels: k8s:k8s-app: node-local-dns*, the traffic is now allowed. Hubble confirms a policy verdict of *EGRESS ALLOWED* for UDP traffic on port 53. Because DNS resolution now succeeds, the response populates the Cilium FQDN cache, subsequently allowing the TCP traffic to *github.com* on port 443 as intended.\n\n![]()\n\n### Real-World Example: Restricting Egress with FQDN Policies\n\nHere is a complete *CiliumNetworkPolicy* that locks down a workload to only *access api.example.com.* Note how the DNS rule explicitly allows traffic to both *kube-dns* (for upstream) and *node-local-dns* (for the local cache).\n- apiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy metadata: name: secure-workload-policy spec: endpointSelector: matchLabels: app: critical-workload egress:\n# 1. Allow DNS Resolution (REQUIRED for FQDN policies)\n- toEndpoints:\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: kube-dns\n# Allow traffic to the local cache redirection target\n- matchLabels:\nk8s:io.kubernetes.pod.namespace: kube-system k8s:k8s-app: node-local-dns toPorts:\n- ports:\n- port: \"53\"\nprotocol: ANY rules: dns:\n- matchPattern: \"\\*\"\n\n# 2. Allow specific FQDN traffic (populated via DNS lookups)\n- toFQDNs:\n- matchName: \"api.example.com\"\ntoPorts:\n- ports:\n- port: \"443\"\nprotocol: TCP\n\n## Configuration & Upstream Loops\n\nWhen configuring the *ConfigMap* for *node-local-dns*, use the standard placeholders provided by the image. The binary replaces them at runtime:\n\n- \\_\\_PILLAR\\_\\_CLUSTER\\_\\_DNS\\_\\_: The Upstream Service IP (*kube-dns-upstream*).\n- \\_\\_PILLAR\\_\\_UPSTREAM\\_\\_SERVERS\\_\\_: The system resolvers (usually */etc/resolv.conf*).\n\nEnsure *kube-dns-upstream* exists as a Service selecting the CoreDNS pods so cache misses are forwarded to the actual CoreDNS backends.\n\n## Alternative: AKS LocalDNS\n\n**LocalDNS** is an Azure Kubernetes Services (AKS)-managed node-local DNS proxy/cache.\n\n#### Pros:\n\n- Managed lifecycle at the node pool level.\n- Support for custom configuration via *localdnsconfig.json* (e.g., custom server blocks, cache tuning).\n- No manual DaemonSet management required.\n\n#### Cons & Limitations:\n\n- **Incompatibility with FQDN Policies**: As noted in the [official documentation](https://learn.microsoft.com/en-us/azure/aks/localdns-custom), LocalDNS isn’t compatible with applied FQDN filter policies in ACNS/Cilium; if you rely on FQDN enforcement, prefer a DNS path that preserves FQDN learning/enforcement.\n- Updating configuration requires reimaging the node pool.\n\nFor environments heavily relying on strict Cilium Network Policies and FQDN filtering, the manual deployment method described above (using LRP) can be more reliable and transparent.\n\n> >\n> AKS recommends not enabling both upstream NodeLocal DNSCache and LocalDNS in the same node pool, as DNS traffic is routed through LocalDNS and results may be unexpected.\n> >\n\n## References\n\n1. [Kubernetes Documentation: NodeLocal DNSCache](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/)\n2. [Cilium Documentation: Local Redirect Policy](https://docs.cilium.io/en/stable/network/kubernetes/local-redirect-policy/)\n3. [AKS Documentation: Configure LocalDNS](https://learn.microsoft.com/en-us/azure/aks/localdns-custom)",
  "Link": "https://techcommunity.microsoft.com/t5/linux-and-open-source-blog/scaling-dns-on-aks-with-cilium-nodelocal-dnscache-lrp-and-fqdn/ba-p/4486323"
}
