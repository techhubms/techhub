{
  "Title": "Weird problem when comparing the answers from chat playground and answer from api",
  "Description": "**I'm running into a weird issue with Azure AI Foundry (gpt-4o-mini) and need help.**\n\nI'm building a chatbot that classifies each user message into:\n\n1. follow-up to previous message\n2. repeat of an earlier message\n3. brand-new query\n\nThe classification logic works **perfectly** in the Azure AI Foundry **Chat Playground**. But when I use the **exact same prompt** in Python via:\n\n- AzureChatOpenAI() (LangChain)\n- or the official Azure OpenAI code from **\"View Code\"** (client.chat.completions.create())\n\n…I get **totally different and often wrong** results.\n\nI’ve already verified:\n\n- same deployment name (gpt-4o-mini)\n- same temperature / top\\_p / max\\_tokens\n- same system and user messages\n- even tried copy-pasting the full system prompt from the Playground\n\nBut the API version still behaves very differently.\n\nIt **feels like** Azure AI Foundry’s Chat Playground is using some kind of **hidden system prompt, invisible scaffolding, or extra formatting** that is NOT shown in the UI and NOT included in the “View Code” snippet. The Playground output is consistently more accurate than the raw API call.\n\n**Question:** Does the Chat Playground apply hidden instructions or pre-processing that we can’t see? And is there any way to:\n\n- view those hidden prompts, or\n- replicate Playground behavior exactly through the API or LangChain?\n\nIf anyone has run into this or knows how to get identical behavior outside the Playground, I’d really appreciate the help.",
  "Link": "https://techcommunity.microsoft.com/t5/azure/weird-problem-when-comparing-the-answers-from-chat-playground/m-p/4486090#M22407",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "ProcessedDate": "2026-01-15 21:05:00",
  "FeedName": "Microsoft Tech Community",
  "Tags": [],
  "OutputDir": "_community",
  "EnhancedContent": "**I'm running into a weird issue with Azure AI Foundry (gpt-4o-mini) and need help.**\n\nI'm building a chatbot that classifies each user message into:\n\n1. follow-up to previous message\n2. repeat of an earlier message\n3. brand-new query\n\nThe classification logic works **perfectly** in the Azure AI Foundry **Chat Playground**. But when I use the **exact same prompt** in Python via:\n\n- AzureChatOpenAI() (LangChain)\n- or the official Azure OpenAI code from **\"View Code\"** (client.chat.completions.create())\n\n…I get **totally different and often wrong** results.\n\nI’ve already verified:\n\n- same deployment name (gpt-4o-mini)\n- same temperature / top\\_p / max\\_tokens\n- same system and user messages\n- even tried copy-pasting the full system prompt from the Playground\n\nBut the API version still behaves very differently.\n\nIt **feels like** Azure AI Foundry’s Chat Playground is using some kind of **hidden system prompt, invisible scaffolding, or extra formatting** that is NOT shown in the UI and NOT included in the “View Code” snippet. The Playground output is consistently more accurate than the raw API call.\n\n**Question:** Does the Chat Playground apply hidden instructions or pre-processing that we can’t see? And is there any way to:\n\n- view those hidden prompts, or\n- replicate Playground behavior exactly through the API or LangChain?\n\nIf anyone has run into this or knows how to get identical behavior outside the Playground, I’d really appreciate the help.",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "PubDate": "2026-01-15T20:34:31+00:00",
  "Author": "Rakanid"
}
