{
  "Tags": [],
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "OutputDir": "_community",
  "Author": "Lee_Stott",
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/building-hipaa-compliant-medical-transcription-with-local-ai/ba-p/4490777",
  "Description": "# Building HIPAA-Compliant Medical Transcription with Local AI\n\n## Introduction\n\nHealthcare organizations generate vast amounts of spoken content, patient consultations, research interviews, clinical notes, medical conferences. Transcribing these recordings traditionally requires either manual typing (time-consuming and expensive) or cloud transcription services (creating immediate HIPAA compliance concerns). Every audio file sent to external APIs exposes Protected Health Information (PHI), requires Business Associate Agreements, creates audit trails on third-party servers, and introduces potential breach vectors. This sample solution lies in on-premises voice-to-text systems that process audio entirely locally, never sending PHI beyond organizational boundaries. This article demonstrates building a sample medical transcription application using [FLWhisper](https://github.com/leestott/FLWhisper), ASP.NET Core, C#, and [Microsoft Foundry Local](https://foundrylocal.ai) with OpenAI Whisper models. You'll learn how to build sample HIPAA-compliant audio processing, integrate Whisper models for medical terminology accuracy, design privacy-first API patterns, and build responsive web UIs for healthcare workflows.\n\nWhether you're developing electronic health record (EHR) integrations, building clinical research platforms, or implementing dictation systems for medical practices, this sample could be a great starting point for privacy-first speech recognition.\n\n## Why Local Transcription Is Critical for Healthcare\n\nHealthcare data handling is fundamentally different from general business data due to HIPAA regulations, state privacy laws, and professional ethics obligations. Understanding these requirements explains why cloud transcription services, despite their convenience, create unacceptable risks for medical applications.\n\nHIPAA compliance mandates strict controls over PHI. Every system that touches patient data must implement administrative, physical, and technical safeguards. Cloud transcription APIs require Business Associate Agreements (BAAs), but even with paperwork, you're entrusting PHI to external systems. Every API call creates logs on vendor servers, potentially in multiple jurisdictions. Data breaches at transcription vendors expose patient information, creating liability for healthcare organizations. On-premises processing eliminates these third-party risks entirely, PHI never leaves your controlled environment. US State laws increasingly add requirements beyond HIPAA. California's CCPA, New York's SHIELD Act, and similar legislation create additional compliance obligations. International regulations like GDPR prohibit transferring health data outside approved jurisdictions. Local processing simplifies compliance by keeping data within organizational boundaries.\n\nResearch applications face even stricter requirements. Institutional Review Boards (IRBs) often require explicit consent for data sharing with external parties. Cloud transcription may violate study protocols that promise \"no third-party data sharing.\" Clinical trials in pharmaceutical development handle proprietary information alongside PHI, double jeopardy for data exposure. Local transcription maintains research integrity while enabling audio analysis.\n\nCost considerations favor local deployment at scale. Medical organizations generate substantial audio, thousands of patient encounters monthly. Cloud APIs charge per minute of audio, creating significant recurring costs. Local models have fixed infrastructure costs that scale economically. A modest GPU server can process hundreds of hours monthly at predictable expense.\n\nLatency matters for clinical workflows. Doctors and nurses need transcriptions available immediately after patient encounters to review and edit while details are fresh. Cloud APIs introduce network delays, especially problematic in rural health facilities with limited connectivity. Local inference provides\n\n## Application Architecture: ASP.NET Core with Foundry Local\n\nThe sample FLWhisper application implements clean separation between audio handling, AI inference, and state management using modern .NET patterns:\n\nThe ASP.NET Core 10 minimal API provides HTTP endpoints for health checks, audio transcription, and sample file streaming. Minimal APIs reduce boilerplate while maintaining full middleware support for error handling, authentication, and CORS. The API design follows OpenAI's transcription endpoint specification, enabling drop-in replacement for existing integrations.\n\nThe service layer encapsulates business logic: `FoundryModelService` manages model loading and lifetime, `TranscriptionService` handles audio processing and AI inference, and `SampleAudioService` provides demonstration files for testing. This separation enables easy testing, dependency injection, and service swapping.\n\nFoundry Local integration uses the `Microsoft.AI.Foundry.Local.WinML` SDK. Unlike cloud APIs requiring authentication and network calls, this SDK communicates directly with the local Foundry service via in-process calls. Models load once at startup, remaining resident in memory for sub-second inference on subsequent requests.\n\nThe static file frontend delivers vanilla HTML/CSS/JavaScript, no framework overhead. This simplicity aids healthcare IT security audits and enables deployment on locked-down hospital networks. The UI provides file upload, sample selection, audio preview, transcription requests, and result display with copy-to-clipboard functionality.\n\nHere's the architectural flow for transcription requests:\n\n``` Web UI (Upload Audio File) ↓ POST /v1/audio/transcriptions (Multipart Form Data) ↓ ASP.NET Core API Route ↓ TranscriptionService.TranscribeAudio(audioStream) ↓ Foundry Local Model (Whisper Medium locally) ↓ Text Result + Metadata (language, duration) ↓ Return JSON/Text Response ↓ Display in UI ```\n\nThis architecture embodies several healthcare system design principles:\n\n- **Data never leaves the device**: All processing occurs on-premises, no external API calls\n- **No data persistence by default**: Audio and transcripts are session-only, never saved unless explicitly configured\n- **Comprehensive health checks**: System readiness verification before accepting PHI\n- **Audit logging support**: Structured logging for compliance documentation\n- **Graceful degradation**: Clear error messages when models unavailable rather than silent failures\n\n## Setting Up Foundry Local with Whisper Models\n\nFoundry Local supports multiple Whisper model sizes, each with different accuracy/speed tradeoffs. For medical transcription, accuracy is paramount—misheard drug names or dosages create patient safety risks:\n\n```\n# Install Foundry Local (Windows)\nwinget install Microsoft.FoundryLocal\n\n# Verify installation\nfoundry --version\n\n# Download Whisper Medium model (optimal for medical accuracy)\nfoundry model add openai-whisper-medium-generic-cpu:1\n\n# Check model availability\nfoundry model list ```\n\nWhisper Medium (769M parameters) provides the best balance for medical use. Smaller models (Tiny, Base) miss medical terminology frequently. Larger models (Large) offer marginal accuracy gains at 3x inference time. Medium handles medical vocabulary well, drug names, anatomical terms, procedure names, while processing typical consultation audio (5-10 minutes) in under 30 seconds.\n\nThe application detects and loads the model automatically:\n\n``` // Services/FoundryModelService.cs using Microsoft.AI.Foundry.Local.WinML;\n\npublic class FoundryModelService { private readonly ILogger _logger; private readonly FoundryOptions _options; private ILocalAIModel? _loadedModel;\n\npublic FoundryModelService( ILogger logger, IOptions options) { _logger = logger; _options = options.Value; }\n\npublic async Task InitializeModelAsync() { try { _logger.LogInformation( \"Loading Foundry model: {ModelAlias}\", _options.ModelAlias );\n\n// Load model from Foundry Local _loadedModel = await FoundryClient.LoadModelAsync( modelAlias: _options.ModelAlias, cancellationToken: CancellationToken.None );\n\nif (_loadedModel == null) { _logger.LogWarning(\"Model loaded but returned null instance\"); return false; }\n\n_logger.LogInformation( \"Successfully loaded model: {ModelAlias}\", _options.ModelAlias ); return true;\n\n} catch (Exception ex) { _logger.LogError( ex, \"Failed to load Foundry model: {ModelAlias}\", _options.ModelAlias ); return false; } }\n\npublic ILocalAIModel? GetLoadedModel() => _loadedModel;\n\npublic async Task UnloadModelAsync() { if (_loadedModel != null) { await FoundryClient.UnloadModelAsync(_loadedModel); _loadedModel = null; _logger.LogInformation(\"Model unloaded\"); } } } ```\n\nConfiguration lives in `appsettings.json` , enabling easy customization without code changes:\n\n``` { \"Foundry\": { \"ModelAlias\": \"whisper-medium\", \"LogLevel\": \"Information\" }, \"Transcription\": { \"MaxAudioDurationSeconds\": 300, \"SupportedFormats\": [\"wav\", \"mp3\", \"m4a\", \"flac\"], \"DefaultLanguage\": \"en\" } } ```\n\n## Implementing Privacy-First Transcription Service\n\nThe transcription service handles audio processing while maintaining strict privacy controls. No audio or transcript persists beyond the HTTP request lifecycle unless explicitly configured:\n\n``` // Services/TranscriptionService.cs public class TranscriptionService { private readonly FoundryModelService _modelService; private readonly ILogger _logger;\n\npublic async Task TranscribeAudioAsync( Stream audioStream, string originalFileName, TranscriptionOptions? options = null) {\n\noptions ??= new TranscriptionOptions(); var startTime = DateTime.UtcNow;\n\ntry { // Validate audio format ValidateAudioFormat(originalFileName);\n\n// Get loaded model var model = _modelService.GetLoadedModel(); if (model == null) { throw new InvalidOperationException(\"Whisper model not loaded\"); }\n\n// Create temporary file (automatically deleted after transcription) using var tempFile = new TempAudioFile(audioStream);\n\n// Execute transcription _logger.LogInformation( \"Starting transcription for file: {FileName}\", originalFileName );\n\nvar transcription = await model.TranscribeAsync( audioFilePath: tempFile.Path, language: options.Language, cancellationToken: CancellationToken.None );\n\nvar duration = (DateTime.UtcNow - startTime).TotalSeconds;\n\n_logger.LogInformation( \"Transcription completed in {Duration:F2}s\", duration );\n\nreturn new TranscriptionResult { Text = transcription.Text, Language = transcription.Language ?? options.Language, Duration = transcription.AudioDuration, ProcessingTimeSeconds = duration, FileName = originalFileName, Timestamp = DateTime.UtcNow };\n\n} catch (Exception ex) { _logger.LogError( ex, \"Transcription failed for file: {FileName}\", originalFileName ); throw; } }\n\nprivate void ValidateAudioFormat(string fileName) { var extension = Path.GetExtension(fileName).TrimStart('.'); var supportedFormats = new[] { \"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\" };\n\nif (!supportedFormats.Contains(extension.ToLowerInvariant())) { throw new ArgumentException( $\"Unsupported audio format: {extension}. \" + $\"Supported: {string.Join(\", \", supportedFormats)}\" ); } } }\n\n// Temporary file wrapper that auto-deletes internal class TempAudioFile : IDisposable { public string Path { get; }\n\npublic TempAudioFile(Stream sourceStream) { Path = System.IO.Path.GetTempFileName(); using var fileStream = File.OpenWrite(Path); sourceStream.CopyTo(fileStream); }\n\npublic void Dispose() { try { if (File.Exists(Path)) { File.Delete(Path); } } catch { // Ignore deletion errors in temp folder } } } ```\n\nThis service demonstrates several privacy-first patterns:\n\n- **Temporary file lifecycle management**: Audio written to temp storage, automatically deleted after transcription\n- **No implicit persistence**: Results returned to caller, not saved by service\n- **Format validation**: Accept only supported audio formats to prevent processing errors\n- **Comprehensive logging**: Audit trail for compliance without logging PHI content\n- **Error isolation**: Exceptions contain diagnostic info but no patient data\n\n## Building the OpenAI-Compatible REST API\n\nThe API endpoint mirrors OpenAI's transcription API specification, enabling existing integrations to work without modifications:\n\n``` // Program.cs var builder = WebApplication.CreateBuilder(args);\n\n// Configure services builder.Services.Configure( builder.Configuration.GetSection(\"Foundry\") ); builder.Services.AddSingleton(); builder.Services.AddScoped(); builder.Services.AddHealthChecks() .AddCheck(\"foundry-health\");\n\nvar app = builder.Build();\n\n// Load model at startup var modelService = app.Services.GetRequiredService(); await modelService.InitializeModelAsync();\n\napp.UseHealthChecks(\"/health\"); app.MapHealthChecks(\"/api/health/status\");\n\n// OpenAI-compatible transcription endpoint app.MapPost(\"/v1/audio/transcriptions\", async ( HttpRequest request, TranscriptionService transcriptionService, ILogger logger) => {\n\nif (!request.HasFormContentType) { return Results.BadRequest(new { error = \"Content-Type must be multipart/form-data\" }); }\n\nvar form = await request.ReadFormAsync();\n\n// Extract audio file var audioFile = form.Files.GetFile(\"file\"); if (audioFile == null || audioFile.Length == 0) { return Results.BadRequest(new { error = \"Audio file required in 'file' field\" }); }\n\n// Parse options var format = form[\"format\"].ToString() ?? \"text\"; var language = form[\"language\"].ToString() ?? \"en\";\n\ntry { // Process transcription using var stream = audioFile.OpenReadStream(); var result = await transcriptionService.TranscribeAudioAsync( audioStream: stream, originalFileName: audioFile.FileName, options: new TranscriptionOptions { Language = language } );\n\n// Return in requested format if (format == \"json\") { return Results.Json(new { text = result.Text, language = result.Language, duration = result.Duration }); } else { // Default: plain text return Results.Text(result.Text); }\n\n} catch (Exception ex) { logger.LogError(ex, \"Transcription request failed\"); return Results.StatusCode(500); } }) .DisableAntiforgery() // File uploads need CSRF exemption .WithName(\"TranscribeAudio\") .WithOpenApi();\n\napp.Run(); ```\n\nExample API usage:\n\n```\n# PowerShell\n$audioFile = Get-Item \"consultation-recording.wav\" $response = Invoke-RestMethod ` -Uri \"http://localhost:5192/v1/audio/transcriptions\" ` -Method Post ` -Form @{ file = $audioFile; format = \"json\" }\n\nWrite-Output $response.text\n\n# cURL\ncurl -X POST http://localhost:5192/v1/audio/transcriptions \\ -F \"file=@consultation-recording.wav\" \\ -F \"format=json\" ```\n\n## Building the Interactive Web Frontend\n\nThe web UI provides a user-friendly interface for non-technical medical staff to transcribe recordings:\n\n```\n\n```\n\n![]()\n\n```\n\n```\n\nThe JavaScript handles file uploads and API interactions:\n\n``` // wwwroot/app.js let selectedFile = null;\n\nasync function checkHealth() { try { const response = await fetch('/health'); const statusEl = document.getElementById('status');\n\nif (response.ok) { statusEl.className = 'status-badge online'; statusEl.textContent = '✓ System Ready'; } else { statusEl.className = 'status-badge offline'; statusEl.textContent = '✗ System Unavailable'; } } catch (error) { console.error('Health check failed:', error); } }\n\nfunction handleFileSelect(event) { const file = event.target.files[0]; if (!file) return;\n\nselectedFile = file;\n\n// Show file info const fileInfo = document.getElementById('fileInfo'); fileInfo.textContent = `Selected: ${file.name} (${formatFileSize(file.size)})`; fileInfo.classList.remove('hidden');\n\n// Enable audio preview const preview = document.getElementById('audioPreview'); preview.src = URL.createObjectURL(file); preview.classList.remove('hidden');\n\n// Enable transcribe button document.getElementById('transcribeBtn').disabled = false; }\n\nasync function transcribeAudio() { if (!selectedFile) return;\n\nconst loadingEl = document.getElementById('loadingIndicator'); const resultEl = document.getElementById('resultSection'); const transcribeBtn = document.getElementById('transcribeBtn');\n\n// Show loading state loadingEl.classList.remove('hidden'); resultEl.classList.add('hidden'); transcribeBtn.disabled = true;\n\ntry { const formData = new FormData(); formData.append('file', selectedFile); formData.append('format', 'json');\n\nconst startTime = Date.now();\n\nconst response = await fetch('/v1/audio/transcriptions', { method: 'POST', body: formData });\n\nif (!response.ok) { throw new Error(`HTTP ${response.status}: ${response.statusText}`); }\n\nconst result = await response.json(); const processingTime = ((Date.now() - startTime) / 1000).toFixed(1);\n\n// Display results document.getElementById('transcriptionText').value = result.text; document.getElementById('resultDuration').textContent = `Duration: ${result.duration.toFixed(1)}s`; document.getElementById('resultLanguage').textContent = `Language: ${result.language}`;\n\nresultEl.classList.remove('hidden');\n\nconsole.log(`Transcription completed in ${processingTime}s`);\n\n} catch (error) { console.error('Transcription failed:', error); alert(`Transcription failed: ${error.message}`); } finally { loadingEl.classList.add('hidden'); transcribeBtn.disabled = false; } }\n\nfunction copyToClipboard() { const text = document.getElementById('transcriptionText').value; navigator.clipboard.writeText(text) .then(() => alert('Copied to clipboard')) .catch(err => console.error('Copy failed:', err)); }\n\n// Initialize window.addEventListener('load', () => { checkHealth(); loadSamplesList(); }); ```\n\n## Key Takeaways and Production Considerations\n\nBuilding HIPAA-compliant voice-to-text systems requires architectural decisions that prioritize data privacy over convenience. The FLWhisper application demonstrates that you can achieve accurate medical transcription, fast processing times, and intuitive user experiences entirely on-premises.\n\nCritical lessons for healthcare AI:\n\n- **Privacy by architecture**: Design systems where PHI never exists outside controlled environments, not as a configuration option\n- **No persistence by default**: Audio and transcripts should be ephemeral unless explicitly saved with proper access controls\n- **Model selection matters**: Whisper Medium provides medical terminology accuracy that smaller models miss\n- **Health checks enable reliability**: Systems should verify model availability before accepting PHI\n- **Audit logging without content logging**: Track operations for compliance without storing sensitive data in logs\n\nFor production deployment in clinical settings, integrate with EHR systems via HL7/FHIR interfaces. Implement role-based access control with Active Directory integration. Add digital signatures for transcript authentication. Configure automatic PHI redaction using clinical NLP models. Deploy on HIPAA-compliant infrastructure with proper physical security. Implement comprehensive audit logging meeting compliance requirements.\n\nThe complete implementation with ASP.NET Core API, Foundry Local integration, sample audio files, and comprehensive tests is available at [github.com/leestott/FLWhisper](https://github.com/leestott/FLWhisper). Clone the repository and follow the setup guide to experience privacy-first medical transcription.\n\n## Resources and Further Reading\n\n- [FLWhisper Repository](https://github.com/leestott/FLWhisper) - Complete C# implementation with .NET 10\n- [Quick Start Guide](https://github.com/leestott/FLWhisper/blob/main/README.md) - Installation and usage instructions\n- [Microsoft Foundry Local Documentation](https://foundrylocal.ai) - SDK reference and model catalog\n- [OpenAI Whisper Documentation](https://openai.com/research/whisper) - Model architecture and capabilities\n- [HIPAA Compliance Guidelines](https://www.hhs.gov/hipaa/index.html) - HHS official guidance\n- [Testing Guide](https://github.com/leestott/FLWhisper/blob/main/TESTING.md) - Comprehensive test suite documentation",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "ProcessedDate": "2026-02-19 08:10:11",
  "FeedName": "Microsoft Tech Community",
  "EnhancedContent": "## Privacy-First Voice-to-Text Using Foundry Local\n\n# Building HIPAA-Compliant Medical Transcription with Local AI\n\n## Introduction\n\nHealthcare organizations generate vast amounts of spoken content, patient consultations, research interviews, clinical notes, medical conferences. Transcribing these recordings traditionally requires either manual typing (time-consuming and expensive) or cloud transcription services (creating immediate HIPAA compliance concerns). Every audio file sent to external APIs exposes Protected Health Information (PHI), requires Business Associate Agreements, creates audit trails on third-party servers, and introduces potential breach vectors. This sample solution lies in on-premises voice-to-text systems that process audio entirely locally, never sending PHI beyond organizational boundaries. This article demonstrates building a sample medical transcription application using [FLWhisper](https://github.com/leestott/FLWhisper), ASP.NET Core, C#, and [Microsoft Foundry Local](https://foundrylocal.ai) with OpenAI Whisper models. You'll learn how to build sample HIPAA-compliant audio processing, integrate Whisper models for medical terminology accuracy, design privacy-first API patterns, and build responsive web UIs for healthcare workflows.\n\nWhether you're developing electronic health record (EHR) integrations, building clinical research platforms, or implementing dictation systems for medical practices, this sample could be a great starting point for privacy-first speech recognition.\n\n## Why Local Transcription Is Critical for Healthcare\n\nHealthcare data handling is fundamentally different from general business data due to HIPAA regulations, state privacy laws, and professional ethics obligations. Understanding these requirements explains why cloud transcription services, despite their convenience, create unacceptable risks for medical applications.\n\nHIPAA compliance mandates strict controls over PHI. Every system that touches patient data must implement administrative, physical, and technical safeguards. Cloud transcription APIs require Business Associate Agreements (BAAs), but even with paperwork, you're entrusting PHI to external systems. Every API call creates logs on vendor servers, potentially in multiple jurisdictions. Data breaches at transcription vendors expose patient information, creating liability for healthcare organizations. On-premises processing eliminates these third-party risks entirely, PHI never leaves your controlled environment. US State laws increasingly add requirements beyond HIPAA. California's CCPA, New York's SHIELD Act, and similar legislation create additional compliance obligations. International regulations like GDPR prohibit transferring health data outside approved jurisdictions. Local processing simplifies compliance by keeping data within organizational boundaries.\n\nResearch applications face even stricter requirements. Institutional Review Boards (IRBs) often require explicit consent for data sharing with external parties. Cloud transcription may violate study protocols that promise \"no third-party data sharing.\" Clinical trials in pharmaceutical development handle proprietary information alongside PHI, double jeopardy for data exposure. Local transcription maintains research integrity while enabling audio analysis.\n\nCost considerations favor local deployment at scale. Medical organizations generate substantial audio, thousands of patient encounters monthly. Cloud APIs charge per minute of audio, creating significant recurring costs. Local models have fixed infrastructure costs that scale economically. A modest GPU server can process hundreds of hours monthly at predictable expense.\n\nLatency matters for clinical workflows. Doctors and nurses need transcriptions available immediately after patient encounters to review and edit while details are fresh. Cloud APIs introduce network delays, especially problematic in rural health facilities with limited connectivity. Local inference provides &lt;1 second turnaround for typical consultation lengths.\n\n## Application Architecture: ASP.NET Core with Foundry Local\n\nThe sample FLWhisper application implements clean separation between audio handling, AI inference, and state management using modern .NET patterns:\n\nThe ASP.NET Core 10 minimal API provides HTTP endpoints for health checks, audio transcription, and sample file streaming. Minimal APIs reduce boilerplate while maintaining full middleware support for error handling, authentication, and CORS. The API design follows OpenAI's transcription endpoint specification, enabling drop-in replacement for existing integrations.\n\nThe service layer encapsulates business logic: `FoundryModelService` manages model loading and lifetime, `TranscriptionService` handles audio processing and AI inference, and `SampleAudioService` provides demonstration files for testing. This separation enables easy testing, dependency injection, and service swapping.\n\nFoundry Local integration uses the `Microsoft.AI.Foundry.Local.WinML` SDK. Unlike cloud APIs requiring authentication and network calls, this SDK communicates directly with the local Foundry service via in-process calls. Models load once at startup, remaining resident in memory for sub-second inference on subsequent requests.\n\nThe static file frontend delivers vanilla HTML/CSS/JavaScript, no framework overhead. This simplicity aids healthcare IT security audits and enables deployment on locked-down hospital networks. The UI provides file upload, sample selection, audio preview, transcription requests, and result display with copy-to-clipboard functionality.\n\nHere's the architectural flow for transcription requests:\n\n``` Web UI (Upload Audio File) ↓ POST /v1/audio/transcriptions (Multipart Form Data) ↓ ASP.NET Core API Route ↓ TranscriptionService.TranscribeAudio(audioStream) ↓ Foundry Local Model (Whisper Medium locally) ↓ Text Result + Metadata (language, duration) ↓ Return JSON/Text Response ↓ Display in UI ```\n\nThis architecture embodies several healthcare system design principles:\n\n- **Data never leaves the device**: All processing occurs on-premises, no external API calls\n- **No data persistence by default**: Audio and transcripts are session-only, never saved unless explicitly configured\n- **Comprehensive health checks**: System readiness verification before accepting PHI\n- **Audit logging support**: Structured logging for compliance documentation\n- **Graceful degradation**: Clear error messages when models unavailable rather than silent failures\n\n## Setting Up Foundry Local with Whisper Models\n\nFoundry Local supports multiple Whisper model sizes, each with different accuracy/speed tradeoffs. For medical transcription, accuracy is paramount—misheard drug names or dosages create patient safety risks:\n\n```\n# Install Foundry Local (Windows)\nwinget install Microsoft.FoundryLocal\n\n# Verify installation\nfoundry --version\n\n# Download Whisper Medium model (optimal for medical accuracy)\nfoundry model add openai-whisper-medium-generic-cpu:1\n\n# Check model availability\nfoundry model list ```\n\nWhisper Medium (769M parameters) provides the best balance for medical use. Smaller models (Tiny, Base) miss medical terminology frequently. Larger models (Large) offer marginal accuracy gains at 3x inference time. Medium handles medical vocabulary well, drug names, anatomical terms, procedure names, while processing typical consultation audio (5-10 minutes) in under 30 seconds.\n\nThe application detects and loads the model automatically:\n\n``` // Services/FoundryModelService.cs using Microsoft.AI.Foundry.Local.WinML;\n\npublic class FoundryModelService { private readonly ILogger _logger; private readonly FoundryOptions _options; private ILocalAIModel? _loadedModel;\n\npublic FoundryModelService( ILogger logger, IOptions options) { _logger = logger; _options = options.Value; }\n\npublic async Task InitializeModelAsync() { try { _logger.LogInformation( \"Loading Foundry model: {ModelAlias}\", _options.ModelAlias );\n\n// Load model from Foundry Local _loadedModel = await FoundryClient.LoadModelAsync( modelAlias: _options.ModelAlias, cancellationToken: CancellationToken.None );\n\nif (_loadedModel == null) { _logger.LogWarning(\"Model loaded but returned null instance\"); return false; }\n\n_logger.LogInformation( \"Successfully loaded model: {ModelAlias}\", _options.ModelAlias ); return true;\n\n} catch (Exception ex) { _logger.LogError( ex, \"Failed to load Foundry model: {ModelAlias}\", _options.ModelAlias ); return false; } }\n\npublic ILocalAIModel? GetLoadedModel() => _loadedModel;\n\npublic async Task UnloadModelAsync() { if (_loadedModel != null) { await FoundryClient.UnloadModelAsync(_loadedModel); _loadedModel = null; _logger.LogInformation(\"Model unloaded\"); } } } ```\n\nConfiguration lives in `appsettings.json` , enabling easy customization without code changes:\n\n``` { \"Foundry\": { \"ModelAlias\": \"whisper-medium\", \"LogLevel\": \"Information\" }, \"Transcription\": { \"MaxAudioDurationSeconds\": 300, \"SupportedFormats\": [\"wav\", \"mp3\", \"m4a\", \"flac\"], \"DefaultLanguage\": \"en\" } } ```\n\n## Implementing Privacy-First Transcription Service\n\nThe transcription service handles audio processing while maintaining strict privacy controls. No audio or transcript persists beyond the HTTP request lifecycle unless explicitly configured:\n\n``` // Services/TranscriptionService.cs public class TranscriptionService { private readonly FoundryModelService _modelService; private readonly ILogger _logger;\n\npublic async Task TranscribeAudioAsync( Stream audioStream, string originalFileName, TranscriptionOptions? options = null) {\n\noptions ??= new TranscriptionOptions(); var startTime = DateTime.UtcNow;\n\ntry { // Validate audio format ValidateAudioFormat(originalFileName);\n\n// Get loaded model var model = _modelService.GetLoadedModel(); if (model == null) { throw new InvalidOperationException(\"Whisper model not loaded\"); }\n\n// Create temporary file (automatically deleted after transcription) using var tempFile = new TempAudioFile(audioStream);\n\n// Execute transcription _logger.LogInformation( \"Starting transcription for file: {FileName}\", originalFileName );\n\nvar transcription = await model.TranscribeAsync( audioFilePath: tempFile.Path, language: options.Language, cancellationToken: CancellationToken.None );\n\nvar duration = (DateTime.UtcNow - startTime).TotalSeconds;\n\n_logger.LogInformation( \"Transcription completed in {Duration:F2}s\", duration );\n\nreturn new TranscriptionResult { Text = transcription.Text, Language = transcription.Language ?? options.Language, Duration = transcription.AudioDuration, ProcessingTimeSeconds = duration, FileName = originalFileName, Timestamp = DateTime.UtcNow };\n\n} catch (Exception ex) { _logger.LogError( ex, \"Transcription failed for file: {FileName}\", originalFileName ); throw; } }\n\nprivate void ValidateAudioFormat(string fileName) { var extension = Path.GetExtension(fileName).TrimStart('.'); var supportedFormats = new[] { \"wav\", \"mp3\", \"m4a\", \"flac\", \"ogg\" };\n\nif (!supportedFormats.Contains(extension.ToLowerInvariant())) { throw new ArgumentException( $\"Unsupported audio format: {extension}. \" + $\"Supported: {string.Join(\", \", supportedFormats)}\" ); } } }\n\n// Temporary file wrapper that auto-deletes internal class TempAudioFile : IDisposable { public string Path { get; }\n\npublic TempAudioFile(Stream sourceStream) { Path = System.IO.Path.GetTempFileName(); using var fileStream = File.OpenWrite(Path); sourceStream.CopyTo(fileStream); }\n\npublic void Dispose() { try { if (File.Exists(Path)) { File.Delete(Path); } } catch { // Ignore deletion errors in temp folder } } } ```\n\nThis service demonstrates several privacy-first patterns:\n\n- **Temporary file lifecycle management**: Audio written to temp storage, automatically deleted after transcription\n- **No implicit persistence**: Results returned to caller, not saved by service\n- **Format validation**: Accept only supported audio formats to prevent processing errors\n- **Comprehensive logging**: Audit trail for compliance without logging PHI content\n- **Error isolation**: Exceptions contain diagnostic info but no patient data\n\n## Building the OpenAI-Compatible REST API\n\nThe API endpoint mirrors OpenAI's transcription API specification, enabling existing integrations to work without modifications:\n\n``` // Program.cs var builder = WebApplication.CreateBuilder(args);\n\n// Configure services builder.Services.Configure( builder.Configuration.GetSection(\"Foundry\") ); builder.Services.AddSingleton(); builder.Services.AddScoped(); builder.Services.AddHealthChecks() .AddCheck(\"foundry-health\");\n\nvar app = builder.Build();\n\n// Load model at startup var modelService = app.Services.GetRequiredService(); await modelService.InitializeModelAsync();\n\napp.UseHealthChecks(\"/health\"); app.MapHealthChecks(\"/api/health/status\");\n\n// OpenAI-compatible transcription endpoint app.MapPost(\"/v1/audio/transcriptions\", async ( HttpRequest request, TranscriptionService transcriptionService, ILogger logger) => {\n\nif (!request.HasFormContentType) { return Results.BadRequest(new { error = \"Content-Type must be multipart/form-data\" }); }\n\nvar form = await request.ReadFormAsync();\n\n// Extract audio file var audioFile = form.Files.GetFile(\"file\"); if (audioFile == null || audioFile.Length == 0) { return Results.BadRequest(new { error = \"Audio file required in 'file' field\" }); }\n\n// Parse options var format = form[\"format\"].ToString() ?? \"text\"; var language = form[\"language\"].ToString() ?? \"en\";\n\ntry { // Process transcription using var stream = audioFile.OpenReadStream(); var result = await transcriptionService.TranscribeAudioAsync( audioStream: stream, originalFileName: audioFile.FileName, options: new TranscriptionOptions { Language = language } );\n\n// Return in requested format if (format == \"json\") { return Results.Json(new { text = result.Text, language = result.Language, duration = result.Duration }); } else { // Default: plain text return Results.Text(result.Text); }\n\n} catch (Exception ex) { logger.LogError(ex, \"Transcription request failed\"); return Results.StatusCode(500); } }) .DisableAntiforgery() // File uploads need CSRF exemption .WithName(\"TranscribeAudio\") .WithOpenApi();\n\napp.Run(); ```\n\nExample API usage:\n\n```\n# PowerShell\n$audioFile = Get-Item \"consultation-recording.wav\" $response = Invoke-RestMethod ` -Uri \"http://localhost:5192/v1/audio/transcriptions\" ` -Method Post ` -Form @{ file = $audioFile; format = \"json\" }\n\nWrite-Output $response.text\n\n# cURL\ncurl -X POST http://localhost:5192/v1/audio/transcriptions \\ -F \"file=@consultation-recording.wav\" \\ -F \"format=json\" ```\n\n## Building the Interactive Web Frontend\n\nThe web UI provides a user-friendly interface for non-technical medical staff to transcribe recordings:\n\n```\n\n```\n\n```\n\n```\n\nThe JavaScript handles file uploads and API interactions:\n\n``` // wwwroot/app.js let selectedFile = null;\n\nasync function checkHealth() { try { const response = await fetch('/health'); const statusEl = document.getElementById('status');\n\nif (response.ok) { statusEl.className = 'status-badge online'; statusEl.textContent = '✓ System Ready'; } else { statusEl.className = 'status-badge offline'; statusEl.textContent = '✗ System Unavailable'; } } catch (error) { console.error('Health check failed:', error); } }\n\nfunction handleFileSelect(event) { const file = event.target.files[0]; if (!file) return;\n\nselectedFile = file;\n\n// Show file info const fileInfo = document.getElementById('fileInfo'); fileInfo.textContent = `Selected: ${file.name} (${formatFileSize(file.size)})`; fileInfo.classList.remove('hidden');\n\n// Enable audio preview const preview = document.getElementById('audioPreview'); preview.src = URL.createObjectURL(file); preview.classList.remove('hidden');\n\n// Enable transcribe button document.getElementById('transcribeBtn').disabled = false; }\n\nasync function transcribeAudio() { if (!selectedFile) return;\n\nconst loadingEl = document.getElementById('loadingIndicator'); const resultEl = document.getElementById('resultSection'); const transcribeBtn = document.getElementById('transcribeBtn');\n\n// Show loading state loadingEl.classList.remove('hidden'); resultEl.classList.add('hidden'); transcribeBtn.disabled = true;\n\ntry { const formData = new FormData(); formData.append('file', selectedFile); formData.append('format', 'json');\n\nconst startTime = Date.now();\n\nconst response = await fetch('/v1/audio/transcriptions', { method: 'POST', body: formData });\n\nif (!response.ok) { throw new Error(`HTTP ${response.status}: ${response.statusText}`); }\n\nconst result = await response.json(); const processingTime = ((Date.now() - startTime) / 1000).toFixed(1);\n\n// Display results document.getElementById('transcriptionText').value = result.text; document.getElementById('resultDuration').textContent = `Duration: ${result.duration.toFixed(1)}s`; document.getElementById('resultLanguage').textContent = `Language: ${result.language}`;\n\nresultEl.classList.remove('hidden');\n\nconsole.log(`Transcription completed in ${processingTime}s`);\n\n} catch (error) { console.error('Transcription failed:', error); alert(`Transcription failed: ${error.message}`); } finally { loadingEl.classList.add('hidden'); transcribeBtn.disabled = false; } }\n\nfunction copyToClipboard() { const text = document.getElementById('transcriptionText').value; navigator.clipboard.writeText(text) .then(() => alert('Copied to clipboard')) .catch(err => console.error('Copy failed:', err)); }\n\n// Initialize window.addEventListener('load', () => { checkHealth(); loadSamplesList(); }); ```\n\n## Key Takeaways and Production Considerations\n\nBuilding HIPAA-compliant voice-to-text systems requires architectural decisions that prioritize data privacy over convenience. The FLWhisper application demonstrates that you can achieve accurate medical transcription, fast processing times, and intuitive user experiences entirely on-premises.\n\nCritical lessons for healthcare AI:\n\n- **Privacy by architecture**: Design systems where PHI never exists outside controlled environments, not as a configuration option\n- **No persistence by default**: Audio and transcripts should be ephemeral unless explicitly saved with proper access controls\n- **Model selection matters**: Whisper Medium provides medical terminology accuracy that smaller models miss\n- **Health checks enable reliability**: Systems should verify model availability before accepting PHI\n- **Audit logging without content logging**: Track operations for compliance without storing sensitive data in logs\n\nFor production deployment in clinical settings, integrate with EHR systems via HL7/FHIR interfaces. Implement role-based access control with Active Directory integration. Add digital signatures for transcript authentication. Configure automatic PHI redaction using clinical NLP models. Deploy on HIPAA-compliant infrastructure with proper physical security. Implement comprehensive audit logging meeting compliance requirements.\n\nThe complete implementation with ASP.NET Core API, Foundry Local integration, sample audio files, and comprehensive tests is available at [github.com/leestott/FLWhisper](https://github.com/leestott/FLWhisper). Clone the repository and follow the setup guide to experience privacy-first medical transcription.\n\n## Resources and Further Reading\n\n- [FLWhisper Repository](https://github.com/leestott/FLWhisper) - Complete C# implementation with .NET 10\n- [Quick Start Guide](https://github.com/leestott/FLWhisper/blob/main/README.md) - Installation and usage instructions\n- [Microsoft Foundry Local Documentation](https://foundrylocal.ai) - SDK reference and model catalog\n- [OpenAI Whisper Documentation](https://openai.com/research/whisper) - Model architecture and capabilities\n- [HIPAA Compliance Guidelines](https://www.hhs.gov/hipaa/index.html) - HHS official guidance\n- [Testing Guide](https://github.com/leestott/FLWhisper/blob/main/TESTING.md) - Comprehensive test suite documentation\n\nUpdated Jan 30, 2026\n\nVersion 1.0\n\n[.net](/tag/.net?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai foundry](/tag/ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[genai](/tag/genai?nodeId=board%3AAzureDevCommunityBlog)\n\n[llm](/tag/llm?nodeId=board%3AAzureDevCommunityBlog)\n\n[performance](/tag/performance?nodeId=board%3AAzureDevCommunityBlog)\n\n[slm](/tag/slm?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[Lee_Stott&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yMTA1NDYtODM5MjVpMDI2ODNGQTMwMzAwNDFGQQ?image-dimensions=50x50)](/users/lee_stott/210546) [Lee_Stott](/users/lee_stott/210546) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined September 25, 2018\n\n[View Profile](/users/lee_stott/210546)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "Title": "Building HIPAA-Compliant Medical Transcription with Local AI",
  "PubDate": "2026-02-19T08:00:00+00:00"
}
