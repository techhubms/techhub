{
  "Link": "https://zure.com/blog/how-reusing-wcf-sql-xml-simplified-our-biztalk-to-azure-migration",
  "Author": "tim.dhaeyer@zure.com (Tim D'haeyer)",
  "PubDate": "2025-10-30T12:50:02+00:00",
  "FeedLevelAuthor": "Blog",
  "Title": "How Reusing WCF-SQL XML Simplified Our BizTalk to Azure Migration",
  "Description": "[!\\[Alt text for image\\](https://zure.com/hubfs/Kuvituskuvat/Zure-By-Fredography-Full-48.jpg)](https://zure.com/blog/how-reusing-wcf-sql-xml-simplified-our-biztalk-to-azure-migration?hsLang=en)\n\nMigrating BizTalk flows to Azure is not always a straightforward task. While some flows are fairly easy to move almost \"as-is\", others can present significant challenges.\n\nIn one of my projects, I had to deal with an integration between SAP and an external SFA (Sales Force Automation) system. The SFA system relied heavily on SQL and used a staging database for integration. In BizTalk, we naturally used the WCF-SQL adapter as our main tool of choice, along with a fair amount of XSLT mapping to convert data to and from SAP RFCs.\n\nGiven tight constraints on time and resources, especially around testing, we set out to explore how we could migrate this to a proper Azure-based solution.",
  "EnhancedContent": "Skip to content\n\nBlog\n\n# How Reusing WCF-SQL XML Simplified Our BizTalk to Azure Migration\n\n![Tim D'haeyer](https://zure.com/hs-fs/hubfs/Profile%20pictures/Tim%20D%E2%80%99haeyer.jpg?width=220&amp;name=Tim%20D%E2%80%99haeyer.jpg)\n\nTim D'haeyer\n\n30.10.2025\n\n![](https://zure.com/hs-fs/hubfs/Kuvituskuvat/Zure-By-Fredography-Full-48.jpg?width=1037&amp;name=Zure-By-Fredography-Full-48.jpg)\n\nMigrating BizTalk flows to Azure is not always a straightforward task. While some flows are fairly easy to move almost \"as-is\", others can present significant challenges.\n\nIn one of my projects, I had to deal with an integration between SAP and an external SFA (Sales Force Automation) system. The SFA system relied heavily on SQL and used a staging database for integration. In BizTalk, we naturally used the WCF-SQL adapter as our main tool of choice, along with a fair amount of XSLT mapping to convert data to and from SAP RFCs.\n\nGiven tight constraints on time and resources, especially around testing, we set out to explore how we could migrate this to a proper Azure-based solution.\n\n## Rethinking the Approach\n\nOn the SQL side, we relied heavily on user-defined table types when calling stored procedures, passing in typed sets of data to be upserted into staging tables. From experience, we knew that the SQL connector in Logic Apps doesn’t handle user-defined table types very well. So we quickly decided to use an Azure Function instead of the built-in connector.\n\nI started by writing a basic Azure Function that could execute SQL statements on on-premises databases (one of the requirements for several integrations).\n\nOnce that was in place, I began migrating one of the first flows. But I hit a roadblock when I looked at the (rather large) XSLT involved. Migrating it seemed like a major challenge. That led me to explore other flows with similar requirements, and I started wondering whether there might be a better way forward.\n\n## Eureka Moment\n\nI kept thinking about what the best solution could be. And then, one morning during my commute, I had a bit of a Eureka moment:\n\nInstead of reimplementing the logic in Logic Apps or recreating the SQL mapping logic, I wondered, could I just use the same XML BizTalk already generates?\n\nWhen you think about it, the XML schema (especially when you're just calling stored procedures) is actually quite straightforward to handle. Let’s look at a simple example:\n\n```\n\n<Request xmlns=\"http://schemas.microsoft.com/Sql/2008/05/TypedProcedures/dbo\"> <CreateCustomers> <CustomersTVP> <CustomerType xmlns=\"http://schemas.microsoft.com/Sql/2008/05/Types/TableTypes/dbo\"> <Row> <CustomerId>1</CustomerId> <Name>Jane Doe</Name> <Email>jane@example.com</Email> </Row> <Row> <CustomerId>2</CustomerId> <Name>John Smith</Name> <Email>john@example.com</Email> </Row> </CustomerType> </CustomersTVP> <CreatedBy>admin_user</CreatedBy> </CreateCustomers> </Request>\n\n```\n\nThis XML calls a stored procedure named **CreateCustomers**. It has two input parameters:\n\n**CustomersTVP**: a user-defined table type called *CustomerType*.\n\n**CreatedBy**: a simple string used for logging or audit purposes.\n\n## Parsing the XML\n\nThe XML is parsed using LINQ to XML, which makes it easy to extract values and map them to SQL input parameters. In this post, I’ll highlight a few key parts of the code and explain how they work. For our use case, we exclusively use stored procedures. While the BizTalk schema also supports inline SQL statements, that feature is not covered in this blog post.\n\n### 1. Stored Procedure(s)\n\nFirst step is to extract the stored procedures we need to call. That can be done with the following line of code:\n\n```\n\nvar spElements = xmlDoc.Root? .Elements() .Where(e => e.Name.Namespace.NamespaceName.Equals(\"http://schemas.microsoft.com/Sql/2008/05/Procedures/dbo\")) .ToList();\n\n```\n\nIt just takes all elements directly under the root node (&lt;Request xmlns=\"http://schemas.microsoft.com/Sql/2008/05/TypedProcedures/dbo\"&gt;) and selects all that have the namespace specific for stored procedures. That should give us a list of all the stored procedures we want to call.\n\n### 2. Parameters\n\nFor each stored procedure we need to find out which parameters we need to add. This can be simple types or user-defined table types.\n\n```\n\nforeach (var spElement in spElements) { // Get the parameters for the stored procedure foreach (var parameterElement in spElement.Elements()) { string parameterName = parameterElement.Name.LocalName; var tableTypeElement = parameterElement.Elements() .FirstOrDefault(e => e.Name.Namespace.NamespaceName.Equals(\"http://schemas.microsoft.com/Sql/2008/05/Types/TableTypes/dbo\"));\n\nif (tableTypeElement != null) { string tableTypeName = tableTypeElement.Name.LocalName; DataTable dataTable = await BuildDataTableFromTableType(sqlXmlRequest.ConnectionStringKey, tableTypeName, tableTypeElement, cancellationToken); sqlStatement.Parameters.Add(parameterName, dataTable); } else { sqlStatement.Parameters.Add(parameterName, parameterElement.Value); } } }\n\n```\n\nTo check if a parameter is a user-defined table type, we can check if the child of the parameter element has the Table-Types namespace. To add that one, we need some extra parsing (which I will explain in the next step). The simple parameters can just be added by name and value.\n\n### 3. User-defined table type\n\nThis step is a little bit tricky. Calling a stored procedure with a user-defined table type expects you to pass a DataTable. To be able to do that, I execute a query on my SQL server to get the definition of the user-defined table-type.\n\n```\n\nstring query = $@\" SELECT c.name AS ColumnName, t.name AS DataType FROM sys.columns c JOIN sys.types t ON c.user_type_id = t.user_type_id WHERE c.object_id = ( SELECT type_table_object_id FROM sys.table_types WHERE name = @TableTypeName) ORDER BY c.column_id;\";\n\n```\n\nWith the result of this query I can easily add columns to my DataTable.\n\n```\n\nforeach (var row in result.First().Data) { string columnName = row[\"ColumnName\"]?.ToString() ?? string.Empty; string dataType = row[\"DataType\"]?.ToString() ?? string.Empty; dataTable.Columns.Add(columnName, GetTypeFromSqlDataType(dataType)); }\n\n```\n\nAnd after that we can add the rows to our DataTable.\n\n```\n\nforeach (var rowElement in tableTypeElement?.Parent?.Elements() ?? []) { DataRow dataRow = dataTable.NewRow(); foreach (var field in rowElement.Elements()) { string fieldValue = field.Value; Type columnType = dataTable.Columns[field.Name.LocalName].DataType;\n\nif (string.IsNullOrWhiteSpace(fieldValue)) dataRow[field.Name.LocalName] = DBNull.Value; else dataRow[field.Name.LocalName] = Convert.ChangeType(fieldValue, columnType); } dataTable.Rows.Add(dataRow); }\n\n```\n\nHow you handle empties is totally up to you. In my case, I needed the DBNull.\n\n### Azure Function\n\nTo bring everything together, I implemented an Azure Function that I call from within my Logic Apps.\n\n![Azure Function diagram](https://zure.com/hs-fs/hubfs/undefined.png?width=924&amp;height=774&amp;name=undefined.png)\n\nOne key design decision was to pass the **ConnectionStringKey** as a parameter. This allows me to select the appropriate database at runtime without modifying the function’s environment variables. Secrets are never hardcoded—the actual connection string is securely retrieved from Azure Key Vault.\n\nThis approach suited my use case, where I needed to support multiple databases (e.g. on-premises and in Azure). However, if you're working with a single, fixed database, you could just as easily configure the connection string as an environment variable instead.\n\nThe **XmlContent** parameter is flexible: it supports either a Base64-encoded XML payload or a reference to a blob containing the XML data.\n\n### Extra\n\nAt the time of writing, this is implemented as an HTTP-triggered Azure Function. In the future, I may also offer an asynchronous version, triggered via Azure Service Bus. This would be useful in scenarios where the SQL query takes longer to complete—especially considering the 2-minute timeout limit for HTTP actions in a consumption-based Logic App.\n\n### Conclusion\n\nThis isn’t how you’d build a new integration from scratch. But for migrating BizTalk flows, it works surprisingly well.\n\nBy reusing the existing XML that BizTalk already understands, we avoided redoing schemas, mappings, and logic. It let us move fast, keep things stable, and get off BizTalk without a full rewrite.\n\nNot perfect, but for legacy migrations? It’s a solid shortcut.\n\n![Tim D'haeyer](https://zure.com/hs-fs/hubfs/Profile%20pictures/Tim%20D%E2%80%99haeyer.jpg?width=608&amp;name=Tim%20D%E2%80%99haeyer.jpg)\n\nTim D'haeyer\n\nIntroducing Tim, our seasoned Azure Consultant at Zure! With an impressive 15-year journey in Microsoft technologies, Tim has honed his skills as both a developer and a team lead across a multitude of projects. His enthusiasm for problem-solving is not just a job requirement; it's a genuine passion that drives excellence in every task. Join us in experiencing the blend of expertise and passion Tim brings to our team, making him an invaluable asset in navigating the complex world of Azure solutions.\n\n##\n\nRead these next\n\nBlog\n\n### [Welcome Aleksi and Olli!](https://zure.com/blog/welcome-aleksi-and-olli?hsLang=en)\n\nGuest Writer\n\n[News](https://zure.com/blog/tag/news)\n\nBlog\n\n### [Welcome Aleksi and Olli!](https://zure.com/blog/welcome-aleksi-and-olli?hsLang=en)\n\nGuest Writer\n\n[News](https://zure.com/blog/tag/news)\n\nBlog\n\n### [Fabric IQ – The new semantic layer for your organizational data](https://zure.com/blog/fabric-iq-the-new-semantic-layer-for-your-organizational-data?hsLang=en)\n\n![Lauri Lehman](https://zure.com/hs-fs/hubfs/Profile%20pictures/LauriLehman-01-lowres.jpg?width=220&amp;name=LauriLehman-01-lowres.jpg)\n\nLauri Lehman\n\nBlog\n\n### [Fabric IQ – The new semantic layer for your organizational data](https://zure.com/blog/fabric-iq-the-new-semantic-layer-for-your-organizational-data?hsLang=en)\n\n![Lauri Lehman](https://zure.com/hs-fs/hubfs/Profile%20pictures/LauriLehman-01-lowres.jpg?width=220&amp;name=LauriLehman-01-lowres.jpg)\n\nLauri Lehman\n\n[All articles](https://zure.com/insights?hsLang=en)",
  "FeedUrl": "https://zure.com/blog/rss.xml",
  "OutputDir": "_blogs",
  "FeedName": "Zure Data & AI Blog",
  "Tags": [],
  "ProcessedDate": "2026-01-01 15:29:23"
}
