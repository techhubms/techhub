{
  "Tags": [
    "AI",
    "ai-assisted-development",
    "copilot",
    "Development",
    "DevOps",
    "github",
    "series",
    "spec-kit",
    "specification-driven-development",
    "vibe-coding"
  ],
  "Description": "> >\n> ğŸ¯ **This is Part 3 of a 5-part series on mastering AI-assisted development.**\n> >\n> Weâ€™ve covered the why and the how. Now letâ€™s tackle the messy reality: debugging, iteration, and real-world troubleshooting.\n> >\n\n## Series overview\n\n| Part | Topic | Status | | --- | --- | --- | | [Part 1](/from-vibe-coding-to-spec-driven-development) | The problem and the solution | âœ“ | | [Part 2](/from-vibe-coding-to-spec-driven-development-part2) | Deep dive into the Spec-Kit workflow | âœ“ | | **Part 3** | **Best practices and troubleshooting** | ğŸ“ | | Part 4 | Team collaboration and advanced patterns | Jan 26 | | Part 5 | Case studies and lessons learned | Feb 2 |\n\n## The reality of spec-driven development\n\nLast week, I was helping a team at a European airline build a proof-of-concept for managing contractors. Everything was going smoothly until the AI started generating code that referenced a workflow library that simply didnâ€™t exist. Once weâ€™d untangled the hallucination, it got me thinking: **we need a field guide for when things go wrong.**\n\nIn Part 2, we walked through a clean, linear workflow: constitution â†’ spec â†’ plan â†’ tasks â†’ implementation. In reality, development is rarely that smooth.\n\nYouâ€™ll encounter:\n\n| Challenge | What happens | | --- | --- | | **AI hallucinations** | The AI invents APIs that donâ€™t exist | | **Spec ambiguities** | Unclear requirements lead to wrong implementations | | **Performance issues** | Code works but doesnâ€™t scale | | **Integration failures** | Components donâ€™t play well together | | **Requirement changes** | What you built isnâ€™t what users need |\n\nThis post is about handling those messy situations. **Think of it as your field guide for when things go wrong.**\n\n## Section 1: Advanced specification techniques\n\n### Writing specs that survive reality\n\nThe quality of your specification directly determines the quality of your output. Hereâ€™s how to write specs that work in practice.\n\n> >\n> ğŸ’¡ **Quick win**\n> >\n> Start with Technique #1 (Given-When-Then). It alone will significantly improve your AI output quality.\n> >\n\n#### Technique #1: Use Given-When-Then scenarios\n\nInstead of vague descriptions, use structured scenarios:\n\n**DONâ€™T** (Vague):\n\n``` Users should be able to filter tasks\n\n```\n\n**DO** (Structured):\n\n``` **Scenario: Filter tasks by assignee**\n\n**Given** I'm viewing the team dashboard **And** there are 50 tasks assigned to various team members **When** I select \"John Smith\" from the assignee filter **Then** I see only tasks assigned to John Smith (expected: 12 tasks) **And** the count updates to show \"Showing 12 of 50 tasks\" **And** the filter persists when I navigate away and return\n\n```\n\n**Why this works**: The AI has concrete inputs, actions, and expected outputs. No room for interpretation.\n\n#### Technique #2: Specify error messages exactly\n\nDonâ€™t say â€œshow an error.â€ Say exactly what the error should say.\n\n**DONâ€™T** (Vague):\n\n``` If the user enters an invalid email, show an error\n\n```\n\n**DO** (Specific):\n\n``` **Validation: Email format**\n\n**Invalid inputs:**\n- Empty string â†’ \"Email is required\"\n- \"notanemail\" â†’ \"Please enter a valid email address\"\n- \"test@\" â†’ \"Please enter a valid email address\"\n- \"test@domain\" â†’ \"Please enter a valid email address\"\n\n**Valid inputs:**\n- \"user@example.com\" â†’ Validation passes\n- \"user+tag@example.co.uk\" â†’ Validation passes\n\n**Display:**\n- Error appears below input field\n- Input border turns red (#dc3545)\n- Error icon displayed to the left of message\n- Error clears immediately when user starts typing\n\n```\n\n#### Technique #3: Mock the UI with ASCII art\n\nDonâ€™t assume the AI knows what you want visually.\n\n```\n## Task Card Layout\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ [âœ“] Task title goes here [Edit] â”‚ â”‚ â”‚ â”‚ Brief description preview shown here, â”‚ â”‚ truncated at 100 characters... â”‚ â”‚ â”‚ â”‚ ğŸ‘¤ Assigned to: John Smith â”‚ â”‚ ğŸ“… Due: Jan 25, 2026 â”‚ â”‚ ğŸ·ï¸ Status: In Progress â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n**Interactions:**\n- Checkbox: Click to toggle complete/incomplete\n- Card body: Click to open detail view\n- Edit button: Click to enter edit mode\n- Hover: Box shadow elevation (0.5rem)\n\n```\n\n#### Technique #4: Define the negative space\n\nExplicitly state what youâ€™re NOT building.\n\n```\n## Out of Scope (Explicitly NOT Implementing)\n\n### Comments on tasks\n**Why**: Adds complexity without MVP validation **Future consideration**: Phase 2 after user feedback\n\n### File attachments\n**Why**: Requires file storage infrastructure **Alternative**: Use task description to link to external files\n\n### Recurring tasks\n**Why**: Complex scheduling logic **Alternative**: Manual task duplication for now\n\n### Subtasks/nested tasks\n**Why**: Violates \"Simplicity First\" principle **Alternative**: Use task dependencies in future phase\n\n```\n\n**Why this matters**: Prevents the AI from â€œhelpfullyâ€ adding features you didnâ€™t ask for.\n\n## Section 2: Debugging strategies\n\n### When the AI goes off track\n\nAI hallucinations are real. Hereâ€™s how to diagnose and fix them.\n\n> >\n> ğŸ“‹ **TL;DR: The 5-step debug checklist**\n> >\n> 1. Validate dependencies exist\n> 2. Check for hallucinated APIs\n> 3. Reset context if issues repeat\n> 4. Provide minimal reproductions\n> 5. Audit dependencies after updates\n> >\n\n### Strategy #1: The validation loop\n\nAfter implementation, run this three-step validation:\n\n```\n# Step 1: Verify dependencies exist\nnpm list # Check all packages are installed dotnet list package # For .NET projects\n\n# Step 2: Check for type errors\nnpm run typecheck # TypeScript dotnet build # C#\n\n# Step 3: Run tests\nnpm test # Run test suite dotnet test # Run unit tests\n\n```\n\n**Common issues caught:**\n\n- AI referenced packages that donâ€™t exist\n- API endpoints that were never implemented\n- Type mismatches between frontend and backend\n\n### Strategy #2: The hallucination detector\n\nAI models sometimes invent plausible-sounding but non-existent APIs.\n\n**Example hallucination:**\n\n``` // AI-generated code import { usePrismaQuery } from '@prisma/react';\n\n// This hook doesn't exist! const { data } = usePrismaQuery('users', { where: { email } });\n\n```\n\n**How to catch it:**\n\n1. Check official documentation\n2. Search GitHub for the import\n3. Test in isolation\n\n**Fix:**\n\n``` // Correct approach import { prisma } from '@/lib/prisma';\n\n// Manual query in server component const data = await prisma.users.findUnique({ where: { email } });\n\n```\n\n### Strategy #3: The context refresh\n\nWhen the AI keeps making the same mistake, itâ€™s context-poisoned.\n\n**Symptom**: You fix a bug, the AI reintroduces it in the next iteration.\n\n**Solution**: Reset context with explicit correction:\n\n``` /implement fix and memorize: The TaskService uses async/await, NOT promises with .then()\n\nINCORRECT PATTERN (never use): ```typescript taskService.createTask(data).then(result => { ... })\n\n```\n\nCORRECT PATTERN (always use):\n\n``` const result = await taskService.createTask(data);\n\n```\n\nUpdate all existing code that uses .then() to use async/await. Update the plan to reflect this as a coding standard.\n\n```\n\nThe AI will:\n1. Fix all instances\n2. Update the plan to document the pattern\n3. Remember the pattern for future tasks\n\n### Strategy #4: The minimal reproduction\n\nWhen something breaks, isolate it.\n\n**INEFFECTIVE bug report**:\n\n```\n\nThe app crashes when I try to create a task\n\n```\n\n**EFFECTIVE bug report**:\n\n```\n\n/implement fix bug: Creating a task with empty title crashes the application\n\n**Steps to reproduce:**\n\n1. Log in as user test@example.com\n2. Navigate to /dashboard\n3. Click â€œNew Taskâ€ button\n4. Leave title field empty\n5. Enter description: â€œTest descriptionâ€\n6. Click â€œCreate Taskâ€\n\n**Expected**: Validation error â€œTitle is requiredâ€ **Actual**: Server crashes with 500 error\n\n**Error log:**\n\n``` TypeError: Cannot read property 'trim' of undefined at TaskService.createTask (services/TaskService.cs:47) at TaskController.Create (controllers/TaskController.cs:23)\n\n```\n\n**Hypothesis**: Title validation happens after attempting to trim, should happen before.\n\n```\n\n### Strategy #5: The dependency audit\n\nWhen things mysteriously break after an update, check dependencies.\n\n```bash\n# Check for breaking changes\nnpm outdated # Shows available updates npm audit # Security vulnerabilities\n\n# Lock down working versions\nnpm ci # Install exact versions from lockfile\n\n```\n\n**Real example**: Express 5.0 (released late 2024) changed middleware signatures. Code using Express 4.x patterns breaks silently.\n\n**Solution**: Pin major versions in constitution:\n\n```\n## Technical Constraints\n\n### Dependency Versions (Pinned for Stability)\n- express: ^4.18.0 (NOT 5.x until migration planned)\n- react: ^18.2.0\n- typescript: ^5.0.0\n\n**Rationale**: Avoid breaking changes mid-development\n\n```\n\n## Section 3: Iteration patterns\n\n### Evolving specs without chaos\n\nRequirements change. Your spec needs to evolve. Hereâ€™s how to do it systematically.\n\n> >\n> ğŸ“ **Rule of thumb**\n> >\n> **Add** â†’ Do it freely (safe)\n> **Modify** â†’ Document everything (risky)\n> **Remove** â†’ Phase it out gradually (dangerous)\n> >\n\n### Pattern #1: Additive changes (safe)\n\n**Scenario**: Add a new feature without modifying existing behavior.\n\n**Example**: Add task comments to existing task system.\n\n**Process:**\n\n1. Update spec with new section:\n\n```\n## Feature: Task Comments (Added 2026-01-19)\n\n**As a team member**, I want to add comments to tasks to discuss details with my team.\n\n### User Stories\n[... detailed scenarios ...]\n\n### Acceptance Criteria\n[... specific criteria ...]\n\n```\n\n1. Run `/speckit.plan`\nâ†’ Generates updated plan\n2. Run `/speckit.tasks`\nâ†’ Adds new tasks (existing tasks unchanged)\n3. Run `/speckit.implement`\nâ†’ Implements only new tasks\n\n**Result**: Clean addition, no regression risk.\n\n### Pattern #2: Modificative changes (risky)\n\n**Scenario**: Change existing behavior.\n\n**Example**: Change task status from boolean (complete/incomplete) to enum (open/in-progress/completed).\n\n**Process:**\n\n1. Document the change explicitly:\n\n```\n## Breaking Change: Task Status Model (Changed 2026-01-19)\n\n**Previous behavior:**\n- Task.complete: boolean (true/false)\n\n**New behavior:**\n- Task.status: enum (open | in_progress | completed)\n\n**Migration plan:**\n1. Add new status column\n2. Migrate data: false â†’ open, true â†’ completed\n3. Update API to accept status enum\n4. Update UI to show three states\n5. Remove old complete column\n\n**Affected components:**\n- Database schema (tasks table)\n- TaskService.createTask, updateTask\n- TaskCard component\n- TaskDetailModal component\n- API endpoints: PUT /api/tasks/:id\n\n```\n\n1. Run `/speckit.analyze`\nto find all affected artifacts\n2. Update each artifact explicitly:\n\n``` /implement migration: Task.complete (boolean) â†’ Task.status (enum)\n\nFollow the migration plan in spec.md exactly:\n1. Add status column to database\n2. Run data migration script\n3. Update TypeScript types\n4. Update API validation\n5. Update UI components\n6. Remove complete column\n\nTest each step before proceeding to next.\n\n```\n\n**Result**: Controlled migration, clear rollback path.\n\n### Pattern #3: Subtractive changes (dangerous)\n\n**Scenario**: Remove a feature.\n\n**Example**: Remove task due dates (users donâ€™t use them).\n\n**Process:**\n\n1. Document removal and rationale:\n\n```\n## Feature Removal: Task Due Dates (Removed 2026-01-19)\n\n**Rationale**: Usage analytics show only 3% of tasks have due dates. Feature adds UI complexity without user value.\n\n**Affected components:**\n- Database: tasks.due_date column (keep for rollback, but don't display)\n- API: Remove dueDate from POST/PUT payloads\n- UI: Remove date picker from TaskDetailModal\n- UI: Remove date display from TaskCard\n\n**Rollback plan**: Set UI flag to re-enable if users complain.\n\n**Migration:**\n1. Hide UI controls (don't delete database column yet)\n2. Monitor for 2 weeks\n3. If no complaints, remove database column\n4. If complaints, re-enable UI controls\n\n```\n\n1. Implement in phases:\n\n``` Phase 1: Hide UI /implement: Remove due date UI controls but keep database column\n\nPhase 2: Monitor (2 weeks wait)\n\nPhase 3: Remove infrastructure /implement: Remove due_date column and API support\n\n```\n\n**Result**: Safe removal with rollback option.\n\n## Section 4: Performance optimization\n\n### Keeping the AI focused and fast\n\nLarge projects can overwhelm the AIâ€™s context window. Hereâ€™s how to maintain performance.\n\n### Optimization #1: Scope reduction\n\nDonâ€™t send everything to the AI at once.\n\n**SLOW**:\n\n``` /implement entire application from spec\n\n```\n\n**FAST**:\n\n``` /implement phase 3: User authentication only\n\n```\n\n**Why**: Smaller context = faster responses = fewer errors.\n\n### Optimization #2: Reference by location\n\nInstead of pasting entire files, reference them:\n\n**BLOATED context**:\n\n``` Here's my entire spec.md (5000 lines) ...\n\n```\n\n**EFFICIENT reference**:\n\n``` /implement task 4.3 from tasks.md: Create TaskCard component\n\nRefer to spec.md section \"Task Card Layout\" for requirements. Refer to plan.md section \"Component Architecture\" for structure.\n\n```\n\n### Optimization #3: Checkpoint saves\n\nSave progress frequently to avoid re-generation.\n\n```\n# After each phase\ngit add . git commit -m \"Phase 3 complete: Authentication working\" git tag phase-3-complete\n\n# If AI goes off track, reset to checkpoint\ngit reset --hard phase-3-complete\n\n```\n\n### Optimization #4: Parallel work streams\n\nSplit independent features across multiple AI sessions.\n\n**Example**: Task management and user authentication are independent.\n\n**Session 1** (AI Agent A):\n\n``` /implement phase 3: User authentication\n\n```\n\n**Session 2** (AI Agent B):\n\n``` /implement phase 4: Task CRUD operations (stub out authentication checks for now)\n\n```\n\n**Integration step** (merge):\n\n``` /implement integration: Connect task CRUD to authentication\n\n```\n\n**Result**: 2x faster implementation, no conflicts.\n\n## Section 5: Real-world edge cases\n\n### Lessons from production deployments\n\nThese are real issues encountered in production spec-driven projects.\n\n> >\n> âš ï¸ **Warning**\n> >\n> These arenâ€™t hypothetical. Each of these cost real teams real time. Learn from their mistakes.\n> >\n\n### Edge Case #1: Concurrent updates conflict\n\n**Scenario**: Two users edit the same task simultaneously. Last write should win with conflict notification.\n\n**Spec requirement**:\n\n``` **Scenario: Concurrent task editing**\n\n**Given** User A and User B both open Task #42 for editing **When** User A changes title to \"Updated Title A\" and saves **And** User B changes title to \"Updated Title B\" and saves **Then** User B sees notification: \"This task was updated by User A. Your changes may conflict. Reload to see latest version?\" **And** Latest saved version (User B's) is stored **And** User B can choose to reload or force save\n\n```\n\n**Implementation challenge**: AI initially used naive timestamp comparison. Under load, race conditions caused data loss.\n\n**Solution**: Implement optimistic concurrency with version tokens:\n\n``` public class Task { public int Id { get; set; } public string Title { get; set; }\n\n[Timestamp] // EF Core concurrency token public byte[] RowVersion { get; set; } }\n\n// In TaskService public async TaskTask> UpdateTaskAsync(Task task) { try { _context.Tasks.Update(task); await _context.SaveChangesAsync(); return task; } catch (DbUpdateConcurrencyException ex) { // Conflict detected var entry = ex.Entries.Single(); var databaseValues = await entry.GetDatabaseValuesAsync(); var databaseTask = (Task)databaseValues.ToObject();\n\nthrow new ConcurrencyException( $\"Task was modified by {databaseTask.LastModifiedBy}. Reload to see latest version.\", databaseTask ); } }\n\n```\n\n**Lesson**: Specify concurrency handling explicitly in spec. Donâ€™t assume AI will get it right.\n\n### Edge Case #2: N+1 query problem\n\n**Scenario**: Loading 100 tasks takes 5 seconds. Performance requirement:\n\n**Root cause**: AI generated naive queries:\n\n``` // AI-generated (slow) public async TaskListTask>> GetTeamTasksAsync(int teamId) { var tasks = await _context.Tasks .Where(t => t.TeamId == teamId) .ToListAsync();\n\n// Loops for each task - N+1 queries! foreach (var task in tasks) { task.Creator = await _context.Users.FindAsync(task.CreatorId); task.Assignee = await _context.Users.FindAsync(task.AssigneeId); }\n\nreturn tasks; }\n\n```\n\n**Problem**: 100 tasks = 1 query + 100 queries for creators + 100 queries for assignees = 201 queries.\n\n**Solution**: Use eager loading:\n\n``` // Optimized public async TaskListTask>> GetTeamTasksAsync(int teamId) { return await _context.Tasks .Include(t => t.Creator) .Include(t => t.Assignee) .Where(t => t.TeamId == teamId) .ToListAsync(); }\n\n```\n\n**Result**: 100 tasks = 1 query. Performance improved from 5s to 120ms.\n\n**Lesson**: Add performance requirements to constitution:\n\n```\n## Quality Standards\n\n### Database Query Performance\n- All list queries MUST use eager loading (.Include())\n- Queries returning > 10 rows MUST include execution plan review\n- N+1 queries are PROHIBITED - PR will be rejected\n\n```\n\n### Edge Case #3: Memory leak in real-time connections\n\n**Scenario**: Application memory grows unbounded. After 2 hours, server crashes.\n\n**Root cause**: SignalR connections not cleaned up:\n\n``` // AI-generated (leaky) public class TaskHub : Hub { private static readonly Liststring> _connections = new Liststring>();\n\npublic override async Task OnConnectedAsync() { _connections.Add(Context.ConnectionId); // Added but never removed! await base.OnConnectedAsync(); } }\n\n```\n\n**Problem**: `_connections` list grows forever. Disconnected clients never removed.\n\n**Solution**: Use proper cleanup:\n\n``` // Fixed public class TaskHub : Hub { // Use ConcurrentDictionary for thread-safe operations private static readonly ConcurrentDictionarystring, string> _connections = new ConcurrentDictionarystring, string>();\n\npublic override async Task OnConnectedAsync() { _connections.TryAdd(Context.ConnectionId, Context.UserIdentifier); await base.OnConnectedAsync(); }\n\npublic override async Task OnDisconnectedAsync(Exception exception) { _connections.TryRemove(Context.ConnectionId, out _); // Cleanup! await base.OnDisconnectedAsync(exception); } }\n\n```\n\n**Lesson**: Specify cleanup explicitly in spec:\n\n```\n## Real-Time Requirements\n\n### WebSocket Connection Management\n- Connections MUST be tracked in thread-safe collection\n- Disconnections MUST remove connection from tracking\n- Server MUST handle abrupt disconnections (network failures)\n- Connection cleanup MUST complete within 30 seconds\n\n```\n\n### Edge Case #4: XSS vulnerability in task descriptions\n\n**Scenario**: User enters `` as task description. Alert executes when other users view the task.\n\n**Root cause**: AI didnâ€™t sanitize user input:\n\n```cshtml @* AI-generated (vulnerable) *@\n\n@((MarkupString)Task.Description) @* Renders raw HTML! *@\n\n```\n\n**Problem**: User-supplied HTML executed in other usersâ€™ browsers.\n\n**Solution**: Sanitize and escape:\n\n```cshtml @* Fixed - Razor automatically escapes *@\n\n@Task.Description @* Escaped by Blazor automatically *@\n\n```\n\nOr if Markdown support is needed:\n\n``` using Markdig; using Ganss.Xss;\n\npublic string SanitizeMarkdown(string markdown) { // Convert Markdown to HTML var html = Markdown.ToHtml(markdown);\n\n// Sanitize HTML (HtmlSanitizer uses whitelist approach by default) // Safe tags like p, strong, em, ul, ol, li are allowed by default // Dangerous tags like script, iframe are NOT in the default allowlist var sanitizer = new HtmlSanitizer();\n\n// Optionally restrict to only specific tags if needed: // sanitizer.AllowedTags.Clear(); // sanitizer.AllowedTags.Add(\"p\"); // sanitizer.AllowedTags.Add(\"strong\"); // etc.\n\nreturn sanitizer.Sanitize(html); }\n\n```\n\n**Lesson**: Add security requirements to constitution:\n\n```\n## Security Standards\n\n### Input Validation and Sanitization\n- ALL user input MUST be sanitized before display\n- HTML rendering MUST use whitelist approach (never blacklist)\n- Markdown MUST be sanitized before rendering\n- SQL queries MUST use parameterized queries (no string concatenation)\n- OWASP Top 10 MUST be addressed for all features\n\n```\n\n## Section 6: Common gotchas and fixes\n\n### Quick reference guide\n\n#### Gotcha #1: AI forgets the constitution mid-implementation\n\n**Symptom**: Early tasks follow standards, later tasks deviate.\n\n**Fix**:\n\n``` /implement reminder: Review constitution.md before each task Verify compliance before proceeding.\n\n```\n\n#### Gotcha #2: Generated tests donâ€™t run\n\n**Symptom**: Tests pass when AI runs them, fail when you run them.\n\n**Cause**: AI assumes mocked data, you have real database.\n\n**Fix**: Add test data setup to tasks:\n\n```\n- [ ] 5.1 Write unit test for TaskService.createTask\n- **Setup**: Seed test database with team and user\n- **Test**: Call createTask with valid data\n- **Assert**: Task created with correct properties\n- **Teardown**: Clean up test data\n\n```\n\n#### Gotcha #3: Environment variables missing in production\n\n**Symptom**: Works locally, crashes in production with â€œundefined environment variable.â€\n\n**Cause**: AI hard-coded localhost values.\n\n**Fix**: Add environment configuration to constitution:\n\n```\n## Deployment Requirements\n\n### Environment Variables (REQUIRED in production)\n- DATABASE_URL: PostgreSQL connection string\n- JWT_SECRET: Random 32-character string\n- SENDGRID_API_KEY: Email service API key\n- NODE_ENV: \"production\"\n\n**Validation**: Application MUST fail to start if required env vars are missing. Show error: \"Missing required environment variable: DATABASE_URL\"\n\n```\n\n#### Gotcha #4: API versioning not considered\n\n**Symptom**: Breaking API changes force mobile app update.\n\n**Cause**: Spec didnâ€™t mention API versioning.\n\n**Fix**: Add to constitution:\n\n```\n## API Design Principles\n\n### Versioning Strategy\n- All API endpoints MUST include version: /api/v1/tasks\n- Breaking changes require new version: /api/v2/tasks\n- Previous version MUST be supported for 6 months minimum\n- Deprecation warnings in headers: `X-API-Deprecated: v1 will be removed on 2026-07-01`\n\n```\n\n#### Gotcha #5: AI uses deprecated patterns\n\n**Symptom**: Code works but uses old syntax (e.g., `var` instead of `let/const` , callback-style instead of async/await).\n\n**Cause**: AI trained on older codebases.\n\n**Fix**: Add coding style requirements to constitution:\n\n```\n## Code Style Requirements\n\n### Modern Patterns (REQUIRED)\n- Use async/await, NOT .then() callbacks\n- Use const by default, let when reassignment needed, NEVER var\n- Use arrow functions for callbacks\n- Use template literals, NOT string concatenation\n\n```\n\n#### Gotcha #6: AI generates over-engineered solutions\n\n**Symptom**: Simple feature implemented with 5 classes, 3 interfaces, and a factory pattern.\n\n**Cause**: AI saw enterprise patterns in training data.\n\n**Fix**: Add simplicity constraints to constitution:\n\n```\n## Simplicity Constraints\n\n### YAGNI Principle\n- No interfaces with single implementations\n- No factory patterns unless 3+ concrete types exist\n- No dependency injection for classes with no dependencies\n- Maximum 3 levels of class inheritance\n\nBefore adding abstraction, AI MUST justify with: \"This abstraction is needed because...\"\n\n```\n\n## Section 7: Measuring success\n\n### How do you know if spec-driven development is working?\n\nTrack these five key metrics:\n\n| Metric | What to Measure | ğŸŸ¢ Good | ğŸŸ¡ Warning | | --- | --- | --- | --- | | **Velocity** | Tasks completed per day | 5-10/day | | | **Regression** | Bugs reintroduced after fix | | > 20% | | **Stability** | Spec changes per week | | > 5 major | | **Coverage** | Test coverage % | > 80% | | | **Recovery** | Bug report â†’ fix deployed | | > 4 hours |\n\n#### Fixing slow velocity\n\n- Break tasks smaller (2-4 hours each)\n- Simplify constitution (too many constraints?)\n- Provide more context in spec\n\n#### Fixing high regression\n\n- Use `/implement fix and memorize`\ninstead of just `/implement fix`\n- Update plan.md with lessons learned\n- Add regression tests to tasks\n\n#### Fixing high spec churn\n\n- Spend more time on spec before implementation\n- Validate spec with stakeholders early\n- Build MVP, then iterate based on feedback\n\n#### Fixing low coverage\n\n- Add test requirements to tasks explicitly\n- Require test task for each feature task\n- Run coverage reports in CI/CD\n\n#### Fixing slow recovery\n\n- Improve bug report templates (see Strategy #4 above)\n- Add health monitoring endpoints\n- Implement feature flags for quick rollback\n\n## Section 8: Tools and automation\n\n### Helpers that make spec-driven development smoother\n\n### Tool #1: Specification validator\n\nCreate a script to validate spec completeness:\n\n``` #!/bin/bash\n# validate-spec.sh\n\necho \"Validating specification...\"\n\n# Check for required sections\nrequired_sections=(\"User Stories\" \"Acceptance Criteria\" \"Data Requirements\" \"Edge Cases\")\n\nfor section in \"${required_sections[@]}\"; do if ! grep -q \"## $section\" .speckit/spec.md; then echo \"[FAIL] Missing required section: $section\" exit 1 fi done\n\n# Check for TODO markers\nif grep -q \"TODO\" .speckit/spec.md; then echo \"[FAIL] Specification contains TODO markers\" exit 1 fi\n\n# Check for vague language\nvague_terms=(\"might\" \"maybe\" \"probably\" \"hopefully\" \"basically\") for term in \"${vague_terms[@]}\"; do if grep -i -q \"$term\" .speckit/spec.md; then echo \"[WARN] Spec contains vague term '$term'\" fi done\n\necho \"[PASS] Specification validation passed\"\n\n```\n\nUsage:\n\n``` ./validate-spec.sh\n\n```\n\n### Tool #2: Constitution compliance checker\n\nVerify code follows constitution standards:\n\n``` #!/bin/bash\n# check-compliance.sh\n\necho \"Checking constitution compliance...\"\n\n# Check nullable reference types enabled\nif ! grep -q \"enable\" *.csproj; then echo \"[FAIL] Nullable reference types not enabled (constitution requirement)\" exit 1 fi\n\n# Check for raw SQL (prohibited)\nif grep -r \"ExecuteSqlRaw\\|FromSqlRaw\" ./Services ./Controllers; then echo \"[FAIL] Raw SQL detected (constitution prohibits)\" exit 1 fi\n\n# Check for console.log in production code (debugging leftovers)\nif grep -r \"Console.WriteLine\" ./Services ./Controllers | grep -v \"// DEBUG\"; then echo \"[WARN] Console.WriteLine found in production code\" fi\n\necho \"[PASS] Constitution compliance check passed\"\n\n```\n\n### Tool #3: Performance testing automation\n\nAdd to CI/CD pipeline:\n\n```\n# .github/workflows/performance.yml\nname: Performance Tests\n\non: [push, pull_request]\n\njobs: performance: runs-on: ubuntu-latest steps:\n- uses: actions/checkout@v3\n\n- name: Build application\nrun: dotnet build\n\n- name: Start application\nrun: dotnet run & env: ASPNETCORE_ENVIRONMENT: Development\n\n- name: Wait for startup\nrun: sleep 10\n\n- name: Run performance tests\nrun: |\n# Test API response times\nresponse_time=$(curl -w \"%{time_total}\" -s -o /dev/null http://localhost:5000/api/health)\n\nif (( $(echo \"$response_time > 0.5\" | bc -l) )); then echo \"[FAIL] Health check took ${response_time}s (requirement: exit 1 fi\n\necho \"[PASS] Health check took ${response_time}s\"\n\n```\n\n### Tool #4: Spec change impact analyzer\n\nWhen you change the spec, see whatâ€™s affected:\n\n``` #!/usr/bin/env python3\n# analyze-impact.py\n\nimport re import sys\n\ndef analyze_spec_changes(old_spec, new_spec): \"\"\"Compare two spec versions and identify affected components\"\"\"\n\nchanges = { 'added_sections': [], 'modified_sections': [], 'removed_sections': [], 'affected_tasks': [], 'affected_files': [] }\n\n# Simple diff logic (extend as needed)\nold_sections = re.findall(r'^## (.+)$', old_spec, re.MULTILINE) new_sections = re.findall(r'^## (.+)$', new_spec, re.MULTILINE)\n\nchanges['added_sections'] = [s for s in new_sections if s not in old_sections] changes['removed_sections'] = [s for s in old_sections if s not in new_sections]\n\nprint(\"\\n=== Spec Change Impact Analysis ===\\n\")\n\nif changes['added_sections']: print(\"[+] Added sections:\") for section in changes['added_sections']: print(f\" - {section}\")\n\nif changes['removed_sections']: print(\"\\n[-] Removed sections:\") for section in changes['removed_sections']: print(f\" - {section}\")\n\nif not changes['added_sections'] and not changes['removed_sections']: print(\"[OK] No structural changes detected\") else: print(\"\\n[!] Actions required:\") print(\" 1. Run /speckit.plan to update technical plan\") print(\" 2. Run /speckit.tasks to regenerate task breakdown\") print(\" 3. Review affected files before re-implementation\")\n\nif __name__ == '__main__': if len(sys.argv) 3: print(\"Usage: ./analyze-impact.py old-spec.md new-spec.md\") sys.exit(1)\n\nwith open(sys.argv[1]) as f: old_spec = f.read()\n\nwith open(sys.argv[2]) as f: new_spec = f.read()\n\nanalyze_spec_changes(old_spec, new_spec)\n\n```\n\nUsage:\n\n``` cp .speckit/spec.md .speckit/spec-old.md\n# Make changes to spec.md\n./analyze-impact.py .speckit/spec-old.md .speckit/spec.md\n\n```\n\n## Section 9: When to abandon spec-driven development\n\n### Itâ€™s not always the right tool\n\nAs Addy Osmani notes in [Beyond Vibe Coding](https://beyond.addy.ie/), the key is knowing when structure helps and when it hinders. Spec-driven development has overhead. Know when to use simpler approaches.\n\n| âŒ Donâ€™t use spec-driven for | âœ… Do use spec-driven for | | --- | --- | | **One-off scripts**: vibe code is fine | **Production applications**: users depend on it | | **Proof-of-concept demos**: iterate fast | **Team projects**: shared understanding needed | | **Extremely simple apps**: 3 features max | **Regulated industries**: audit trails required | | **Rapidly changing requirements**: daily pivots | **Long-lived systems**: years of maintenance | | **Learning new tech**: experimentation first | **Complex domains**: intricate business logic |\n\n## Recap: Best practices checklist\n\nUse this before starting any spec-driven project:\n\n| Phase | Checklist Item | | | --- | --- | --- | | **ğŸ“œ Constitution** | Specific technology constraints listed | â˜ | | | Performance targets are measurable numbers | â˜ | | | Security requirements reference OWASP Top 10 | â˜ | | | Explicit non-goals prevent feature creep | â˜ | | | Amendment process defined | â˜ | | **ğŸ“ Specification** | User stories use Given-When-Then format | â˜ | | | Error messages specified exactly | â˜ | | | UI mockups provided (ASCII art is fine) | â˜ | | | Edge cases documented with expected behavior | â˜ | | | Success metrics are measurable | â˜ | | **ğŸ—ºï¸ Planning** | Technology choices align with constitution | â˜ | | | Data model includes indexes for performance | â˜ | | | API design is RESTful and consistent | â˜ | | | Security patterns specified | â˜ | | | Open questions documented | â˜ | | **ğŸ“‹ Tasks** | Tasks are 2-4 hours each | â˜ | | | Dependencies are explicit | â˜ | | | Acceptance criteria per task | â˜ | | | Parallel opportunities identified | â˜ | | | Test tasks included for each feature | â˜ | | **ğŸš€ Implementation** | Test after each phase | â˜ | | | Rich context for bug reports | â˜ | | | Use `/implement fix and memorize`<br> for recurring issues | â˜ | | | Checkpoint progress with git tags | â˜ | | | Run automated compliance checks | â˜ |\n\n## Whatâ€™s next\n\nIn **Part 4** (next week), weâ€™ll explore:\n\n- **Team workflows**: How multiple developers use Spec-Kit together\n- **Review processes**: Code review checklist for spec-driven projects\n- **CI/CD integration**: Automating validation and deployment\n- **Advanced patterns**: Microservices, event-driven architectures\n- **Scaling strategies**: When your application outgrows the initial architecture\n\n## Key takeaways from Part 3\n\n| # | Takeaway | Remember | | --- | --- | --- | | 1 | **Specifications need precision** | Given-When-Then, exact error messages, UI mocks | | 2 | **Debug systematically** | Validation loops, hallucination detection, context refresh | | 3 | **Iterate carefully** | Add = safe, Modify = risky, Remove = dangerous | | 4 | **Performance matters** | Watch for N+1 queries, memory leaks, concurrency | | 5 | **Measure success** | Track velocity, regression rate, time to recovery |\n\n**Next week in Part 4**, weâ€™ll tackle team collaboration and how Spec-Kit scales beyond solo developers.\n\n## Resources\n\n| Resource | Description | | --- | --- | | [**Spec-Kit GitHub**](https://github.com/github/spec-kit) | Official toolkit repository | | [**Beyond Vibe Coding**](https://beyond.addy.ie/) | Addy Osmaniâ€™s guide to AI-assisted development | | [**OWASP Top 10**](https://owasp.org/www-project-top-ten/) | Security requirements reference | | [**Exploring Gen AI**](https://martinfowler.com/articles/exploring-gen-ai.html) | Martin Fowlerâ€™s AI development series | | [**Given-When-Then**](https://martinfowler.com/bliki/GivenWhenThen.html) | BDD specification pattern |\n\n## Series navigation\n\n- **Previous**: [Part 2 - The Spec-Kit workflow](/from-vibe-coding-to-spec-driven-development-part2)\n- **ğŸ“ You are here: Part 3 - Best practices and troubleshooting**\n- **Next**: Part 4 - Team collaboration and advanced patterns (Coming January 26, 2026)\n- Part 5 - Case studies and lessons learned (Coming February 2, 2026)\n\n> >\n> ğŸ’¬ **Found a gotcha not covered here?**\n> >\n> Connect with me on [LinkedIn](https://linkedin.com/in/hiddedesmet) to share your experience.\n> >\n> **Want to get notified when Part 4 drops?** Follow me for updates!\n> >",
  "Title": "From Vibe Coding to Spec-Driven Development: Part 3 - Best Practices and Troubleshooting",
  "ProcessedDate": "2026-01-19 09:06:24",
  "EnhancedContent": "Search for Blog\n\n[ai-assisted-development](/tags#ai-assisted-development)\n\n[spec-kit](/tags#spec-kit)\n\n[github](/tags#github)\n\n[copilot](/tags#copilot)\n\n[vibe-coding](/tags#vibe-coding)\n\n[specification-driven-development](/tags#specification-driven-development)\n\n[series](/tags#series)\n\nâ€¢ Jan 19, 2026\n\nâ€¢\n\n35 min read\n\n# From Vibe Coding to Spec-Driven Development: Part 3 - Best Practices and Troubleshooting\n\nPart 3 of our series on mastering AI-assisted development. Learn advanced specification techniques, debugging strategies, iteration patterns, and real-world troubleshooting for production-ready AI-generated code.\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by [Hidde de Smet](/hidde)\n\n- https://twitter.com/intent/tweet?text=From%20Vibe%20Coding%20to%20Spec-Driven%20Development:%20Part%203%20-%20Best%20Practices%20and%20Troubleshooting&url=https://hiddedesmet.com/from-vibe-coding-to-spec-driven-development-part3\n- https://www.facebook.com/sharer/sharer.php?u=https://hiddedesmet.com/from-vibe-coding-to-spec-driven-development-part3\n- http://pinterest.com/pin/create/button/?url=https://hiddedesmet.com/from-vibe-coding-to-spec-driven-development-part3&amp;media=https://hiddedesmet.com/images/spec-kit/image-03.png&amp;description=From%20Vibe%20Coding%20to%20Spec-Driven%20Development:%20Part%203%20-%20Best%20Practices%20and%20Troubleshooting\n- https://www.linkedin.com/shareArticle?mini=true&url=https://hiddedesmet.com/from-vibe-coding-to-spec-driven-development-part3&title=From%20Vibe%20Coding%20to%20Spec-Driven%20Development:%20Part%203%20-%20Best%20Practices%20and%20Troubleshooting&summary=Part%203%20of%20our%20series%20on%20mastering%20AI-assisted%20development.%20Learn%20advanced%20specification%20techniques,%20debugging%20strategies,%20iteration%20patterns,%20and%20real-world%20troubleshooting%20for%20production-ready%20AI-generated%20code.&source=myblog\n\n![From Vibe Coding to Spec-Driven Development: Part 3 - Best Practices and Troubleshooting]()\n\n## Table of Contents\n\n1. Series overview\n2. The reality of spec-driven development\n3. Section 1: Advanced specification techniques\n1. Writing specs that survive reality\n1. Technique #1: Use Given-When-Then scenarios\n2. Technique #2: Specify error messages exactly\n3. Technique #3: Mock the UI with ASCII art\n4. Technique #4: Define the negative space\n4. Section 2: Debugging strategies\n1. When the AI goes off track\n2. Strategy #1: The validation loop\n3. Strategy #2: The hallucination detector\n4. Strategy #3: The context refresh\n5. Section 3: Iteration patterns\n1. Evolving specs without chaos\n2. Pattern #1: Additive changes (safe)\n3. Pattern #2: Modificative changes (risky)\n4. Pattern #3: Subtractive changes (dangerous)\n6. Section 4: Performance optimization\n1. Keeping the AI focused and fast\n2. Optimization #1: Scope reduction\n3. Optimization #2: Reference by location\n4. Optimization #3: Checkpoint saves\n5. Optimization #4: Parallel work streams\n7. Section 5: Real-world edge cases\n1. Lessons from production deployments\n2. Edge Case #1: Concurrent updates conflict\n3. Edge Case #2: N+1 query problem\n4. Edge Case #3: Memory leak in real-time connections\n5. Edge Case #4: XSS vulnerability in task descriptions\n8. Section 6: Common gotchas and fixes\n1. Quick reference guide\n1. Gotcha #1: AI forgets the constitution mid-implementation\n2. Gotcha #2: Generated tests donâ€™t run\n3. Gotcha #3: Environment variables missing in production\n4. Gotcha #4: API versioning not considered\n5. Gotcha #5: AI uses deprecated patterns\n6. Gotcha #6: AI generates over-engineered solutions\n9. Section 7: Measuring success\n1. How do you know if spec-driven development is working?\n1. Fixing slow velocity\n2. Fixing high regression\n3. Fixing high spec churn\n4. Fixing low coverage\n5. Fixing slow recovery\n10. Section 8: Tools and automation\n1. Helpers that make spec-driven development smoother\n2. Tool #1: Specification validator\n3. Tool #2: Constitution compliance checker\n4. Tool #3: Performance testing automation\n5. Tool #4: Spec change impact analyzer\n11. Section 9: When to abandon spec-driven development\n1. Itâ€™s not always the right tool\n12. Recap: Best practices checklist\n13. Whatâ€™s next\n14. Key takeaways from Part 3\n15. Resources\n16. Series navigation\n\n> >\n> ğŸ¯ **This is Part 3 of a 5-part series on mastering AI-assisted development.**\n> >\n> Weâ€™ve covered the why and the how. Now letâ€™s tackle the messy reality: debugging, iteration, and real-world troubleshooting.\n> >\n\n## Series overview\n\n| Part | Topic | Status | | --- | --- | --- | | [Part 1](/from-vibe-coding-to-spec-driven-development) | The problem and the solution | âœ“ | | [Part 2](/from-vibe-coding-to-spec-driven-development-part2) | Deep dive into the Spec-Kit workflow | âœ“ | | **Part 3** | **Best practices and troubleshooting** | ğŸ“ | | Part 4 | Team collaboration and advanced patterns | Jan 26 | | Part 5 | Case studies and lessons learned | Feb 2 |\n\n## The reality of spec-driven development\n\nLast week, I was helping a team at a European airline build a proof-of-concept for managing contractors. Everything was going smoothly until the AI started generating code that referenced a workflow library that simply didnâ€™t exist. Once weâ€™d untangled the hallucination, it got me thinking: **we need a field guide for when things go wrong.**\n\nIn Part 2, we walked through a clean, linear workflow: constitution â†’ spec â†’ plan â†’ tasks â†’ implementation. In reality, development is rarely that smooth.\n\nYouâ€™ll encounter:\n\n| Challenge | What happens | | --- | --- | | **AI hallucinations** | The AI invents APIs that donâ€™t exist | | **Spec ambiguities** | Unclear requirements lead to wrong implementations | | **Performance issues** | Code works but doesnâ€™t scale | | **Integration failures** | Components donâ€™t play well together | | **Requirement changes** | What you built isnâ€™t what users need |\n\nThis post is about handling those messy situations. **Think of it as your field guide for when things go wrong.**\n\n## Section 1: Advanced specification techniques\n\n### Writing specs that survive reality\n\nThe quality of your specification directly determines the quality of your output. Hereâ€™s how to write specs that work in practice.\n\n> >\n> ğŸ’¡ **Quick win**\n> >\n> Start with Technique #1 (Given-When-Then). It alone will significantly improve your AI output quality.\n> >\n\n#### Technique #1: Use Given-When-Then scenarios\n\nInstead of vague descriptions, use structured scenarios:\n\n**DONâ€™T** (Vague):\n\n``` Users should be able to filter tasks\n\n```\n\n**DO** (Structured):\n\n``` **Scenario: Filter tasks by assignee**\n\n**Given** I'm viewing the team dashboard **And** there are 50 tasks assigned to various team members **When** I select \"John Smith\" from the assignee filter **Then** I see only tasks assigned to John Smith (expected: 12 tasks) **And** the count updates to show \"Showing 12 of 50 tasks\" **And** the filter persists when I navigate away and return\n\n```\n\n**Why this works**: The AI has concrete inputs, actions, and expected outputs. No room for interpretation.\n\n#### Technique #2: Specify error messages exactly\n\nDonâ€™t say â€œshow an error.â€ Say exactly what the error should say.\n\n**DONâ€™T** (Vague):\n\n``` If the user enters an invalid email, show an error\n\n```\n\n**DO** (Specific):\n\n``` **Validation: Email format**\n\n**Invalid inputs:**\n- Empty string â†’ \"Email is required\"\n- \"notanemail\" â†’ \"Please enter a valid email address\"\n- \"test@\" â†’ \"Please enter a valid email address\"\n- \"test@domain\" â†’ \"Please enter a valid email address\"\n\n**Valid inputs:**\n- \"user@example.com\" â†’ Validation passes\n- \"user+tag@example.co.uk\" â†’ Validation passes\n\n**Display:**\n- Error appears below input field\n- Input border turns red (#dc3545)\n- Error icon displayed to the left of message\n- Error clears immediately when user starts typing\n\n```\n\n#### Technique #3: Mock the UI with ASCII art\n\nDonâ€™t assume the AI knows what you want visually.\n\n```\n## Task Card Layout\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ [âœ“] Task title goes here [Edit] â”‚ â”‚ â”‚ â”‚ Brief description preview shown here, â”‚ â”‚ truncated at 100 characters... â”‚ â”‚ â”‚ â”‚ ğŸ‘¤ Assigned to: John Smith â”‚ â”‚ ğŸ“… Due: Jan 25, 2026 â”‚ â”‚ ğŸ·ï¸ Status: In Progress â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n**Interactions:**\n- Checkbox: Click to toggle complete/incomplete\n- Card body: Click to open detail view\n- Edit button: Click to enter edit mode\n- Hover: Box shadow elevation (0.5rem)\n\n```\n\n#### Technique #4: Define the negative space\n\nExplicitly state what youâ€™re NOT building.\n\n```\n## Out of Scope (Explicitly NOT Implementing)\n\n### Comments on tasks\n**Why**: Adds complexity without MVP validation **Future consideration**: Phase 2 after user feedback\n\n### File attachments\n**Why**: Requires file storage infrastructure **Alternative**: Use task description to link to external files\n\n### Recurring tasks\n**Why**: Complex scheduling logic **Alternative**: Manual task duplication for now\n\n### Subtasks/nested tasks\n**Why**: Violates \"Simplicity First\" principle **Alternative**: Use task dependencies in future phase\n\n```\n\n**Why this matters**: Prevents the AI from â€œhelpfullyâ€ adding features you didnâ€™t ask for.\n\n## Section 2: Debugging strategies\n\n### When the AI goes off track\n\nAI hallucinations are real. Hereâ€™s how to diagnose and fix them.\n\n> >\n> ğŸ“‹ **TL;DR: The 5-step debug checklist**\n>\n> 1. Validate dependencies exist\n> 2. Check for hallucinated APIs\n> 3. Reset context if issues repeat\n> 4. Provide minimal reproductions\n> 5. Audit dependencies after updates\n> >\n\n### Strategy #1: The validation loop\n\nAfter implementation, run this three-step validation:\n\n```\n# Step 1: Verify dependencies exist\nnpm list # Check all packages are installed dotnet list package # For .NET projects\n\n# Step 2: Check for type errors\nnpm run typecheck # TypeScript dotnet build # C#\n\n# Step 3: Run tests\nnpm test # Run test suite dotnet test # Run unit tests\n\n```\n\n**Common issues caught:**\n\n- AI referenced packages that donâ€™t exist\n- API endpoints that were never implemented\n- Type mismatches between frontend and backend\n\n### Strategy #2: The hallucination detector\n\nAI models sometimes invent plausible-sounding but non-existent APIs.\n\n**Example hallucination:**\n\n``` // AI-generated code import { usePrismaQuery } from '@prisma/react';\n\n// This hook doesn't exist! const { data } = usePrismaQuery('users', { where: { email } });\n\n```\n\n**How to catch it:**\n\n1. Check official documentation\n2. Search GitHub for the import\n3. Test in isolation\n\n**Fix:**\n\n``` // Correct approach import { prisma } from '@/lib/prisma';\n\n// Manual query in server component const data = await prisma.users.findUnique({ where: { email } });\n\n```\n\n### Strategy #3: The context refresh\n\nWhen the AI keeps making the same mistake, itâ€™s context-poisoned.\n\n**Symptom**: You fix a bug, the AI reintroduces it in the next iteration.\n\n**Solution**: Reset context with explicit correction:\n\n``` /implement fix and memorize: The TaskService uses async/await, NOT promises with .then()\n\nINCORRECT PATTERN (never use): ```typescript taskService.createTask(data).then(result => { ... })\n\n```\n\nCORRECT PATTERN (always use):\n\n``` const result = await taskService.createTask(data);\n\n```\n\nUpdate all existing code that uses .then() to use async/await. Update the plan to reflect this as a coding standard.\n\n```\n\nThe AI will:\n1. Fix all instances\n2. Update the plan to document the pattern\n3. Remember the pattern for future tasks\n\n### Strategy #4: The minimal reproduction\n\nWhen something breaks, isolate it.\n\n**INEFFECTIVE bug report**:\n\n```\n\nThe app crashes when I try to create a task\n\n```\n\n**EFFECTIVE bug report**:\n\n```\n\n/implement fix bug: Creating a task with empty title crashes the application\n\n**Steps to reproduce:**\n\n1. Log in as user test@example.com\n2. Navigate to /dashboard\n3. Click â€œNew Taskâ€ button\n4. Leave title field empty\n5. Enter description: â€œTest descriptionâ€\n6. Click â€œCreate Taskâ€\n\n**Expected**: Validation error â€œTitle is requiredâ€ **Actual**: Server crashes with 500 error\n\n**Error log:**\n\n``` TypeError: Cannot read property 'trim' of undefined at TaskService.createTask (services/TaskService.cs:47) at TaskController.Create (controllers/TaskController.cs:23)\n\n```\n\n**Hypothesis**: Title validation happens after attempting to trim, should happen before.\n\n```\n\n### Strategy #5: The dependency audit\n\nWhen things mysteriously break after an update, check dependencies.\n\n```bash\n# Check for breaking changes\nnpm outdated # Shows available updates npm audit # Security vulnerabilities\n\n# Lock down working versions\nnpm ci # Install exact versions from lockfile\n\n```\n\n**Real example**: Express 5.0 (released late 2024) changed middleware signatures. Code using Express 4.x patterns breaks silently.\n\n**Solution**: Pin major versions in constitution:\n\n```\n## Technical Constraints\n\n### Dependency Versions (Pinned for Stability)\n- express: ^4.18.0 (NOT 5.x until migration planned)\n- react: ^18.2.0\n- typescript: ^5.0.0\n\n**Rationale**: Avoid breaking changes mid-development\n\n```\n\n## Section 3: Iteration patterns\n\n### Evolving specs without chaos\n\nRequirements change. Your spec needs to evolve. Hereâ€™s how to do it systematically.\n\n> >\n> ğŸ“ **Rule of thumb**\n> >\n> **Add** â†’ Do it freely (safe)\n> **Modify** â†’ Document everything (risky)\n> **Remove** â†’ Phase it out gradually (dangerous)\n> >\n\n### Pattern #1: Additive changes (safe)\n\n**Scenario**: Add a new feature without modifying existing behavior.\n\n**Example**: Add task comments to existing task system.\n\n**Process:**\n\n1. Update spec with new section:\n\n```\n## Feature: Task Comments (Added 2026-01-19)\n\n**As a team member**, I want to add comments to tasks to discuss details with my team.\n\n### User Stories\n[... detailed scenarios ...]\n\n### Acceptance Criteria\n[... specific criteria ...]\n\n```\n\n1. Run `/speckit.plan`\nâ†’ Generates updated plan\n2. Run `/speckit.tasks`\nâ†’ Adds new tasks (existing tasks unchanged)\n3. Run `/speckit.implement`\nâ†’ Implements only new tasks\n\n**Result**: Clean addition, no regression risk.\n\n### Pattern #2: Modificative changes (risky)\n\n**Scenario**: Change existing behavior.\n\n**Example**: Change task status from boolean (complete/incomplete) to enum (open/in-progress/completed).\n\n**Process:**\n\n1. Document the change explicitly:\n\n```\n## Breaking Change: Task Status Model (Changed 2026-01-19)\n\n**Previous behavior:**\n- Task.complete: boolean (true/false)\n\n**New behavior:**\n- Task.status: enum (open | in_progress | completed)\n\n**Migration plan:**\n1. Add new status column\n2. Migrate data: false â†’ open, true â†’ completed\n3. Update API to accept status enum\n4. Update UI to show three states\n5. Remove old complete column\n\n**Affected components:**\n- Database schema (tasks table)\n- TaskService.createTask, updateTask\n- TaskCard component\n- TaskDetailModal component\n- API endpoints: PUT /api/tasks/:id\n\n```\n\n1. Run `/speckit.analyze`\nto find all affected artifacts\n2. Update each artifact explicitly:\n\n``` /implement migration: Task.complete (boolean) â†’ Task.status (enum)\n\nFollow the migration plan in spec.md exactly:\n1. Add status column to database\n2. Run data migration script\n3. Update TypeScript types\n4. Update API validation\n5. Update UI components\n6. Remove complete column\n\nTest each step before proceeding to next.\n\n```\n\n**Result**: Controlled migration, clear rollback path.\n\n### Pattern #3: Subtractive changes (dangerous)\n\n**Scenario**: Remove a feature.\n\n**Example**: Remove task due dates (users donâ€™t use them).\n\n**Process:**\n\n1. Document removal and rationale:\n\n```\n## Feature Removal: Task Due Dates (Removed 2026-01-19)\n\n**Rationale**: Usage analytics show only 3% of tasks have due dates. Feature adds UI complexity without user value.\n\n**Affected components:**\n- Database: tasks.due_date column (keep for rollback, but don't display)\n- API: Remove dueDate from POST/PUT payloads\n- UI: Remove date picker from TaskDetailModal\n- UI: Remove date display from TaskCard\n\n**Rollback plan**: Set UI flag to re-enable if users complain.\n\n**Migration:**\n1. Hide UI controls (don't delete database column yet)\n2. Monitor for 2 weeks\n3. If no complaints, remove database column\n4. If complaints, re-enable UI controls\n\n```\n\n1. Implement in phases:\n\n``` Phase 1: Hide UI /implement: Remove due date UI controls but keep database column\n\nPhase 2: Monitor (2 weeks wait)\n\nPhase 3: Remove infrastructure /implement: Remove due_date column and API support\n\n```\n\n**Result**: Safe removal with rollback option.\n\n## Section 4: Performance optimization\n\n### Keeping the AI focused and fast\n\nLarge projects can overwhelm the AIâ€™s context window. Hereâ€™s how to maintain performance.\n\n### Optimization #1: Scope reduction\n\nDonâ€™t send everything to the AI at once.\n\n**SLOW**:\n\n``` /implement entire application from spec\n\n```\n\n**FAST**:\n\n``` /implement phase 3: User authentication only\n\n```\n\n**Why**: Smaller context = faster responses = fewer errors.\n\n### Optimization #2: Reference by location\n\nInstead of pasting entire files, reference them:\n\n**BLOATED context**:\n\n``` Here's my entire spec.md (5000 lines) ...\n\n```\n\n**EFFICIENT reference**:\n\n``` /implement task 4.3 from tasks.md: Create TaskCard component\n\nRefer to spec.md section \"Task Card Layout\" for requirements. Refer to plan.md section \"Component Architecture\" for structure.\n\n```\n\n### Optimization #3: Checkpoint saves\n\nSave progress frequently to avoid re-generation.\n\n```\n# After each phase\ngit add . git commit -m \"Phase 3 complete: Authentication working\" git tag phase-3-complete\n\n# If AI goes off track, reset to checkpoint\ngit reset --hard phase-3-complete\n\n```\n\n### Optimization #4: Parallel work streams\n\nSplit independent features across multiple AI sessions.\n\n**Example**: Task management and user authentication are independent.\n\n**Session 1** (AI Agent A):\n\n``` /implement phase 3: User authentication\n\n```\n\n**Session 2** (AI Agent B):\n\n``` /implement phase 4: Task CRUD operations (stub out authentication checks for now)\n\n```\n\n**Integration step** (merge):\n\n``` /implement integration: Connect task CRUD to authentication\n\n```\n\n**Result**: 2x faster implementation, no conflicts.\n\n## Section 5: Real-world edge cases\n\n### Lessons from production deployments\n\nThese are real issues encountered in production spec-driven projects.\n\n> >\n> âš ï¸ **Warning**\n> >\n> These arenâ€™t hypothetical. Each of these cost real teams real time. Learn from their mistakes.\n> >\n\n### Edge Case #1: Concurrent updates conflict\n\n**Scenario**: Two users edit the same task simultaneously. Last write should win with conflict notification.\n\n**Spec requirement**:\n\n``` **Scenario: Concurrent task editing**\n\n**Given** User A and User B both open Task #42 for editing **When** User A changes title to \"Updated Title A\" and saves **And** User B changes title to \"Updated Title B\" and saves **Then** User B sees notification: \"This task was updated by User A. Your changes may conflict. Reload to see latest version?\" **And** Latest saved version (User B's) is stored **And** User B can choose to reload or force save\n\n```\n\n**Implementation challenge**: AI initially used naive timestamp comparison. Under load, race conditions caused data loss.\n\n**Solution**: Implement optimistic concurrency with version tokens:\n\n``` public class Task { public int Id { get; set; } public string Title { get; set; }\n\n[Timestamp] // EF Core concurrency token public byte[] RowVersion { get; set; } }\n\n// In TaskService public async Task<Task> UpdateTaskAsync(Task task) { try { _context.Tasks.Update(task); await _context.SaveChangesAsync(); return task; } catch (DbUpdateConcurrencyException ex) { // Conflict detected var entry = ex.Entries.Single(); var databaseValues = await entry.GetDatabaseValuesAsync(); var databaseTask = (Task)databaseValues.ToObject();\n\nthrow new ConcurrencyException( $\"Task was modified by {databaseTask.LastModifiedBy}. Reload to see latest version.\", databaseTask ); } }\n\n```\n\n**Lesson**: Specify concurrency handling explicitly in spec. Donâ€™t assume AI will get it right.\n\n### Edge Case #2: N+1 query problem\n\n**Scenario**: Loading 100 tasks takes 5 seconds. Performance requirement: &lt; 500ms.\n\n**Root cause**: AI generated naive queries:\n\n``` // AI-generated (slow) public async Task<List<Task>> GetTeamTasksAsync(int teamId) { var tasks = await _context.Tasks .Where(t => t.TeamId == teamId) .ToListAsync();\n\n// Loops for each task - N+1 queries! foreach (var task in tasks) { task.Creator = await _context.Users.FindAsync(task.CreatorId); task.Assignee = await _context.Users.FindAsync(task.AssigneeId); }\n\nreturn tasks; }\n\n```\n\n**Problem**: 100 tasks = 1 query + 100 queries for creators + 100 queries for assignees = 201 queries.\n\n**Solution**: Use eager loading:\n\n``` // Optimized public async Task<List<Task>> GetTeamTasksAsync(int teamId) { return await _context.Tasks .Include(t => t.Creator) .Include(t => t.Assignee) .Where(t => t.TeamId == teamId) .ToListAsync(); }\n\n```\n\n**Result**: 100 tasks = 1 query. Performance improved from 5s to 120ms.\n\n**Lesson**: Add performance requirements to constitution:\n\n```\n## Quality Standards\n\n### Database Query Performance\n- All list queries MUST use eager loading (.Include())\n- Queries returning > 10 rows MUST include execution plan review\n- N+1 queries are PROHIBITED - PR will be rejected\n\n```\n\n### Edge Case #3: Memory leak in real-time connections\n\n**Scenario**: Application memory grows unbounded. After 2 hours, server crashes.\n\n**Root cause**: SignalR connections not cleaned up:\n\n``` // AI-generated (leaky) public class TaskHub : Hub { private static readonly List<string> _connections = new List<string>();\n\npublic override async Task OnConnectedAsync() { _connections.Add(Context.ConnectionId); // Added but never removed! await base.OnConnectedAsync(); } }\n\n```\n\n**Problem**: `_connections` list grows forever. Disconnected clients never removed.\n\n**Solution**: Use proper cleanup:\n\n``` // Fixed public class TaskHub : Hub { // Use ConcurrentDictionary for thread-safe operations private static readonly ConcurrentDictionary<string, string> _connections = new ConcurrentDictionary<string, string>();\n\npublic override async Task OnConnectedAsync() { _connections.TryAdd(Context.ConnectionId, Context.UserIdentifier); await base.OnConnectedAsync(); }\n\npublic override async Task OnDisconnectedAsync(Exception exception) { _connections.TryRemove(Context.ConnectionId, out _); // Cleanup! await base.OnDisconnectedAsync(exception); } }\n\n```\n\n**Lesson**: Specify cleanup explicitly in spec:\n\n```\n## Real-Time Requirements\n\n### WebSocket Connection Management\n- Connections MUST be tracked in thread-safe collection\n- Disconnections MUST remove connection from tracking\n- Server MUST handle abrupt disconnections (network failures)\n- Connection cleanup MUST complete within 30 seconds\n\n```\n\n### Edge Case #4: XSS vulnerability in task descriptions\n\n**Scenario**: User enters `<script>alert('XSS')</script>` as task description. Alert executes when other users view the task.\n\n**Root cause**: AI didnâ€™t sanitize user input:\n\n```cshtml @* AI-generated (vulnerable) *@ <div class=\"task-description\"> @((MarkupString)Task.Description) @* Renders raw HTML! *@ </div>\n\n```\n\n**Problem**: User-supplied HTML executed in other usersâ€™ browsers.\n\n**Solution**: Sanitize and escape:\n\n```cshtml @* Fixed - Razor automatically escapes *@ <div class=\"task-description\"> @Task.Description @* Escaped by Blazor automatically *@ </div>\n\n```\n\nOr if Markdown support is needed:\n\n``` using Markdig; using Ganss.Xss;\n\npublic string SanitizeMarkdown(string markdown) { // Convert Markdown to HTML var html = Markdown.ToHtml(markdown);\n\n// Sanitize HTML (HtmlSanitizer uses whitelist approach by default) // Safe tags like p, strong, em, ul, ol, li are allowed by default // Dangerous tags like script, iframe are NOT in the default allowlist var sanitizer = new HtmlSanitizer();\n\n// Optionally restrict to only specific tags if needed: // sanitizer.AllowedTags.Clear(); // sanitizer.AllowedTags.Add(\"p\"); // sanitizer.AllowedTags.Add(\"strong\"); // etc.\n\nreturn sanitizer.Sanitize(html); }\n\n```\n\n**Lesson**: Add security requirements to constitution:\n\n```\n## Security Standards\n\n### Input Validation and Sanitization\n- ALL user input MUST be sanitized before display\n- HTML rendering MUST use whitelist approach (never blacklist)\n- Markdown MUST be sanitized before rendering\n- SQL queries MUST use parameterized queries (no string concatenation)\n- OWASP Top 10 MUST be addressed for all features\n\n```\n\n## Section 6: Common gotchas and fixes\n\n### Quick reference guide\n\n#### Gotcha #1: AI forgets the constitution mid-implementation\n\n**Symptom**: Early tasks follow standards, later tasks deviate.\n\n**Fix**:\n\n``` /implement reminder: Review constitution.md before each task Verify compliance before proceeding.\n\n```\n\n#### Gotcha #2: Generated tests donâ€™t run\n\n**Symptom**: Tests pass when AI runs them, fail when you run them.\n\n**Cause**: AI assumes mocked data, you have real database.\n\n**Fix**: Add test data setup to tasks:\n\n```\n- [ ] 5.1 Write unit test for TaskService.createTask\n- **Setup**: Seed test database with team and user\n- **Test**: Call createTask with valid data\n- **Assert**: Task created with correct properties\n- **Teardown**: Clean up test data\n\n```\n\n#### Gotcha #3: Environment variables missing in production\n\n**Symptom**: Works locally, crashes in production with â€œundefined environment variable.â€\n\n**Cause**: AI hard-coded localhost values.\n\n**Fix**: Add environment configuration to constitution:\n\n```\n## Deployment Requirements\n\n### Environment Variables (REQUIRED in production)\n- DATABASE_URL: PostgreSQL connection string\n- JWT_SECRET: Random 32-character string\n- SENDGRID_API_KEY: Email service API key\n- NODE_ENV: \"production\"\n\n**Validation**: Application MUST fail to start if required env vars are missing. Show error: \"Missing required environment variable: DATABASE_URL\"\n\n```\n\n#### Gotcha #4: API versioning not considered\n\n**Symptom**: Breaking API changes force mobile app update.\n\n**Cause**: Spec didnâ€™t mention API versioning.\n\n**Fix**: Add to constitution:\n\n```\n## API Design Principles\n\n### Versioning Strategy\n- All API endpoints MUST include version: /api/v1/tasks\n- Breaking changes require new version: /api/v2/tasks\n- Previous version MUST be supported for 6 months minimum\n- Deprecation warnings in headers: `X-API-Deprecated: v1 will be removed on 2026-07-01`\n\n```\n\n#### Gotcha #5: AI uses deprecated patterns\n\n**Symptom**: Code works but uses old syntax (e.g., `var` instead of `let/const` , callback-style instead of async/await).\n\n**Cause**: AI trained on older codebases.\n\n**Fix**: Add coding style requirements to constitution:\n\n```\n## Code Style Requirements\n\n### Modern Patterns (REQUIRED)\n- Use async/await, NOT .then() callbacks\n- Use const by default, let when reassignment needed, NEVER var\n- Use arrow functions for callbacks\n- Use template literals, NOT string concatenation\n\n```\n\n#### Gotcha #6: AI generates over-engineered solutions\n\n**Symptom**: Simple feature implemented with 5 classes, 3 interfaces, and a factory pattern.\n\n**Cause**: AI saw enterprise patterns in training data.\n\n**Fix**: Add simplicity constraints to constitution:\n\n```\n## Simplicity Constraints\n\n### YAGNI Principle\n- No interfaces with single implementations\n- No factory patterns unless 3+ concrete types exist\n- No dependency injection for classes with no dependencies\n- Maximum 3 levels of class inheritance\n\nBefore adding abstraction, AI MUST justify with: \"This abstraction is needed because...\"\n\n```\n\n## Section 7: Measuring success\n\n### How do you know if spec-driven development is working?\n\nTrack these five key metrics:\n\n| Metric | What to Measure | ğŸŸ¢ Good | ğŸŸ¡ Warning | | --- | --- | --- | --- | | **Velocity** | Tasks completed per day | 5-10/day | &lt; 3/day | | **Regression** | Bugs reintroduced after fix | &lt; 5% | &gt; 20% | | **Stability** | Spec changes per week | &lt; 2 major | &gt; 5 major | | **Coverage** | Test coverage % | &gt; 80% | &lt; 50% | | **Recovery** | Bug report â†’ fix deployed | &lt; 30 min | &gt; 4 hours |\n\n#### Fixing slow velocity\n\n- Break tasks smaller (2-4 hours each)\n- Simplify constitution (too many constraints?)\n- Provide more context in spec\n\n#### Fixing high regression\n\n- Use `/implement fix and memorize`\ninstead of just `/implement fix`\n- Update plan.md with lessons learned\n- Add regression tests to tasks\n\n#### Fixing high spec churn\n\n- Spend more time on spec before implementation\n- Validate spec with stakeholders early\n- Build MVP, then iterate based on feedback\n\n#### Fixing low coverage\n\n- Add test requirements to tasks explicitly\n- Require test task for each feature task\n- Run coverage reports in CI/CD\n\n#### Fixing slow recovery\n\n- Improve bug report templates (see Strategy #4 above)\n- Add health monitoring endpoints\n- Implement feature flags for quick rollback\n\n## Section 8: Tools and automation\n\n### Helpers that make spec-driven development smoother\n\n### Tool #1: Specification validator\n\nCreate a script to validate spec completeness:\n\n``` #!/bin/bash\n# validate-spec.sh\n\necho \"Validating specification...\"\n\n# Check for required sections\nrequired_sections=(\"User Stories\" \"Acceptance Criteria\" \"Data Requirements\" \"Edge Cases\")\n\nfor section in \"${required_sections[@]}\"; do if ! grep -q \"## $section\" .speckit/spec.md; then echo \"[FAIL] Missing required section: $section\" exit 1 fi done\n\n# Check for TODO markers\nif grep -q \"TODO\" .speckit/spec.md; then echo \"[FAIL] Specification contains TODO markers\" exit 1 fi\n\n# Check for vague language\nvague_terms=(\"might\" \"maybe\" \"probably\" \"hopefully\" \"basically\") for term in \"${vague_terms[@]}\"; do if grep -i -q \"$term\" .speckit/spec.md; then echo \"[WARN] Spec contains vague term '$term'\" fi done\n\necho \"[PASS] Specification validation passed\"\n\n```\n\nUsage:\n\n``` ./validate-spec.sh\n\n```\n\n### Tool #2: Constitution compliance checker\n\nVerify code follows constitution standards:\n\n``` #!/bin/bash\n# check-compliance.sh\n\necho \"Checking constitution compliance...\"\n\n# Check nullable reference types enabled\nif ! grep -q \"<Nullable>enable</Nullable>\" *.csproj; then echo \"[FAIL] Nullable reference types not enabled (constitution requirement)\" exit 1 fi\n\n# Check for raw SQL (prohibited)\nif grep -r \"ExecuteSqlRaw\\|FromSqlRaw\" ./Services ./Controllers; then echo \"[FAIL] Raw SQL detected (constitution prohibits)\" exit 1 fi\n\n# Check for console.log in production code (debugging leftovers)\nif grep -r \"Console.WriteLine\" ./Services ./Controllers | grep -v \"// DEBUG\"; then echo \"[WARN] Console.WriteLine found in production code\" fi\n\necho \"[PASS] Constitution compliance check passed\"\n\n```\n\n### Tool #3: Performance testing automation\n\nAdd to CI/CD pipeline:\n\n```\n# .github/workflows/performance.yml\nname: Performance Tests\n\non: [push, pull_request]\n\njobs: performance: runs-on: ubuntu-latest steps:\n- uses: actions/checkout@v3\n\n- name: Build application\nrun: dotnet build\n\n- name: Start application\nrun: dotnet run & env: ASPNETCORE_ENVIRONMENT: Development\n\n- name: Wait for startup\nrun: sleep 10\n\n- name: Run performance tests\nrun: |\n# Test API response times\nresponse_time=$(curl -w \"%{time_total}\" -s -o /dev/null http://localhost:5000/api/health)\n\nif (( $(echo \"$response_time > 0.5\" | bc -l) )); then echo \"[FAIL] Health check took ${response_time}s (requirement: < 500ms)\" exit 1 fi\n\necho \"[PASS] Health check took ${response_time}s\"\n\n```\n\n### Tool #4: Spec change impact analyzer\n\nWhen you change the spec, see whatâ€™s affected:\n\n``` #!/usr/bin/env python3\n# analyze-impact.py\n\nimport re import sys\n\ndef analyze_spec_changes(old_spec, new_spec): \"\"\"Compare two spec versions and identify affected components\"\"\"\n\nchanges = { 'added_sections': [], 'modified_sections': [], 'removed_sections': [], 'affected_tasks': [], 'affected_files': [] }\n\n# Simple diff logic (extend as needed)\nold_sections = re.findall(r'^## (.+)$', old_spec, re.MULTILINE) new_sections = re.findall(r'^## (.+)$', new_spec, re.MULTILINE)\n\nchanges['added_sections'] = [s for s in new_sections if s not in old_sections] changes['removed_sections'] = [s for s in old_sections if s not in new_sections]\n\nprint(\"\\n=== Spec Change Impact Analysis ===\\n\")\n\nif changes['added_sections']: print(\"[+] Added sections:\") for section in changes['added_sections']: print(f\" - {section}\")\n\nif changes['removed_sections']: print(\"\\n[-] Removed sections:\") for section in changes['removed_sections']: print(f\" - {section}\")\n\nif not changes['added_sections'] and not changes['removed_sections']: print(\"[OK] No structural changes detected\") else: print(\"\\n[!] Actions required:\") print(\" 1. Run /speckit.plan to update technical plan\") print(\" 2. Run /speckit.tasks to regenerate task breakdown\") print(\" 3. Review affected files before re-implementation\")\n\nif __name__ == '__main__': if len(sys.argv) < 3: print(\"Usage: ./analyze-impact.py old-spec.md new-spec.md\") sys.exit(1)\n\nwith open(sys.argv[1]) as f: old_spec = f.read()\n\nwith open(sys.argv[2]) as f: new_spec = f.read()\n\nanalyze_spec_changes(old_spec, new_spec)\n\n```\n\nUsage:\n\n``` cp .speckit/spec.md .speckit/spec-old.md\n# Make changes to spec.md\n./analyze-impact.py .speckit/spec-old.md .speckit/spec.md\n\n```\n\n## Section 9: When to abandon spec-driven development\n\n### Itâ€™s not always the right tool\n\nAs Addy Osmani notes in [Beyond Vibe Coding](https://beyond.addy.ie/), the key is knowing when structure helps and when it hinders. Spec-driven development has overhead. Know when to use simpler approaches.\n\n| âŒ Donâ€™t use spec-driven for | âœ… Do use spec-driven for | | --- | --- | | **One-off scripts**: vibe code is fine | **Production applications**: users depend on it | | **Proof-of-concept demos**: iterate fast | **Team projects**: shared understanding needed | | **Extremely simple apps**: 3 features max | **Regulated industries**: audit trails required | | **Rapidly changing requirements**: daily pivots | **Long-lived systems**: years of maintenance | | **Learning new tech**: experimentation first | **Complex domains**: intricate business logic |\n\n## Recap: Best practices checklist\n\nUse this before starting any spec-driven project:\n\n| Phase | Checklist Item | | | --- | --- | --- | | **ğŸ“œ Constitution** | Specific technology constraints listed | â˜ | | | Performance targets are measurable numbers | â˜ | | | Security requirements reference OWASP Top 10 | â˜ | | | Explicit non-goals prevent feature creep | â˜ | | | Amendment process defined | â˜ | | **ğŸ“ Specification** | User stories use Given-When-Then format | â˜ | | | Error messages specified exactly | â˜ | | | UI mockups provided (ASCII art is fine) | â˜ | | | Edge cases documented with expected behavior | â˜ | | | Success metrics are measurable | â˜ | | **ğŸ—ºï¸ Planning** | Technology choices align with constitution | â˜ | | | Data model includes indexes for performance | â˜ | | | API design is RESTful and consistent | â˜ | | | Security patterns specified | â˜ | | | Open questions documented | â˜ | | **ğŸ“‹ Tasks** | Tasks are 2-4 hours each | â˜ | | | Dependencies are explicit | â˜ | | | Acceptance criteria per task | â˜ | | | Parallel opportunities identified | â˜ | | | Test tasks included for each feature | â˜ | | **ğŸš€ Implementation** | Test after each phase | â˜ | | | Rich context for bug reports | â˜ | | | Use `/implement fix and memorize`<br> for recurring issues | â˜ | | | Checkpoint progress with git tags | â˜ | | | Run automated compliance checks | â˜ |\n\n## Whatâ€™s next\n\nIn **Part 4** (next week), weâ€™ll explore:\n\n- **Team workflows**: How multiple developers use Spec-Kit together\n- **Review processes**: Code review checklist for spec-driven projects\n- **CI/CD integration**: Automating validation and deployment\n- **Advanced patterns**: Microservices, event-driven architectures\n- **Scaling strategies**: When your application outgrows the initial architecture\n\n## Key takeaways from Part 3\n\n| # | Takeaway | Remember | | --- | --- | --- | | 1 | **Specifications need precision** | Given-When-Then, exact error messages, UI mocks | | 2 | **Debug systematically** | Validation loops, hallucination detection, context refresh | | 3 | **Iterate carefully** | Add = safe, Modify = risky, Remove = dangerous | | 4 | **Performance matters** | Watch for N+1 queries, memory leaks, concurrency | | 5 | **Measure success** | Track velocity, regression rate, time to recovery |\n\n**Next week in Part 4**, weâ€™ll tackle team collaboration and how Spec-Kit scales beyond solo developers.\n\n## Resources\n\n| Resource | Description | | --- | --- | | [**Spec-Kit GitHub**](https://github.com/github/spec-kit) | Official toolkit repository | | [**Beyond Vibe Coding**](https://beyond.addy.ie/) | Addy Osmaniâ€™s guide to AI-assisted development | | [**OWASP Top 10**](https://owasp.org/www-project-top-ten/) | Security requirements reference | | [**Exploring Gen AI**](https://martinfowler.com/articles/exploring-gen-ai.html) | Martin Fowlerâ€™s AI development series | | [**Given-When-Then**](https://martinfowler.com/bliki/GivenWhenThen.html) | BDD specification pattern |\n\n## Series navigation\n\n- **Previous**: [Part 2 - The Spec-Kit workflow](/from-vibe-coding-to-spec-driven-development-part2)\n- **ğŸ“ You are here: Part 3 - Best practices and troubleshooting**\n- **Next**: Part 4 - Team collaboration and advanced patterns (Coming January 26, 2026)\n- Part 5 - Case studies and lessons learned (Coming February 2, 2026)\n\n> >\n> ğŸ’¬ **Found a gotcha not covered here?**\n> >\n> Connect with me on [LinkedIn](https://linkedin.com/in/hiddedesmet) to share your experience.\n> >\n> **Want to get notified when Part 4 drops?** Follow me for updates!\n> >\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by\n\n### [Hidde de Smet](/hidde)\n\nAs a certified Azure Solution Architect, I specialize in designing, implementing, and managing cloud-based solutions using Scrum and DevOps methodologies.\n\n### Start the conversation\n\n## Related\n\n[See all ai-assisted-development](/tags#ai-assisted-development)\n\n[!\\[From Vibe Coding to Spec-Driven Development: Part 2 - The Spec-Kit Workflow\\]()](/from-vibe-coding-to-spec-driven-development-part2)\n\n[ai-assisted-development](/tags#ai-assisted-development)\n\n[spec-kit](/tags#spec-kit)\n\n[github](/tags#github)\n\n[copilot](/tags#copilot)\n\n[vibe-coding](/tags#vibe-coding)\n\n[specification-driven-development](/tags#specification-driven-development)\n\n[series](/tags#series)\n\nâ€¢Jan 12, 2026\n\n## [From Vibe Coding to Spec-Driven Development: Part 2 - The Spec-Kit Workflow](/from-vibe-coding-to-spec-driven-development-part2)\n\nPart 2 of our series on mastering AI-assisted development. A hands-on walkthrough of the complete Spec-Kit workflow: creating constitutions, writing specs, generating plans, and implementing production-ready code.\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by [Hidde de Smet](/hidde)\n\n[!\\[From Vibe Coding to Spec-Driven Development: Part 1 - The problem and the solution\\]()](/from-vibe-coding-to-spec-driven-development)\n\n[ai-assisted-development](/tags#ai-assisted-development)\n\n[spec-kit](/tags#spec-kit)\n\n[github](/tags#github)\n\n[claude](/tags#claude)\n\n[copilot](/tags#copilot)\n\n[vibe-coding](/tags#vibe-coding)\n\n[specification-driven-development](/tags#specification-driven-development)\n\n[series](/tags#series)\n\nâ€¢Jan 05, 2026\n\n## [From Vibe Coding to Spec-Driven Development: Part 1 - The problem and the solution](/from-vibe-coding-to-spec-driven-development)\n\nPart 1 of our series on mastering AI-assisted development. Discover why 'vibe coding' gets you only 70% there, and why spec-driven development is the answer. This is your roadmap to production-ready AI-generated code.\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by [Hidde de Smet](/hidde)",
  "OutputDir": "_blogs",
  "FeedUrl": "https://hiddedesmet.com/feed.xml",
  "Link": "https://hiddedesmet.com/from-vibe-coding-to-spec-driven-development-part3",
  "FeedName": "Hidde de Smet's Blog",
  "FeedLevelAuthor": "Hidde de Smet",
  "PubDate": "2026-01-19T00:00:00+00:00",
  "Author": "Hidde de Smet"
}
