{
  "ProcessedDate": "2026-02-25 11:15:05",
  "FeedUrl": "https://hiddedesmet.com/feed.xml",
  "EnhancedContent": "Search for Blog\n\n[prompt-engineering](/tags#prompt-engineering)\n\n[llm](/tags#llm)\n\n[ai](/tags#ai)\n\n[claude](/tags#claude)\n\n[chatgpt](/tags#chatgpt)\n\n[github-copilot](/tags#github-copilot)\n\n[context-engineering](/tags#context-engineering)\n\n[agents](/tags#agents)\n\nâ€¢ Feb 25, 2026\n\nâ€¢\n\n28 min read\n\n# Prompt Engineering That Actually Works\n\nFrom vague prompts to reliable results: practical frameworks and mental models for working with LLMs, covering the 5 Principles, few-shot learning, Chain of Thought, RAG, context engineering, and how agents change everything.\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by [Hidde de Smet](/hidde)\n\n- https://twitter.com/intent/tweet?text=Prompt%20Engineering%20That%20Actually%20Works&url=https://hiddedesmet.com/prompt-engineering-that-actually-works\n- https://www.facebook.com/sharer/sharer.php?u=https://hiddedesmet.com/prompt-engineering-that-actually-works\n- http://pinterest.com/pin/create/button/?url=https://hiddedesmet.com/prompt-engineering-that-actually-works&amp;media=https://hiddedesmet.com/images/promptengineering.png&amp;description=Prompt%20Engineering%20That%20Actually%20Works\n- https://www.linkedin.com/shareArticle?mini=true&url=https://hiddedesmet.com/prompt-engineering-that-actually-works&title=Prompt%20Engineering%20That%20Actually%20Works&summary=From%20vague%20prompts%20to%20reliable%20results:%20practical%20frameworks%20and%20mental%20models%20for%20working%20with%20LLMs,%20covering%20the%205%20Principles,%20few-shot%20learning,%20Chain%20of%20Thought,%20RAG,%20context%20engineering,%20and%20how%20agents%20change%20everything.&source=myblog\n\n![Prompt Engineering That Actually Works]()\n\n## Table of Contents\n\n1. The problem everyone recognizes\n2. â€œPrompt engineering is deadâ€\n3. The 80/20 of prompt engineering\n1. 1. Be specific\n2. 2. Show, donâ€™t tell (few-shot)\n3. 3. Iterate, donâ€™t restart\n4. How LLMs actually work\n1. LLMs are document completers\n2. How LLMs see the world differently\n3. The context window\n4. Bigger isnâ€™t always better\n5. Practical rules\n6. The Loop Framework\n5. Core techniques\n1. The 5 Principles\n2. The Playwriting metaphor\n3. Anatomy of a great prompt\n4. How prompt elements interact\n5. Choose your document type\n6. Few-shot: the technique that always works\n6. Advanced patterns\n1. Chain of Thought (CoT) reasoning\n2. Prewarming\n3. ReAct Pattern\n4. Prompt Chaining\n5. RAG: Retrieval-Augmented Generation\n6. Context Engineering\n7. Anatomy of a system prompt\n8. From chat to agents\n9. Instructing agents\n10. Meta Prompting\n11. Self-Critique Prompting\n12. Text Style Unbundling\n7. Model settings and behavior\n1. Temperature\n2. The alignment tax\n8. Practice and evaluation\n1. Common pitfalls\n2. Real rewrites\n3. Iteration in practice\n4. The SOMA framework for evaluation\n5. Which technique when?\n6. Prompt engineering at work\n9. Your prompt toolkit\n1. Templates\n2. Rules\n10. Key takeaways\n11. Resources\n\nThis week I gave a talk at the Xebia Azure AI Winter Jam: *Prompt Engineering That Actually Works*. The session drew from two Oâ€™Reilly books, [*Prompt Engineering for LLMs*](https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/) by Berryman & Ziegler and [*Prompt Engineering for Generative AI*](https://www.oreilly.com/library/view/prompt-engineering-for/9781098153427/) by Phoenix & Taylor, along with the latest 2025-2026 research.\n\nThis is an expanded writeup of that talk. The goal: get more predictable results out of LLMs, for quick daily tasks and for building AI features into products.\n\n## The problem everyone recognizes\n\nYou type a prompt. The result isâ€¦ fine. Kind of useful. But not exactly what you needed.\n\nSo you try again with slightly different wording. Still not quite right. After the fourth attempt, you either settle for â€œgood enoughâ€ or give up.\n\nSound familiar?\n\nHereâ€™s the same scenario with and without intentional prompting:\n\n| The vague prompt | The engineered prompt | | --- | --- | | â€œTell me about AIâ€ | â€œYou are a tech analyst writing for a CTO audience. Compare the top 3 AI approaches for enterprise use in 2026, in a table with pros and cons.â€ | | 500 words of generic overview | Focused comparison table with CTO-appropriate depth | | Probably hallucinated dates | Actionable insights in the format you asked for |\n\nSame model. Same topic. Completely different result.\n\n## â€œPrompt engineering is deadâ€\n\nYouâ€™ve seen the headlines. Letâ€™s be honest about what actually changed.\n\nModels got smarter. You donâ€™t need magic spells anymore. In 2024, phrasing mattered enormously. In 2026, Claude and GPT understand vague intent much better. The â€œprompt wizardâ€ era is over.\n\n| What died | What evolved | | --- | --- | | Memorizing magic phrases | Clear communication with AI | | â€œAct as aâ€¦â€ formulas | System prompts for agents | | Prompt libraries of 500 templates | CLAUDE.md & copilot-instructions.md | | Prompt engineer as a job title | Context architecture at scale |\n\n**What matters in 2026:**\n\n- Instructing agents, not just chatbots\n- Designing workflows, not sentences\n- Evaluation and reliability\n- The skill inside every role\n\nThis talk teaches the skill that survived: communicating effectively with AI systems, from chat to agents.\n\n## The 80/20 of prompt engineering\n\nDozens of techniques exist. Three of them deliver 80% of the value. Start here.\n\n### 1. Be specific\n\nSay who itâ€™s for, what format you want, how long, and what good looks like. Vagueness is the number one cause of bad output.\n\n``` âŒ \"Summarize this\" âœ“ \"Summarize this for a CTO in 3 bullets, max 50 words each\"\n\n```\n\n### 2. Show, donâ€™t tell (few-shot)\n\nGive 2-3 examples of what you want. Models learn patterns faster from examples than from instructions.\n\n``` âŒ \"Write a professional reply\" âœ“ [paste example reply] â†’ \"Write one like this for [new situation]\"\n\n```\n\n### 3. Iterate, donâ€™t restart\n\nYour first prompt is a rough draft. Read the output, tell the model what to fix. Three rounds of iteration beats one â€œperfectâ€ prompt.\n\n``` \"Good, but make it shorter and remove the jargon. Keep the data points.\"\n\n```\n\nEverything else in this post is for the remaining 20%. Get these three right first.\n\n## How LLMs actually work\n\nKnowing how the model actually works changes how you write prompts. Two mental models help.\n\n### LLMs are document completers\n\nLLMs donâ€™t â€œunderstandâ€ or â€œthink.â€ They predict the most likely next token based on patterns in their training data. Your prompt is the beginning of a document. The model writes what should come next.\n\n> >\n> â€œThe beginning of your prompt is the start of a document. The model will try to write the most plausible continuation of that document.â€\n> â€” Berryman & Ziegler, *Prompt Engineering for LLMs*\n> >\n\nThis is why framing matters so much. If you start with â€œHereâ€™s why AI is overhypedâ€¦â€, youâ€™ll get a very different continuation than â€œHereâ€™s what enterprises are getting right with AIâ€¦â€.\n\n### How LLMs see the world differently\n\nThree differences that affect how you write prompts:\n\n| Difference | Implication | | --- | --- | | **One token at a time** (sequential, canâ€™t skip ahead) | Order matters. Put critical constraints early | | **Tokens, not words** (â€˜ChatGPTâ€™ = [â€˜Chatâ€™, â€˜Gâ€™, â€˜PTâ€™]) | Spelling, counting, and character-level tasks trip them up | | **Pattern matching, not reasoning** | Few-shot examples speak their native language |\n\n### The context window\n\nThe modelâ€™s working memory: everything it can see at once. 1 token â‰ˆ Â¾ of a word.\n\nContext windows have grown dramatically. As of early 2026:\n\n| Model | Context window | Max output | | --- | --- | --- | | **OpenAI** | | | | GPT-4o / GPT-4o mini | 128K | 16K | | o3 / o4-mini | 200K | 100K | | GPT-4.1 / 4.1 mini / 4.1 nano | 1M | 32K | | **Anthropic** | | | | Claude 3 Opus | 200K | 4K | | Claude 3.5 Sonnet | 200K | 8K | | Claude 3.5 Haiku | 200K | 8K | | Claude Haiku 4.5 | 200K | 64K | | Claude Sonnet 4 / 4.5 / 4.6 | 200K (1M beta, API only) | 64K | | Claude Opus 4.6 | 200K (1M beta, API only) | 128K | | **Google** | | | | Gemini 2.0 Flash / 2.5 Flash | 1M | â€” | | Gemini 2.5 Pro | 1M (2M in testing) | â€” | | **Meta** | | | | Llama 3.1 / 3.2 / 3.3 | 128K | â€” | | Llama 4 Maverick | 1M | â€” | | Llama 4 Scout | 10M | â€” | | **Mistral** | | | | Mistral Small 3.1 / Medium 3 | 128K | â€” | | Mistral Large 3 / Codestral | 256K | â€” | | **Other** | | | | DeepSeek V3 | 128K | â€” |\n\n> >\n> Note: Claude 3.5 Haiku has been retired by Anthropic. API calls will error. Use Haiku 4.5 instead. The 1M beta for Claude 4-series requires a `context-1m-2025-08-07`\n> header and is restricted to API tier 4.\n> >\n\n**1M tokens is becoming the new baseline** for frontier models. Llama 4 Scout holds the current record at 10M (roughly 7.5 million words).\n\n### Bigger isnâ€™t always better\n\nTwo research findings are worth knowing before you start stuffing context.\n\n**Lost in the middle** ([Liu et al., 2024](https://aclanthology.org/2024.tacl-1.9/)): performance degrades significantly when relevant information is placed in the middle of a long context. Models recall information at the start (primacy) and end (recency) much more reliably. In tested models, accuracy dropped over 30% when relevant content shifted from the edges to the middle.\n\n**Context rot** ([Chroma Research, 2025](https://research.trychroma.com/context-rot)): performance becomes increasingly unreliable as input length grows, even within the advertised window. The common benchmark (â€œneedle in a haystackâ€) (find a specific phrase buried in text) â€” is a weak proxy for real tasks. Summarization, reasoning, and agentic tasks degrade faster. At ~133K tokens, response latency can jump 50x.\n\n### Practical rules\n\n- **Put critical information at the edges** of your prompt, not in the middle\n- **Long context â‰  better RAG**: use RAG when the knowledge base is large or dynamic; use full-context stuffing when you need holistic reasoning over a fixed document\n- **Compress before you stuff**: summarize and trim irrelevant context rather than dumping everything in\n- **Be skeptical of window sizes for complex tasks**: a model passing needle-in-a-haystack at 1M tokens doesnâ€™t guarantee reliable reasoning across that full context in production\n\n### The Loop Framework\n\nThe best mental model for debugging when things go wrong:\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"ðŸ§‘ User's Problem\\nExpressed in human terms\"] --> B[\"ðŸ”„ Convert to Model Domain\\nAdd role, context, examples\"] B --> C[\"ðŸ¤– LLM Completes\\nGenerates continuation\"] C --> D[\"âœ… Transform Back\\nParse, validate, extract\"] D --> A\n\nstyle A fill:#fff3e0,stroke:#ff9800,stroke-width:2px style B fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style C fill:#e8f5e9,stroke:#4caf50,stroke-width:2px style D fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n\n```\n\n**Youâ€™re the translator** between human intent and AI patterns. When output goes wrong, check which step failed: bad conversion? Wrong completion? Poor transformation?\n\n## Core techniques\n\n### The 5 Principles\n\nFive principles that work together:\n\n| # | Principle | What it means | | --- | --- | --- | | 1 | **Give Direction** | Assign a role, set the tone and persona | | 2 | **Specify Format** | Define output structure: bullets, JSON, table, length | | 3 | **Provide Examples** | Few-shot learning: show what you want | | 4 | **Evaluate Quality** | Set criteria: â€œRate confidence 1-5â€ or require sources | | 5 | **Divide Labor** | Break complex tasks into sequential, focused prompts |\n\n### The Playwriting metaphor\n\nBerrymanâ€™s key insight: youâ€™re not asking a question. Youâ€™re writing a script for a brilliant actor.\n\n| Script element | Prompting equivalent | | --- | --- | | **Set the Stage** | Context & background: whatâ€™s the situation? | | **Cast the Role** | Persona & expertise: who should the AI be? | | **Write the Lines** | Examples & patterns: show the style you want | | **Direct the Scene** | Constraints & format: set the boundaries |\n\n### Anatomy of a great prompt\n\nEach layer adds precision:\n\n``` Role: \"You are a senior Python developer\" Context: \"reviewing code for a REST API\" Task: \"Review for: security, bugs, performance\" Format: \"For each issue: describe, rate severity, give fix\" Examples: [Include 1-2 examples of ideal output] CoT: \"Think through each concern systematically\"\n\n```\n\n### How prompt elements interact\n\nYour prompt is a system of interacting parts, not a flat list. Three things to keep in mind:\n\n| Property | What it means | Practical rule | | --- | --- | --- | | **Position** | Elements at the start and end get the most attention | Put critical constraints early â€” donâ€™t bury them in the middle | | **Importance** | Role and format dominate output. Examples outweigh descriptions. Constraints beat suggestions | Invest effort in role and examples before tweaking wording | | **Dependency** | Elements affect each other. Role shapes how examples are interpreted. Format can conflict with task complexity | Design prompts as a system â€” changing one element changes how others are read |\n\n### Choose your document type\n\nMost people default to casual conversation. But different task types have different optimal frames:\n\n| Type | Best for | Example use | | --- | --- | --- | | **Advice/Conversation** | Questions, brainstorming, exploration | Ideation, Q&A | | **Analytic/Report** | Formal analysis, code review, data analysis | Evaluation, comparison | | **Structured/Document** | Fixed format output, JSON, tables, templates | Data extraction, forms |\n\n### Few-shot: the technique that always works\n\nWithout examples, results are inconsistent. With 3 examples, results are predictable:\n\n``` Without examples: \"Convert casual text to formal tone\" â†’ Inconsistent results, different styles each time\n\nWith 3 examples: \"Hey, can u help?\" â†’ \"Could you please assist?\" \"This is broken lol\" â†’ \"This is not functioning.\" \"Thx so much!!!\" â†’ \"Thank you.\" â†’ Consistent, predictable, matches your style\n\n```\n\n**Why it works**: LLMs are pattern matchers. Examples speak their language better than any description. Use 2-3 diverse ones, include edge cases, mark them clearly.\n\n## Advanced patterns\n\n### Chain of Thought (CoT) reasoning\n\nAsking the model to show its work dramatically improves accuracy on complex tasks.\n\n``` Without reasoning: \"Solve this problem\" â†’ Answer: 42 (can't verify, can't debug)\n\nWith reasoning: \"Let's think step by step\" â†’ First... Then... Therefore: 42 (verifiable, debuggable)\n\n```\n\n> >\n> **2026 update**: Modern reasoning models (o3, Claude with extended thinking, Gemini 2.5 Pro) now do this automatically. But the concept still matters for debugging and for smaller/faster models where you want explicit reasoning.\n> >\n\n### Prewarming\n\nPrime the model before asking your real question. Ask it to gather relevant knowledge first, then use that as context.\n\n``` Without prewarming: \"Give me 5 product names for shoes that fit any size.\" â†’ UniFit, FlexShoe, AnySize... (generic)\n\nWith prewarming: Step 1: \"List 10 expert tips for naming consumer products.\" Step 2: \"Now using those tips, give me 5 product names for shoes that fit any size.\" â†’ iMorph, SoleFlex, TrueForm... (expert-informed, creative)\n\n```\n\nAlso called **Internal Retrieval** or **Generate Knowledge Prompting**: the model retrieves from its own training data before answering.\n\n### ReAct Pattern\n\nReasoning + Action for complex tasks. Instead of letting the model jump straight to an answer, force it to show its thinking before each action.\n\n``` Thought: Reason about what to do next Action: Use a tool or execute a step Observation: See the result and analyze it Repeat: Until the task is complete\n\n```\n\nThis is the backbone of most modern agent frameworks. In practice: ask the model to explain what itâ€™s doing before it does it. This makes errors visible and debuggable, rather than silently baked into the final output.\n\n### Prompt Chaining\n\nDonâ€™t ask the model to do everything at once. Break complex tasks into a pipeline of focused prompts:\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"1. Extract\\nPull key data\\nfrom raw input\"] --> B[\"2. Transform\\nRestructure into\\nuseful format\"] B --> C[\"3. Analyze\\nReason over\\nstructured data\"] C --> D[\"4. Generate\\nProduce the\\nfinal output\"]\n\nstyle A fill:#e3f2fd,stroke:#2196f3 style B fill:#e8f5e9,stroke:#4caf50 style C fill:#fff3e0,stroke:#ff9800 style D fill:#f3e5f5,stroke:#9c27b0\n\n```\n\nEach step is independently testable. When something breaks, you know exactly where.\n\n**Example: analyzing a contract**\n\n1. â€œExtract all dates, names, and amounts from this contractâ€\n2. â€œGiven this data, create a JSON summary withâ€¦â€\n3. â€œCompare these values and flag anomalies over 10%â€\n4. â€œWrite a 3-paragraph executive summary from this analysisâ€\n\n### RAG: Retrieval-Augmented Generation\n\nLLMs donâ€™t know your internal data. RAG fetches relevant documents at query time and injects them as context. No retraining needed.\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"ðŸ§‘ User asks\\nNatural language query\"] --> B[\"ðŸ” Retrieve\\nSearch knowledge base\"] B --> C[\"ðŸ“„ Inject\\nAdd docs to prompt\"] C --> D[\"ðŸ’¬ Generate\\nGrounded response\"]\n\nstyle A fill:#fff3e0,stroke:#ff9800,stroke-width:2px style B fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style C fill:#e8f5e9,stroke:#4caf50,stroke-width:2px style D fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n\n```\n\n**Use cases**: documentation bots, code search, customer support, enterprise knowledge bases.\n\n### Context Engineering\n\nThe prompt is only part of what the model reads. Context engineering means designing the *full information architecture*:\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#1a1a2e', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"âš™ï¸ System Instructions\\nStable rules, identity, behavior\"] --> G B[\"ðŸ§  Memory\\nPersisted facts across sessions\"] --> G C[\"ðŸ“„ Retrieved Docs (RAG)\\nDynamic knowledge per query\"] --> G D[\"ðŸ”§ Tool Schemas\\nAvailable actions & contracts\"] --> G E[\"ðŸ’¬ Conversation History\\nRecent messages & context\"] --> G F[\"ðŸŽ¯ Current Task\\nThe actual user request\"] --> G G([\"ðŸ¤– LLM\"])\n\nstyle A fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style B fill:#e8f5e9,stroke:#4caf50,stroke-width:2px style C fill:#fff3e0,stroke:#ff9800,stroke-width:2px style D fill:#fce4ec,stroke:#e91e63,stroke-width:2px style E fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px style F fill:#e0f7fa,stroke:#00bcd4,stroke-width:2px style G fill:#1a237e,color:#fff,stroke:#1a237e,stroke-width:3px\n\n```\n\n> >\n> â€œPrompt engineering is now being rebranded as context engineeringâ€ â€” promptingguide.ai\n> >\n\nEach layer can be tuned independently.\n\n### Anatomy of a system prompt\n\nWhen you build an AI feature, the system prompt is where you do most of the work. It has four distinct components:\n\n| Component | Purpose | Example | | --- | --- | --- | | **Identity** | Who the model is and what it knows | â€œYou are a senior software engineer specializing in Python and API design.â€ | | **Rules** | What the model must or must not do | â€œAlways explain your reasoning. Never output code without a brief explanation.â€ | | **Format** | How output should be structured | â€œRespond in markdown. Use bullet points for lists, code blocks for code.â€ | | **Guardrails** | What to do when things go out of scope | â€œIf asked about topics outside software engineering, politely redirect.â€ |\n\nA minimal but effective system prompt covers all four. You can leave Identity implicit (the model will infer from rules and context), but skipping Rules or Format usually leads to inconsistent output.\n\n``` You are a technical writing assistant helping developers write clear API documentation.\n\nRules:\n- Use plain language. Avoid jargon unless the term is defined first.\n- Always include a request example and response example.\n- Flag any ambiguous parameter descriptions for human review.\n\nFormat:\n- Use markdown with H3 headings for each endpoint.\n- Code blocks use the language tag (json, bash, etc.)\n\nIf asked to document something outside the API, ask for clarification.\n\n```\n\nThe guardrail at the end is short on purpose. Over-specifying edge cases in the system prompt leads to brittle behavior. Define the center, not all the edges.\n\n### From chat to agents\n\nIn 2026, the mode has shifted:\n\n| Chat mode | Agent mode | | --- | --- | | You type, AI responds | You instruct, AI executes | | You write the prompt each time | Instructions run autonomously | | You iterate manually | Agent self-corrects in a loop | | You manage context yourself | Context managed by the system | | One task at a time | Multi-step, multi-file workflows |\n\n> >\n> â€œGPT-3.5 in an agentic workflow outperforms GPT-4 with a single prompt.â€\n> â€” Andrew Ng, Sequoia AI Ascent\n> >\n\nYour â€œpromptâ€ is now a configuration file. With Claude Code, GitHub Copilot, or Cursor, you write instructions once and they execute thousands of times. The surface has changed but the skill hasnâ€™t: be specific, give examples, set constraints.\n\n### Instructing agents\n\nEach tool has its own instruction file. Hereâ€™s where your prompting skills live in 2026:\n\n| File | Tool | Purpose | | --- | --- | --- | | `CLAUDE.md` | Claude Code | Global rules for the agent: conventions, workflow, preferences | | `agent.md` | Claude Code sub-agents | Role and scope for a specialized sub-agent | | `.github/copilot-instructions.md` | GitHub Copilot | Workspace-level coding conventions | | `SKILL.md` | Claude Code skills | Reusable task definitions the agent can invoke | | System prompt | Custom agents / APIs | Core identity, rules, and guardrails for production agents |\n\n```\n# CLAUDE.md\nUse TypeScript strict mode. Run tests before committing. Prefer async/await patterns.\n\n```\n\n```\n# agent.md (security review sub-agent)\nYou are a security reviewer. Focus: authentication, injection vulnerabilities, secrets in code. Flag severity as: critical / warning / info.\n\n```\n\n```\n# skills/deploy/SKILL.md\nUse Bicep for all infrastructure. Target: Azure. Naming convention: rg-{app}-{env}. Always output a plan before applying changes.\n\n```\n\nSkills are particularly useful for repeated workflows: deploy, test, review, document. You define the skill once and the agent can invoke it by name.\n\n**MCP (Model Context Protocol)** extends this further. Itâ€™s a standard for connecting agents to external tools: databases, APIs, file systems, internal services. When you define what tools an agent can use and how, youâ€™re doing context engineering at the infrastructure level.\n\nThe same 80/20 principles apply. But with agents, vague instructions run at scale: across every file, every commit, every customer request.\n\n### Meta Prompting\n\nStop writing prompts from scratch. Use a stronger model to generate and refine prompts for your target model.\n\n1. **Describe your task**: tell the LLM what you want to achieve\n2. **LLM generates prompt**: it produces a structured prompt using best practices\n3. **Test and iterate**: run the prompt, give feedback, let it refine further\n\nAvailable tools: Anthropic Console prompt generator, OpenAI Playground system prompt builder, or simply ask Claude/GPT to write your prompt for you.\n\n### Self-Critique Prompting\n\nFirst drafts from AI are fine. Second drafts are better. Third drafts are great.\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"âœï¸ Generate\\nWrite first draft\"] --> B[\"ðŸ”Ž Critique\\nWhat's weak?\\nWhat would make\\nsomeone delete this?\"] B --> C[\"â™»ï¸ Revise\\nFix every issue\\nidentified\"] C -->|\"Still not great?\"| B\n\nstyle A fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style B fill:#fff3e0,stroke:#ff9800,stroke-width:2px style C fill:#e8f5e9,stroke:#4caf50,stroke-width:2px\n\n```\n\n**Why it works**: LLMs are better critics than creators. They spot issues they wouldnâ€™t have caught on first pass. Ask â€œWhatâ€™s missing?â€ to surface gaps the model skipped over.\n\n**Power move**: Give Claudeâ€™s output to GPT for critique, then back. Different models catch different things.\n\n### Text Style Unbundling\n\nDonâ€™t describe the style you want. Extract it from examples.\n\n1. **Provide sample**: paste your best blog post, email, or brand copy\n2. **Extract features**: â€œAnalyze this text. Identify: tone, sentence structure, vocabulary level, rhetorical devices, and formatting.â€\n3. **Generate in style**: â€œWrite a new [text] using EXACTLY these style features: [paste extracted features]â€\n\n**Use cases**: brand consistency, ghostwriting, matching someoneâ€™s writing style from samples.\n\nEssentially advanced few-shot: instead of pasting examples, you extract the underlying rules and apply them explicitly.\n\n## Model settings and behavior\n\n### Temperature\n\n``` temp = 0.1 â†’ Deterministic â†’ Facts, code, data extraction temp = 0.5 â†’ Professional â†’ Emails, reports, analysis temp = 0.9 â†’ Creative â†’ Brainstorming, fiction, copy\n\n```\n\nWhen in doubt, start at 0.3-0.5 and adjust from there.\n\n### The alignment tax\n\nRLHF made models helpful but also verbose, hedging, and overly cautious. Modern models handle this better, but it still shows up:\n\n``` âŒ \"What's the capital of France?\" â†’ \"Great question! The capital of France is Paris. I hope that helps!\"\n\nâœ“ Add: \"Be concise. No disclaimers. Direct answers only.\" â†’ \"Paris.\"\n\n```\n\nWhen output feels â€œoffâ€ or unnecessarily padded, add explicit constraints about tone and length.\n\n## Practice and evaluation\n\n### Common pitfalls\n\n| Pitfall | Fix | | --- | --- | | **Too vague** | Add specifics: who, what, how many, format, for whom | | **Negative instructions** | Say what TO do, not what NOT to do. â€œBe conciseâ€ beats â€œDonâ€™t be longâ€ | | **Monolithic prompts** | Break into steps. Complex tasks need sequential, focused prompts | | **Ignoring hallucinations** | Ask for sources. Add â€œIf unsure, say soâ€ | | **Not iterating** | First prompt rarely perfect. Refine based on output |\n\n### Real rewrites\n\n**Too vague â†’ Specific:**\n\n``` âŒ \"Summarize these meeting notes\" âœ“ \"Extract: (1) key decisions, (2) action items with owners, (3) open questions. Max 150 words.\"\n\n```\n\n**Negative â†’ Positive:**\n\n``` âŒ \"Don't be too technical and don't use jargon\" âœ“ \"Explain at high school level. Everyday analogies. Under 100 words.\"\n\n```\n\n**Monolith â†’ Chained:**\n\n``` âŒ \"Analyze Q3 sales, find trends, compare Q2, write report\" âœ“ \"Step 1: Top 5 by revenue. Step 2: Compare to Q2. Step 3: Biggest trend. Step 4: 3-sentence summary.\"\n\n```\n\n### Iteration in practice\n\nNobody writes the perfect prompt on the first try. Hereâ€™s what three rounds looks like:\n\n**Round 1:**\n\n``` \"Write a product description for our new headphones\" â†’ Generic, bland. Missing audience, tone, differentiators. Fix: add who it's for and what makes it premium\n\n```\n\n**Round 2:**\n\n``` \"You are a copywriter for premium audio. 50-word description. Target: remote workers. Tone: premium but approachable.\" â†’ Better tone. Still missing the actual product specs. Fix: add the features that matter\n\n```\n\n**Round 3:**\n\n``` \"...Same. Must mention: 40hr battery, spatial audio, ANC. End with a call to action.\" â†’ Specific, compelling, on-brand. âœ“\n\n```\n\nEach round takes 30 seconds. Three rounds of iteration consistently beats one attempt at the perfect prompt.\n\n### The SOMA framework for evaluation\n\nWhen building AI features, test systematically:\n\n| Letter | Principle | What it means | | --- | --- | --- | | **S** | Specificity | Test specific scenarios with concrete inputs and expected outputs | | **O** | Objective Measurement | Define success criteria. Automate scoring. Track accuracy, consistency, format compliance | | **M** | Multiple Scenarios | Test happy path AND edge cases, different input variations | | **A** | Automation | Integrate testing into CI/CD. Treat prompts like production code |\n\n### Which technique when?\n\n| Situation | Technique | | --- | --- | | Output is vague or off-target | Be specific + format constraints | | Need consistent results | Few-shot examples | | First draft not good enough | Self-Critique: â€œNow critique thisâ€ | | Donâ€™t know how to prompt it | Meta Prompting: ask the AI to help | | Complex multi-step task | Prompt Chaining | | Need domain expertise primed | Prewarming first, then ask | | Need to match a voice or style | Text Style Unbundling | | High-stakes, canâ€™t be wrong | CoT + Self-Consistency | | Building an AI feature | Context Engineering + System Prompts | | Need to measure quality at scale | SOMA framework + automated eval |\n\nStart at the top. Move down only when the simpler approach isnâ€™t enough.\n\n### Prompt engineering at work\n\nThese arenâ€™t tricks. Theyâ€™re just clear communication applied to common tasks:\n\n**Code review:**\n\n``` You are a senior Python developer reviewing code for a REST API. Review for: bugs, performance, readability. For each issue: describe it, rate severity (high/med/low), suggest a fix.\n\n```\n\n**Data analysis:**\n\n``` Analyze this CSV of monthly sales data. Identify the top 3 trends, any anomalies, and explain what's driving them. Format: numbered list.\n\n```\n\n**Email drafting:**\n\n``` Write a follow-up email to a client whose project is 2 weeks late due to API issues. Tone: firm but professional. Goal: get a new commitment date from them. Max 100 words.\n\n```\n\n**Content creation:**\n\n``` You are a B2B content strategist. Write a 300-word blog intro about cloud migration, targeted at CTOs. Tone: authoritative, not salesy.\n\n```\n\n## Your prompt toolkit\n\n### Templates\n\n**The Expert:**\n\n``` You are a [role] with [X] years of experience. [Task] for [audience]. Format: [format]. Max [N] words.\n\n```\n\n**The Extractor:**\n\n``` From [source], extract: [field 1], [field 2], [field 3]. Return as [JSON/table]. If unsure: unknown.\n\n```\n\n**The Analyst:**\n\n``` Analyze [data]. Top [N] trends. Compare to [benchmark]. Present as [format].\n\n```\n\n**The Creator:**\n\n``` Write [type] about [topic]. Audience: [who]. Tone: [tone]. Include: [elements]. [N] words.\n\n```\n\n### Rules\n\n1. Start with a clear verb: Summarize, Compare, Create, Extract\n2. Assign a role and expertise level\n3. Set constraints: length, format, audience, tone\n4. Show 2-3 examples (few-shot)\n5. Use delimiters to separate instructions from data\n6. Break complex tasks into steps or chains\n7. Iterate: first prompt is a rough draft\n8. Say what TO do, not what NOT to do\n\n## Key takeaways\n\n| # | Takeaway | | --- | --- | | 1 | **LLMs are document completers**: they predict continuations. Design prompts as document beginnings | | 2 | **The Loop is your mental model**: youâ€™re the translator between human intent and AI patterns | | 3 | **80/20 applies**: specificity, examples, and iteration deliver most of the value | | 4 | **Context engineering &gt; prompt engineering**: design all six layers, not just the message | | 5 | **Agents change the surface, not the skill**: the same principles apply, with higher stakes | | 6 | **Evaluate systematically**: SOMA framework â€” Specificity, Objective measurement, Multiple scenarios, Automation |\n\nThe skill that survived the â€œprompt engineering is deadâ€ wave isnâ€™t about magic phrases. Itâ€™s about communicating clearly with AI systems, in chat or in production.\n\nStart today: pick one technique from this post, apply it to a real task, and iterate.\n\n## Resources\n\n| Resource | Description | | --- | --- | | [**Prompt Engineering for LLMs**](https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/) | Oâ€™Reilly book by Berryman & Ziegler | | [**Prompt Engineering for Generative AI**](https://www.oreilly.com/library/view/prompt-engineering-for/9781098153427/) | Oâ€™Reilly book by Phoenix & Taylor | | [**Anthropic Prompt Library**](https://docs.anthropic.com/en/prompt-library/library) | Ready-to-use prompt templates from Anthropic | | [**Prompt Engineering Guide**](https://www.promptingguide.ai/) | Comprehensive community resource | | [**OpenAI Prompt Engineering**](https://platform.openai.com/docs/guides/prompt-engineering) | Official OpenAI guidance |\n\n> >\n> ðŸ’¬ **Whatâ€™s your go-to prompt technique?**\n> >\n> Iâ€™d love to hear what works for your workflow. Connect with me on [LinkedIn](https://linkedin.com/in/hiddedesmet).\n> >\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by\n\n### [Hidde de Smet](/hidde)\n\nAs a certified Azure Solution Architect, I specialize in designing, implementing, and managing cloud-based solutions using Scrum and DevOps methodologies.\n\n### Start the conversation\n\n## Related\n\n[See all prompt-engineering](/tags#prompt-engineering)\n\n[!\\[From Vibe Coding to Spec-Driven Development: Part 1 - The problem and the solution\\]()](/from-vibe-coding-to-spec-driven-development)\n\n[ai-assisted-development](/tags#ai-assisted-development)\n\n[spec-kit](/tags#spec-kit)\n\n[github](/tags#github)\n\n[claude](/tags#claude)\n\n[copilot](/tags#copilot)\n\n[vibe-coding](/tags#vibe-coding)\n\n[specification-driven-development](/tags#specification-driven-development)\n\n[series](/tags#series)\n\nâ€¢Jan 05, 2026\n\n## [From Vibe Coding to Spec-Driven Development: Part 1 - The problem and the solution](/from-vibe-coding-to-spec-driven-development)\n\nPart 1 of our series on mastering AI-assisted development. Discover why 'vibe coding' gets you only 70% there, and why spec-driven development is the answer. This is your roadmap to production-ready AI-generated code.\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by [Hidde de Smet](/hidde)\n\n[!\\[Building a Center of Excellence for AI: A strategic approach to enterprise AI adoption\\]()](/creating-ccoe-for-ai)\n\n[ai](/tags#ai)\n\n[governance](/tags#governance)\n\n[ccoe](/tags#ccoe)\n\nâ€¢Jul 14, 2025\n\n## [Building a Center of Excellence for AI: A strategic approach to enterprise AI adoption](/creating-ccoe-for-ai)\n\nA comprehensive guide to establishing and operating a successful Center of Excellence (CCoE) for Artificial Intelligence in enterprise organizations. Learn the key components, governance frameworks, and best practices for scaling AI initiatives across your organization.\n\n[!\\[Hidde de Smet\\]()](/hidde)\n\nWritten by [Hidde de Smet](/hidde)",
  "FeedName": "Hidde de Smet's Blog",
  "Title": "Prompt Engineering That Actually Works",
  "Link": "https://hiddedesmet.com/prompt-engineering-that-actually-works",
  "FeedLevelAuthor": "Hidde de Smet",
  "Author": "Hidde de Smet",
  "Description": "This week I gave a talk at the Xebia Azure AI Winter Jam: *Prompt Engineering That Actually Works*. The session drew from two Oâ€™Reilly books, [*Prompt Engineering for LLMs*](https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/) by Berryman & Ziegler and [*Prompt Engineering for Generative AI*](https://www.oreilly.com/library/view/prompt-engineering-for/9781098153427/) by Phoenix & Taylor, along with the latest 2025-2026 research.\n\nThis is an expanded writeup of that talk. The goal: get more predictable results out of LLMs, for quick daily tasks and for building AI features into products.\n\n## The problem everyone recognizes\n\nYou type a prompt. The result isâ€¦ fine. Kind of useful. But not exactly what you needed.\n\nSo you try again with slightly different wording. Still not quite right. After the fourth attempt, you either settle for â€œgood enoughâ€ or give up.\n\nSound familiar?\n\nHereâ€™s the same scenario with and without intentional prompting:\n\n| The vague prompt | The engineered prompt | | --- | --- | | â€œTell me about AIâ€ | â€œYou are a tech analyst writing for a CTO audience. Compare the top 3 AI approaches for enterprise use in 2026, in a table with pros and cons.â€ | | 500 words of generic overview | Focused comparison table with CTO-appropriate depth | | Probably hallucinated dates | Actionable insights in the format you asked for |\n\nSame model. Same topic. Completely different result.\n\n## â€œPrompt engineering is deadâ€\n\nYouâ€™ve seen the headlines. Letâ€™s be honest about what actually changed.\n\nModels got smarter. You donâ€™t need magic spells anymore. In 2024, phrasing mattered enormously. In 2026, Claude and GPT understand vague intent much better. The â€œprompt wizardâ€ era is over.\n\n| What died | What evolved | | --- | --- | | Memorizing magic phrases | Clear communication with AI | | â€œAct as aâ€¦â€ formulas | System prompts for agents | | Prompt libraries of 500 templates | CLAUDE.md & copilot-instructions.md | | Prompt engineer as a job title | Context architecture at scale |\n\n**What matters in 2026:**\n\n- Instructing agents, not just chatbots\n- Designing workflows, not sentences\n- Evaluation and reliability\n- The skill inside every role\n\nThis talk teaches the skill that survived: communicating effectively with AI systems, from chat to agents.\n\n## The 80/20 of prompt engineering\n\nDozens of techniques exist. Three of them deliver 80% of the value. Start here.\n\n### 1. Be specific\n\nSay who itâ€™s for, what format you want, how long, and what good looks like. Vagueness is the number one cause of bad output.\n\n``` âŒ \"Summarize this\" âœ“ \"Summarize this for a CTO in 3 bullets, max 50 words each\"\n\n```\n\n### 2. Show, donâ€™t tell (few-shot)\n\nGive 2-3 examples of what you want. Models learn patterns faster from examples than from instructions.\n\n``` âŒ \"Write a professional reply\" âœ“ [paste example reply] â†’ \"Write one like this for [new situation]\"\n\n```\n\n### 3. Iterate, donâ€™t restart\n\nYour first prompt is a rough draft. Read the output, tell the model what to fix. Three rounds of iteration beats one â€œperfectâ€ prompt.\n\n``` \"Good, but make it shorter and remove the jargon. Keep the data points.\"\n\n```\n\nEverything else in this post is for the remaining 20%. Get these three right first.\n\n## How LLMs actually work\n\nKnowing how the model actually works changes how you write prompts. Two mental models help.\n\n### LLMs are document completers\n\nLLMs donâ€™t â€œunderstandâ€ or â€œthink.â€ They predict the most likely next token based on patterns in their training data. Your prompt is the beginning of a document. The model writes what should come next.\n\n> >\n> â€œThe beginning of your prompt is the start of a document. The model will try to write the most plausible continuation of that document.â€\n> â€” Berryman & Ziegler, *Prompt Engineering for LLMs*\n> >\n\nThis is why framing matters so much. If you start with â€œHereâ€™s why AI is overhypedâ€¦â€, youâ€™ll get a very different continuation than â€œHereâ€™s what enterprises are getting right with AIâ€¦â€.\n\n### How LLMs see the world differently\n\nThree differences that affect how you write prompts:\n\n| Difference | Implication | | --- | --- | | **One token at a time** (sequential, canâ€™t skip ahead) | Order matters. Put critical constraints early | | **Tokens, not words** (â€˜ChatGPTâ€™ = [â€˜Chatâ€™, â€˜Gâ€™, â€˜PTâ€™]) | Spelling, counting, and character-level tasks trip them up | | **Pattern matching, not reasoning** | Few-shot examples speak their native language |\n\n### The context window\n\nThe modelâ€™s working memory: everything it can see at once. 1 token â‰ˆ Â¾ of a word.\n\nContext windows have grown dramatically. As of early 2026:\n\n| Model | Context window | Max output | | --- | --- | --- | | **OpenAI** | | | | GPT-4o / GPT-4o mini | 128K | 16K | | o3 / o4-mini | 200K | 100K | | GPT-4.1 / 4.1 mini / 4.1 nano | 1M | 32K | | **Anthropic** | | | | Claude 3 Opus | 200K | 4K | | Claude 3.5 Sonnet | 200K | 8K | | Claude 3.5 Haiku | 200K | 8K | | Claude Haiku 4.5 | 200K | 64K | | Claude Sonnet 4 / 4.5 / 4.6 | 200K (1M beta, API only) | 64K | | Claude Opus 4.6 | 200K (1M beta, API only) | 128K | | **Google** | | | | Gemini 2.0 Flash / 2.5 Flash | 1M | â€” | | Gemini 2.5 Pro | 1M (2M in testing) | â€” | | **Meta** | | | | Llama 3.1 / 3.2 / 3.3 | 128K | â€” | | Llama 4 Maverick | 1M | â€” | | Llama 4 Scout | 10M | â€” | | **Mistral** | | | | Mistral Small 3.1 / Medium 3 | 128K | â€” | | Mistral Large 3 / Codestral | 256K | â€” | | **Other** | | | | DeepSeek V3 | 128K | â€” |\n\n> >\n> Note: Claude 3.5 Haiku has been retired by Anthropic. API calls will error. Use Haiku 4.5 instead. The 1M beta for Claude 4-series requires a `context-1m-2025-08-07`\n> header and is restricted to API tier 4.\n> >\n\n**1M tokens is becoming the new baseline** for frontier models. Llama 4 Scout holds the current record at 10M (roughly 7.5 million words).\n\n### Bigger isnâ€™t always better\n\nTwo research findings are worth knowing before you start stuffing context.\n\n**Lost in the middle** ([Liu et al., 2024](https://aclanthology.org/2024.tacl-1.9/)): performance degrades significantly when relevant information is placed in the middle of a long context. Models recall information at the start (primacy) and end (recency) much more reliably. In tested models, accuracy dropped over 30% when relevant content shifted from the edges to the middle.\n\n**Context rot** ([Chroma Research, 2025](https://research.trychroma.com/context-rot)): performance becomes increasingly unreliable as input length grows, even within the advertised window. The common benchmark (â€œneedle in a haystackâ€) (find a specific phrase buried in text) â€” is a weak proxy for real tasks. Summarization, reasoning, and agentic tasks degrade faster. At ~133K tokens, response latency can jump 50x.\n\n### Practical rules\n\n- **Put critical information at the edges** of your prompt, not in the middle\n- **Long context â‰  better RAG**: use RAG when the knowledge base is large or dynamic; use full-context stuffing when you need holistic reasoning over a fixed document\n- **Compress before you stuff**: summarize and trim irrelevant context rather than dumping everything in\n- **Be skeptical of window sizes for complex tasks**: a model passing needle-in-a-haystack at 1M tokens doesnâ€™t guarantee reliable reasoning across that full context in production\n\n### The Loop Framework\n\nThe best mental model for debugging when things go wrong:\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"ðŸ§‘ User's Problem\\nExpressed in human terms\"] --> B[\"ðŸ”„ Convert to Model Domain\\nAdd role, context, examples\"] B --> C[\"ðŸ¤– LLM Completes\\nGenerates continuation\"] C --> D[\"âœ… Transform Back\\nParse, validate, extract\"] D --> A\n\nstyle A fill:#fff3e0,stroke:#ff9800,stroke-width:2px style B fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style C fill:#e8f5e9,stroke:#4caf50,stroke-width:2px style D fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n\n```\n\n**Youâ€™re the translator** between human intent and AI patterns. When output goes wrong, check which step failed: bad conversion? Wrong completion? Poor transformation?\n\n## Core techniques\n\n### The 5 Principles\n\nFive principles that work together:\n\n| # | Principle | What it means | | --- | --- | --- | | 1 | **Give Direction** | Assign a role, set the tone and persona | | 2 | **Specify Format** | Define output structure: bullets, JSON, table, length | | 3 | **Provide Examples** | Few-shot learning: show what you want | | 4 | **Evaluate Quality** | Set criteria: â€œRate confidence 1-5â€ or require sources | | 5 | **Divide Labor** | Break complex tasks into sequential, focused prompts |\n\n### The Playwriting metaphor\n\nBerrymanâ€™s key insight: youâ€™re not asking a question. Youâ€™re writing a script for a brilliant actor.\n\n| Script element | Prompting equivalent | | --- | --- | | **Set the Stage** | Context & background: whatâ€™s the situation? | | **Cast the Role** | Persona & expertise: who should the AI be? | | **Write the Lines** | Examples & patterns: show the style you want | | **Direct the Scene** | Constraints & format: set the boundaries |\n\n### Anatomy of a great prompt\n\nEach layer adds precision:\n\n``` Role: \"You are a senior Python developer\" Context: \"reviewing code for a REST API\" Task: \"Review for: security, bugs, performance\" Format: \"For each issue: describe, rate severity, give fix\" Examples: [Include 1-2 examples of ideal output] CoT: \"Think through each concern systematically\"\n\n```\n\n### How prompt elements interact\n\nYour prompt is a system of interacting parts, not a flat list. Three things to keep in mind:\n\n| Property | What it means | Practical rule | | --- | --- | --- | | **Position** | Elements at the start and end get the most attention | Put critical constraints early â€” donâ€™t bury them in the middle | | **Importance** | Role and format dominate output. Examples outweigh descriptions. Constraints beat suggestions | Invest effort in role and examples before tweaking wording | | **Dependency** | Elements affect each other. Role shapes how examples are interpreted. Format can conflict with task complexity | Design prompts as a system â€” changing one element changes how others are read |\n\n### Choose your document type\n\nMost people default to casual conversation. But different task types have different optimal frames:\n\n| Type | Best for | Example use | | --- | --- | --- | | **Advice/Conversation** | Questions, brainstorming, exploration | Ideation, Q&A | | **Analytic/Report** | Formal analysis, code review, data analysis | Evaluation, comparison | | **Structured/Document** | Fixed format output, JSON, tables, templates | Data extraction, forms |\n\n### Few-shot: the technique that always works\n\nWithout examples, results are inconsistent. With 3 examples, results are predictable:\n\n``` Without examples: \"Convert casual text to formal tone\" â†’ Inconsistent results, different styles each time\n\nWith 3 examples: \"Hey, can u help?\" â†’ \"Could you please assist?\" \"This is broken lol\" â†’ \"This is not functioning.\" \"Thx so much!!!\" â†’ \"Thank you.\" â†’ Consistent, predictable, matches your style\n\n```\n\n**Why it works**: LLMs are pattern matchers. Examples speak their language better than any description. Use 2-3 diverse ones, include edge cases, mark them clearly.\n\n## Advanced patterns\n\n### Chain of Thought (CoT) reasoning\n\nAsking the model to show its work dramatically improves accuracy on complex tasks.\n\n``` Without reasoning: \"Solve this problem\" â†’ Answer: 42 (can't verify, can't debug)\n\nWith reasoning: \"Let's think step by step\" â†’ First... Then... Therefore: 42 (verifiable, debuggable)\n\n```\n\n> >\n> **2026 update**: Modern reasoning models (o3, Claude with extended thinking, Gemini 2.5 Pro) now do this automatically. But the concept still matters for debugging and for smaller/faster models where you want explicit reasoning.\n> >\n\n### Prewarming\n\nPrime the model before asking your real question. Ask it to gather relevant knowledge first, then use that as context.\n\n``` Without prewarming: \"Give me 5 product names for shoes that fit any size.\" â†’ UniFit, FlexShoe, AnySize... (generic)\n\nWith prewarming: Step 1: \"List 10 expert tips for naming consumer products.\" Step 2: \"Now using those tips, give me 5 product names for shoes that fit any size.\" â†’ iMorph, SoleFlex, TrueForm... (expert-informed, creative)\n\n```\n\nAlso called **Internal Retrieval** or **Generate Knowledge Prompting**: the model retrieves from its own training data before answering.\n\n### ReAct Pattern\n\nReasoning + Action for complex tasks. Instead of letting the model jump straight to an answer, force it to show its thinking before each action.\n\n``` Thought: Reason about what to do next Action: Use a tool or execute a step Observation: See the result and analyze it Repeat: Until the task is complete\n\n```\n\nThis is the backbone of most modern agent frameworks. In practice: ask the model to explain what itâ€™s doing before it does it. This makes errors visible and debuggable, rather than silently baked into the final output.\n\n### Prompt Chaining\n\nDonâ€™t ask the model to do everything at once. Break complex tasks into a pipeline of focused prompts:\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"1. Extract\\nPull key data\\nfrom raw input\"] --> B[\"2. Transform\\nRestructure into\\nuseful format\"] B --> C[\"3. Analyze\\nReason over\\nstructured data\"] C --> D[\"4. Generate\\nProduce the\\nfinal output\"]\n\nstyle A fill:#e3f2fd,stroke:#2196f3 style B fill:#e8f5e9,stroke:#4caf50 style C fill:#fff3e0,stroke:#ff9800 style D fill:#f3e5f5,stroke:#9c27b0\n\n```\n\nEach step is independently testable. When something breaks, you know exactly where.\n\n**Example: analyzing a contract**\n\n1. â€œExtract all dates, names, and amounts from this contractâ€\n2. â€œGiven this data, create a JSON summary withâ€¦â€\n3. â€œCompare these values and flag anomalies over 10%â€\n4. â€œWrite a 3-paragraph executive summary from this analysisâ€\n\n### RAG: Retrieval-Augmented Generation\n\nLLMs donâ€™t know your internal data. RAG fetches relevant documents at query time and injects them as context. No retraining needed.\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"ðŸ§‘ User asks\\nNatural language query\"] --> B[\"ðŸ” Retrieve\\nSearch knowledge base\"] B --> C[\"ðŸ“„ Inject\\nAdd docs to prompt\"] C --> D[\"ðŸ’¬ Generate\\nGrounded response\"]\n\nstyle A fill:#fff3e0,stroke:#ff9800,stroke-width:2px style B fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style C fill:#e8f5e9,stroke:#4caf50,stroke-width:2px style D fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px\n\n```\n\n**Use cases**: documentation bots, code search, customer support, enterprise knowledge bases.\n\n### Context Engineering\n\nThe prompt is only part of what the model reads. Context engineering means designing the *full information architecture*:\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#1a1a2e', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"âš™ï¸ System Instructions\\nStable rules, identity, behavior\"] --> G B[\"ðŸ§  Memory\\nPersisted facts across sessions\"] --> G C[\"ðŸ“„ Retrieved Docs (RAG)\\nDynamic knowledge per query\"] --> G D[\"ðŸ”§ Tool Schemas\\nAvailable actions & contracts\"] --> G E[\"ðŸ’¬ Conversation History\\nRecent messages & context\"] --> G F[\"ðŸŽ¯ Current Task\\nThe actual user request\"] --> G G([\"ðŸ¤– LLM\"])\n\nstyle A fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style B fill:#e8f5e9,stroke:#4caf50,stroke-width:2px style C fill:#fff3e0,stroke:#ff9800,stroke-width:2px style D fill:#fce4ec,stroke:#e91e63,stroke-width:2px style E fill:#f3e5f5,stroke:#9c27b0,stroke-width:2px style F fill:#e0f7fa,stroke:#00bcd4,stroke-width:2px style G fill:#1a237e,color:#fff,stroke:#1a237e,stroke-width:3px\n\n```\n\n> >\n> â€œPrompt engineering is now being rebranded as context engineeringâ€ â€” promptingguide.ai\n> >\n\nEach layer can be tuned independently.\n\n### Anatomy of a system prompt\n\nWhen you build an AI feature, the system prompt is where you do most of the work. It has four distinct components:\n\n| Component | Purpose | Example | | --- | --- | --- | | **Identity** | Who the model is and what it knows | â€œYou are a senior software engineer specializing in Python and API design.â€ | | **Rules** | What the model must or must not do | â€œAlways explain your reasoning. Never output code without a brief explanation.â€ | | **Format** | How output should be structured | â€œRespond in markdown. Use bullet points for lists, code blocks for code.â€ | | **Guardrails** | What to do when things go out of scope | â€œIf asked about topics outside software engineering, politely redirect.â€ |\n\nA minimal but effective system prompt covers all four. You can leave Identity implicit (the model will infer from rules and context), but skipping Rules or Format usually leads to inconsistent output.\n\n``` You are a technical writing assistant helping developers write clear API documentation.\n\nRules:\n- Use plain language. Avoid jargon unless the term is defined first.\n- Always include a request example and response example.\n- Flag any ambiguous parameter descriptions for human review.\n\nFormat:\n- Use markdown with H3 headings for each endpoint.\n- Code blocks use the language tag (json, bash, etc.)\n\nIf asked to document something outside the API, ask for clarification.\n\n```\n\nThe guardrail at the end is short on purpose. Over-specifying edge cases in the system prompt leads to brittle behavior. Define the center, not all the edges.\n\n### From chat to agents\n\nIn 2026, the mode has shifted:\n\n| Chat mode | Agent mode | | --- | --- | | You type, AI responds | You instruct, AI executes | | You write the prompt each time | Instructions run autonomously | | You iterate manually | Agent self-corrects in a loop | | You manage context yourself | Context managed by the system | | One task at a time | Multi-step, multi-file workflows |\n\n> >\n> â€œGPT-3.5 in an agentic workflow outperforms GPT-4 with a single prompt.â€\n> â€” Andrew Ng, Sequoia AI Ascent\n> >\n\nYour â€œpromptâ€ is now a configuration file. With Claude Code, GitHub Copilot, or Cursor, you write instructions once and they execute thousands of times. The surface has changed but the skill hasnâ€™t: be specific, give examples, set constraints.\n\n### Instructing agents\n\nEach tool has its own instruction file. Hereâ€™s where your prompting skills live in 2026:\n\n| File | Tool | Purpose | | --- | --- | --- | | `CLAUDE.md` | Claude Code | Global rules for the agent: conventions, workflow, preferences | | `agent.md` | Claude Code sub-agents | Role and scope for a specialized sub-agent | | `.github/copilot-instructions.md` | GitHub Copilot | Workspace-level coding conventions | | `SKILL.md` | Claude Code skills | Reusable task definitions the agent can invoke | | System prompt | Custom agents / APIs | Core identity, rules, and guardrails for production agents |\n\n```\n# CLAUDE.md\nUse TypeScript strict mode. Run tests before committing. Prefer async/await patterns.\n\n```\n\n```\n# agent.md (security review sub-agent)\nYou are a security reviewer. Focus: authentication, injection vulnerabilities, secrets in code. Flag severity as: critical / warning / info.\n\n```\n\n```\n# skills/deploy/SKILL.md\nUse Bicep for all infrastructure. Target: Azure. Naming convention: rg-{app}-{env}. Always output a plan before applying changes.\n\n```\n\nSkills are particularly useful for repeated workflows: deploy, test, review, document. You define the skill once and the agent can invoke it by name.\n\n**MCP (Model Context Protocol)** extends this further. Itâ€™s a standard for connecting agents to external tools: databases, APIs, file systems, internal services. When you define what tools an agent can use and how, youâ€™re doing context engineering at the infrastructure level.\n\nThe same 80/20 principles apply. But with agents, vague instructions run at scale: across every file, every commit, every customer request.\n\n### Meta Prompting\n\nStop writing prompts from scratch. Use a stronger model to generate and refine prompts for your target model.\n\n1. **Describe your task**: tell the LLM what you want to achieve\n2. **LLM generates prompt**: it produces a structured prompt using best practices\n3. **Test and iterate**: run the prompt, give feedback, let it refine further\n\nAvailable tools: Anthropic Console prompt generator, OpenAI Playground system prompt builder, or simply ask Claude/GPT to write your prompt for you.\n\n### Self-Critique Prompting\n\nFirst drafts from AI are fine. Second drafts are better. Third drafts are great.\n\n```mermaid %%{init: { 'theme': 'base', 'themeVariables': { 'primaryColor': '#e3f2fd', 'primaryTextColor': '#0d47a1', 'primaryBorderColor': '#2196f3', 'lineColor': '#546e7a', 'fontSize': '14px' } }}%% flowchart LR A[\"âœï¸ Generate\\nWrite first draft\"] --> B[\"ðŸ”Ž Critique\\nWhat's weak?\\nWhat would make\\nsomeone delete this?\"] B --> C[\"â™»ï¸ Revise\\nFix every issue\\nidentified\"] C -->|\"Still not great?\"| B\n\nstyle A fill:#e3f2fd,stroke:#2196f3,stroke-width:2px style B fill:#fff3e0,stroke:#ff9800,stroke-width:2px style C fill:#e8f5e9,stroke:#4caf50,stroke-width:2px\n\n```\n\n**Why it works**: LLMs are better critics than creators. They spot issues they wouldnâ€™t have caught on first pass. Ask â€œWhatâ€™s missing?â€ to surface gaps the model skipped over.\n\n**Power move**: Give Claudeâ€™s output to GPT for critique, then back. Different models catch different things.\n\n### Text Style Unbundling\n\nDonâ€™t describe the style you want. Extract it from examples.\n\n1. **Provide sample**: paste your best blog post, email, or brand copy\n2. **Extract features**: â€œAnalyze this text. Identify: tone, sentence structure, vocabulary level, rhetorical devices, and formatting.â€\n3. **Generate in style**: â€œWrite a new [text] using EXACTLY these style features: [paste extracted features]â€\n\n**Use cases**: brand consistency, ghostwriting, matching someoneâ€™s writing style from samples.\n\nEssentially advanced few-shot: instead of pasting examples, you extract the underlying rules and apply them explicitly.\n\n## Model settings and behavior\n\n### Temperature\n\n``` temp = 0.1 â†’ Deterministic â†’ Facts, code, data extraction temp = 0.5 â†’ Professional â†’ Emails, reports, analysis temp = 0.9 â†’ Creative â†’ Brainstorming, fiction, copy\n\n```\n\nWhen in doubt, start at 0.3-0.5 and adjust from there.\n\n### The alignment tax\n\nRLHF made models helpful but also verbose, hedging, and overly cautious. Modern models handle this better, but it still shows up:\n\n``` âŒ \"What's the capital of France?\" â†’ \"Great question! The capital of France is Paris. I hope that helps!\"\n\nâœ“ Add: \"Be concise. No disclaimers. Direct answers only.\" â†’ \"Paris.\"\n\n```\n\nWhen output feels â€œoffâ€ or unnecessarily padded, add explicit constraints about tone and length.\n\n## Practice and evaluation\n\n### Common pitfalls\n\n| Pitfall | Fix | | --- | --- | | **Too vague** | Add specifics: who, what, how many, format, for whom | | **Negative instructions** | Say what TO do, not what NOT to do. â€œBe conciseâ€ beats â€œDonâ€™t be longâ€ | | **Monolithic prompts** | Break into steps. Complex tasks need sequential, focused prompts | | **Ignoring hallucinations** | Ask for sources. Add â€œIf unsure, say soâ€ | | **Not iterating** | First prompt rarely perfect. Refine based on output |\n\n### Real rewrites\n\n**Too vague â†’ Specific:**\n\n``` âŒ \"Summarize these meeting notes\" âœ“ \"Extract: (1) key decisions, (2) action items with owners, (3) open questions. Max 150 words.\"\n\n```\n\n**Negative â†’ Positive:**\n\n``` âŒ \"Don't be too technical and don't use jargon\" âœ“ \"Explain at high school level. Everyday analogies. Under 100 words.\"\n\n```\n\n**Monolith â†’ Chained:**\n\n``` âŒ \"Analyze Q3 sales, find trends, compare Q2, write report\" âœ“ \"Step 1: Top 5 by revenue. Step 2: Compare to Q2. Step 3: Biggest trend. Step 4: 3-sentence summary.\"\n\n```\n\n### Iteration in practice\n\nNobody writes the perfect prompt on the first try. Hereâ€™s what three rounds looks like:\n\n**Round 1:**\n\n``` \"Write a product description for our new headphones\" â†’ Generic, bland. Missing audience, tone, differentiators. Fix: add who it's for and what makes it premium\n\n```\n\n**Round 2:**\n\n``` \"You are a copywriter for premium audio. 50-word description. Target: remote workers. Tone: premium but approachable.\" â†’ Better tone. Still missing the actual product specs. Fix: add the features that matter\n\n```\n\n**Round 3:**\n\n``` \"...Same. Must mention: 40hr battery, spatial audio, ANC. End with a call to action.\" â†’ Specific, compelling, on-brand. âœ“\n\n```\n\nEach round takes 30 seconds. Three rounds of iteration consistently beats one attempt at the perfect prompt.\n\n### The SOMA framework for evaluation\n\nWhen building AI features, test systematically:\n\n| Letter | Principle | What it means | | --- | --- | --- | | **S** | Specificity | Test specific scenarios with concrete inputs and expected outputs | | **O** | Objective Measurement | Define success criteria. Automate scoring. Track accuracy, consistency, format compliance | | **M** | Multiple Scenarios | Test happy path AND edge cases, different input variations | | **A** | Automation | Integrate testing into CI/CD. Treat prompts like production code |\n\n### Which technique when?\n\n| Situation | Technique | | --- | --- | | Output is vague or off-target | Be specific + format constraints | | Need consistent results | Few-shot examples | | First draft not good enough | Self-Critique: â€œNow critique thisâ€ | | Donâ€™t know how to prompt it | Meta Prompting: ask the AI to help | | Complex multi-step task | Prompt Chaining | | Need domain expertise primed | Prewarming first, then ask | | Need to match a voice or style | Text Style Unbundling | | High-stakes, canâ€™t be wrong | CoT + Self-Consistency | | Building an AI feature | Context Engineering + System Prompts | | Need to measure quality at scale | SOMA framework + automated eval |\n\nStart at the top. Move down only when the simpler approach isnâ€™t enough.\n\n### Prompt engineering at work\n\nThese arenâ€™t tricks. Theyâ€™re just clear communication applied to common tasks:\n\n**Code review:**\n\n``` You are a senior Python developer reviewing code for a REST API. Review for: bugs, performance, readability. For each issue: describe it, rate severity (high/med/low), suggest a fix.\n\n```\n\n**Data analysis:**\n\n``` Analyze this CSV of monthly sales data. Identify the top 3 trends, any anomalies, and explain what's driving them. Format: numbered list.\n\n```\n\n**Email drafting:**\n\n``` Write a follow-up email to a client whose project is 2 weeks late due to API issues. Tone: firm but professional. Goal: get a new commitment date from them. Max 100 words.\n\n```\n\n**Content creation:**\n\n``` You are a B2B content strategist. Write a 300-word blog intro about cloud migration, targeted at CTOs. Tone: authoritative, not salesy.\n\n```\n\n## Your prompt toolkit\n\n### Templates\n\n**The Expert:**\n\n``` You are a [role] with [X] years of experience. [Task] for [audience]. Format: [format]. Max [N] words.\n\n```\n\n**The Extractor:**\n\n``` From [source], extract: [field 1], [field 2], [field 3]. Return as [JSON/table]. If unsure: unknown.\n\n```\n\n**The Analyst:**\n\n``` Analyze [data]. Top [N] trends. Compare to [benchmark]. Present as [format].\n\n```\n\n**The Creator:**\n\n``` Write [type] about [topic]. Audience: [who]. Tone: [tone]. Include: [elements]. [N] words.\n\n```\n\n### Rules\n\n1. Start with a clear verb: Summarize, Compare, Create, Extract\n2. Assign a role and expertise level\n3. Set constraints: length, format, audience, tone\n4. Show 2-3 examples (few-shot)\n5. Use delimiters to separate instructions from data\n6. Break complex tasks into steps or chains\n7. Iterate: first prompt is a rough draft\n8. Say what TO do, not what NOT to do\n\n## Key takeaways\n\n| # | Takeaway | | --- | --- | | 1 | **LLMs are document completers**: they predict continuations. Design prompts as document beginnings | | 2 | **The Loop is your mental model**: youâ€™re the translator between human intent and AI patterns | | 3 | **80/20 applies**: specificity, examples, and iteration deliver most of the value | | 4 | **Context engineering > prompt engineering**: design all six layers, not just the message | | 5 | **Agents change the surface, not the skill**: the same principles apply, with higher stakes | | 6 | **Evaluate systematically**: SOMA framework â€” Specificity, Objective measurement, Multiple scenarios, Automation |\n\nThe skill that survived the â€œprompt engineering is deadâ€ wave isnâ€™t about magic phrases. Itâ€™s about communicating clearly with AI systems, in chat or in production.\n\nStart today: pick one technique from this post, apply it to a real task, and iterate.\n\n## Resources\n\n| Resource | Description | | --- | --- | | [**Prompt Engineering for LLMs**](https://www.oreilly.com/library/view/prompt-engineering-for/9781098156145/) | Oâ€™Reilly book by Berryman & Ziegler | | [**Prompt Engineering for Generative AI**](https://www.oreilly.com/library/view/prompt-engineering-for/9781098153427/) | Oâ€™Reilly book by Phoenix & Taylor | | [**Anthropic Prompt Library**](https://docs.anthropic.com/en/prompt-library/library) | Ready-to-use prompt templates from Anthropic | | [**Prompt Engineering Guide**](https://www.promptingguide.ai/) | Comprehensive community resource | | [**OpenAI Prompt Engineering**](https://platform.openai.com/docs/guides/prompt-engineering) | Official OpenAI guidance |\n\n> >\n> ðŸ’¬ **Whatâ€™s your go-to prompt technique?**\n> >\n> Iâ€™d love to hear what works for your workflow. Connect with me on [LinkedIn](https://linkedin.com/in/hiddedesmet).\n> >",
  "OutputDir": "_blogs",
  "Tags": [
    "agents",
    "ai",
    "chatgpt",
    "claude",
    "context-engineering",
    "Development",
    "github-copilot",
    "llm",
    "prompt-engineering"
  ],
  "PubDate": "2026-02-25T00:00:00+00:00"
}
