{
  "FeedUrl": "https://devblogs.microsoft.com/dotnet/feed/",
  "ProcessedDate": "2026-02-26 18:12:11",
  "Title": "Vector Data in .NET – Building Blocks for AI Part 2",
  "FeedLevelAuthor": ".NET Blog",
  "Description": "Discover how Microsoft.Extensions.VectorData brings unified vector database access to .NET - one interface for semantic search across any vector store with built-in support for embeddings, filtering, and RAG patterns.\n\nThe post [Vector Data in .NET – Building Blocks for AI Part 2](https://devblogs.microsoft.com/dotnet/vector-data-in-dotnet-building-blocks-for-ai-part-2/) appeared first on [.NET Blog](https://devblogs.microsoft.com/dotnet).",
  "OutputDir": "_news",
  "PubDate": "2026-02-26T18:00:00+00:00",
  "EnhancedContent": "Welcome back to the building blocks for AI in .NET series! In [part one](https://devblogs.microsoft.com/dotnet/dotnet-ai-essentials-the-core-building-blocks-explained/), we explored Microsoft Extensions for AI (MEAI) and how it provides a unified interface for working with large language models. Today, we’re diving into the second building block: **Microsoft.Extensions.VectorData**.\n\nIn the first post, we learned how to ask questions and even share some content for context with an LLM. Most applications, however, require more than just a simple question or small markdown file for context. You may want the LLM to have access to all of your product manuals to help troubleshoot customer issues, or provide your employee handbook for an HR chatbot.\n\nAnother feature that is common in intelligent apps is semantic search. A semantic search uses the meaning of a query, not just the words or letters, to conduct the search. It does this by converting text into *embeddings* which are numerical representations of the semantic meaning of text, and vectors that provide insights into how they are related.\n\nImagine you have a simple database with just three entries:\n\n1. Hall pass\n2. Mountain pass\n3. Pass (verb)\n\nA traditional approach to finding the answer to queries like “How do I get over the pass?” or “Where do I pick up a pass?” breaks the query down into parts to search for. The word “pass” appears in all three database items, so I receive all three entries back despite the different contexts of my queries. Here is a simplified visualization:\n\n```text \"How do I get over the pass?\" How | do | I | get | over | the | pass Pass - matches all three entries\n\n\"Where do I pick up my pass?\" Where | do | I | pick | up | my | pass Pass - matches all three entries ```\n\nNow let’s assume I use an embedding to encode the semantic meaning of the word. The database has already been encoded, but I need to create embeddings from my query. This time, however, the embeddings provide me with a semantic result, not a text-based one. The semantic approach looks like this:\n\n```text \"How do I get over the pass?\" 0 | 5 | etc. | 2 2 - matches the 2nd entry, \"Mountain pass\"\n\n\"Where do I pick up my pass?\" 6 | 9 | etc. | 1 1 - matches the 1st entry, \"Hall pass\" ```\n\nA special embeddings model is used to create the embeddings and is trained to understand the semantic meaning of words through context such as the related terms that appear before and after it. Instead of generating embeddings every time the application runs, it makes much more sense to store them in a database. This has the added bonus of being able to use the database’s ability to query and return results, rather than coding the logic yourself or doing it in a suboptimal way.\n\n*Vector databases* are designed specifically to store vectors and embeddings. Qdrant, Redis, SQL Server and Cosmos DB are examples of services and products that support storing vector data. Just like MEAI unified LLM access, the vector data extensions provide a common abstraction for working with vector stores.\n\n## Why vectors matter for AI applications\n\nBefore we jump into the code, let’s look a little more closely at vectors. When you ask an LLM a question about your company’s documentation, the model doesn’t magically know your content. Instead, your application typically:\n\n1. **Converts your documents into embeddings** – numerical representations that capture semantic meaning\n2. **Stores those embeddings in a vector database** along with the original content\n3. **Converts the user’s query into an embedding** using the same model\n4. **Performs a similarity search** to find the most relevant documents\n5. **Passes the relevant context to the LLM** along with the user’s query\n\n[![rag diagram image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAvcAAAQAAQMAAABf9uRjAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAdUlEQVR4nO3BAQ0AAADCoPdP7ewBFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAN4APAAHuoNYAAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2026/02/rag-diagram-scaled.webp)\n\nThis pattern, known as RAG (Retrieval-Augmented Generation), allows models to provide accurate, grounded responses based on your specific data. The challenge? Every vector database has its own SDK, data structures, and query patterns. That’s where Microsoft.Extensions.VectorData comes in.\n\n## One interface, many vector stores\n\nThe Microsoft Extensions for Vector Data library provides abstractions that work across different vector database providers. Here’s what that looks like in practice. First, let’s look at using an example vector database, Qdrant, directly and without the abstractions:\n\n```csharp var qdrantClient = new QdrantClient(\"localhost\", 6334);\n\nvar collection = \"my_collection\"; await qdrantClient.CreateCollectionAsync(collection, new VectorParams { Size = 1536, Distance = Distance.Cosine });\n\nvar points = new List<PointStruct> { new() { Id = new PointId { Uuid = Guid.NewGuid().ToString() }, Vectors = embedding, Payload = { [\"text\"] = \"Sample document text\", [\"category\"] = \"documentation\" } } };\n\nawait qdrantClient.UpsertAsync(collection, points);\n\nvar searchResults = await qdrantClient.SearchAsync(collection, queryEmbedding, limit: 5); ```\n\nNow let’s see the same thing using the universal abstractions:\n\n```csharp // Configure embedding generation once on the vector store var embeddingGenerator = new OpenAIClient(apiKey) .GetEmbeddingClient(\"text-embedding-3-small\") .AsIEmbeddingGenerator();\n\nvar vectorStore = new QdrantVectorStore( new QdrantClient(\"localhost\"), ownsClient: true, new QdrantVectorStoreOptions { EmbeddingGenerator = embeddingGenerator });\n\nvar collection = vectorStore.GetCollection<string, DocumentRecord>(\"my_collection\"); await collection.EnsureCollectionExistsAsync();\n\nvar record = new DocumentRecord { Key = Guid.NewGuid().ToString(), Text = \"Sample document text\", Category = \"documentation\" };\n\nawait collection.UpsertAsync(record);\n\nvar searchResults = collection.SearchAsync(\"find documents about sample topics\", top: 5); ```\n\nThe second example works with any supported vector store by simply changing the `VectorStore` implementation. Your business logic stays the same.\n\n## Defining your data model\n\nThe vector data abstractions use attributes to map your C# classes to vector database schemas. Here’s a practical example for a document store:\n\n```csharp public class DocumentRecord { [VectorStoreKey] public string Key { get; set; }\n\n[VectorStoreData] public string Text { get; set; }\n\n[VectorStoreData(IsIndexed = true)] public string Category { get; set; }\n\n[VectorStoreData(IsIndexed = true)] public DateTimeOffset Timestamp { get; set; }\n\n// The vector is automatically generated from Text when an // IEmbeddingGenerator is configured on the collection or vector store [VectorStoreVector(1536, DistanceFunction.CosineSimilarity)] public string Embedding => this.Text; } ```\n\nThe attributes tell the library:\n\n- **`VectorStoreKey`** – This property uniquely identifies each record\n- **`VectorStoreData`** – These are metadata fields you can filter and retrieve\n- **`VectorStoreVector`** – This is the embedding vector with its dimensions and distance function\n\n## Working with collections\n\nOnce you’ve defined your data model, working with collections is straightforward. The library provides a consistent interface regardless of your underlying vector store:\n\n```csharp // Get or create a collection var collection = vectorStore.GetCollection<string, DocumentRecord>(\"documents\");\n\n// Check if the collection exists bool exists = await collection.CollectionExistsAsync(); await collection.EnsureCollectionExistsAsync();\n\n// Insert or update records await collection.UpsertAsync(documentRecord);\n\n// Batch operations are supported await collection.UpsertBatchAsync(documentRecords);\n\n// Retrieve by key var record = await collection.GetAsync(\"some-key\");\n\n// Delete records await collection.DeleteAsync(\"some-key\"); await collection.DeleteBatchAsync([\"key1\", \"key2\", \"key3\"]); ```\n\n## Semantic search\n\nThe real power comes when you perform semantic searches using the `SearchAsync` method. When an `IEmbeddingGenerator` is configured on the vector store or collection, simply pass your query text and embeddings are generated automatically:\n\n```csharp // Embeddings are generated automatically when IEmbeddingGenerator is configured await foreach (var result in collection.SearchAsync(\"What is semantic search?\", top: 5)) { Console.WriteLine($\"Score: {result.Score}, Text: {result.Record.Text}\"); } ```\n\nIf you already have a pre-computed `ReadOnlyMemory<float>` embedding—for example, when batching embeddings yourself—you can pass it directly instead:\n\n```csharp // Pass a pre-computed embedding vector directly ReadOnlyMemory<float> precomputedEmbedding = /* your embedding */; await foreach (var result in collection.SearchAsync(precomputedEmbedding, top: 5)) { Console.WriteLine($\"Score: {result.Score}, Text: {result.Record.Text}\"); } ```\n\n## Filtering results\n\nYou can combine vector similarity with metadata filtering to narrow down results:\n\n```csharp var searchOptions = new VectorSearchOptions<DocumentRecord> { Filter = r => r.Category == \"documentation\" && r.Timestamp > DateTimeOffset.UtcNow.AddDays(-30) };\n\nvar results = collection.SearchAsync(\"find relevant documentation\", top: 10, searchOptions); ```\n\nFilters use standard LINQ expressions. The supported operations include:\n\n- Equality comparisons (`==`\n, `!=` )\n- Range queries (`>`\n, `<` , `>=` , `<=` )\n- Logical operators (`&&`\n, `||` )\n- Collection membership (`.Contains()`\n)\n\n## Integrating with embeddings\n\nThe recommended approach is to configure an `IEmbeddingGenerator` on the vector store or collection. Embeddings are then generated automatically during both upsert and search—no manual preprocessing required:\n\n```csharp // Configure an embedding generator on the vector store var embeddingGenerator = new OpenAIClient(apiKey) .GetEmbeddingClient(\"text-embedding-3-small\") .AsIEmbeddingGenerator();\n\nvar vectorStore = new InMemoryVectorStore(new() { EmbeddingGenerator = embeddingGenerator }); var collection = vectorStore.GetCollection<string, DocumentRecord>(\"documents\"); await collection.EnsureCollectionExistsAsync();\n\n// Embeddings are generated automatically on upsert var record = new DocumentRecord { Key = Guid.NewGuid().ToString(), Text = \"Sample text to store\" }; await collection.UpsertAsync(record);\n\n// Embeddings are also generated automatically on search await foreach (var result in collection.SearchAsync(\"find similar text\", top: 5)) { Console.WriteLine($\"Score: {result.Score}, Text: {result.Record.Text}\"); } ```\n\n## Implementing RAG patterns\n\nBringing it all together, here’s a simplified RAG implementation using both Microsoft.Extensions.AI and Microsoft.Extensions.VectorData:\n\n```csharp public async Task<string> AskQuestionAsync(string question) { // Find relevant documents - embeddings are generated automatically var contextParts = new List<string>(); await foreach (var result in collection.SearchAsync(question, top: 3)) { contextParts.Add(result.Record.Text); }\n\n// Build context from results var context = string.Join(\"\\n\\n\", contextParts);\n\n// Create prompt with context var messages = new List<ChatMessage> { new(ChatRole.System, \"Answer questions based on the provided context. If the context doesn't contain relevant information, say so.\"), new(ChatRole.User, $\"Context:\\n{context}\\n\\nQuestion: {question}\") };\n\n// Get response from LLM var response = await chatClient.GetResponseAsync(messages); return response.Message.Text; } ```\n\n## Supported vector stores\n\nMicrosoft.Extensions.VectorData works with a wide range of vector databases through official connectors:\n\n- **Azure AI Search** – `Microsoft.Extensions.VectorData.AzureAISearch`\n- **Qdrant** – `Microsoft.SemanticKernel.Connectors.Qdrant`\n- **Redis** – `Microsoft.SemanticKernel.Connectors.Redis`\n- **PostgreSQL** – `Microsoft.SemanticKernel.Connectors.Postgres`\n- **Azure Cosmos DB (NoSQL)** – `Microsoft.SemanticKernel.Connectors.AzureCosmosDBNoSQL`\n- **SQL Server** – `Microsoft.SemanticKernel.Connectors.SqlServer`\n- **SQLite** – `Microsoft.SemanticKernel.Connectors.Sqlite`\n- **In-Memory** – `Microsoft.SemanticKernel.Connectors.InMemory`\n(great for testing and development)\n\nFor the full list of supported connectors—including Elasticsearch, MongoDB, Weaviate, Pinecone, and more—see the [out-of-the-box connectors documentation](https://learn.microsoft.com/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/?pivots=programming-language-csharp).\n\n## Why separate from the core AI extensions?\n\nYou might wonder why vector data is in a separate library from the core Microsoft.Extensions.AI package. The answer is simple: not every intelligent application needs vector storage. Many scenarios – like chatbots, content generation, or classification tasks – work perfectly fine with just the LLM abstractions. By keeping vector data separate, the core library remains lightweight and focused.\n\nWhen you do need vectors for semantic search, RAG, or long-term memory, you can add the vector data package and immediately benefit from the same consistent patterns you’re already using with MEAI.\n\n## Summary\n\nMicrosoft.Extensions.VectorData brings the same benefits to vector databases that Microsoft.Extensions.AI brings to LLMs: a unified, provider-agnostic interface that makes your code portable and your architecture flexible. Whether you’re implementing RAG patterns, building semantic search, or creating long-term memory for AI agents, these abstractions let you focus on your application logic instead of database-specific SDKs.\n\nIn the next post, we’ll explore the Microsoft Agent Framework and see how these building blocks come together to create sophisticated agentic workflows. Until then, here are some resources to help you get started with vector data in .NET:\n\n- Learn by code\n- [AI samples repository](https://github.com/dotnet/ai-samples)\n- Learn by following tutorials\n- [.NET AI documentation](https://learn.microsoft.com/dotnet/ai/)\n- Learn by watching videos\n- [AI building blocks](https://youtu.be/qcp6ufe_XYo)\n- [Building intelligent apps with .NET](https://youtu.be/N0DzWMkEnzk)\n\nHappy coding!",
  "Link": "https://devblogs.microsoft.com/dotnet/vector-data-in-dotnet-building-blocks-for-ai-part-2/",
  "Tags": [
    ".NET",
    "AI",
    "C#",
    "Embeddings",
    "Microsoft.Extensions.VectorData",
    "rag",
    "semantic search",
    "vector search"
  ],
  "Author": "Jeremy Likness",
  "FeedName": "Microsoft .NET Blog"
}
