{
  "Description": "In the rapidly evolving landscape of large language models (LLMs), achieving precise control over model behavior while maintaining quality has become a critical challenge. While models like GPT-4 demonstrate impressive capabilities, ensuring their outputs align with human preferences—whether for safety, helpfulness, or style—requires sophisticated fine-tuning techniques. Direct Preference Optimization (DPO) represents a breakthrough approach that […]\n\nThe post [DPO Fine-Tuning Using Microsoft Foundry SDK](https://devblogs.microsoft.com/foundry/dpo-fine-tuning-using-microsoft-foundry-sdk/) appeared first on [Microsoft Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "FeedLevelAuthor": "Microsoft Foundry Blog",
  "OutputDir": "_news",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/",
  "ProcessedDate": "2026-02-14 00:06:51",
  "Author": "Jayesh Tanna",
  "Title": "DPO Fine-Tuning Using Microsoft Foundry SDK",
  "EnhancedContent": "In the rapidly evolving landscape of large language models (LLMs), achieving precise control over model behavior while maintaining quality has become a critical challenge. While models like GPT-4 demonstrate impressive capabilities, ensuring their outputs align with human preferences—whether for safety, helpfulness, or style—requires sophisticated fine-tuning techniques. Direct Preference Optimization (DPO) represents a breakthrough approach that simplifies this alignment process while delivering exceptional results.\n\nThis comprehensive guide explores DPO fine-tuning, explaining what it is, how it works, when to use it, and how to implement it using [Microsoft Foundry SDK](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/sdk-overview?view=foundry&amp;pivots=programming-language-python). Whether you’re building a customer service chatbot that needs to be consistently helpful, a content generation system that should avoid harmful outputs, or any AI application where response quality matters, understanding DPO will empower you to create better-aligned models.\n\n## What is Direct Preference Optimization (DPO)?\n\nDirect Preference Optimization is an innovative technique for training language models to align with human preferences without the complexity of traditional Reinforcement Learning from Human Feedback (RLHF). Introduced in the groundbreaking paper [Direct Preference Optimization: Your Language Model is Secretly a Reward Model” by Rafailov, Sharma,…](https://arxiv.org/abs/2305.18290), DPO fundamentally reimagines how we teach models to generate preferred outputs.\n\nUnlike traditional supervised fine-tuning where you show a model “what to say,” DPO teaches models by showing them comparative examples: “this response is better than that one.” For each prompt, you provide:\n\n- **Preferred response**: A high-quality, desirable output\n- **Non-preferred response**: A lower-quality or undesirable output\n\n[![dpo image image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAAI9AQMAAAANU8cvAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAWElEQVR4nO3BMQEAAADCoPVP7W8GoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4A0ILQABU8X3dwAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2026/01/dpo-image.webp)\n\nThe model learns to increase the likelihood of generating preferred responses while decreasing the probability of non-preferred ones, all without requiring explicit reward modeling or complex reinforcement learning pipelines.\n\n**Best Use Cases for DPO:**\n\n- Response Quality & Accuracy Improvement\n- Reading Comprehension & Summarization\n- Safety & Harmfulness Reduction\n- Style, Tone, & Brand Voice Alignment\n- Helpfulness & User Preference Optimization\n\n## How Direct Preference Optimization Works\n\nThe following code demonstrates DPO fine-tuning using the [Microsoft Foundry Projects SDK](https://pypi.org/project/azure-ai-projects/):\n\n```py import os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient\n\n# Load environment variables\nload_dotenv()\n\nendpoint = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\") model_name = os.environ.get(\"MODEL_NAME\")\n\n# Define dataset file paths\ntraining_file_path = \"training.jsonl\" validation_file_path = \"validation.jsonl\"\n\ncredential = DefaultAzureCredential() project_client = AIProjectClient(endpoint=endpoint, credential=credential) openai_client = project_client.get_openai_client()\n\n# Upload training and validation files\nwith open(training_file_path, \"rb\") as f: train_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n\nwith open(validation_file_path, \"rb\") as f: validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\")\n\nopenai_client.files.wait_for_processing(train_file.id) openai_client.files.wait_for_processing(validation_file.id)\n\n# Create DPO Fine Tuning job\nfine_tuning_job = openai_client.fine_tuning.jobs.create( training_file=train_file.id, validation_file=validation_file.id, model=model_name, method={ \"type\": \"dpo\", \"dpo\": { \"hyperparameters\": { \"n_epochs\": 3, \"batch_size\": 1, \"learning_rate_multiplier\": 1.0 } } }, extra_body={\"trainingType\": \"GlobalStandard\"} ) ```\n\nDPO Fine-Tuning Results:\n\n```py print(f\"Testing fine-tuned model via deployment: {deployment_name}\")\n\nresponse = openai_client.responses.create( model=deployment_name, input=[{\"role\": \"user\", \"content\": \"Explain machine learning in simple terms.\"}] )\n\nprint(f\"Model response: {response.output_text}\") ```\n\nInference result:\n\n```py Model response: Machine learning is like teaching a computer to learn from experience, similar to how people do. Instead of programming specific instructions for every task, we give the computer a lot of data and it figures out patterns on its own. Then, it can use what it learned to make decisions or predictions. For example, if you show a machine learning system lots of pictures of cats and dogs, it will learn to recognize which is which by itself. ```\n\nData format example:\n\n```json { \"input\": { \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of France?\"} ] }, \"preferred_output\": [ {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"} ], \"non_preferred_output\": [ {\"role\": \"assistant\", \"content\": \"I think it's London.\"} ] } ```\n\n## Comparing DPO to Other Methods\n\n| **Aspect** | **DPO** | **SFT** | **RFT** | | --- | --- | --- | --- | | Learning signal | Comparative preferences | Input-output pairs | Graded exploration | | Data requirement | Preference pairs | Example demonstrations | Problems + grader | | Best for | Quality alignment | Task learning | Complex reasoning | | Computational cost | Moderate | Low | High |\n\n## Learn more\n\n- Watch on-demand: [AI fine-tuning in Microsoft Foundry to make your agents unstoppable](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fignite.microsoft.com%2Fen-US%2Fsessions%2FBRK188%3Fsource%3Dsessions&amp;data=05%7C02%7Ckingernupur%40microsoft.com%7Cb6ae5ecd53ee4d3bf8e008de6989e2a6%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C639064238111810769%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=gydnYWKyZiI22zsjFGcGwvuPv6KGV%2FmfT8Wheck8Suk%3D&amp;reserved=0)\n- Join the next Model Mondays Livestream [Model Mondays | Microsoft Reactor](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fdeveloper.microsoft.com%2Fen-us%2Freactor%2Fseries%2FS-1485%2F&amp;data=05%7C02%7Ckingernupur%40microsoft.com%7Cb6ae5ecd53ee4d3bf8e008de6989e2a6%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C639064238111821245%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=nuaivTskhS7t4xe7cFp17kdN8lWBN3MEAP1uy5XO19M%3D&amp;reserved=0)\n- Learn more about fine-tuning on Microsoft Foundry [Fine-tune models with Microsoft Foundry – Microsoft Foundry | Microsoft Learn](https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fazure%2Fai-foundry%2Fconcepts%2Ffine-tuning-overview%3Fview%3Dfoundry-classic&amp;data=05%7C02%7Ckingernupur%40microsoft.com%7Cb6ae5ecd53ee4d3bf8e008de6989e2a6%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C639064238111831782%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&amp;sdata=L%2B96SWkHeG5t%2BZ%2FcbRQadiF91wrI7hfoGZoKhsUdZCE%3D&amp;reserved=0)",
  "Tags": [
    "Microsoft Foundry"
  ],
  "FeedName": "Microsoft AI Foundry Blog",
  "PubDate": "2026-02-13T23:13:44+00:00",
  "Link": "https://devblogs.microsoft.com/foundry/dpo-fine-tuning-using-microsoft-foundry-sdk/"
}
