{
  "FeedName": "Microsoft AI Foundry Blog",
  "Link": "https://devblogs.microsoft.com/foundry/beyond-the-prompt-why-and-how-to-fine-tune-your-own-models/",
  "FeedLevelAuthor": "Microsoft Foundry Blog",
  "PubDate": "2026-02-11T17:29:15+00:00",
  "EnhancedContent": "Large Language Models (LLMs) have reached a point where general intelligence is no longer the bottleneck. The real challenge in enterprise AI systems behavioral alignment ensuring models that produce consistent, reliable, policy-compliant outputs on a scale. Prompt engineering and Retrieval-Augmented Generation (RAG) are powerful but they do not change model behavior. Fine-tuning will solve this by customizing a pretrained AI model with additional training on a specific task or dataset to improve performance, add new skills, or enhance accuracy.\n\nThis post explores what [Microsoft Foundry](https://ai.azure.com/) fine-tuning is, when using it, the fine-tuning approaches it supports **** and code examples on how to run Fine-tuning on Microsoft Foundry.\n\n[![image 1 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABAAAAAKMAQMAAABPcVhZAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAZ0lEQVR4nO3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA8G5ImwAByJT81QAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2026/01/image-1.webp)\n\n**What Is Microsoft Foundry Fine-Tuning:**\n\n[Microsoft Foundry fine-tuning](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/fine-tuning-overview?view=foundry-classic) allows you to customize pre-trained foundation models (OpenAI and open models) using task-specific datasets, producing a specialized model that behaves predictably for your use case while maintaining Azure’s enterprise-grade security, governance, and observability.\n\n**Key Benefits and top use cases of Fine-tuning:**\n\n[![image 2 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA80AAAJKAQMAAAAsujViAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAXUlEQVR4nO3BMQEAAADCoPVPbQo/oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICvARmdAAGc52EeAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2026/01/image-2.webp)\n\n- **Domain Specialization:** Adapt a language model for specialized domains like medicine, finance, or law to understand technical jargon and deliver more accurate, domain-specific responses.\n- **Task Performance:** Optimize a model for tasks like sentiment analysis, code generation, translation, or summarization to achieve higher performance than a general-purpose model.\n- **Style and Tone:** Fine-tune the model to match your preferred communication style, such as formal business, brand voice, or technical writing.\n- **Instruction Following:** Enhance the model’s ability to follow formatting rules, multi-step instructions, and structured outputs, including selecting the right agent in multi-agent workflows.\n- **Compliance and Safety:** Train a fine-tuned model to adhere to organizational policies, regulatory requirements, or other guidelines unique to your application.\n- **Language or Cultural Adaptation:** Tailor a language model to a specific language, dialect, or cultural context when general-purpose models fall short, without the cost of training from scratch.\n\n**Supported Finetuning Methods:**\n\n- Supervised Finetuning\n- Direct Preference Optimization\n- Reinforcement Finetuning\n\n**Supervised Fine-Tuning (SFT)** is foundational training technique that trains a pre-trained model on input-output pairs for a specific task. It helps the model give more accurate, consistent, and task-specific responses, such as summarizing text, answering questions, or generating code, while keeping the knowledge from the original base model.\n\n**Best use cases for SFT:**\n\n1. Text Classification & Labeling\n2. Question Answering & Knowledge Extraction\n3. Text Summarization\n4. Code Generation & Analysis\n5. Structured Output & Formatting\n6. Domain-Specific Language or Style Alignment\n7. Multi-Agent or Tool-Calling Workflows\n\n**How it works:** You provide the model with a fixed set of examples, and it learns to produce the desired output for a given input. It’s a “learn by example” approach.\n\n[![image 3 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA9UAAAJzAQMAAAA7tqIbAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAYklEQVR4nO3BMQEAAADCoPVPbQo/oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHgZL8MAAe81XOwAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2026/01/image-3.webp)\n\n**Supervised Finetuning Code Snippet:** It is supported by Microsoft Foundry SDK and Foundry UI. This demonstrates the code snipped using Microsoft Foundry SDK:\n\n```python import os from dotenv import load_dotenv from azure.identity import DefaultAzureCredential from azure.ai.projects import AIProjectClient load_dotenv() endpoint = os.environ.get(\"AZURE_AI_PROJECT_ENDPOINT\") model_name = os.environ.get(\"MODEL_NAME\") # Define dataset file paths training_file_path = \"training.jsonl\" validation_file_path = \"validation.jsonl\" credential = DefaultAzureCredential() project_client = AIProjectClient(endpoint=endpoint, credential=credential) openai_client = project_client.get_openai_client() with open(validation_file_path, \"rb\") as f: validation_file = openai_client.files.create(file=f, purpose=\"fine-tune\") openai_client.files.wait_for_processing(validation_file.id) with open(training_file_path, \"rb\") as f: train_file = openai_client.files.create(file=f, purpose=\"fine-tune\") openai_client.files.wait_for_processing(train_file.id) fine_tune_job = openai_client.fine_tuning.jobs.create( model=model_name, training_file=train_file.id, validation_file=validation_file.id, method={ \"type\": \"supervised\", \"supervised\": {\"hyperparameters\": {\"n_epochs\": 3, \"batch_size\": 1, \"learning_rate_multiplier\": 1.0}}, }, suffix=\"pubmed-summarization\" ) ```\n\n**Data Format Example:** The training data sample should contain min. of 10 lines\n\n```json { \"messages\": [ { \"role\": \"system\", \"content\": \"You are a medical research summarization assistant. Create concise, accurate abstracts of medical research articles that capture the key findings and methodology.\" }, { \"role\": \"user\", \"content\": \"Summarize this medical research article:\\n\\n[full article text]\" }, { \"role\": \"assistant\", \"content\": \"[generated abstract]\" } ] } ```\n\n**Cookbooks:** SFT with PubMed Medical Research Summarization Dataset\n\nThis cookbook [fine-tuning/Demos/SFT_PubMed_Summarization at main · microsoft-foundry/fine-tuning](https://github.com/microsoft-foundry/fine-tuning/tree/main/Demos/SFT_PubMed_Summarization) demonstrates how to fine-tune language models using Supervised Fine-Tuning (SFT) with the PubMed Medical Research Summarization dataset on Azure AI.\n\nAfter executing the cookbook, one can navigate to Foundry Portal to monitor the job details.\n\n**Fine-tuning job view in Microsoft Foundry:**\n\nNavigate to Microsoft Foundry at https://ai.azure.com then the fine-tuning section to view the job details and execution progress\n\n[![Training Loss image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABRoAAAJEAQMAAABDyAphAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAc0lEQVR4nO3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAlwF14wABhzjCkQAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2026/01/Training-Loss.webp)\n\n**Key highlights:**\n\n1. This cookbook uses GPT4.1 as the base model and [PubMed Article Summarization Dataset on Kaggle](https://www.kaggle.com/datasets/thedevastator/pubmed-article-summarization-dataset) as the reference training data set.\n2. **Prerequisites:** Ensure you have Azure subscription with Microsoft Foundry project; you must have Azure AI User role, have access to the required models and set up an AI Foundry project\n3. **Dataset Preparation:** Finetuning expects the datasets in JSONL formats, the JSONL can be found here: Use the training.jsonl and validation.jsonl available in this Cookbook sample\n4. **Finetune Job:** Configure with default hyper parameters and run the finetune job\n5. **Deployment:** Optionally, deploy the finetuned model to a serverless endpoint and perform sample inferences.\n\n**Results of Finetuning**\n\n| Metric | Base Model | Fine-Tuned Model | | --- | --- | --- | | Task Accuracy | 70–80% | 88–95% | | Prompt Length | 800–1200 tokens | 200–400 tokens | | Inference Cost | Baseline (1.0x) | 0.5–0.7x |",
  "Description": "Large Language Models (LLMs) have reached a point where general intelligence is no longer the bottleneck. The real challenge in enterprise AI systems behavioral alignment ensuring models that produce consistent, reliable, policy-compliant outputs on a scale. Prompt engineering and Retrieval-Augmented Generation (RAG) are powerful but they do not change model behavior. Fine-tuning will solve this […]\n\nThe post [Beyond the Prompt – Why and How to Fine-tune Your Own Models](https://devblogs.microsoft.com/foundry/beyond-the-prompt-why-and-how-to-fine-tune-your-own-models/) appeared first on [Microsoft Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "Tags": [
    "Microsoft Foundry"
  ],
  "OutputDir": "_news",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/",
  "Title": "Beyond the Prompt – Why and How to Fine-tune Your Own Models",
  "ProcessedDate": "2026-02-11 18:16:13",
  "Author": "Radhika Bollineni"
}
