{
  "Description": "Think of Continuous AI as background agents that operate in your repository for tasks that require reasoning.\n\nThe post [Continuous AI in practice: What developers can automate today with agentic CI](https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/) appeared first on [The GitHub Blog](https://github.blog).",
  "ProcessedDate": "2026-02-05 17:15:50",
  "EnhancedContent": "Software engineering has always included work that’s repetitive, necessary, and historically difficult to automate. This isn’t because it lacks values, but because it resists deterministic rules.\n\nContinuous integration (CI) solved part of this by handling tests, builds, formatting, and static analysis—anything that can be described with deterministic rules. CI excels when correctness can be expressed unambiguously: a test passes or fails, a build succeeds or doesn’t, a rule is violated or isn’t.\n\nBut CI is intentionally limited to problems that can be reduced to heuristics and rules.\n\nFor most teams, the hardest work isn’t writing code. It’s everything that requires judgment around that code: reviewing changes, keeping documentation accurate, managing dependencies, tracking regressions, maintaining tests, monitoring quality, and responding to issues that only surface after code ships.\n\nBut a lot of engineering work goes into work that requires interpretation, synthesis, and context, rather than deterministic validation. And an increasing share of engineering tasks fall into a category CI was never designed to handle: work that depends on understanding intent.\n\n“Any task that requires judgment goes beyond heuristics,” says Idan Gazit, head of GitHub Next, which works on research and development initiatives.\n\n> >\n> Any time something can’t be expressed as a rule or a flow chart is a place where AI becomes incredibly helpful.\n> >\n> Idan Gazit, head of GitHub Next\n\nThis is why GitHub Next has been exploring a new pattern: [Continuous AI](https://githubnext.com/projects/continuous-ai/), or background agents that operate in your repository the way CI jobs do, but only for tasks that require reasoning instead of rules.\n\n## Why CI isn’t enough anymore\n\nCI isn’t failing. It’s doing exactly what it was designed to do.\n\nCI is designed for binary outcomes. Tests pass or fail. Builds succeed or don’t. Linters flag well-defined violations. That works well for rule-based automation.\n\nBut many of the hardest and most time-consuming parts of engineering are judgment-heavy and context-dependent.\n\nConsider these scenarios:\n\n- A docstring says one thing, but the implementation says another.\n- Text passes accessibility linting but is still confusing to users.\n- A dependency adds a new flag, altering behavior without a major version bump.\n- A regex is compiled inside a loop, tanking performance in subtle ways.\n- UI behavior changes are only visible when interacting with the product.\n\nThese problems are about whether intent still holds.\n\n“The first era of AI for code was about code generation,” Idan explains. “The second era involves cognition and tackling the cognitively heavy chores off of developers.”\n\nThis is the gap Continuous AI fills: not more automation, but a different class of automation. CI handles deterministic work. Continuous AI applies where correctness depends on reasoning, interpretation, and intent.\n\n## What Continuous AI actually means\n\nContinuous AI is not a new product or CI replacement. Traditional CI remains essential.\n\nContinuous AI is a pattern:\n\n| **Continuous AI = natural-language rules + agentic reasoning, executed continuously inside your repository.** | | --- |\n\nIn practice, Continuous AI means expressing in plain language what should be true about your code, especially when that expectation cannot be reduced to rules or heuristics. An agent then evaluates the repository and produces artifacts a developer can review: suggested patches, issues, discussions, or insights.\n\nDevelopers rarely author agentic workflows in a single pass. In practice, they collaborate with an agent to refine intent, add constraints, and define acceptable outputs. The workflow emerges through iteration, not a single sentence.\n\nFor example:\n\n- “Check whether documented behavior matches implementation, explain any mismatches, and propose a concrete fix.”\n- “Generate a weekly report summarizing project activity, emerging bug trends, and areas of increased churn.”\n- “Flag performance regressions in critical paths.”\n- “Detect semantic regressions in user flows.”\n\nThese workflows are not defined by brevity. They combine intent, constraints, and permitted outputs to express expectations that would be awkward or impossible to encode as deterministic rules.\n\n“In the future, it’s not about agents running in your repositories,” Idan says. “It’s about being able to presume you can cheaply define agents for anything you want off your plate permanently.”\n\n> >\n> Think about what your work looks like when you can delegate more of it to AI, and what parts of your work you want to retain: your judgment, your taste.\n> >\n> Idan Gazit, head of GitHub Next\n\n## Guardrails by design: Permissions and Safe Outputs\n\nIn our work, we define agentic workflows with safety as a first principle. By default, agents operate with read-only access to repositories. They cannot create issues, open pull requests, or modify content unless explicitly permitted.\n\nWe call this **Safe Outputs**, which provides a deterministic contract for what an agent is allowed to do. When defining a workflow, developers specify exactly which artifacts an agent may produce, such as opening a pull request or filing an issue, and under what constraints.\n\nAnything outside those boundaries is forbidden.\n\nThis model assumes agents can fail or behave unexpectedly. Outputs are sanitized, permissions are explicit, and all activity is logged and auditable. The blast radius is deterministic.\n\nThis isn’t “AI taking over software development.” It’s AI operating within guardrails developers explicitly define.\n\n## Why natural language complements YAML\n\nAs we’ve developed this, we’ve heard a common question: why not just extend CI with more rules?\n\nWhen a problem can be expressed deterministically, extending CI is exactly the right approach. YAML, schemas, and heuristics remain the correct tools for those jobs.\n\nBut many expectations cannot be reduced to rules without losing meaning.\n\nIdan puts it simply: “**There’s a larger class of chores and tasks we can’t express in heuristics.**”\n\nA rule like “whenever documentation and code diverge, identify and fix it” cannot be expressed in a regex or schema. It requires understanding semantics and intent. A natural-language instruction can express that expectation clearly enough for an agent to reason over it.\n\nNatural language doesn’t replace YAML, but instead complements it. CI remains the foundation. Continuous AI expands automation into commands CI was never designed to cover.\n\n## Developers stay in the loop, by design\n\nAgentic workflows don’t make autonomous commits. Instead, they can create the same kinds of artifacts developers would (pull requests, issues, comments, or discussions) depending on what the workflow is permitted to do.\n\nPull requests remain the most common outputs because they align with how developers already review and reason about change.\n\n“The PR is the existing noun where developers expect to review work,” Idan says. “It’s the checkpoint everyone rallies around.”\n\nThat means:\n\n- Agents don’t merge code\n- Developers retain full control\n- Everything is visible and reviewable\n\nDeveloper judgment remains the final authority. Continuous AI helps scale that judgment across a codebase.\n\n## How GitHub Next is experimenting with these ideas\n\nThe [GitHub Next prototype](https://githubnext.github.io/gh-aw/) (or you can find the repository at [gh aw](https://github.com/githubnext/gh-aw)) uses a deliberately simple pattern:\n\n1. Write an agentic workflow\n2. Compile it into a GitHub Action\n3. Push it\n4. Let an agent run on any GitHub Actions trigger (pull requests, pushes, issues, comments, or schedules)\n\nNothing is hidden; everything is transparent and visible.\n\n“You want an action to look for style violations like misplaced brackets, that’s heuristics,” Idan explains. “But when you want deeper intent checks, you need AI.”\n\n## What Continuous AI can automate today\n\nThese aren’t theoretical examples. GitHub Next has tested these patterns in real repositories.\n\n### 1. Fix mismatches between documentation and behavior\n\nThis is one of the hardest problems for CI because it requires understanding *intent*.\n\nAn agentic workflow can:\n\n- Read a function’s docstring\n- Compare it to the implementation\n- Detect mismatches\n- Suggest updates to either the code or the docs\n- Open a pull request\n\nIdan calls this one of the most meaningful categories of work Continuous AI can address: “You don’t want to worry every time you ship code if the documentation is still right. That wasn’t possible to automate before AI.”\n\n### 2. Generate ongoing project reports with reasoning\n\nMaintainers and managers spend significant time answering the same questions repeatedly: What changed yesterday? Are bugs trending up or down? Which parts of the codebase are most active?\n\nAgentic workflows can generate recurring reports that pull from multiple data sources (issues, pull requests, commits, and CI results), and apply reasoning on top.\n\nFor example, an agent can:\n\n- Summarize daily or weekly activity\n- Highlight emerging bug trends\n- Correlate recent changes with test failures\n- Surfaces areas of increased churn\n\nThe value isn’t the report itself. It’s the synthesis across multiple data sources that would otherwise require manual analysis.\n\n### 3. Keep translations up to date automatically\n\nAnyone who has worked with localized applications knows the pattern: Content changes in English, translations fall behind, and teams batch work late in the cycle (often right before a release).\n\nAn agent can:\n\n- Detect when English text changes\n- Re-generate translations for all languages\n- Open a single pull request containing the updates\n\nThe workflow becomes continuous, not episodic. Machine translations might not be perfect out of the box, but having a draft translation ready for review in a pull request makes it that much easier to engage help from professional translators or community contributors.\n\n### 4. Detect dependency drift and undocumented changes\n\nDependencies often change behavior without changing major versions. New flags appear. Defaults shift. Help output evolves.\n\nIn one demo, an agent:\n\n- Installed dependencies\n- Inspected CLI help text\n- Diffed it against previous days\n- Found an undocumented flag\n- Filed an issue before maintainers even noticed\n\nThis requires semantic interpretation, not just diffs, which is why classical CI cannot handle it.\n\n“This is the first harbinger of the new phase of AI,” Idan says. “We’re moving from generation to reasoning.”\n\n### 5. Automated test-coverage burn down\n\nIn one experiment:\n\n- Test coverage went from ~5% to near 100%\n- 1,400+ tests were written\n- Across 45 days\n- For about ~$80 worth of tokens\n\nAnd because the agent produced small pull requests daily, developers reviewed changes incrementally.\n\n### 6. Background performance improvements\n\nLinters and analyzers don’t always catch performance pitfalls that depend on understanding the code’s intent.\n\nExample: compiling a regex inside a function call so it compiles on every invocation.\n\nAn agent can:\n\n- Recognize the inefficiency\n- Rewrite the code to pre-compile the regex\n- Open a pull request with an explanation\n\nSmall things add up, especially in frequently called code paths.\n\n### 7. Automated interaction testing (using agents as deterministic play-testers)\n\nThis was one of the more creative demos from Universe: using agents to play a simple platformer game thousands of times to detect UX regressions.\n\nStrip away the game, and the pattern is widely useful:\n\n- Onboarding flows\n- Multi-step forms\n- Retry loops\n- Input validation\n- Accessibility patterns under interaction\n\nAgents can simulate user behavior at scale and compare variants.\n\n## How to build your first agentic workflow\n\nDevelopers don’t need a new CI system or separate infrastructure to try this. The GitHub Next prototype (gh aw) uses a simple pattern:\n\n**1. Write a natural-language rule in a Markdown file**\n\nFor example:\n\n``` --- on: daily permissions: read safe-outputs: create-issue: title-prefix: \"[news] \" --- Analyze the recent activity in the repository and:\n- create an upbeat daily status report about the activity\n- proviate an agentic task description to improve the project based on the activity.\nCreate an issue with the report. ```\n\n**2. Compile it into an action**\n\n``` gh aw compile daily-team-status ```\n\nThis generates a GitHub Actions workflow.\n\n**3. Review the YAML**\n\nNothing is hidden. You can see exactly what the agent will do.\n\n**4. Push to your repository**\n\nThe agentic workflow begins executing in response to repository events or on a schedule you define, **just like any other action**.\n\n**5. Review the issue it creates**\n\n## Patterns to watch next\n\nWhile still early, several trends are already emerging in developer workflows:\n\n**Pattern 1: Natural-language rules will become a part of automation**\n\nDevelopers will write short English rules that express intent:\n\n- “Keep translations current”\n- “Flag performance regressions”\n- “Warn on auth patterns that look unsafe”\n\n**Pattern 2: Repositories will begin hosting a fleet of small agents**\n\nNot one general agent, but many small ones with each responsible for one chore, one check, or one rule of thumb.\n\n**Pattern 3: Tests, docs, localization, and cleanup will shift into “continuous” mode**\n\nThis mirrors the early CI movement: Not replacing developers, but changing when chores happen from “when someone remembers” to “every day.”\n\n**Pattern 4: Debuggability will win over complexity**\n\nDevelopers will adopt agentic patterns that are transparent, auditable, and diff-based—not opaque systems that act without visibility.\n\n## What developers should take away\n\n“Custom agents for offline tasks, that’s what Continuous AI is,” Idan says. “Anything you couldn’t outsource before, you now can.”\n\n**More precisely: many judgment-heavy chores that were previously manual can now be made continuous.**\n\nThis requires a mental shift, like moving from owning files to streaming music.\n\n“You already had all the music,” Idan says. “But suddenly the player is helping you discover more.”\n\n## Start with one small workflow\n\nContinuous AI is not an all-or-nothing paradigm. You don’t need to overhaul your pipeline. Start with something small:\n\n- Translate strings\n- Add missing tests\n- Check for docstring drift\n- Detect dependency changes\n- Flag subtle performance issues\n\nEach of these is something agents can meaningfully assist with today.\n\nIdentify the recurring judgment-heavy tasks that quietly drain attention, and make those tasks continuous instead of episodic.\n\nIf CI automated rule-based work over the past decade, Continuous AI may do the same for select categories of judgment-based work, when applied deliberately and safely.\n\n[Explore Continuous AI Actions and frameworks &gt;](https://github.com/githubnext/awesome-continuous-ai)",
  "PubDate": "2026-02-05T17:00:00+00:00",
  "FeedUrl": "https://github.blog/feed/",
  "Title": "Continuous AI in practice: What developers can automate today with agentic CI",
  "FeedName": "The GitHub Blog",
  "Author": "GitHub Staff",
  "FeedLevelAuthor": "The GitHub Blog",
  "Link": "https://github.blog/ai-and-ml/generative-ai/continuous-ai-in-practice-what-developers-can-automate-today-with-agentic-ci/",
  "Tags": [
    "AI & ML",
    "Continuous AI",
    "Generative AI"
  ],
  "OutputDir": "_news"
}
