{
  "FeedLevelAuthor": "The GitHub Blog",
  "Tags": [
    "Company news",
    "GitHub Availability Report",
    "News & insights"
  ],
  "EnhancedContent": "In December, we experienced five incidents that resulted in degraded performance across GitHub services.\n\n**December 08 19:51 UTC (lasting 1 hour and 15 minutes)**\n\nBetween November 26, 2025, at 02:24 UTC and December 8, 2025 at 20:26 UTC, enterprise administrators experienced a disruption when viewing agent session activities in the Enterprise AI Controls page. During this period, users were unable to list agent session activity in the AI Controls view. This did not impact viewing agent session activity in audit logs, directly navigating to individual agent session logs, or otherwise managing AI agents.\n\nThe issue was caused by a misconfiguration in a change deployed on November 25 that unintentionally prevented data from being published to an internal Kafka topic responsible for feeding the AI Controls page with agent session activity information.\n\nThe problem was identified and mitigated on December 8 by correcting the configuration issue. GitHub is improving monitoring for data pipeline dependencies and enhancing pre-deployment validation to catch configuration issues before they reach production.\n\n**December 15 17:43 UTC (lasting 39 minutes)**\n\nOn December 15, 2025, between 15:15 UTC and 18:22 UTC, Copilot Code Review experienced a service degradation that caused 46.97% of pull request review requests to fail, requiring users to re-request a review. Impacted users saw the error message: “Copilot encountered an error and was unable to review this pull request. You can try again by re-requesting a review.” The remaining requests completed successfully.\n\nThe degradation was caused by elevated response times in an internal, model-backed dependency, which led to request timeouts and backpressure in the review processing pipeline, resulting in sustained queue growth and failed review completion.\n\nWe mitigated the issue by temporarily bypassing fix suggestions to reduce latency, increasing worker capacity to drain the backlog, and rolling out a model configuration change that reduced end-to-end latency. Queue depth and request success rates returned to normal and remained stable through peak traffic.\n\nFollowing the incident, we increased baseline worker capacity, added instrumentation for worker utilization and queue health, and are improving automatic load-shedding, fallback behavior, and alerting to reduce time to detection and mitigation for similar issues.\n\n**December 18 16:33 UTC (lasting 1 hour and 8 minutes)**\n\nOn December 18, 2025, from 08:15 UTC to 17:11 UTC, some GitHub Actions runners experienced intermittent timeouts for Github API calls, which led to failures during runner setup and workflow execution. This was caused by network packet loss between runners in the West US region and one of GitHub’s edge sites. Approximately 1.5% of jobs on larger and standard hosted runners in the West US region (0.28% of all Actions jobs) were impacted during this period.\n\nBy 17:11 UTC, all traffic was routed away from the affected edge site, mitigating the timeouts. We are working to improve early detection of cross-cloud connectivity issues and faster mitigation paths to reduce the impact of similar issues in the future.\n\n**December 18 17:36 UTC (lasting 1 hour and 33 minutes)**\n\nOn December 18, 2025, between 16:25 UTC and 19:09 UTC, the service underlying Copilot policies was degraded and users, organizations, and enterprises were not able to update any policies related to Copilot. No other GitHub services, including other Copilot services, were impacted. This was due to a database migration causing a schema drift.\n\nWe mitigated the incident by synchronizing the schema. We have hardened the service to make sure schema drift does not cause any further incidents, and we will investigate improvements in our deployment pipeline to shorten time to mitigation in the future.\n\n**December 22 22:31 UTC (lasting 1 hour and 46 minutes)**\n\nOn December 22, 2025, between 22:01 UTC and 22:32 UTC, unauthenticated requests to github.com were degraded, resulting in slow or timed out page loads and API requests. Unauthenticated requests from Actions jobs, such as release downloads, were also impacted. Authenticated traffic was not impacted. This was due to a severe spike in traffic, primarily to search endpoints.\n\nOur immediate response focused on identifying and mitigating the source of the traffic increase, which along with automated traffic management restored full service for our users.\n\nWe improved limiters for load to relevant endpoints and are continuing work to more proactively identify these large changes in traffic volume, improve resilience in critical request flows, and improve our time to mitigation.\n\nFollow our [status page](https://www.githubstatus.com/) for real-time updates on status changes and post-incident recaps. To learn more about what we’re working on, check out the engineering section on the [GitHub Blog](https://github.blog/category/engineering/).\n\n## Tags:\n\n- [GitHub Availability Report](https://github.blog/tag/github-availability-report/)\n\n## Written by\n\n![Jakub Oleksy](https://avatars.githubusercontent.com/u/6147691?v=4&#038;s=200)",
  "PubDate": "2026-01-14T22:06:49+00:00",
  "OutputDir": "_news",
  "Description": "In December, we experienced five incidents that resulted in degraded performance across GitHub services.\n\nThe post [GitHub Availability Report: December 2025](https://github.blog/news-insights/company-news/github-availability-report-december-2025/) appeared first on [The GitHub Blog](https://github.blog).",
  "Link": "https://github.blog/news-insights/company-news/github-availability-report-december-2025/",
  "ProcessedDate": "2026-01-14 23:01:57",
  "Author": "Jakub Oleksy",
  "FeedName": "The GitHub Blog",
  "FeedUrl": "https://github.blog/feed/",
  "Title": "GitHub Availability Report: December 2025"
}
