{
  "FeedUrl": "https://www.microsoft.com/en-us/security/blog/feed/",
  "EnhancedContent": "The rapid adoption of AI applications, including agents, orchestrators, and autonomous workflows, represents a significant shift in how software systems are built and operated. Unlike traditional applications, these systems are active participants in execution. They make decisions, invoke tools, and interact with other systems on behalf of users. While this evolution enables new capabilities, it also introduces an expanded and less familiar attack surface.\n\nSecurity discussions often focus on prompt-level protections, and that focus is justified. However, prompt security addresses only one layer of risk. Equally important is securing the AI application supply chain, including the frameworks, SDKs, and orchestration layers used to build and operate these systems. Vulnerabilities in these components can allow attackers to influence AI behavior, access sensitive resources, or compromise the broader application environment.\n\nThe recent disclosure of **[CVE-2025-68664](https://nvd.nist.gov/vuln/detail/CVE-2025-68664)**, known as LangGrinch, in LangChain Core highlights the importance of securing the AI supply chain. This blog uses that real-world vulnerability to illustrate how Microsoft Defender posture management capabilities can help organizations identify and mitigate AI supply chain risks.\n\n## **Case example: Serialization injection in LangChain (CVE-2025-68664)**\n\nA recently disclosed vulnerability in LangChain Core highlights how AI frameworks can become conduits for exploitation when workloads are not properly secured. Tracked as CVE-2025-68664 and commonly referred to as LangGrinch, this flaw exposes risks associated with insecure deserialization in agentic ecosystems that rely heavily on structured metadata exchange.\n\n### **Vulnerability summary**\n\nCVE-2025-68664 is a serialization injection vulnerability affecting the `langchain-core` Python package. The issue stems from improper handling of internal metadata fields during the serialization and deserialization process. If exploited, an attacker could:\n\n- Extract secrets such as environment variables without authorization\n- Instantiate unintended classes during object reconstruction\n- Trigger side effects through malicious object initialization\n\nThe vulnerability carries a CVSS score of 9.3, highlighting the risks that arise when AI orchestration systems do not adequately separate control signals from user-supplied data.\n\n**Understanding the root cause: The lc marker**\n\nLangChain utilizes a custom serialization format to maintain state across different components of an AI chain. To distinguish between standard data and serialized LangChain objects, the framework uses a reserved key called lc. During deserialization, when the framework encounters a dictionary containing this key, it interprets the content as a trusted object rather than plain user data.\n\nThe vulnerability originates in the `dumps()` and `dumpd()` functions in affected versions of the `langchain-core` package. These functions did not properly escape or neutralize the `lc` key when processing user-controlled dictionaries. As a result, if an attacker is able to inject a dictionary containing the `lc` key into a data stream that is later serialized and deserialized, the framework may reconstruct a malicious object.\n\nThis is a classic example of an injection flaw where data and control signals are not properly separated, allowing untrusted input to influence the execution flow.\n\n**Mitigation and protection guidance**\n\nMicrosoft recommends that all organizations using LangChain review their deployments and apply the following mitigations immediately.\n\n**1. Update LangChain Core**\n\nThe most effective defense is to upgrade to a patched version of the langchain-core package.\n\n- For 0.3.x users: [Update](https://github.com/langchain-ai/langchain/pull/34455) to version 0.3.81 or later.\n- For 1.x users: [Update](https://github.com/langchain-ai/langchain/pull/34458) to version 1.2.5 or later.\n\n**2. Query the security explorer to identify any instances of LangChain in your environment**\n\nTo identify instances of LangChain package in the assets protected by Defender for Cloud, customers can use the [Cloud Security Explorer](https://learn.microsoft.com/en-us/azure/defender-for-cloud/how-to-manage-cloud-security-explorer):\n\n![](https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2026/01/image-34.webp)\n\n\\*Identification in cloud compute resources requires D-CSPM / Defender for Containers / Defender for Servers plan.\n\n\\*Identification in code environment requires connecting your code environment to Defender for Cloud [Learn how to set up connectors](https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-github)\n\n**3. Remediate based on Defender for Cloud recommendations across the software development cycle: Code, Ship, Runtime**\n\n\\*Identification in cloud compute resources requires D-CSPM / Defender for Containers / Defender for Servers plan.\n\n\\*Identification in code environment requires connecting your code environment to Defender for Cloud [Learn how to set up connectors](https://learn.microsoft.com/azure/defender-for-cloud/quickstart-onboard-github)\n\n**4. Create GitHub issues with runtime context directly from Defender for Cloud, track progress, and use Copilot coding agent for AI-powered automated fix**\n\n![](https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2026/01/image-35.webp)\n\n[Learn more about Defender](https://learn.microsoft.com/en-us/azure/defender-for-cloud/github-advanced-security-overview) for Cloud seamless workflows with GitHub to shorten remediation times for security issues.\n\n**Microsoft Defender XDR detections**\n\nMicrosoft security products provide several layers of defense to help organizations identify and block exploitation attempts related to AI vulnerable software.\n\nMicrosoft Defender provides visibility into vulnerable AI workloads through its [Cloud Security Posture Management (DCSPM)](https://learn.microsoft.com/en-us/azure/defender-for-cloud/concept-cloud-security-posture-management).\n\n**Vulnerability Assessment:** Defender for Cloud scanners have been updated to identify containers and virtual machines running vulnerable versions of langchain-core. Microsoft Defender is actively working to expand coverage to additional platforms and this blog will be updated when more information is available.\n\n![](https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2026/01/image-36.webp)\n\n**Hunting queries**\n\n**Microsoft Defender XDR**\n\nSecurity teams can use the advanced hunting capabilities in Microsoft Defender XDR to proactively look for indicators of exploitation. A common sign of exploitation is a Python process associated with LangChain attempting to access sensitive environment variables or making unexpected network connections immediately following an LLM interaction.\n\nThe following Kusto Query Language (KQL) query can be used to identify devices that are using the vulnerable software:\n\n```\n\nDeviceTvmSoftwareInventory | where SoftwareName has \"langchain\" and ( // Lower version ranges SoftwareVersion startswith \"0.\" and toint(split(SoftwareVersion, \".\")[1]) ```\n\n**References**\n\n- [NVD - CVE-2025-68664](https://nvd.nist.gov/vuln/detail/CVE-2025-68664)\n- [All I Want for Christmas is Your Secrets: LangGrinch hits LangChain Core (CVE-2025-68664) - Cyata | The Control Plane for Agentic Identity](https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/)\n- [fix(core): serialization patch by mdrxy · Pull Request #34458 · langchain-ai/langchain · GitHub](https://github.com/langchain-ai/langchain/pull/34458)\n- [fix(core): serialization patch by ccurme · Pull Request #34455 · langchain-ai/langchain · GitHub](https://github.com/langchain-ai/langchain/pull/34455)\n- [What is Cloud Security Posture Management (CSPM) - Microsoft Defender for Cloud | Microsoft Learn](https://learn.microsoft.com/en-us/azure/defender-for-cloud/concept-cloud-security-posture-management)\n- [Build queries with cloud security explorer - Microsoft Defender for Cloud | Microsoft Learn](https://learn.microsoft.com/en-us/azure/defender-for-cloud/how-to-manage-cloud-security-explorer)\n\n*This research is provided by Microsoft Defender Security Research with contributions from Tamer Salman, Astar Lev, Yossi Weizman, Hagai Ran Kestenberg, and Shai Yannai.*\n\n**Learn more**\n\nReview our documentation to learn more about our real-time protection capabilities and see how to enable them within your organization.\n\nLearn more about [securing Copilot Studio agents with Microsoft Defender](https://learn.microsoft.com/en-us/defender-cloud-apps/ai-agent-protection)\n\nLearn more about [Protect your agents in real-time during runtime (Preview) – Microsoft Defender for Cloud Apps | Microsoft Learn](https://learn.microsoft.com/en-us/defender-cloud-apps/real-time-agent-protection-during-runtime)\n\nExplore [how to build and customize agents with Copilot Studio Agent Builder](https://eurppc-word-edit.officeapps.live.com/we/%E2%80%A2%09https:/learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/copilot-studio-agent-builder)",
  "Title": "Case study: Securing AI application supply chains",
  "ProcessedDate": "2026-01-30 21:05:11",
  "OutputDir": "_news",
  "FeedName": "Microsoft Security Blog",
  "FeedLevelAuthor": "Microsoft Security Blog",
  "Tags": [],
  "Link": "https://www.microsoft.com/en-us/security/blog/2026/01/30/case-study-securing-ai-application-supply-chains/",
  "Author": "Microsoft Defender Security Research Team",
  "PubDate": "2026-01-30T18:49:44+00:00",
  "Description": "Securing AI-powered applications requires more than just safeguarding prompts. Organizations must adopt a holistic approach that includes monitoring the AI supply chain, assessing frameworks, SDKs, and orchestration layers for vulnerabilities, and enforcing strong runtime controls for agents and tools. Leveraging visibility into these components allows security teams to detect, respond to, and remediate risks before they can be exploited.\n\nThe post [Case study: Securing AI application supply chains](https://www.microsoft.com/en-us/security/blog/2026/01/30/case-study-securing-ai-application-supply-chains/) appeared first on [Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog)."
}
