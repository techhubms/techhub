{
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "Author": "Microsoft Fabric Blog",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/fabric-february-2026-feature-summary/",
  "Title": "Fabric February 2026 Feature Summary",
  "OutputDir": "_news",
  "PubDate": "2026-02-25T10:00:45+00:00",
  "FeedName": "Microsoft Fabric Blog",
  "Description": "Welcome to the February 2026 Microsoft Fabric update! This month brings a wide range of enhancements across the Fabric platform—from improvements to the OneLake Catalog and developer experiences, to meaningful updates in Data Engineering, Data Factory, Real‑Time Intelligence, and more. Whether you’re building, operating, or scaling solutions in Fabric, there’s plenty here to explore. And …\n\n[Continue reading “Fabric February 2026 Feature Summary”](https://blog.fabric.microsoft.com/en-us/blog/fabric-february-2026-feature-summary/)",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "Tags": [],
  "ProcessedDate": "2026-02-25 17:21:15",
  "EnhancedContent": "## Welcome to the February 2026 Microsoft Fabric update!\n\nThis month brings a wide range of enhancements across the Fabric platform—from improvements to the OneLake Catalog and developer experiences, to meaningful updates in Data Engineering, Data Factory, Real‑Time Intelligence, and more. Whether you’re building, operating, or scaling solutions in Fabric, there’s plenty here to explore. And with [FabCon](https://aka.ms/fabcon) just weeks away, February’s updates are a great preview of what’s ahead.\n\n### Don’t miss your chance to get Fabric certified for FREE\n\nIf you are ready to take your Fabric exam in the next month, the Fabric team would like to give you a 100% voucher to cover the cost.\n\n[Request a voucher](https://aka.ms/fabcert/2026) by February 28, 2026. Terms and conditions apply.\n\n### Three weeks until FabCon – will we see you there?\n\nJoin us for the ultimate Power BI, Microsoft Fabric SQL, Real-Time Intelligence, AI, and Databases community-led event from March 16-20, 2026, in Atlanta, GA. The third annual FabCon Americas will feature sessions from your favorite Microsoft and community speakers, keynotes, more opportunities to Ask the Experts for 1:1 support, an engaging community lounge with opportunities to network and connect with your peers, a dedicated partner pre-day, a packed expo hall, attendee favorites Power Hour and the Data Viz World Championships live finals, and a can’t-miss attendee party at the Georgia Aquarium.\n\n[Register](https://aka.ms/fabcon) with code FABCOMM to save $200.\n\n## Contents\n\n- - Fabric Platform\n- Workspace Apps now in the OneLake Catalog\n- Streamlined item details\n- Managing Fabric Identity limits within your tenant\n- Horizontal Tab Display Settings\n- Data Engineering\n- Enhanced notebook version history with multiple sources\n- Python notebooks add %run support\n- Full size mode in Fabric notebook 11\n- Announcing Private Link Support for Microsoft Fabric API for GraphQL\n- CI/CD for API for GraphQL (Generally Available)\n- Support for default arguments for Fabric user data functions\n- Microsoft ODBC Driver for Microsoft Fabric Data Engineering (Preview)\n- Customer Managed Key Encryption Support for Notebook Code\n- Data Science\n- Semantic Link 0.13.0 is Live\n- Monitoring Real-Time Scoring Model Endpoints\n- Data Warehouse\n- Export migration summary\n- SQL Pool Insights\n- Real-Time Intelligence\n- Effortless Real-Time Data Connection\n- Streaming real-time data from private networks into RTI with Eventstream connectors\n- Faster insights: real-time dashboard performance improvements\n- Data Factory\n- Recent data: Get back to your data faster (Preview)\n- Improvements to the Fabric variable libraries integration in Dataflow Gen2\n- Relative references with Fabric connectors in Dataflow Gen2\n- Introducing Dataflow Gen2’s just-in-time publishing mechanism\n- Modern Evaluator for Dataflow Gen2 (Generally Available)\n- Incremental copy from Fabric Lakehouse now supports both CDF and watermark-based methods in Copy job\n- SAP Datasphere outbound for Amazon S3 and Google cloud storage in Copy job\n- Column Mapping in CDC for Copy Job\n- Rowversion now supported as an incremental column in SQL database Copy job\n- Copy job activity now supports Service Principal and Workspace identity authentication\n- Parallel Read Support for Large CSV Dataset\n- Adaptive Performance Tuning: Intelligent Optimization for Data Movement (Preview)\n- Other\n- Fabric VS Code extension for browsing, editing item definitions, and MCP support\n\n## February Monthly Update Video\n\n## Fabric Platform\n\n### Workspace Apps now in the OneLake Catalog\n\nWorkspace Apps (Apps V2) are now supported in the OneLake Catalog, so you can discover, browse, and open apps directly from the **Insights category**. This category focuses on business-ready content designed to help you analyze, visualize, and report on data to drive actionable insights.\n\nWith this update, Workspace Apps appear alongside other business-facing content such as organizational apps and reports, **making it easier to explore all relevant insights in one place without needing to switch between different experiences**.\n\nThe Catalog also surfaces key metadata for each Workspace App, helping you quickly understand what the app contains before opening it. From there, you can open the app directly and start exploring insights right away.\n\nWith the addition of Workspace Apps, **the OneLake Catalog now includes all item types available in Microsoft Fabric**, making it the central place to discover, understand, and access your Fabric content.\n\n![OneLake Catalog Insights category view showing Workspace Apps available for discovery and direct access, alongside other business-ready content.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/onelake-catalog-insights-category-view-showing-wor.png)\n\n*Figure: Workspace Apps displayed in the OneLake Catalog under the Insights category.*\n\nLearn more about the OneLake catalog in the [OneLake catalog overview](https://learn.microsoft.com/fabric/governance/onelake-catalog-overview) documentation.\n\n### Streamlined item details\n\nThe updated [Item Details experience](https://blog.fabric.microsoft.com/blog/processing-cdc-streams-using-fabric-eventstreams-sql/) now extends beyond the OneLake Catalog to include full-page experiences for items accessed from outside the catalog. For example, when you open a Semantic Model directly from Workspaces, you will now see a modern, unified details page that matches the streamlined in-context experience found within the OneLake Catalog.\n\nThis update brings a consistent design language across Fabric, offering improved usability and access to richer metadata: the enhanced details page now features the complete schema of all OneLake stored data items, making it easier to understand item characteristics.\n\nIt also shows and visualizes item-level lineage, managing permissions and monitoring run/refresh history, all in one place. You can quickly find this key information, whether they’re navigating through the catalog or opening items in a standalone context.\n\n![The new item details page, showing the full schema of the Lakehouse among other key metadata and actions.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/the-new-item-details-page-showing-the-full-schema.png)\n\n*Figure: A Lakehouse in the new item details experience*\n\n### Managing Fabric Identity limits within your tenant\n\nFabric Identity governance at scale just got easier. We are introducing a new tenant admin setting that gives you control over the maximum number of Fabric identities (hence Workspace identities) in your organization.\n\n**With this update, Fabric tenant admins can:**\n\n- Scale beyond previous constraint—the default limit for number of Fabric identities in an organization increases from 1,000 to 10,000 identities.\n- Set custom limits for how many Fabric identities can be created in their tenant.\n- Manage limits programmatically by using the [Update Tenant Setting REST API](https://learn.microsoft.com/rest/api/fabric/admin/tenants/update-tenant-setting).\n\n**How it works:**\n\nThe new setting “Define maximum number of Fabric identities in a tenant” is in the Fabric Admin portal in Tenant settings, within Developer settings.\n\nWhen the setting is disabled (the default), your tenant supports up to 10,000 Fabric identities—a 10x increase from the previous limit. Enable the setting to specify your own maximum. The value you enter becomes the upper limit for Fabric identity creation across your tenant.\n\n**Note:** Fabric doesn’t validate that your custom limit falls within your Entra ID resource quota. Before setting a custom limit, check your organizations [Entra ID service limits](https://learn.microsoft.com/entra/identity/users/directory-service-limits-restrictions).\n\nIf a workspace admin tries to create a new workspace identity that would exceed the limit, they’ll see a clear error message explaining the reason.\n\n![Figure1. Configuring Maximum number of Fabric identities in a tenant. In the Admin portal, under Developer settings, you can enable the ability to configure the maximum number of identities in the tenant. Default is 10000.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure1-configuring-maximum-number-of-fabric-iden.png)\n\n*Figure: Configuring Maximum number of Fabric Identities in a tenant.*\n\nYou can also manage this setting programmatically using the [Update Tenant Setting API](https://learn.microsoft.com/rest/api/fabric/admin/tenants/update-tenant-setting).\n\n**Sample HTTP request:**\n\n``` POST https://api.fabric.microsoft.com/v1/admin/tenantsettings/ConfigureFabricIdentityTenantLimit/update{ \"enabled\": true, \"properties\": [ { \"name\": \"FabricIdentityTenantLimit\", \"value\": \"100\", \"type\": \"int\" } ] }Sample JSON response:Status code: 200{ \"tenantSettings\": [ { \"settingName\": \"ConfigureFabricIdentityTenantLimit\", \"title\": \"Define maximum number of Fabric identities in a tenant\", \"enabled\": true, \"canSpecifySecurityGroups\": false, \"tenantSettingGroup\": \"Developer settings\", \"properties\": [ { \"name\": \"FabricIdentityTenantLimit\", \"value\": \"100\", \"type\": \"Integer\" } ] } ] } ```\n\nTo learn more about identities in Fabric, see the [documentation](https://learn.microsoft.com/fabric/security/workspace-identity).\n\nFor more information about all the tenant admin settings in Fabric, see the [Tenant settings index](https://learn.microsoft.com/fabric/admin/tenant-settings-index).\n\n### Horizontal Tab Display Settings\n\nTo give developers more control over how they navigate open items in Microsoft Fabric, we’ve introduced new **horizontal tab display settings**. These settings let you tailor how tabs appear across the top of the Fabric interface—helping you stay organized and maintain focus during complex multitasking workflows.\n\nWhat’s new:\n\n**Open the tab settings menu** to quickly access tab display options by right‑clicking any tab.\n\n![Right-click menu on a tab showing the “Open horizontal tab setting” option highlighted.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/right-click-menu-on-a-tab-showing-the-open-horizo.png)\n\n*Figure: Open the horizontal tab display settings directly from a tab’s right-click menu.*\n\n**Two display modes**\n\n- - **Full tab names** always show each tab’s full name for maximum clarity.\n- **Adaptive truncated names** automatically shorten names when space is limited, allowing more tabs to remain visible.\n\n![Preferences page in Settings showing horizontal tab settings with options for full tab names and adaptive truncated names](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/preferences-page-in-settings-showing-horizontal-ta.png)\n\n*Figure: Horizontal tab display modes, including full tab names and adaptive truncated names, configured in Preferences page in Settings.*\n\n**Overflow menu** When space runs out, tabs automatically collapse into a clean overflow list, making it easy to jump to any open item. ![Overflow menu displaying a list of open tabs when horizontal tab space is limited.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/overflow-menu-displaying-a-list-of-open-tabs-when.png)\n\n*Figure: Tabs automatically move into an overflow menu when there is insufficient space in the horizontal tab bar.*\n\nThese enhancements streamline navigation for developers working across multiple items and workspaces, reducing friction and improving overall multitasking efficiency. Find more details in this [documentation](https://review.learn.microsoft.com/fabric/fundamentals/fabric-home?branch=pr-en-us-12549#customize-horizontal-tab-display).\n\n## Data Engineering\n\n### Enhanced notebook version history with multiple sources\n\nKeeping track of how a notebook evolves gets tricky when changes can come from different entry points—editing in the Fabric portal, syncing from source control, or other update flows. Fabric notebooks seamlessly integrate with Git, deployment pipelines, and Visual Studio Code. Each saved version is automatically captured in the notebook’s version history. Versions may originate from direct edits within the notebook, Git synchronizations, deployment pipeline, or publishing via VS Code. The source of each version is clearly labeled in version history to provide full traceability.\n\n![Image of version history list with multiple sources indicator. ](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/image-of-version-history-list-with-multiple-source.png)*Figure: Multiple sources records in notebook version history*\n\nWith the Enhanced Notebook Version History with Multiple Sources support, Fabric Notebooks now surface a clearer, more trustworthy history by reflecting versions from multiple origins, helping you trace changes, collaborate with confidence, and roll back to the right point when needed. Especially in CI/CD workflows (Git sync, deployment pipeline, public API), and team-authored workflows.\n\nLearn more about version history in the [Version history](https://learn.microsoft.com/fabric/data-engineering/how-to-use-notebook#version-history) documentation.\n\n### Python notebooks add %run support\n\nPython developers often want to keep notebooks modular—shared utilities, setup logic, and reusable helpers shouldn’t be copy‑pasted everywhere. Python notebooks now support %run, enabling a familiar pattern for executing shared “code modules” and reusing logic across notebooks. This makes it easier to structure projects cleanly, iterate faster, and maintain common code in a single place.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/word-image-33579-8.png)*Figure: Reference another python notebook with intellisense*\n\n![Reference run python notebook](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/reference-run-python-notebook.png)*Figure: Reference run python notebook*\n\nYou can use %run to reference and execute other notebooks within the same execution context, allowing you to directly call functions and reuse variables defined in those notebooks.\n\nCurrently, %run in Python notebooks supports referencing notebook items only. Support for running code modules (such as .py files) from the notebook resources folder is coming soon—stay tuned.\n\nYou can reference the [reference run a notebook](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#reference-run-a-notebook) documentation for the detailed usage.\n\n### Full size mode in Fabric notebook\n\nFull-size mode of cells is now available on Fabric notebook. When you’re working on a long or complex cell, the surrounding UI can get in the way. Full Size Mode lets you expand a single cell to fill the notebook for distraction‑free editing—ideal for deep refactors, large SQL or Python blocks, or screensharing. In full‑size mode, you retain full editing capability, stay focused on the selected cell, and can conveniently navigate to the previous or next cell without leaving the focused view.\n\n![Entry of full size mode](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/entry-of-full-size-mode.png)\n\n*Figure: Enable full size mode on cell toolbar.*\n\n![Image of a full size mode notebook example](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/image-of-a-full-size-mode-notebook-example.png)\n\n*Figure: Example of full-size mode.*\n\nLearn more: [Develop, execute, and manage notebooks](https://learn.microsoft.com/fabric/data-engineering/author-execute-notebook#fullsize-mode-on-a-cell)\n\n### Announcing Private Link Support for Microsoft Fabric API for GraphQL\n\n**Microsoft Fabric API for GraphQL now supports Tenant Level Private Link**, bringing enterprise-grade network security to your data APIs. This highly requested feature enables organizations to access their GraphQL APIs through private connectivity, ensuring data traffic never traverses the public internet.\n\n- **Secure data access with Private Link:** This feature allows organizations to access GraphQL APIs through Microsoft’s private backbone network, improving security by preventing exposure to public internet threats and supporting compliance requirements.\n- **Simplified network management:** Private Link reduces the need for complex firewall rules or VPN setups by allowing API calls only through approved private endpoints, easing governance and integration with existing Azure Private Link infrastructure.\n- **Enterprise-ready security model:** Enabling Private Link at the tenant level integrates GraphQL APIs into a secured network environment complemented by Microsoft Entra ID authentication and flexible security options like single sign-on and saved credentials.\n\nTo enable private link, update your tenant admin settings to enable **Azure private link**.\n\n![Screenshot showing Azure Private Link tenant setting.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-showing-azure-private-link-tenant-setti.png)\n\n*Figure: Enable private link for your tenant*\n\nLearn more [Private link support for API for GraphQL](https://aka.ms/graphql-privatelink).\n\n### CI/CD for API for GraphQL (Generally Available)\n\nWith this release, we have made improvements to reliability and performance on of the experience with Fabric CI/CD and deployment pipelines experience. Your teams can manage GraphQL artifacts in Git, collaborate with familiar pull-request workflows, and promote changes across environments using CI/CD—bringing the same engineering rigor to APIs that you already use for code and data.\n\nWith CI/CD support, you can do the following:\n\n- **Git-enabled source control** for your GraphQL API artifacts so you can version, review, and roll back changes.\n- **Support with Fabric deployment pipelines** that allow you to build release pipelines for managing API for GraphQL items.\n- **Improved collaboration** with pull requests, code reviews, and branching strategies applied to API changes.\n\n![Screenshot of source control changes for API for graphQL item](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-of-source-control-changes-for-api-for-g.png)\n\n*Figure: Screenshot of source control for API for GraphQL*\n\nLearn more about [API for GraphQL CI/CD and source control](https://aka.ms/graphql-cicd-ga).\n\n### Support for default arguments for Fabric user data functions\n\nFabric User data functions now support default argument values, allowing omitted arguments to use preset defaults, which simplifies function calls and enhances code flexibility. This feature supports various input types including strings, boolean, floats, int, arrays, and objects. Functions become more versatile as they can handle common use cases with fewer arguments, while still allowing for customization when needed.\n\n![Screenshot of user data function with sample code using default arguments to a label for a record](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-of-user-data-function-with-sample-code.png)\n\n*Figure: Code snippet of using function with default arguments*\n\nLearn more about [default arguments for user data functions](https://aka.ms/udf-defaultargs).\n\n### Microsoft ODBC Driver for Microsoft Fabric Data Engineering (Preview)\n\nODBC (Open Database Connectivity) is a widely adopted industry standard that enables applications to connect to and work with data across databases and big data platforms. Today, we’re introducing the Microsoft ODBC Driver for Microsoft Fabric Data Engineering (Preview) – an enterprise‑grade connector that delivers secure, reliable, and flexible Spark SQL connectivity for .NET, Python, and other ODBC‑compatible applications and BI tools, all powered through Microsoft Fabric’s Livy APIs.\n\nBuilt specifically for Fabric Data Engineering, this driver offers deep integration with OneLake and Lakehouse data, supports environment‑based execution, and enables flexible Spark configuration tailored to your workloads. With full ODBC 3.x compliance, Microsoft Entra ID authentication, comprehensive Spark SQL and data type support, performance optimizations for large datasets, and enterprise‑ready features like proxy support and session reuse, the Microsoft ODBC Driver helps teams accelerate Spark‑powered data engineering with the security, reliability, and performance expected in modern enterprise environments.\n\n![Screenshot of ODBC Data Source Administrator window showing System Data Sources tab with one data source named &quot;Driver&quot; listed. Buttons for Add, Remove, and Configure are on the right side, with a description below explaining ODBC system data sources store information about how to connect to Fabric Spark.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-of-odbc-data-source-administrator-windo-1.gif)\n\n*Figure: The animated GIF demonstrates how to get started using ODBC driver*\n\nTo download and learn more about the Microsoft ODBC Driver for Microsoft Fabric Data Engineering, please refer to official documentation: [Microsoft ODBC Driver for Microsoft Fabric Data Engineering](https://learn.microsoft.com/fabric/data-engineering/spark-odbc-driver).\n\n### Customer Managed Key Encryption Support for Notebook Code\n\nEnterprise teams can now run **Microsoft Fabric Notebooks** in CMK‑enabled workspaces with **Notebook content and metadata encrypted at rest using customer‑owned keys** in Azure Key Vault, supporting stricter governance and compliance requirements without changing developer workflows.\n\nWhat’s new: Notebooks are fully supported in CMK‑enabled workspaces\n\nWith this update, **Notebooks can be created and used in workspaces where CMK encryption is enabled**, and the **Notebook content and associated Notebook metadata** stored as part of Data Engineering items are protected using the workspace’s customer‑managed key.\n\nConcretely, this covers core Notebook content artifacts such as **cell source, cell output, and cell attachments,** so the key you control can be applied consistently to what developers author and what the system stores for notebook execution and collaboration.\n\nTo enable CMK for your Fabric workspace (and use Notebooks in that CMK‑enabled workspace), follow the official documentation: [Customer‑managed keys for Fabric workspaces](https://learn.microsoft.com/fabric/security/workspace-customer-managed-keys)\n\n## Data Science\n\n### Semantic Link 0.13.0 is Live\n\nWith the 0.13.0 release, Semantic Link continues to expand its Fabric coverage and management capabilities. This update introduces new modules for lakehouse, reports, semantic models, SQL endpoints, and Spark, enabling end‑to‑end workspace operations—from creating and managing lakehouses and tables, to cloning and rebinding reports, refreshing and monitoring semantic models, and administering SQL and Spark settings.\n\nSeveral Fabric APIs are now surfaced consistently across modules, simplifying common workflows and improving API discoverability. The release also includes targeted API refinements and bug fixes, improving reliability for service principal authentication and correctness when evaluating measures. Overall, 0.13.0 makes it easier to manage Fabric assets programmatically at scale with stronger consistency and control. Explore the [release notes](https://pypi.org/project/semantic-link/0.13.0/#description).\n\nTo help you explore these scenarios in practice, we’ve also published three short demos showcasing [Sempy for data science](https://www.youtube.com/watch?v=F59zX6y2MqA), [Sempy for Power BI automation](https://www.youtube.com/watch?v=L6fmElJ3LE8), and [Sempy for data engineering](https://www.youtube.com/watch?v=YapQbuFUUNY), illustrating how Semantic Link can unify workflows across personas and accelerate development within Fabric.\n\n[Cast your vote](https://github.com/microsoft/semantic-link-labs/discussions/991) for additional capabilities from Semantic Link Labs to be included in Semantic Link.\n\n### Monitoring Real-Time Scoring Model Endpoints\n\nThe new monitoring experience for real‑time scoring endpoints in Microsoft Fabric provides clear visibility into request volume, error rates, and latency as models run in production. Teams can easily compare these metrics across endpoint versions to validate improvements, catch regressions early, and make confident rollout or rollback decisions based on real usage. From tracking adoption to diagnosing issues and ensuring consistent performance underload, endpoint monitoring helps teams move faster from insight to action—delivering more reliable ML experiences while staying focused on scaling impact and business value.\n\n## Data Warehouse\n\n### Export migration summary\n\n**Export Migration Summary** is a new capability in Migration Assistant that makes it simple, reliable, and secure to download your full migration results in formats that best fit your workflow.\n\nThe export option is available directly from the Migration Assistant’s summary view and full screen view.\n\n![Screenshot of a dropdown menu showing export options for data download. Options include &quot;Download as Excel file&quot; and &quot;Download as CSV,&quot; with an additional choice to &quot;Resize migration assistant pane.&quot;](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-of-a-dropdown-menu-showing-export-optio.png)\n\nFigure 1: Export menu\n\n![Screenshot of a user interface showing options to export data as Excel or CSV files under an &quot;Export&quot; dropdown menu. The interface also includes a &quot;Refresh&quot; button and a &quot;Submitted by Priyanka Langade&quot; label below the export options.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-of-a-user-interface-showing-options-to.png)\n\n*Figure 2: Export file formats*\n\nOnce triggered, the export runs reliably in the background, even if the Migration Assistant window is closed, ensuring a smooth workflow for large, multi-object migrations. The following output formats are supported:\n\n**Excel**\n\n- Fully structured workbook with two worksheets: Migrated Objects and Objects To Fix\n- MIP-compliant and aligned with your organization’s sensitivity labels.\n\n**CSV**\n\n- Lightweight and tool-friendly\n\nEach exported file provides a structured, comprehensive view of your migration results, including:\n\n| **Field** | **Description** | | --- | --- | | Object name | Name of the SQL object | | Object type | SQL object types such as table, view, function, stored procedure. | | State | Translation State<br><ul><br><li>Adjusted: Fabric Data Warehouse compatible updates are applied</li><br><br><li>Not adjusted: No change in the original script</li><br><br></ul> | | Details | List of adjustments applied or error messages | | Type of error | Type of error as Translation message, Translation error, Translation apply error |\n\n*Figure 3: Fields in exported file*\n\nThis structure enables teams to aggregate migration details at object or object type level and identify patterns across objects.\n\nExport Migration Summary removes a major blocker for customers who need shareable, reliable artifacts that reflect the true state of their migration progress.\n\nLearn more in the [Migration Assistant for Fabric](https://learn.microsoft.com/fabric/data-warehouse/migrate-with-migration-assistant)[Data Warehouse](https://learn.microsoft.com/fabric/data-warehouse/migrate-with-migration-assistant) documentation.\n\n### SQL Pool Insights\n\nUnderstanding why workloads slow down often require visibility beyond individual queries. SQL Pool Insights extends the existing Query Insights experience with pool‑level telemetry, helping you understand how resources are allocated and when pools are under pressure in Microsoft Fabric Data Warehouse.\n\n![The image shows a data monitoring dashboard with various tables and metrics, such as resource usage, workloads, and schema information. AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/the-image-shows-a-data-monitoring-dashboard-with-v.png)*Figure – The image shows the sql\\_pool\\_insights schema*\n\nWith SQL Pool Insights, you can:\n\nMonitor the health of built‑in **SELECT** and **NON SELECT** SQL pools.\n\n- Track **pressure events**, configuration changes, and capacity updates over time.\n- Correlate pool‑level pressure with query performance using existing Query Insights views.\n- Validate **resource isolation** between read‑optimized and write‑optimized workloads.\n- This feature adds a new system view — queryinsights.sql\\_pool\\_insights — that logs pool state changes and sustained pressure events, giving you actionable signals for troubleshooting performance issues and planning capacity more effectively.\n\n[Learn more about SQL Pool Insights](https://blog.fabric.microsoft.com/blog/30370/preview)\n\n## Real-Time Intelligence\n\n### Effortless Real-Time Data Connection\n\nConnecting data is often the first step on a user’s Real-Time Intelligence journey—and it should be effortless.\n\nPreviously, the left navigation in the Real-Time hub (RTH) included two separate entries for connecting data—Data sources and Azure sources. While well intentioned, this distinction didn’t always match how users think about the task at hand. The most common question wasn’t about categories; it was just how to add data.\n\nTo better reflect that reality, we’ve unified these entry points into a single menu item: **Add data**.\n\nWith this update:\n\n- There’s now one clear place to begin when connecting data.\n- The navigation focuses on intent, not source taxonomy.\n- Users can move faster without second‑guessing their choices.\n\nUnder the hood, nothing has changed. You still have access to the same rich set of data sources, including all out-of-box data connectors, Azure sources, Azure Diagnostics logs, and more. What’s changed is the experience—clearer, simpler, and designed to help you get value faster.\n\n**Note**: we are rolling this change out gradually, so you may see it in the coming weeks.\n\n![old design for Connecting to data in Real-Time hub](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/old-design-for-connecting-to-data-in-real-time-hub.png)\n\n*Figure: Real-Time hub left navigation before the change*\n\n![New design for Connecting to data in Real-Time hub](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/new-design-for-connecting-to-data-in-real-time-hub.png)*Figure: Real-Time hub left-navigation after the change*\n\n![New design for data connector](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/new-design-for-data-connector.png)\n\n*Figure: New data connector context menu*\n\nLearn more about [Real-Time Hub.](https://aka.ms/realtimehub) Try it out and share your [feedback](mailto:askrth@microsoft.com).\n\n### Streaming real-time data from private networks into RTI with Eventstream connectors\n\nReal-Time Intelligence Eventstream is designed to bring real-time data from diverse sources, transform it, and effortlessly route it to various destinations. For sources that run in private network environments, such as cloud virtual network or on-premises infrastructures, a secure method is required to allow Eventstream to access the source.\n\nThe streaming connector’s support for virtual networks (vNet) and on-premises environments offers a secure, managed pathway, enabling Eventstream to reliably connect with these private-network streaming sources.\n\nTo enable data transfer from a source within a private network into Eventstream, it is necessary to establish an Azure managed virtual network as an intermediary bridge, as illustrated in the diagram. The Azure virtual network should be connected to the private network hosting the data source using appropriate methods, such as VPN or ExpressRoute for on-premises scenarios, and private endpoints or network peering for Azure sources, etc. Subsequently, the Eventstream streaming connector instance will be injected into this virtual network through SWIFT injection, allowing secure connectivity between the connector and the data source located within the private network.\n\n![Diagram illustrating a data integration architecture connecting streaming data sources to a VPN ExpressRoute Gateway within an Azure Customer vNet. It highlights connections from on-prem Kafka clusters and CDC databases, third-party cloud Kafka clusters via VPN, and shows data flowing through a connector injected into the customer vNet, with screenshots of configuration interfaces included.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/diagram-illustrating-a-data-integration-architectu.png)\n\n*Figure: Eventstream connectors private network support overview*\n\nTo facilitate streaming connector vNet injection into an Azure virtual network you’ve created, Fabric provides a centralized location for network or data engineers to manage the references to Azure virtual network resources. The **streaming virtual network data gateway** in Fabric serves this purpose for Eventstream.\n\nUnlike ‘Virtual network data gateways’ and ‘On-premises data gateways’, this new option does not require cluster provisioning or additional capacity. However, the user experience across all three gateway types remains largely similar. Create and manage the ‘**streaming virtual network data gateway**’ in the ‘**Manage Connections and Gateways**’ page in Fabric. Select it when setting up streaming connections for Eventstream sources using the Get Events wizard or ‘Streaming virtual network’. After that, you can configure your Eventstream data source as usual.\n\n![Screenshot collage showing configuration steps for managing streaming virtual network data gateways in a cloud platform interface. ](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-collage-showing-configuration-steps-for.png)\n\n*Figure: Streaming virtual network data gateway configuration for Eventstream*\n\nFor a step-by-step guide on getting started, please refer to the document: [Connect to Streaming Sources in Virtual Network or On Premises with Eventstream](https://aka.ms/ESConnectorPrivateNetworkSupport).\n\n### Faster insights: real-time dashboard performance improvements\n\nBased directly on community feedback, we’ve optimized the Real-Time Dashboard from the ground up, ensuring a snappier, high-performance experience.\n\nThanks to a series of performance optimizations across the dashboard experience, we’ve achieved a significant double-digit reduction in full dashboard load time, along with major improvements in common interactions:‑digit reduction in full dashboard load time, along with major improvements in common interactions:\n\n- **Much faster initial dashboard load**—in some scenarios, up to **6× faster**\n- **Large dataset visualizations load dramatically quicker**, reducing wait time and friction\n- **Charts render more efficiently** (including up to **10× faster** pie charts)\n- **Smoother, more responsive UI**, with freezes and visual jumps eliminated\n\nWhether you are loading large datasets or refreshing live visuals, the UI is now smoother and significantly more responsive, ensuring your data keeps pace with your decisions.\n\n*Note: The video demonstrates performance benchmarks conducted in a controlled internal environment.*\n\nLearn more about [What is Real-Time Dashboard?](https://learn.microsoft.com/fabric/real-time-intelligence/real-time-dashboards-overview)\n\n## Data Factory\n\n### Recent data: Get back to your data faster (Preview)\n\nWhen you work with the same data sources repeatedly in Dataflow Gen2, the new Recent data (Preview) module helps you access your most frequently used data faster.\n\n**Why this matters**\n\n- Provides quick access to frequently used tables, files, folders, databases, sheets, etc.\n- Eliminates repetitive navigation steps\n- Improves productivity in transforming data by efficiently connecting and ingesting data\n\n**How to access Recent data**\n\nGetting started is straightforward. Open any Dataflow Gen2 in your Fabric workspace and you’ll find two convenient ways to access Recent data. First, you can select **Recent data** directly from the **Power Query ribbon** for immediate access to your history.\n\n![It shows how to find the Recent data entry in the Power Query ribbon of Dataflow gen2 ](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/it-shows-how-to-find-the-recent-data-entry-in-the.jpeg)\n\n*Figure: Recent data in the Power Query Ribbon*\n\nAlternatively, select **Get data** and choose the **Recent data** module from the home tab or dedicated tab.\n\n![This shows how to find Recent data module inside Get Data. And it also shows the way to use Browse location](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/this-shows-how-to-find-recent-data-module-inside-g.jpeg)\n\n*Figure: Recent data in Modern Get Data*\n\nWhen you **select an item** from your Recent data module, it **loads directly into the Power Query editor** without additional navigation steps required by default. You can start applying transformations immediately. If you need to explore related items in the same location, select **Browse location** to discover other tables or files in the same folder or database, making it easy to include additional related data in your dataflow.\n\nThis Preview feature is available now, learn more in the [Recent data documentation](https://aka.ms/recentdatadocs).\n\n### Improvements to the Fabric variable libraries integration in Dataflow Gen2\n\nIn September 2025, we released a preview of the Fabric variable libraries integration with Dataflow Gen2. This update address two of the most common feedback themes:\n\n- **Variable limit**: Dataflows no longer have a limit on how many variables it can retrieve per evaluation.\n- **Power Query editor support**: the data preview shown in the Power Query editor evaluates the variables. This includes both the usage of the [Variable.Value](https://learn.microsoft.com/powerquery-m/variable-value) and [Variable.ValueOrDefault](https://learn.microsoft.com/powerquery-m/variable-valueordefault) functions.\n\n![Screenshot of the Power Query editor for Dataflow Gen2 rendering the output of the Variable.Value function for a Variable Library with the name My Library and a variable with the name MyDateTime](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-of-the-power-query-editor-for-dataflow.png)\n\n*Figure: Screenshot of the Power Query editor for Dataflow Gen2 rendering the output of the Variable.Value function for a Variable Library with the name My Library and a variable with the name MyDateTime*\n\nWe also identified and fixed issues that caused saving issues when variables were used in data destinations, and we improved the overall Power Query editor experience when variables are used in navigation steps.\n\nWe’re continuing to improve our experience and will share updates in the coming months. Be sure to leave your feedback in the [Data Factory community forum](https://community.fabric.microsoft.com/t5/Dataflow/bd-p/df_dataflows) where you can engage directly with us if you have any questions or suggestions.\n\nLearn more from the [Use Fabric variable libraries in Dataflow Gen2 (Preview)](https://learn.microsoft.com/fabric/data-factory/dataflow-gen2-variable-library-integration) documentation.\n\n### Relative references with Fabric connectors in Dataflow Gen2\n\nOne of the core principles of Fabric and Data Factory is enabling solutions that are **CI/CD-ready**. In **Dataflow Gen2**, you can already use **public parameters** and **Fabric variable libraries** to make your solutions dynamic and compatible across deployment pipelines.\n\nWe’re introducing a new capability to simplify CI/CD scenarios when using **Fabric connectors**: **Relative References**.\n\n**What’s changing?**\n\nPreviously, when you used Fabric connectors (Lakehouse, Warehouse, or SQL Database), the generated script relied on **absolute references**—such as Workspace ID and item IDs (e.g., Lakehouse ID, Warehouse ID).\n\nWith **Relative References**, you’ll see a new node in the navigation dialog called **(Current Workspace)**. This allows you to select items within the current workspace context. Once selected, the script will reference the **item name** instead of unique IDs.\n\n![Diagram comparing the possible experiences between absolute and relative references and the M script crated for each](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/diagram-comparing-the-possible-experiences-between.png)\n\n*Figure: Diagram comparing the possible experiences between absolute and relative references and the M script crated for each.*\n\n**Why it matters**\n\nThis approach ensures that when you move your solution from **development** to **testing** or **production**, **no script changes are required**. Your Dataflow will continue to work based on item names, making deployments seamless without adding any extra components\n\n**Learn more**\n\nCheck out the documentation for [Fabric Lakehouse](https://learn.microsoft.com/fabric/data-factory/connector-lakehouse), [Fabric Warehouse](https://learn.microsoft.com/fabric/data-factory/connector-data-warehouse), and [Fabric SQL connectors](https://learn.microsoft.com/fabric/data-factory/connector-sql-database) for more information.\n\n### Introducing Dataflow Gen2’s just-in-time publishing mechanism\n\nPreviously, Dataflow Gen2 required you to manually trigger a publishing operation before running or refreshing a dataflow whenever unpublished changes were present.\n\nWith the updated experience, the run/refresh operation now automatically checks if a publication is needed and completes it as part of the job. This simplifies the workflow and ensures that runs succeed without requiring an explicit publish step.\n\nYou can still rely on the following behaviors:\n\n- **Explicit publish control:** You can continue to trigger a publish directly using the Publish job when you need full control.\n- **Saving in the UI:** Saving a dataflow in the authoring UI still performs a publication as part of the save process.\n- **Longer first refresh:** The first refresh after making changes may take longer, because publishing now happens automatically as part of that initial run.\n- **CI/CD deployments:** When deploying across environments, a separate publishing step is no longer required. The first run in the target environment will be published automatically if needed.\n\nLearn more about this new mechanism from: [Dataflow Gen2 with CI/CD and Git integration.](https://learn.microsoft.com/fabric/data-factory/dataflow-gen2-cicd-and-git-integration#just-in-time-publishing)\n\n### Modern Evaluator for Dataflow Gen2 (Generally Available)\n\nThe **Modern Query Evaluation Engine (Modern Evaluator)** for Dataflow Gen2 brings substantial performance and reliability improvements to data transformation workloads across Microsoft Fabric.\n\nBuilt on **.NET 8**, this engine delivers faster execution, more efficient processing, and improved scalability for complex dataflows. As part of its GA rollout, the Modern Evaluator now supports **more than 80 connectors**, significantly expanding coverage across enterprise and SaaS data sources.\n\n**Key improvements in this release**\n\n- **Broad connector support**: the Modern Evaluator now works with 80+ connectors—including Azure Data Explorer, Lakehouse, Warehouse, Salesforce, Google Analytics, Fabric-native sources, and many more. This includes some SQL-based connectors such as Fabric SQL Database, SQL Server Database and others. This expanded coverage ensures that most Dataflow Gen2 scenarios can benefit from the improved engine.\n- **Faster and more efficient Web requests**: Enhancements to Web connector handling result in lower overhead for HTTP-based data sources. Customers can expect smoother query execution and improved resilience when working with REST APIs or other web endpoints-based data sources. Customers can expect smoother query execution and improved resilience when working with REST APIs or other web endpoints. ‑based data sources. Customers can expect smoother query execution and improved resilience when working with REST APIs or other web endpoints.\n\nLearn more about the modern query evaluator in Dataflow Gen2 and its compatible connectors: [Modern Evaluator for Dataflow Gen2 with CI/CD](https://learn.microsoft.com/fabric/data-factory/dataflow-gen2-modern-evaluator).\n\n### Incremental copy from Fabric Lakehouse now supports both CDF and watermark-based methods in Copy job\n\nWhen performing incremental copy from a Fabric Lakehouse table, we strongly recommend using CDF (Delta Change Data Feed) to capture row inserts, updates, and deletions, and replicate them to supported destinations.\n\n![Incremental copy from Fabric Lakehouse where CDF is enabled ](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/incremental-copy-from-fabric-lakehouse-where-cdf-i.png)\n\n*Figure: Incremental copy from Fabric Lakehouse via CDF.*\n\nHowever, you can now also optionally use watermark-based incremental copy without enabling CDF. In this mode, you can select an incremental column for each table to identify changes.\n\nTo enable this, go to the Advanced Settings button after creating the Copy job, where you will have the option to switch from CDF to using a watermark column.\n\n![Incremental copy from Fabric Lakehouse using watermark-based method](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/incremental-copy-from-fabric-lakehouse-using-water.png)\n\n*Figure: Incremental copy from Fabric Lakehouse via watermark-based method*\n\nLearn more from the [What is Copy job in Data Factory](https://learn.microsoft.com/fabric/data-factory/what-is-copy-job#incremental-copy-cdc-watermark) documentation.\n\n### SAP Datasphere outbound for Amazon S3 and Google cloud storage in Copy job\n\nPreviously, you could use SAP Datasphere Outbound for ADLS Gen2 in Copy job to perform CDC replication from SAP to any supported destination. For more details, see [Tutorial: Copy job with SAP Datasphere Outbound (Preview)](https://learn.microsoft.com/fabric/data-factory/copy-job-tutorial-sap-datasphere).\n\nYou can now also use SAP Datasphere Outbound for Amazon S3 and SAP Datasphere Outbound for Google Cloud Storage, expanding staging storage support across multiple clouds so you can choose the option that best fits your scenario.\n\n![Selecting Copy data from SAP Datasphere Outbound for Amazon S3 and Google Cloud Storage](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/selecting-copy-data-from-sap-datasphere-outbound-f.png)\n\n*Figure: Selecting Copy data from SAP Datasphere Outbound for Amazon S3 and Google Cloud Storage*\n\nLearn more from [Tutorial: Copy job with SAP Datasphere Outbound (Preview) – Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/fabric/data-factory/copy-job-tutorial-sap-datasphere)\n\n### Column Mapping in CDC for Copy Job\n\nColumn mapping from source to destination is now supported during CDC replication within the Copy job. This is useful when you want to rename columns, change data types, or otherwise customize the schema in the destination store.\n\nColumn mapping now is supported across all data movement patterns in Copy job, including full copy, watermark-based incremental copy and CDC replication.\n\n![Column Mapping in Copy job when CDC replicated is enabled](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/column-mapping-in-copy-job-when-cdc-replicated-is.png)\n\n*Figure: Column Mapping in Copy job.*\n\nLearn more from [Change data capture (CDC) in Copy Job – Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/fabric/data-factory/cdc-copy-job).\n\n### Rowversion now supported as an incremental column in SQL database Copy job\n\nCopy job simplifies data movement from many sources to many destinations by natively supporting multiple delivery styles, including bulk copy, incremental copy, and change data capture (CDC) replication. For incremental copy, the first run performs a full copy, and subsequent runs transfer only new or changed data from the last run to save time and resources.\n\nIf CDC is not enabled on your database, you must select an incremental column for each table. This column acts as a marker, allowing Copy job to identify rows that are new or updated since the last run. Previously, this column was limited to date/time values or increasing numeric values.\n\nYou can now also select RowVersion as the incremental column when performing incremental copy from SQL Server, Azure SQL Database, SQL Managed Instance, or SQL in Fabric.\n\n![Selecting RowVersion to identify changes when incremental copy from SQL database](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/selecting-rowversion-to-identify-changes-when-incr.png)\n\n*Figure: Selecting RowVersion to identify changes when incremental copy from SQL database*\n\nLearn more from [What is Copy job in Data Factory – Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/fabric/data-factory/what-is-copy-job#incremental-copy-cdc-watermark).\n\n### Copy job activity now supports Service Principal and Workspace identity authentication\n\nWe are expanding authentication support in the Copy job activity in pipeline, making it easier than ever to securely connect, integrate, and move data across a broad range of enterprise and SaaS systems. This enhancement reflects our continued commitment to delivering an enterprise-ready data integration platform that balances security, flexibility, and ease of use.\n\nThe Copy job activity now supports additional authentication methods to your Copy job item in pipeline, enabling customers to choose the security model that best fits their organizational standards and compliance requirements. The added authentication types are service principal and workspace identity.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/word-image-33579-33.png)\n\n*Figure: Authentication kind in Copy job activity.*\n\n**Why** **this matters**\n\nAuthentication is no longer just a connection detail; it is a foundational requirement for enterprise-scale data integration. With expanded authentication support in the Copy job activity, customers can now:\n\n- **Strengthen security posture** by minimizing long-lived secrets and adopting identity-based access.\n- **Accelerate time to value** by connecting to copy job items using native, first-class authentication mechanisms.\n- **Simplify compliance and audits** through standardized authentication patterns aligned with enterprise security policies.\n- **Improve operational reliability** by leveraging managed identity and token-based access.\n\nThese improvements are especially impactful for organizations operating in regulated industries or managing large-scale hybrid environments where security consistency is non-negotiable.\n\n**Getting started**\n\nThe new authentication options are available within the Copy job activity setting in Fabric Data Factory. Explore these new capabilities and start standardizing modern, secure authentication patterns across your data integration workflows. Learn more about [the authentication capability in copy job activity](https://learn.microsoft.com/fabric/data-factory/copy-job-activity).\n\n### Parallel Read Support for Large CSV Dataset\n\nAs we continue improving ingestion performance, we’ve introduced a new enhancement for reading CSV datasets in Data Factory. It significantly boosts ingestion throughput for large CSV files—a common customer challenge when a single file can’t fully take advantage of parallel reads.\n\nWith this update, Data Factory can now read large CSV files in parallel when the format allows for safe partitioning, delivering better performance and scalability while preserving correctness. By using your multiline configuration, the service can determine how to split the file and process it in parallel, dramatically improving read performance.\n\nWhen multiline behavior is explicitly defined, the service can:\n\n- Safely identify record boundaries even in large files.\n- Partition the file into multiple logical chunks.\n- Read and process those chunks concurrently.\n\n![Screenshot showing source file format settings.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-showing-source-file-format-settings-.png)\n\n*Figure: Multiline rows setting for reading delimited text.*\n\nThis enables higher throughput without compromising data correctness.\n\n**Why multiline configuration matters**\n\nCSV files that contain multiline records such as fields with embedded line breaks enclosed in quotes which require special handling. Without explicit configuration, the system must assume the most conservative parsing model, which prevents parallelization.\n\nBy specifying multiline information on the source, users provide the necessary context for the service to:\n\n- Correctly interpret row boundaries.\n- Avoid recording corruption during parallel reads.\n- Confidently enable parallelism where it is safe.\n\nThis opt-in design ensures that performance improvements are applied only when they are valid for the data format.\n\n**Getting started**\n\nWe encourage customers working with large CSV datasets to review the source configurations and unlock the benefits of this new capability to take advantage of parallel reads for large CSV files. Learn more about [the performance optimization for copying delimited text files](https://learn.microsoft.com/fabric/data-factory/format-delimited-text#performance-optimization-for-copying-delimited-text-files).\n\n### Adaptive Performance Tuning: Intelligent Optimization for Data Movement (Preview)\n\nAdaptive Performance Tuning is designed to intelligently optimize data movement performance based on your configuration and runtime context. This feature represents a major step forward in making performance tuning simpler, safer, and more effective without requiring deep manual expertise or trial-and-error adjustments.\n\nAs data volumes grow and integration scenarios become more diverse, achieving optimal performance has become increasingly complex. Customers must balance throughput, reliability, cost, and data correctness across a wide range of sources, destinations, formats, and network environments. Adaptive Performance Tuning addresses this challenge by allowing the service to dynamically apply performance optimizations informed by customer configurations and real execution conditions.\n\nAdaptive Performance Tuning is designed with safety and predictability as first principles. Optimizations are applied only when they are compatible with the configured semantics of the task, ensuring that performance gains do not compromise data accuracy or expected behavior.\n\nAs a preview feature, Adaptive Performance Tuning is:\n\n- **Explicitly opt-in**, giving customers full control.\n- **Non-breaking**, with no required changes to existing configurations.\n- **Incrementally evolving**, informed by customer feedback and real-world use.\n\nAdaptive Performance Tuning is part of a broader vision to make Data Factory a more intelligent, self-optimizing platform. By combining rich configuration signals with service-side intelligence, we aim to help customers focus less on infrastructure tuning and more on delivering business value from their data.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/word-image-33579-35.png)\n\n*Figure: Adaptive performance tuning setting.*\n\nAs the preview evolves, we plan to expand the range of supported optimization scenarios, continuously improving performance outcomes.\n\n**Getting started with the preview**\n\nCustomers can enable Adaptive Performance Tuning directly within their pipeline settings and begin benefiting from service-driven performance optimization. We encourage users to try the preview, monitor performance improvements, and share feedback to help shape the future of this capability.\n\nLearn more about [the Adaptive Performance Tuning feature](https://learn.microsoft.com/fabric/data-factory/copy-data-activity#configure-your-other-settings-under-settings-tab).\n\n## Other\n\n### Fabric VS Code extension for browsing, editing item definitions, and MCP support\n\nThe Microsoft Fabric extension for Visual Studio Code has been enhanced to improve the user experience in exploring, editing, and managing Fabric items directly within the editor and through integration with Fabric MCP server and GitHub Copilot chat.\n\n- **Browse workspace folders:** Users can now view and drill into folders and their contents within the workspace to better understand the organization of Fabric content without leaving VS Code.\n- **View and edit Fabric item definitions:** The extension supports viewing item definitions in read-only mode by default, but you can enable editing through extension’s settings. Changes saved directly update the Fabric item in the workspace, but users should proceed cautiously to avoid breaking changes.\n- **Fabric MCP server integration:** The [Fabric MCP server](https://marketplace.visualstudio.com/items?itemName=fabric.vscode-fabric-mcp-server) extension can be enabled alongside the Fabric and [GitHub Copilot Chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat) extensions, offering tailored tools for working with Fabric artifacts, including CRUD operations, generating design documents, and accessing Microsoft Fabric documentation through a specialized agent mode.\n\n![Screenshot of using Fabric MCP and GitHub Copilot with Fabric extension in VS Code to Create a spec for data analytics architecture](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/screenshot-of-using-fabric-mcp-and-github-copilot.png)\n\n*Figure: Using Fabric MCP in VS Code to design a data analytics solution.*\n\nLearn more about the [new features enabled for Fabric extension for VS code](https://aka.ms/fabric-extension-updates).\n\n## **That’s a wrap for February!**\n\nWe hope these updates help you work faster, build with confidence, and get even more value from Microsoft Fabric. With FabCon right around the corner, it’s an exciting time to connect with the community, learn from experts, and see these capabilities come to life. If you’re heading to Atlanta, we can’t wait to see you there—and if not, there’s still plenty to dig into until next month’s update."
}
