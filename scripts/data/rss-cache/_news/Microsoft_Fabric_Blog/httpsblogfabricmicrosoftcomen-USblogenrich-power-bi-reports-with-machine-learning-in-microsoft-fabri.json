{
  "PubDate": "2026-02-12T12:00:00+00:00",
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/enrich-power-bi-reports-with-machine-learning-in-microsoft-fabric/",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "Description": "Many organizations want to go beyond descriptive Power BI reports and start answering forward‑looking questions with machine learning—identifying emerging trends, at-risk accounts, and where to focus effort to maximize impact. In practice, this is hard. Adding machine learning to Power BI often means moving data out of semantic models, rebuilding logic, managing separate storage and …\n\n[Continue reading “Enrich Power BI reports with machine learning in Microsoft Fabric”](https://blog.fabric.microsoft.com/en-us/blog/enrich-power-bi-reports-with-machine-learning-in-microsoft-fabric/)",
  "ProcessedDate": "2026-02-12 19:21:41",
  "Author": "Microsoft Fabric Blog",
  "Title": "Enrich Power BI reports with machine learning in Microsoft Fabric",
  "Tags": [],
  "FeedName": "Microsoft Fabric Blog",
  "EnhancedContent": "Many organizations want to go beyond descriptive Power BI reports and start answering forward‑looking questions with machine learning—identifying emerging trends, at-risk accounts, and where to focus effort to maximize impact.\n\nIn practice, this is hard. Adding machine learning to Power BI often means moving data out of semantic models, rebuilding logic, managing separate storage and security, and assembling custom pipelines. As teams cross these boundaries, definitions drift, refreshes break, and insights rarely return to the tools business users rely on.\n\nMicrosoft Fabric takes a different approach.\n\nFabric unifies data engineering, data science, and business intelligence on a single, unified platform built on OneLake. Instead of breaking apart your BI stack, Fabric lets you extend it—reusing governed semantic models, training models where the data lives, and operationalize predictions in the same flow without duplicating logic or rebuilding pipelines.\n\nThis post shows an end‑to‑end pattern for enriching a Power BI report with machine learning in Fabric. We start with a governed semantic model, train a churn-prediction model, and operationalize predictions with batch and real‑time scoring. The result is predictive insight aligned with business logic, reliably refreshed, and surfaced directly in Power BI for monitoring, sharing, and action.\n\n## Scenario: Predicting bank customer churn\n\nAcross industries, teams use Power BI to understand what has already happened. Dashboards show trends, highlight performance, and keep organizations aligned around a shared view of the business.\n\nBut leaders are asking new questions—not just what happened, but what is likely next and how outcomes might change if they act. They want insights that help teams prioritize, intervene earlier, and focus effort where it matters. This is why many organizations look to enrich Power BI reports with machine learning.\n\nThis challenge is especially common in financial services.\n\nConsider a bank that uses Power BI to track customer activity, balances, and service usage. Historical analysis shows that around 20% of customers churn, with churn tied to factors such as customer tenure, product usage, service interactions, and balance changes.\n\n![Figure 1 Bank customer churn overview in Power BI](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-1-bank-customer-churn-overview-in-power-bi.png)*Figure 1 Bank customer churn overview in Power BI*\n\nAt this point, descriptive reporting is no longer enough. The real questions become:\n\n- Which customers are most likely to churn next?\n- How confident are those predictions?\n- Can churn risk update automatically as customer data changes?\n- Can accounts teams monitor high‑risk customers and act early?\n\nMicrosoft Fabric makes it possible to answer those questions with a fully integrated, end-to-end workflow.\n\n### Architecture\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/word-image-33384-2-1-1024x570.png)*Figure 2 Architecture to enrich Power BI reports with ML Model*\n\nFirst, let’s look briefly at the architecture:\n\n1. **Semantic model** defines business logic and metrics.\n2. **Semantic Link** to access that model directly from notebooks.\n3. **Fabric ML experiment** to train and evaluate a churn prediction model.\n4. **Batch scoring** applies to the model at scale and saves predictions to OneLake.\n5. **Real-time scoring endpoints** support low-latency inference where needed.\n6. **Dataflow Gen2** calls the real-time scoring endpoint to enrich data during ingestion.\n7. **Power BI** **Report** visualizing churn risk and driving action.\n\nAll components live natively in Fabric and share the same security, governance, and storage layer.\n\n### Prerequisites\n\nBefore starting, you should have:\n\n- A Fabric-enabled workspace.\n- A “Bank Customer Churn Analysis” semantic model published in the workspace.\n\nYou can find a sample semantic model and notebooks in [this repository](https://aka.ms/enrich-pbi-with-ML).\n\n## Step 1: Explore the semantic model with Semantic Link\n\nA common challenge in analytics organizations is **logic duplication**. Business concepts like *active customer*, *average monthly balance*, or *churned user* are first defined in DAX for reporting, then redefined again in SQL or Spark for pipelines, and rebuilt once more in Python during feature engineering. Over time, those definitions drift, numbers diverge, and teams spend more time reconciling metrics than using them.\n\nIn Fabric, the Power BI semantic model becomes the contract. Instead of exporting data and reimplementing logic, **Semantic Link lets you query the semantic model directly from a Fabric** **notebook.**\n\nMeasures, filters, and calculated columns behave exactly as they do in Power BI. There is no separate authentication step, and no risk of logic drifting over time—the Fabric security context applies automatically.\n\nWith one line of Python code, you can explore all semantic models, tables, and columns.\n\n```\n# Semantic Link uses your Fabric identity — no separate credential handling.\nimport sempy.fabric as fabric\n# Discover semantic models (datasets) you can access\ndf_datasets = fabric.list_datasets() display(df_datasets)\n# Inspect tables in a specific semantic model\ndataset_name = \"Bank Customer Churn Analysis\" df_tables = fabric.list_tables(dataset_name) display(df_tables)\n# Inspect columns and metadata (useful for feature planning)\ndf_columns = fabric.list_columns(dataset_name) display(df_columns) ```\n\n| ![Figure 3 Semantic model discovery](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-3-semantic-model-discovery.png) | | --- |\n\nFigure 3 Semantic model discovery\n\nWhen you query a Power BI semantic model through Semantic Link, you work with the same logic that already powers your reports, not a parallel copy for data science. The result is a **FabricDataFrame**, which looks and feels like a pandas DataFrame but carries much more value. It preserves semantic context—such as measures, calculations, and lineage—directly from the semantic model into the data science environment. Because it subclasses pandas, you can use it with familiar libraries and workflows, while also calling Power BI measures directly from your notebook.\n\n```\n# Measures are the business logic. Keeping them centralized prevents drift. df_measures = fabric.list_measures(dataset_name) display(df_measures)\n```\n\n![Figure 4 Inspect measure definitions](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-4-inspect-measure-definitions.png)*Figure 4 Inspect measure definitions*\n\n```\n# Pull governed data from the semantic model.\n# This keeps your ML features aligned with what Power BI shows.\ndf_raw = fabric.read_table(dataset=dataset_name, table=\"churn\") display(df_raw)\n\n```\n\n![Figure 5 Pull Semantic model table as a SparkDataFrame](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-5-pull-semantic-model-table-as-a-sparkdataf.png)*Figure 5 Pull Semantic model table as a SparkDataFrame*\n\nThis lets you build features, explore data, and train models on the same business definitions your stakeholders trust in Power BI—without rewriting logic, managing credentials, or worrying about drift over time.\n\nOnce the data is available in the notebook, you can perform exploratory analysis with familiar tools such as pandas, seaborn, or matplotlib. Any additional features you derive build on centrally governed definitions instead of replacing them. That foundation makes everything that follows safe, repeatable, and trustworthy.\n\n``` import seaborn as sns sns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)}) import matplotlib.pyplot as plt import matplotlib.ticker as mticker from matplotlib import rc, rcParams import numpy as np import pandas as pd import itertools\n\n```\n\nDrop the duplicated rows, rows with missing data and drop the columns that you do not need.\n\n``` def clean_data(df):\n# Drop rows with missing data across all columns\ndf = df.dropna()\n# Drop duplicate rows in columns: 'CustomerId', 'RowNumber'\ndf = df.drop_duplicates(subset=['CustomerId', 'RowNumber'])\n# Drop columns: 'RowNumber', 'Surname'\nreturn df df_clean = clean_data(df_raw.copy()) df_clean.head()\n\n```\n\n![Figure 6 Cleaned training dataset](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-6-cleaned-training-dataset.png)*Figure 6 Cleaned training dataset*\n\n### Data exploration\n\nDisplay some summaries and visualizations of the cleaned data. Use this code to determine categorical, numerical, and target attributes.\n\n```\n# Determine the dependent (target) attribute\ndependent_variable_name = \"Exited\" print(dependent_variable_name)\n# Determine the categorical attributes\ncategorical_variables = [col for col in df_clean.columns if col in \"O\" or df_clean[col].nunique() <=5 and col not in \"Exited\"] print(categorical_variables)\n# Determine the numerical attributes\nnumeric_variables = [col for col in df_clean.columns if df_clean[col].dtype != \"object\" and df_clean[col].nunique() >5 and col not in \"CustomerId\"] print(numeric_variables)\n\n```\n\nShow the five-number summary (the minimum score, first quartile, median, third quartile, the maximum score) for the numerical attributes, using box plots.\n\n``` df_num_cols = df_raw[numeric_variables] sns.set(font_scale = 0.7) fig, axes = plt.subplots(nrows = 2, ncols = 3, gridspec_kw = dict(hspace=0.3), figsize = (17,8)) fig.tight_layout() for ax,col in zip(axes.flatten(), df_num_cols.columns): sns.boxplot(x = df_num_cols[col], color='green', ax = ax) fig.delaxes(axes[1,2])\n\n```\n\n![Figure 7 Five-number summary for numeric attributes](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-7-five-number-summary-for-numeric-attribute.png)*Figure 7 Five-number summary for numeric attributes*\n\nShow the distribution of exited versus non-exited customers across the categorical attributes.\n\n``` df_raw['Exited'] = df_raw['Exited'].astype(str) attr_list = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember', 'NumOfProducts', 'Tenure'] df_raw['Exited'] = df_raw['Exited'].astype(str) fig, axarr = plt.subplots(2, 3, figsize=(15, 4)) for ind, item in enumerate (attr_list): print(ind, item) sns.countplot(x = item, hue = 'Exited', data = df_raw, ax = axarr[ind%2][ind//2]) fig.subplots_adjust(hspace=0.7)\n\n```\n\n![Figure 8 Distribution of exited versus non-exited customers across the categorical attributes](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-8-distribution-of-exited-versus-non-exited.png)*Figure 8 Distribution of exited versus non-exited customers across the categorical attributes*\n\nShow the frequency distribution of numerical attributes using histogram.\n\n``` columns = df_num_cols.columns[: len(df_num_cols.columns)] fig = plt.figure() fig.set_size_inches(18, 8) length = len(columns) for i,j in itertools.zip_longest(columns, range(length)): plt.subplot((length // 2), 3, j+1) plt.subplots_adjust(wspace = 0.2, hspace = 0.5) df_num_cols[i].hist(bins = 20, edgecolor = 'black') plt.title(i) plt.show()\n\n```\n\n![Figure 9 Frequency distribution of numerical attributes](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-9-frequency-distribution-of-numerical-attri.png)*Figure 9 Frequency distribution of numerical attributes*\n\nThe exploratory analysis highlights a few clear patterns. Most customers are based in France, while Spain—despite having fewer customers—shows the lowest churn rate compared to France and Germany. Product adoption is limited, with very few customers using more than two bank products. Customer activity stands out as a strong signal: inactive customers are significantly more likely to churn. In contrast, gender and tenure length show little influence on whether a customer decides to close their account.\n\n### Feature engineering\n\nNow let us perform feature engineering to generate new attributes based on current attributes:\n\n``` df_clean['Tenure'] = df_clean['Tenure'].astype(int)\n\ndf_clean[\"NewTenure\"] = df_clean[\"Tenure\"]/df_clean[\"Age\"]\n\ndf_clean[\"NewCreditsScore\"] = pd.qcut(df_clean['CreditScore'], 6, labels = [1, 2, 3, 4, 5, 6])\n\ndf_clean[\"NewAgeScore\"] = pd.qcut(df_clean['Age'], 8, labels = [1, 2, 3, 4, 5, 6, 7, 8])\n\ndf_clean[\"NewBalanceScore\"] = pd.qcut(df_clean['Balance'].rank(method=\"first\"), 5, labels = [1, 2, 3, 4, 5])\n\ndf_clean[\"NewEstSalaryScore\"] = pd.qcut(df_clean['EstimatedSalary'], 10, labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) ```\n\nPerform one-hot encoding for geography and gender columns:\n\n```\n\nimport pandas as pd\n\ndef clean_data(df_clean):\n\n# One-hot encode columns: 'Geography', 'Gender'\n\ndf_clean = pd.get_dummies(df_clean, columns=['Geography', 'Gender'])\n\nreturn df_clean df_clean_1 = clean_data(df_clean.copy())\n\ndf_clean_1.head()\n\n```\n\nCreate a delta table for the cleaned data to get ready for model training.\n\n```\n\ntable_name = \"df_clean\"\n\n# Create Spark DataFrame from pandas\n\nsparkDF=spark.createDataFrame(df_clean_1)\n\nsparkDF.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(f\"Tables/{table_name}\")\n\nprint(f\"Spark dataframe saved to delta table: {table_name}\") ```\n\n## Step 2: Build a machine learning model in Fabric\n\nWith governed data in hand, the focus shifts from alignment to prediction: learning which customers are most likely to leave.\n\n### Model training\n\nTo predict churn, we train a **LightGBM classifier**, a strong fit for large tabular datasets like customer records. LightGBM captures non‑linear patterns with minimal tuning and integrates natively with Fabric’s MLflow experience, making experiments easy to track and models easy to operationalize.\n\nBecause churn data is naturally imbalanced, we apply **SMOTE** during training to ensure the model learns equally from both churned and retained customers. This combination keeps the notebook focused on insight and outcomes—building a reliable model that can be registered once and reused consistently across batch and real‑time scoring—rather than on managing infrastructure or custom training code.\n\n```\n# Install imblearn for SMOTE using pip\n%pip install imblearn\n\n```\n\nPrior to training any machine learning model, you need to load the delta table from the Lakehouse to read the cleaned data you created in the previous step.\n\n```\n\nimport pandas as pd\n\nSEED = 12345\n\ndf_clean = spark.read.format(\"delta\").load(\"Tables/df_clean\").toPandas() ```\n\nNow let us generate experiments for tracking and set the experiment.\n\n```\n\nimport mlflow\n\n# Setup experiment name\n\nEXPERIMENT_NAME = \"bank-churn-experiment\"\n\nmlflow.set_experiment(EXPERIMENT_NAME) ```\n\nImport the required libraries for model training\n\n```\n\nfrom sklearn.model_selection import train_test_split\n\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, confusion_matrix, recall_score, roc_auc_score, classification_report ```\n\nUse the train\\_test\\_split function from scikit-learn to split the data into training, validation, and test sets.\n\n```\n\n#Split the dataset to 60%, 20%, 20% for training, validation, and test datasets\n\ny = df_clean[\"Exited\"]\n\nX = df_clean.drop(\"Exited\",axis=1)\n\n#Train-Test Separation\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=SEED)\n\n#Train-Validation Separation\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=SEED) ```\n\nAnd save the test data to a delta table\n\n```\n\n# Save the test data to a delta table\n\ntable_name = \"df_test\"\n\ndf_test=spark.createDataFrame(X_test)\n\ndf_test.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").format(\"delta\").save(f\"Tables/{table_name}\")\n\nprint(f\"Spark test DataFrame saved to delta table: {table_name}\") ```\n\nThe data exploration in Step 1 showed that of 10,000 customers, only 2,037 customers (around 20%) left the bank, indicating a highly imbalanced dataset. With so few examples of the minority class, a model struggles to learn the decision boundary. SMOTE is the most widely used approach for synthesizing new minority-class samples.\n\nApply SMOTE to the training data to synthesize new samples for the minority class.\n\n```\n\nfrom collections import Counter\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=1234)\n\nX_res, y_res = sm.fit_resample(X_train, y_train)\n\nnew_train = pd.concat([X_res, y_res], axis=1) ```\n\nTrain the model using LightGBM and register the trained model as a ML model artifact.\n\nAt the time of this writing, Fabric Real-Time Scoring Endpoint did not support tensor-based models, so we created a non-tensor-based output schema for the model.\n\n``` import mlflow.pyfunc import pandas as pd import numpy as np class Predictor(mlflow.pyfunc.PythonModel): def __init__(self, base_model): self.base_model = base_model\n\ndef predict(self, context, model_input: pd.DataFrame) -> pd.DataFrame:\n# prediction\npred = self.base_model.predict(model_input) pred = np.asarray(pred, dtype=np.int64)\n# probability (positive class)\nproba = self.base_model.predict_proba(model_input) proba = np.asarray(proba[:, 1], dtype=np.float64)\n\nreturn pd.DataFrame({ \"prediction\": pred, \"probability\": proba })\n\n```\n\n``` import mlflow from mlflow.models.signature import infer_signature import numpy as np import pandas as pd from lightgbm import LGBMClassifier\n# Define the LightGMB model\nlgbm_sm_model = LGBMClassifier( learning_rate=0.07, max_delta_step=2, n_estimators=100, max_depth=10, eval_metric=\"logloss\", objective='binary', random_state=42 ) with mlflow.start_run(run_name=\"lgbm_sm_non_tensor\") as run: lgbm_non_tensor_sm_run_id = run.info.run_id\n\n# Train on balanced data\nlgbm_sm_model.fit(X_res, y_res.ravel())\n# Validation predictions (convert to pandas for non-tensor signature)\ny_pred = lgbm_sm_model.predict(X_val)\n# Optionally include probability for positive class as a tabular output example for signature\ny_proba = lgbm_sm_model.predict_proba(X_val)[:, 1] y_proba_series = pd.Series(y_proba.astype(float), name=\"probability\")\n# Build output example for signature (TWO cols)\npred_ex = np.asarray(lgbm_sm_model.predict(X_val), dtype=np.int64) proba_ex = np.asarray(lgbm_sm_model.predict_proba(X_val)[:, 1], dtype=np.float64) y_example = pd.DataFrame({\"prediction\": pred_ex, \"probability\": proba_ex}) signature = infer_signature(X_val, y_example)\n\n# Compute metrics\naccuracy = accuracy_score(y_val, y_pred) cr_lgbm_sm = classification_report(y_val, y_pred) cm_lgbm_sm = confusion_matrix(y_val, y_pred) roc_auc_lgbm_sm = roc_auc_score(y_res, lgbm_sm_model.predict_proba(X_res)[:, 1])\n\n# Log metrics\nmlflow.log_metric(\"val_accuracy\", accuracy) mlflow.log_metric(\"train_roc_auc\", roc_auc_lgbm_sm)\n# Classification report\nmlflow.log_text(cr_lgbm_sm, \"classification_report.txt\")\n\n# Confusion matrix\ncm_df = pd.DataFrame( cm_lgbm_sm, index=[\"Actual_0\", \"Actual_1\"], columns=[\"Pred_0\", \"Pred_1\"]) mlflow.log_table(cm_df, \"confusion_matrix.json\")\n\n# Log model\nmlflow.pyfunc.log_model( artifact_path=\"model\", python_model=Predictor(lgbm_sm_model), signature=signature, input_example=X_val.head(5), registered_model_name=\"lgbm_sm_non_tensor\" ) ```\n\n### View the machine learning experiment\n\nThe experiment runs are automatically saved in the experiment artifact that can be found from the workspace. You can select the link from the cell output and open the experiment view. Parameters, metrics, and artifacts are all captured in the experiment, making every run reproducible and auditable.\n\n![Figure 10 Fabric machine learning experiment view](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-10-fabric-machine-learning-experiment-view.png)*Figure 10 Fabric machine learning experiment view*\n\nValidate the model performance\n\nOnce machine learning model training is complete, you can assess its performance.\n\nOpen the saved experiment, load the machine learning models, and evaluate them on the validation dataset.\n\n```\n\nimport mlflow.pyfunc\n\n# Fetch the model\n\nload_model_lgbm1_sm = mlflow.pyfunc.load_model(f\"runs:/{lgbm_non_tensor_sm_run_id}/model\")\n\n# Assess the performance of the loaded model on validation dataset\n\nypred_lgbm1_sm_v1 = load_model_lgbm1_sm.predict(X_val) ```\n\nNext, you will develop a script to plot the confusion matrix to evaluate the accuracy of the classification using the validation dataset.\n\n```\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\", palette=\"tab10\", rc = {'figure.figsize':(9,6)})\n\nimport matplotlib.pyplot as plt\n\nimport matplotlib.ticker as mticker\n\nfrom matplotlib import rc, rcParams\n\nimport numpy as np\n\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n\nprint(cm)\n\nplt.figure(figsize=(4,4))\n\nplt.rcParams.update({'font.size': 10})\n\nplt.imshow(cm, interpolation='nearest', cmap=cmap)\n\nplt.title(title)\n\nplt.colorbar()\n\ntick_marks = np.arange(len(classes))\n\nplt.xticks(tick_marks, classes, rotation=45, color=\"blue\")\n\nplt.yticks(tick_marks, classes, color=\"blue\")\n\nfmt = '.2f' if normalize else 'd'\n\nthresh = cm.max() / 2.\n\nfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n\nplt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",\n\ncolor=\"red\" if cm[i, j] > thresh else \"black\")\n\nplt.tight_layout()\n\nplt.ylabel('True label')\n\nplt.xlabel('Predicted label')\n\n```\n\n```\n\ncfm = confusion_matrix(y_val, y_pred=ypred_lgbm1_sm_v1[\"prediction\"])\n\nplot_confusion_matrix(cfm, classes=['Non Churn','Churn'], title='LightGBM-non-tensor')\n\ntn, fp, fn, tp = cfm.ravel() ```\n\n![Figure 11 Confusion matrix for the churn prediction model](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-11-confusion-matrix-for-the-churn-predictio.png)*Figure 11 Confusion matrix for the churn prediction model*\n\nOnce performance is validated, the model is **registered in the MLflow Model Registry**. Moving forward, the model becomes a managed Fabric asset with a stable name and version, ready to be consumed by downstream pipelines.\n\n## Step 3: Batch scoring on active customers\n\nOnce the model is registered, the fastest way to make it usable is to score data at Spark scale and persist the results in OneLake—in our case, scoring active customers to identify who is likely to churn next.\n\nFabric’s **PREDICT** capability is built for this: you point to a named model version in the Fabric registry and run inference where the data lives. That model name and version keeps scoring repeatable, letting you rerun the job with the same model and upgrade only when ready.\n\nStart by loading the latest snapshot of active customers from OneLake and applying the same feature-engineering logic to the active users.\n\n```\n\n# Load the active users\n\nactive_users = spark.read.format(\"delta\").load(\"Tables/active\")\n\ndisplay(active_users)\n\n```\n\n``` from pyspark.sql import functions as F from pyspark.sql.window import Window\n# Start with Spark DataFrame\ndf = active_users.drop(\"RowNumber\", \"Surname\")\n# Tenure as integer\ndf = df.withColumn(\"Tenure\", F.col(\"Tenure\").cast(\"int\"))\n# NewTenure = Tenure / Age\ndf = df.withColumn(\"NewTenure\", F.col(\"Tenure\") / F.col(\"Age\"))\n# CreditScore qcut into 6 bins\ncredit_window = Window.orderBy(\"CreditScore\") df = df.withColumn(\"CreditIdx\", F.row_number().over(credit_window)) credit_count = df.count() df = df.withColumn(\"NewCreditsScore\", F.ceil(F.lit(6) * F.col(\"CreditIdx\") / F.lit(credit_count)))\n# Age qcut into 8 bins\nage_window = Window.orderBy(\"Age\") df = df.withColumn(\"AgeIdx\", F.row_number().over(age_window)) age_count = df.count() df = df.withColumn(\"NewAgeScore\", F.ceil(F.lit(8) * F.col(\"AgeIdx\") / F.lit(age_count)))\n# Balance qcut with rank(method=\"first\") into 5 bins\nbalance_window = Window.orderBy(\"Balance\") df = df.withColumn(\"BalanceRank\", F.row_number().over(balance_window)) balance_count = df.count() df = df.withColumn(\"NewBalanceScore\", F.ceil(F.lit(5) * F.col(\"BalanceRank\") / F.lit(balance_count)))\n# EstimatedSalary qcut into 10 bins\nsalary_window = Window.orderBy(\"EstimatedSalary\") df = df.withColumn(\"SalaryIdx\", F.row_number().over(salary_window)) salary_count = df.count() df = df.withColumn(\"NewEstSalaryScore\", F.ceil(F.lit(10) * F.col(\"SalaryIdx\") / F.lit(salary_count)))\n# Cleanup temporary columns\ndf_clean = df.drop(\"CreditIdx\",\"AgeIdx\",\"BalanceRank\",\"SalaryIdx\")\n\n```\n\n``` from pyspark.sql import functions as F def clean_data_spark(df): df = ( df\n# One-hot Geography\n.withColumn(\"Geography_France\", F.when(F.col(\"Geography\") == \"France\", 1).otherwise(0)) .withColumn(\"Geography_Germany\", F.when(F.col(\"Geography\") == \"Germany\", 1).otherwise(0)) .withColumn(\"Geography_Spain\", F.when(F.col(\"Geography\") == \"Spain\", 1).otherwise(0))\n\n# One-hot Gender\n.withColumn(\"Gender_Male\", F.when(F.col(\"Gender\") == \"Male\", 1).otherwise(0)) .withColumn(\"Gender_Female\", F.when(F.col(\"Gender\") == \"Female\", 1).otherwise(0))\n\n# Drop original categorical columns\n.drop(\"Geography\", \"Gender\") ) return df\n\ndf_clean_1 = clean_data_spark(df_clean) display(df_clean_1)\n\n```\n\nIn practice, you have a few equally supported ways to invoke PREDICT, depending on how you like to work. If you prefer a notebook‑first workflow, the **Transformer API** gives you a clean wrapper around your registered MLflow model. You create an MLFlowTransformer, tell it which input columns to use, and choose an output column name (for example, predictions).\n\n``` from synapse.ml.predict import MLFlowTransformer model = MLFlowTransformer( inputCols=list(df_clean_1.columns), outputCol='predictions', modelName='lgbm_sm_non_tensor', modelVersion=1 )\n\n```\n\nFrom there, scoring is a standard Spark transform: you call transform() and you get back a DataFrame that includes predicted labels and probabilities—ready to save as a Delta table. The payoff is simple: the scoring step feels like any other Spark pipeline step, and it scales with your data.\n\n``` import pandas from pyspark.sql.functions import col predictions = model.transform(df_clean_1) predictions = predictions.select( \"*\", col(\"predictions.prediction\").alias(\"prediction\"), col(\"predictions.probability\").alias(\"probability\") ).drop(\"predictions\")\n\n```\n\n``` ay(predictions)\n\n```\n\nIf your team prefers more declarative pipelines—or you want scoring that reads like a query—Fabric also supports calling PREDICT through **Spark SQL**. That is useful when you want inference to sit alongside familiar SQLstyle transformations, or when you are handing off a scoring step to engineers who live in SQL. style transformations, or\n\n``` from pyspark.ml.feature import SQLTransformer\n# Substitute \"model_name\", \"model_version\", and \"features\" below with values for your own model name, model version, and feature columns\nmodel_name = 'lgbm_sm_non_tensor' model_version = 1 features = df_clean_1.columns sqlt = SQLTransformer().setStatement( f\"SELECT PREDICT('{model_name}/{model_version}', {','.join(features)}) as predictions FROM __THIS__\")\n# Substitute \"X_test\" below with your own test dataset\ndisplay(sqlt.transform(df_clean_1))\n\n```\n\nAnd if you need maximum flexibility inside PySpark, you can convert the registered model into a **PySpark UDF** and call it like any other function over columns. This is handy when scoring needs to be embedded inside more customized DataFrame logic, while keeping the model reference centralized in the registry.\n\n``` from pyspark.sql.functions import col, pandas_udf, udf, lit\n# Substitute \"model\" and \"features\" below with values for your own model name and feature columns\nmy_udf = model.to_udf() features = df_clean_1.columns display(df_clean_1.withColumn(\"predictions\", my_udf(*[col(f) for f in features])))\n\n```\n\nWhatever you choose, the goal is the same: **write predictions back to OneLake** as a durable table. At that point, churn risk stops being “ML output in a notebook” and becomes governed data you can join, track over time, and surface directly in Power BI.\n\n```\n# Save predictions to lakehouse to be used for generating a Power BI report\ntable_name = \"active_predictions\" predictions.write.format('delta').mode(\"overwrite\").save(f\"Tables/{table_name}\") print(f\"Spark DataFrame saved to delta table: {table_name}\")\n\n```\n\n## Step 4: Enable real-time scoring\n\nBatch scoring supports many reporting scenarios, but some decisions need predictions as soon as data changes—for example, a customer updating their profile, opening a support ticket, or onboarding. Waiting for the next scheduled refresh is often too late.\n\nMicrosoft Fabric addresses this with **real-time scoring endpoints**. The same model used for batch scoring can be activated as an online endpoint without rewriting or redeploying it. **‑time scoring endpoints**‑deploying\n\nAn endpoint loads a specific model version once and serves low latency predictions through a secure REST interface. It uses the same feature schema and versioning model as batch inference, keeping training and serving cleanly separated. You can iterate on models without disrupting live scoring and always know which version is serving predictions.‑latency predictions through a secure REST interface. It uses the same feature schema and versioning model as batch inference,\n\nFabric makes this simple. Endpoints are a built-in property of ML models. You can enable them directly from the Fabric UI with a low code experience, preview sample predictions immediately, and then integrate them wherever needed—inside Fabric or from external systems—through a scalable, managed API.‑in property of ML models. You can enable them directly from the Fabric UI with a low‑code experience, preview sample predictions immediately, and then integrate them wherever\n\n![Figure 12 Activate real-time scoring endpoint for a ML model version](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-12-activate-real-time-scoring-endpoint-for.png)*Figure 12 Activate real-time scoring endpoint for a ML model version*\n\n## Step 5: Enrich data during ingestion with Dataflow Gen2\n\nRealtime scoring becomes most powerful when it is embedded in the data ingestion pipeline. Instead of scoring after the fact, predictions are generated as data arrives or changes.‑time scoring becomes most powerful when it is embedded\n\nWith **Dataflow Gen2**, you can call the real-time scoring endpoint as part of a transformation step. New or updated customer records are sent to the model, enriched with churn prediction and probability, and written back to OneLake as part of the ingestion flow.‑time scoring endpoint as part of a transformation step. New or updated customer records are sent to the model, enriched with churn prediction and probability, and written back to OneLake as part of the ingestion flow.\n\nThis keeps your data continuously ML enriched without custom orchestration or brittle glue code. Downstream consumers—reports, dashboards, and analysts—always see the latest predictions, and the scoring logic stays centralized around the managed model, not scattered across pipelines.\n\nBelow is the sample M code for calling the ML endpoint from Dataflow Gen2. Use a service principal as credentials, and ensure it has the necessary permissions on the workspace hosting your ML model endpoint. To maintain workflow reliability, turn off the auto-sleep feature for endpoints.\n\n``` (input as table) as table => let // Feature engineering features = fnFeatureEngineering(input), // Build JSON request body jsonBody = \"{\"\"inputs\"\":\" & Text.FromBinary( Json.FromValue( List.Transform( Table.ToRows(features), each _ ) ) ) & \"}\", // OAuth parameters TenantId     = \"<Your Tenant ID>\", ClientId     = \"<Your Client ID>\", ClientSecret = \"<Your Client Secret>\", Scope        = \"https://api.fabric.microsoft.com/.default\", TokenUrl = \"https://login.microsoftonline.com/\" & TenantId & \"/oauth2/v2.0/token\", FormEncode = (fields as record) as text => Text.Combine( List.Transform( Record.FieldNames(fields), (k) => Uri.EscapeDataString(k) & \"=\" & Uri.EscapeDataString(Text.From(Record.Field(fields, k))) ), \"&\" ),\n\nTokenBodyText = FormEncode([ client_id     = ClientId, client_secret = ClientSecret, grant_type    = \"client_credentials\", scope         = Scope ]),\n\nTokenResponse = Json.Document( Web.Contents( TokenUrl, [ Headers = [#\"Content-Type\" = \"application/x-www-form-urlencoded\"], Content = Text.ToBinary(TokenBodyText) ] ) ), AccessToken = TokenResponse[access_token], // Call ML model endpoint ScoreUrl = \"<Your ML model endpoint url>\", response = Json.Document( Web.Contents( ScoreUrl, [ Headers = [ #\"Content-Type\"  = \"application/json\", #\"Authorization\" = \"Bearer \" & AccessToken ], Content = Text.ToBinary(jsonBody) ] ) ), //  Parse orientation=values output PredictionsRaw = response[predictions], PredictionValues = List.Transform(PredictionsRaw, each _{0}), ProbabilityValues = List.Transform(PredictionsRaw, each _{1}), //  Append columns to table Result = Table.FromColumns( Table.ToColumns(features) & { PredictionValues, ProbabilityValues }, Table.ColumnNames(features) & { \"prediction\", \"probability\" } ) in Result\n\n```\n\n## Step 6: Visualize and act in Power BI\n\nOnce predictions are stored in OneLake, they behave like any other curated dataset in Fabric. You bring them into the semantic model, join them with customer dimensions, and shape them using the same governance patterns you already apply to business data. You can enrich your Power BI report by adding a dedicated view that highlights customers with a high risk of churn.\n\n![Figure 13 Customers with high churn risk](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/02/figure-13-customers-with-high-churn-risk.png)*Figure 13 Customers with high churn risk*\n\nFrom the Power BI user’s perspective, churn risk becomes a clear, refreshed metric that can be filtered, segmented, and tracked over time. Account teams can monitor high-risk customers, prioritize outreach, and act before customers leave.\n\nUnder the hood, this experience is powered by a full machine learning pipeline. On the surface, it feels like standard BI—reliable, explainable, and easy to consume.\n\n## Closing thoughts\n\nThe outcome of this pattern isn’t just a trained model—it’s machine learning insight that behaves like data.\n\nPredictions are written to OneLake as governed; refreshable tables aligned to the Power BI semantic model and surfaced directly in reports. As data changes, insights update. As definitions evolve, logic stays consistent. For business teams, churn risk appears as a trusted metric—filterable, explainable, and ready to act on.\n\nMicrosoft Fabric makes this practical by keeping data, models, and BI on the same platform. There’s no need to export data, rebuild business logic, or maintain custom scoring pipelines. Teams can move from descriptive reporting to predictive insight without adding operational complexity.\n\nThe same approach extends naturally to forecasting, anomaly detection, and propensity modeling. The principle stays the same: start from governed semantics, train models in place, and operationalize predictions where the business already works. In Fabric, machine learning isn’t adjacent to BI—it’s embedded in the analytics experience.\n\n## Try it out\n\n- Explore the [sample repository](https://aka.ms/enrich-pbi-with-ML) and try it out. If you’re already using Power BI and Fabric, you can apply this pattern directly to your own data—starting with a single semantic model, one report, and one prediction, then expand as value becomes clear.\n- Check out [Fabric data science document](https://aka.ms/fabric-ds-docs) for more information.\n- If you’d like to learn more or discuss how this fits your organization, you can [book time with author](https://outlook.office.com/bookwithme/user/45cc8a043ef64c5a8847e619c77d811d@microsoft.com/meetingtype/hcyCyWWlyEyidZZw1oLfZg2?anonymous&amp;ep=mlink).",
  "OutputDir": "_news"
}
