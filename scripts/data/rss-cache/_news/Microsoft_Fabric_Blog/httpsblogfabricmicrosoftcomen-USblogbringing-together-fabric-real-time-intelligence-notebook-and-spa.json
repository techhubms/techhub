{
  "Author": "Microsoft Fabric Blog",
  "EnhancedContent": "Coauthored by QiXiao Wang\n\nBuilding event-driven, real-time applications using Fabric Eventstreams and Spark Notebooks just got a whole lot easier. With the **Preview** of **Spark Notebooks and Real-Time Intelligence integration** — a new capability that brings together the open-source community supported richness of **Spark Structured Streaming** with the real-time stream processing power of **Fabric Eventstreams** — developers can now build **low-latency, end-to-end real-time analytics and AI pipelines** all within Microsoft Fabric.\n\nYou can now **seamlessly access streaming data from Eventstreams directly inside Spark notebooks**, enabling real-time insights and decision-making without the complexity & tediousness of manual coding and configuration.\n\n## Why should you care?\n\nReal-time data is at the heart of modern analytics and AI. If you have ever struggled with stitching together streaming sources, managing secrets, writing and debugging streaming logic, this release changes the game. We have simplified the experience so you can focus on building solutions, not managing boiler-plate code and infrastructure.\n\nHere’s what you can do with these new capabilities:\n\n### Discover real-time sources instantly\n\nExplore Eventstreams and other real-time sources through the **Real-Time hub**, right from within your Fabric notebooks. No more searching for connection details; everything you need is at your fingertips. You can also create new Eventstreams and start ingesting data from nearly 30 (and growing) streaming sources including CDC-enabled databases, message brokers, streaming services and public feeds.\n\n**Example scenario:**\n\nBuilding a fraud detection pipeline? Quickly locate the Eventstream carrying the latest transaction data and start processing it using Spark Structured Streaming without ever leaving the Fabric Notebook experience.\n\n![Screenshot of a Fabric Notebook showing how to use the Explorer to add an Eventstream from the Real-time Hub. The left panel displays the &quot;Explorer&quot; section. This is initially empty with a file icon labeled &quot;No data sources added&quot; and a green button &quot;Add data items,&quot; The main area contains a code editor with a welcome comment.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/screenshot-of-a-fabric-notebook-showing-how-to-use.gif)Real-Time Hub view inside Fabric Notebook — discover Eventstreams in seconds\n\n### Connect and start processing in minutes\n\nKickstart your streaming workflows with **auto-generated PySpark code snippets**. Whether you’re ingesting data or applying transformations, these snippets help you go from zero to streaming in record time. Just click on streams in the Explorer and choose to “Read with Spark”. This automatically generates a PySpark code snippet that has all the boiler plate code to read from the source Eventstream and write the results to “console”. You can now start to add complex business logic and debug using familiar Python/SQL.\n\n**Example scenario:**\n\nNeed to enrich IoT sensor data with historical context for predictive maintenance? Connect to Eventstream and data in your Lakehouse, and start processing within minutes using secure, auto-generated PySpark code.\n\n![Animated screenshot of a Fabric Spark Notebook showing a code editor with a welcome message and a sidebar containing a Eventstream selected in the previous step. The interface includes tabs for Home, Edit, Tools, Run, and various options for managing environments and data connections, highlighting a setup for coding and data analysis. Finally, it shows the steps needed to have the Notebook automatically generate code to connect to the selected Eventstream.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/animated-screenshot-of-a-fabric-spark-notebook-sho.gif)Auto-generated PySpark snippet in Fabric Notebook — your streaming pipeline starts here\n\n### Reuse existing Notebooks\n\nIf your team already has Notebooks built for prototyping or testing, you can now bring them directly into your Eventstream as operational streaming processors. This lets you extend the life of existing assets, reduce duplication, and accelerate development by reusing logic that already works. With seamless notebook loading, you can evolve existing workflows into full production‑grade streaming pipelines with minimal refactoring.\n\n**Example scenario:**\n\nYour data science team has already built Notebooks for real-time anomaly detection. Use them directly from an Eventstream, adding advanced ML models for deeper insights.\n\n![Animated screenshot of a Fabric Eventstream showing interface showing how to select and load an existing Notebook to process a stream containing synthetic Stock market data. A user would add a Spark Notebook as a destination. This opens up a right side panel with drop downs to select a Fabric workspace and a Notebook within. There are options to review &amp; validate the parameters. Once completed, the user can save their configuration and publish the changes.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/animated-screenshot-of-a-fabric-eventstream-showin.gif)Load a Spark Notebook as an Eventstream destination — reuse and collaborate\n\n### Secure, seamless connectivity\n\nForget connection strings and secrets in PySpark code. The enhanced Fabric-optimized, Apache Kafka-based Spark adapter for Eventstreams ensures secure, frictionless connectivity between Fabric Spark jobs and any Eventstream — so your data stays protected while your pipelines run fast. Just specify Eventstream ID and the default/derived stream datasource id and enhanced Kafka adapter takes care of the rest. It validates (using EntraID) and uses the logged in Notebook users token to authorize access to the Eventstream, retrieves the connection details and establishes a secure connection to the Eventstream. This removes a major operational burden while ensuring your pipelines stay fast, reliable, and secure by default.\n\n**Example scenario:**\n\nWorking with sensitive financial data? Built-in security including no-secrets in code means compliance without extra effort.\n\n| from pyspark.sql import SparkSession <br>from pyspark.sql.functions import col <br>from pyspark.sql.types import StringType <br>from pyspark.sql.dataframe import DataFrame<br><br><br>eventstream\\_options = { <br>   “eventstream.itemid”: ‘&lt;ENTER ITEMID FOR YOUR EVENTSTREAM&gt;’, <br>   “eventstream.datasourceid”: ‘&lt;ENTER DATASOURCEID FOR THE NOTEBOOK DESTINATION’}<br><br><br># Read from Kafka using the config map <br>df\\_raw = spark.readStream.format(“kafka”).options(\\*\\*eventstream\\_options).load()<br><br><br>decoded\\_df = df\\_raw.select( <br>   col(“key”).cast(StringType()).alias(“key”), <br>   col(“value”).cast(StringType()).alias(“value”), <br>   col(“partition”), <br>   col(“offset”) <br>)<br><br><br>def showDf(x:DataFrame, y:int): <br>   x.show()<br><br> <br># Print messages to the console <br>query = decoded\\_df.writeStream.foreachBatch(showDf).outputMode(“append”).start() <br>query.awaitTermination() | | --- |\n\nAutomatically generated PySpark snippet using the enhanced Kafka adapter for Eventstreams\n\n## Get started today\n\nThe Spark Notebook integration with Fabric Eventstreams is now available in **Preview**. Try it out and experience how easy real-time data processing can be in Microsoft Fabric. Here are some resources to help you get started:\n\n[Microsoft Fabric Eventstreams Overview – Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/overview?tabs=enhancedcapabilities)\n\n[How to use notebooks – Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/fabric/data-engineering/how-to-use-notebook)\n\n[Real-Time Intelligence in Microsoft Fabric documentation – Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/fabric/real-time-intelligence/)\n\n## We’d love your feedback\n\nIf you find this blog helpful, please give it a thumbs-up! Have ideas for what you’d like to see next? Drop us a comment or reach out with suggestions—we’d love to hear what real-time scenarios you’re exploring and what topics you’d like us to cover in future posts.",
  "FeedName": "Microsoft Fabric Blog",
  "OutputDir": "_news",
  "Tags": [],
  "PubDate": "2026-02-02T14:00:00+00:00",
  "Description": "Coauthored by QiXiao Wang Building event-driven, real-time applications using Fabric Eventstreams and Spark Notebooks just got a whole lot easier. With the Preview of Spark Notebooks and Real-Time Intelligence integration — a new capability that brings together the open-source community supported richness of Spark Structured Streaming with the real-time stream processing power of Fabric Eventstreams …\n\n[Continue reading “Bringing together Fabric Real-time Intelligence, Notebook and Spark Structured Streaming (Preview)”](https://blog.fabric.microsoft.com/en-us/blog/bringing-together-fabric-real-time-intelligence-notebook-and-spark-structured-streaming-preview/)",
  "Title": "Bringing together Fabric Real-time Intelligence, Notebook and Spark Structured Streaming (Preview)",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/bringing-together-fabric-real-time-intelligence-notebook-and-spark-structured-streaming-preview/",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "ProcessedDate": "2026-02-02 23:04:32"
}
