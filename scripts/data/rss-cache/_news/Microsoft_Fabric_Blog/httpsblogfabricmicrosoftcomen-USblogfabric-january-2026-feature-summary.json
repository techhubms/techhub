{
  "Title": "Fabric January 2026 Feature Summary",
  "Tags": [],
  "Description": "Events & Announcements Upgrade your skills and certifications before FabCon Join live sessions with your favorite MVPs and other Fabric experts and request voucher to take the Fabric Analytics Engineer (DP-600) exam or the Fabric Data Engineer (DP-700) exam. Voucher supplies are limited. Register for a live session. Submit your request for a Fabric exam …\n\n[Continue reading “Fabric January 2026 Feature Summary”](https://blog.fabric.microsoft.com/en-us/blog/fabric-january-2026-feature-summary/)",
  "OutputDir": "_news",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/fabric-january-2026-feature-summary/",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "EnhancedContent": "- Events & Announcements\n- Upgrade your skills and certifications before FabCon\n- Have you registered for FabCon Atlanta yet?\n- General\n- Microsoft acquires Osmos to extend Fabric with agentic data engineering\n- Fabric Platform\n- AI Auto-Summary for semantic models (Preview)\n- Parent-Child hierarchy in OneLake catalog\n- Item Reference Variable Type (Preview)\n- Git integration – enhanced support for GitHub Enterprise Cloud with data residency\n- Git integration – Commit to standalone branch\n- Python SDK for Microsoft Fabric REST API (Preview)\n- OneLake\n- Granular APIs for OneLake security (Preview)\n- OneLake security support for Mirrored item types\n- OneLake diagnostics immutable logs\n- Data Engineering\n- High Concurrency mode for Lakehouse operations\n- Fabric connection inside Notebook (Preview)\n- Open and edit workspace’s Notebook inside VS Code\n- Create or replace semantics support for Materialized Lake View\n- Lineage enhancements in Materialized Lake views\n- Data Warehouse\n- Proactive statistics refresh\n- Incremental statistics refresh\n- Result set caching (Generally Available)\n- MERGE Transact-SQL (Generally Available)\n- Real-Time Intelligence\n- Integrating real-time data from streaming sources within private networks into RTI Eventstream\n- MQTT v3 support for the Eventstream MQTT Connector\n- Real-Time Weather Connector for Eventstream (Generally Available)\n- Eventhouse accelerated OneLake shortcuts now support acceleration based on date-time columns\n- Eventhouse accelerated OneLake shortcuts control data freshness latency\n- Simplified KQL syntax for querying shortcuts in Eventhouse\n- Copilot support for querying shortcuts in Eventhouse\n- Data Factory\n- More connectors supported for incremental copy in Copy job\n\n## Events & Announcements\n\n### Upgrade your skills and certifications before FabCon\n\nJoin live sessions with your favorite MVPs and other Fabric experts and request voucher to take the Fabric Analytics Engineer (DP-600) exam or the Fabric Data Engineer (DP-700) exam. Voucher supplies are limited.\n\n**[Register](https://aka.ms/FabricCert/live)** for a live session.\n\n[**Submit your request**](https://aka.ms/FabCert/2026) for a Fabric exam voucher.\n\n### Have you registered for FabCon Atlanta yet?\n\nJoin us from March 16-20, 2026, in Atlanta, GA! For the first time ever, SQLCon will be co-located with FabCon, bringing together the entire data community in one place.\n\nTwo Conferences. One pass. One Epic Week. Join us for the ultimate Microsoft Fabric, SQL, Power BI, Real-Time Intelligence, AI, and Databases community-led event.\n\nMaster SQL Server 2025 internals in the morning, dive into Fabric innovations in the afternoon, attend Power Hour before dinner and network with peers from both communities. The sessions you choose are totally up to you.\n\n**[Register](https://aka.ms/fabcon)** with code FABCOMM to save $200.\n\n## General\n\n### Microsoft acquires Osmos to extend Fabric with agentic data engineering\n\nMicrosoft has acquired Osmos, an agentic AI data engineering platform designed to help simplify complex and time-consuming data workflows. Together, we’re accelerating the future of autonomous data engineering directly in Microsoft Fabric, helping customers turn data in OneLake into analytics- and AI-ready assets faster.\n\nTo learn more, read the [acquisition announcement](https://blogs.microsoft.com/blog/2026/01/05/microsoft-announces-acquisition-of-osmos-to-accelerate-autonomous-data-engineering-in-fabric/).\n\n## Fabric Platform\n\n### AI Auto-Summary for semantic models (Preview)\n\nThe Auto-Summary for semantic models is an AI-generated high-level summary that helps you quickly understand an item’s purpose and main characteristics without opening the item or reviewing its full metadata. It makes it easier to understand unfamiliar items and compare them directly in the OneLake catalog Explorer.\n\nThe summary is created based on the item’s metadata and structure. Users with the appropriate Copilot capacity and permission can generate the summary from the quick actions in the main explore tab or directly from the semantic model’s item details page. Each time you return to the Catalog, a new summary can be generated so that you always see the most up-to-date version.\n\nAfter a summary is generated, you can generate another version, copy the text for use elsewhere, or provide feedback on the quality.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-1.png) ![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-2.png)\n\n### Parent-Child hierarchy in OneLake catalog\n\nOneLake catalog now includes a clear Parent–Child item structure that makes it easier to understand how your data items relate to each other. Instead of showing everything in one flat list, the catalog now groups connected items together and displays them in the appropriate hierarchy.\n\nFor example, a Lakehouse appears with its autogenerated SQL Analytics Endpoint, and an Eventhouse appears with its related KQL DBs. You can easily view or hide these related items using the expand and collapse feature, giving you a cleaner view when you need it and more detail when you want it. This helps you quickly find what you need, reduces confusion between similarly named items, and makes it clearer which item you should connect to for each task.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-3.png)\n\nLearn more from the [OneLake catalog](https://learn.microsoft.com/fabric/governance/onelake-catalog-explore) documentation.\n\n### Item Reference Variable Type (Preview)\n\nThe Variable Library in Microsoft Fabric continues to evolve, and one of the most impactful additions arriving with the upcoming release is the Item Reference variable type, a new way to reference Fabric items in a structured, resilient, and more secure way.\n\nThis feature simplifies managing configuration across different environments by allowing you to reference Fabric items directly instead of hard‑coding values as strings. We will also validate permissions to the referenced item, ensuring stricter governance on changing the values being used within variables.\n\n**Why this matters**\n\nThe new **Item Reference** variable addresses these issues:\n\n- **Stronger security:** Variables can only reference items that the user is authorized to access.\n- **Better clarity:** Instead of obscure GUIDs shown in the UI, references clearly provide meaningful information of the item name, type, and its location. For different value sets, simply choose the item in the proper workspace.\n- **Improved reliability:** Structured metadata eliminates issues caused by renaming items or manually editing IDs.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-4.png) ![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-5.png)\n\n**Better experiences across Fabric**\n\nThe Fabric ‘Select variable’ dialog lets you choose a reference from only the items you’re permitted to access and surfaces helpful details such as item name, type, and location, so you can confidently select the right one. In addition, the experience of configuring it across Fabric in UDF, Lakehouse shortcuts, and other Fabric items is unified through the following dialog where you can easily filter and search your variable:\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-6.png)\n\n**Key points**\n\n- At minimum, **READ** permission on an item to set it as a value.\n- When deploying or updating, Fabric validates that all referenced items in the target stage active value-set exist, and that you have permission for them.\n- Early support will be available in Lakehouse Shortcuts, UDF code and Notebook code (Not supported in %%Configure).\n- [Upcoming items](https://learn.microsoft.com/fabric/cicd/variable-library/variable-library-overview#supported-items) can be tracked in the documentation.\n\n**Looking ahead**\n\nItem reference is the first step toward unified, scalable configuration in Fabric. Next, we will bring **Connection reference**, a new variable type for managing external connections (AWS, Blob Storage, and more) with the same secure and consistent experience. Stay tuned!\n\n**Ready to get started?**\n\nThe **Item reference** variable type is more than “just another variable.” is a foundational step toward a more maintainable, predictable, and scalable Fabric ecosystem. With its structured design, CI/CD-friendly behavior, and shared resolution model, it removes many long-standing pain points around configuration, item linking, and environment consistency. Explore the Variable Library in your Fabric workspace and try creating your first **Item reference** variable today!\n\n### Git integration – enhanced support for GitHub Enterprise Cloud with data residency\n\nFabric workspaces can now connect to GitHub Enterprise Cloud instances with data residency (ghe.com), allowing regulated customers to use Microsoft Fabric – Git Integration.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-7.png) To learn more, refer to the [Microsoft Fabric and GitHub Enterprise Cloud with data residency support](https://learn.microsoft.com/fabric/cicd/github-data-residency-support).\n\n### Git integration – Commit to standalone branch\n\nWe have introduced a highly requested flexibility feature that allows users to create a new branch from their last synchronization point and commit current changes to it in a single action.\n\n**In practice, this means:**\n\n- No commitment to the connected branch\n- Branch off on the fly without switching context\n- Commit to another branch even if there are incoming updates\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-8.png)\n\n**This new functionality can be useful for scenarios like:**\n\n- Handling conflict situation\n- Working in isolation\n- Backing up your current work on specific branch\n- Sharing your work with others\n\nTo learn more, refer to the [Microsoft Fabric – Get started with Git Integration – commit to standalone branch](https://learn.microsoft.com/fabric/cicd/git-integration/git-get-started?tabs=azure-devops%2CAzure%2Ccommit-to-git#commit-changes-to-git)\n\n### Python SDK for Microsoft Fabric REST API (Preview)\n\nPython SDK is now available on PyPI as microsoft-fabric-api and you can get started today.\n\nThe Microsoft Fabric Python SDK is a client library that wraps the Fabric REST APIs, allowing developers to manage, automate, and interact with Microsoft Fabric resources directly from Python. Instead of hand-crafting HTTP requests and managing authentication manually, you can use idiomatic Python objects and methods to perform common tasks.\n\nThis SDK is currently in preview and is installed from PyPI, [microsoft-fabric-api · PyPI](https://pypi.org/project/microsoft-fabric-api/)\n\n**The Fabric REST APIs support automation of tasks:**\n\n- Listing and managing workspaces\n- Automating deployment processes\n- Interacting with Fabric items programmatically\n- Integrating workflows into CI/CD and DevOps pipelines\n\n**The new SDK offers:**\n\n- Built-in support for authentication via Azure identity libraries\n- Typed API clients for REST endpoints\n- Simplified serialization/deserialization\n- Higher-level helpers that make script automation easier\n\n**Install the SDK**\n\nYou can install the SDK using pip:\n\nBash\n\npip install microsoft-fabric-api\n\nThis will grab the latest preview release from PyPI.\n\n**Getting Started**\n\nThe following is a quick example to help you start exploring Fabric resources.\n\n**Step 1: Authenticate**\n\nThe SDK uses Azure identity libraries (such as DefaultAzureCredential) to get tokens automatically. Make sure you’re logged in via Azure CLI or have environment credentials set.\n\npython\n\nfrom azure.identity import DefaultAzureCredential\n\nfrom microsoft\\_fabric\\_api import FabricClient\n\n# Create your credential (Azure CLI login, managed identity, etc.)\n\ncredential = DefaultAzureCredential()\n\nInitialize the client\n\nfabric\\_client = FabricClient(credential)\n\n**Step 2: List Workspaces**\n\nOnce authenticated, you can call Fabric REST APIs through the SDK. For example, to list workspaces you have access to:\n\npython\n\nworkspaces = fabric\\_client.core.workspaces.list\\_workspaces()\n\nprint(f”Found {len(workspaces)} workspaces:”)\n\nfor ws in workspaces: print(f”- {ws.display\\_name} (Capacity ID:{ws.capacity\\_id})”)\n\nThis example demonstrates how straightforward it is to work with Fabric resources using Python.\n\n## OneLake\n\n### Granular APIs for OneLake security (Preview)\n\nMicrosoft Fabric is introducing new **granular REST APIs for security role management**, giving developers finer control over how OneLake permissions are created, retrieved, and managed programmatically. In addition to the existing batch role API, Fabric now supports discrete **Get, Create, and Delete role operations**, enabling users to work with individual roles without submitting a full role collection.\n\nThese new APIs make it easier to build automation‑friendly security workflows and integrate Fabric security into CI/CD pipelines. By combining bulk and granular role management options, Fabric offers greater flexibility for organizations managing security at scale—reinforcing our commitment to open, developer‑first, and interoperable security in OneLake.\n\n![API swagger listing the GET dataAccessRoles/{roleName}, POST dataAccessRoles/, and DELETE dataAccessRoles/{roleName}](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/api-swagger-listing-the-get-dataaccessroles-rolen.png)\n\nGranular APIs to get, create, and delete a single role.\n\nCheck out the [OneLake Data Access Security](https://learn.microsoft.com/rest/api/fabric/core/onelake-data-access-security) documentation.\n\n### OneLake security support for Mirrored item types\n\nMicrosoft Fabric now supports defining OneLake data access roles on all mirrored item types, extending granular, role-based security to data replicated into OneLake from transactional systems. With this update, customers can control access to mirrored data using table-, row-, or column-level security, ensuring permissions are enforced consistently at the OneLake layer regardless of how the data is consumed.\n\n![A user managing OneLake security roles for their mirrored database item.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/a-user-managing-onelake-security-roles-for-their-m.png)\n\n### Manage data access roles in your mirrored database\n\nBy attaching security directly to mirrored data in OneLake, this release enables secure reuse across teams through shortcuts and downstream analytics experiences.\n\nOrganizations can mirror data once, apply fine-grained access controls at the source, and confidently share data without duplication—simplifying governance while scaling analytics across Fabric.\n\nYou can learn more about [OneLake security](https://learn.microsoft.com/fabric/onelake/security/data-access-control-model#permissions-and-supported-items) in the documentation.\n\n### OneLake diagnostics immutable logs\n\nOneLake diagnostic events can now be made immutable, which means that the JSON files that contain diagnostic events can’t be tampered with, or deleted, during the immutability retention period.\n\nTo learn more, please refer to [OneLake diagnostics](https://learn.microsoft.com/fabric/onelake/onelake-diagnostics-overview#enabling-immutable-diagnostic-logs) documentation.\n\n## Data Engineering\n\n### High Concurrency mode for Lakehouse operations\n\nHigh Concurrency mode for Lakehouse operations in Microsoft Fabric is a new capability designed to dramatically optimize Spark resource utilization for common tasks like **Load to Table** and **Preview.**\n\nPreviously, when these operations fell back to Spark execution—especially in environments with Managed Virtual Networks—users often faced significant startup latency of three to five minutes per job or found that a single preview operation could hold a session for up to 20 minutes, blocking concurrency on smaller capacities.\n\nHigh Concurrency mode solves this by allowing up to five independent Lakehouse jobs triggered by the same user within the same workspace to share a single, underlying Spark session.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-11.png)\n\nThis innovative approach delivers immediate improvements in both performance and cost-efficiency. By reusing existing sessions, subsequent table loads or previews can start in under five seconds, even with network security features enabled.\n\nFurthermore, this mode offers a significant price-performance advantage: only the initiating Spark session that starts the shared application is billed, meaning subsequent operations sharing that session incur no additional compute costs.\n\nThese optimized sessions are automatically managed by Fabric and are easily traceable in the Monitoring hub via the HC\\_&lt;lakehouse\\_name&gt; naming convention, ensuring you have full visibility into your accelerated workflows.\n\nTo learn more, refer to the [High concurrency mode for Lakehouse operations in Microsoft Fabric](https://learn.microsoft.com/fabric/data-engineering/high-concurrency-for-lakehouse-operations) documentation.\n\n### Fabric connection inside Notebook (Preview)\n\nWith this update, notebooks now offer the familiar Get Data feature, making it simpler and safer for users to access data from frequently used sources like Azure Blob Storage, PostgreSQL, Azure Key Vault, and S3.\n\nThis update supports connections to cloud data sources. If you need to access on-premises data sources, please use the [Managed Private Endpoint](https://learn.microsoft.com/fabric/security/security-managed-private-endpoints-overview) option.\n\nCreate a Fabric connection inside notebook with the new **Connection** flow. Once the connection is ready, you can generate the code snippet to access the underlying data source. Apply the connection’s credential detail to other data source, if applicable.\n\n![Create a Fabric Connection with the build-in flow inside notebook. User can choose the data source and provide the authentication detail to create the connection. The connection is attached to the Notebook after this flow.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/create-a-fabric-connection-with-the-build-in-flow.png)\n\nCreate Fabric connection inside Notebook\n\n![Generate the python code snippet from the connection. The credential detail will be retrieved from the connection and used to query the data source which this connection is set up for.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/generate-the-python-code-snippet-from-the-connecti.png)\n\nGenerate code snippet to query data source\n\nThe supported authentication types include:\n\n- **Basic Authentication:** Supported for Azure SQL Database and other databases that support basic authentication\n- **Account Key Authentication:** Supported for REST API data sources that require Account key authentication\n- **Token Authentication:** Supported for data sources that require token-based authentication\n- **Workspace Identity Authentication:** Supported for Fabric workspace identity authentication\n- **Service Principal Authentication (SPN):** Supported for data sources that require SPN-based authentication\n\nYou can also create the connection inside the Fabric data source management page, but you need to ensure the toggle named **Allow Code-First Artifacts like Notebooks to access this connection (Preview)** has been enabled. This toggle can only be set during the creation of the connection and can’t be modified later.\n\n![Enable this connection can be used inside the code-first artifact such as Notebook. The connection can be listed and used inside notebook only with this toggle is selected.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/enable-this-connection-can-be-used-inside-the-code.png)\n\nAllow code-first artifact like Notebooks to access this connection\n\nAfter creating the connection, it appears under **Global permissions,** ready to link to the notebook. Select **Connect** in the context menu to link this connection to the notebook.\n\nTo learn more, refer to the [Fabric connection inside Notebook](https://learn.microsoft.com/fabric/data-engineering/fabric-connection-with-notebook) documentation.\n\n### Open and edit workspace’s Notebook inside VS Code\n\nUsers can now directly open and edit the Fabric Notebook within its VS Code editor. Previously, the Fabric Data Engineering VS Code extension enable notebook development in VS Code only after downloading the notebook. With this update, the notebook can be accessed and edited from the selected remote workspace, and any changes saved in VS Code will automatically update the content in the remote workspace.\n\nTo open the notebook, select the **Open Notebook Folder** icon located on the toolbar of the desired Notebook.\n\n![Open this notebook folder in the VFS(Virtual File System) mode. with this view, the notebook content is open in the VS Code explore view without downloading to local desktop.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/open-this-notebook-folder-in-the-vfsvirtual-file.png)\n\n**Direct Open Notebook**\n\nOnce activated, the VS Code Explorer View opens the selected Notebook and shows the Fabric workspace with its notebooks, including the one currently open. With this new view, you can open multiple notebooks from the same Fabric workspace, and even open multiple different Fabric workspaces.\n\n**Multi-Notebook and Multi-workspace view**\n\n![Open multiple fabric workspaces and the notebooks from these workspaces inside the same VS Code window. ](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/open-multiple-fabric-workspaces-and-the-notebooks.png)\n\n### Create or replace semantics support for materialized lake view\n\nMaterialized lake views now support **create or replace** semantics, making it significantly easier to evolve data models as business requirements change. This enhancement allows users to update an existing materialized lake view—whether that involves adding or modifying columns, adjusting transformation logic, or updating metadata—without the need to drop and recreate the view. As a result, it becomes faster to iterate while avoiding the operational risks and disruptions that typically come with deleting and rebuilding core analytic objects.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-17.png)\n\nFor more information, refer to the [Spark SQL Reference for Materialized Lake Views](https://learn.microsoft.com/fabric/data-engineering/materialized-lake-views/create-materialized-lake-view) documentation.\n\n### Lineage enhancements in materialized lake views\n\nLineage for materialized lake views now clearly shows the Notebook Source, making it simple to trace each view to its origin. Deleted sources are also flagged for quicker troubleshooting and more reliable refresh scheduling.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-18.png)\n\nLearn more about this feature in the [materialized lake views](https://learn.microsoft.com/fabric/data-engineering/materialized-lake-views/overview-materialized-lake-view) documentation.\n\n## Data Warehouse\n\n### Proactive statistics refresh\n\nProactive statistics refresh for the Data Warehouse and Lakehouse SQL Analytics Endpoint is a built-in optimization that enriches the automatic management of vital column statistics. With this feature enabled, column statistics that were created during SELECT queries may now be updated by the engine proactively as their data changes. This reduces the likelihood of a query being prolonged by statistic maintenance during plan generation, thus reducing query execution time. Now more than ever, there are fewer reasons to maintain statistics manually.\n\nSee the [statistics](https://learn.microsoft.com/fabric/data-warehouse/statistics) documentation for more information.\n\n### Incremental statistics refresh\n\nIncremental statistics refresh is a performance enhancement in Data Warehouse and Lakehouse SQL Analytics Endpoint that improves the execution time of certain column statistic updates. Columns in long tables that experience mostly **INSERT** or **ADD** operations since the last statistic refresh are eligible for incremental refresh. As statistic operations can sometimes contribute to SELECT queries’ execution time, queries over these types of tables will benefit most from this improvement.\n\n![A “before” visualization showing that the entire column segment is re-sampled for statistic refreshes, and an “after” visualization showing that now only the newly added rows are sampled for statistic refreshes.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/a-before-visualization-showing-that-the-entire-c.gif)\n\nIncremental statistics refresh is a quicker mode of automatically updating statistics, compared to before.\n\nSee the [statistics](https://learn.microsoft.com/fabric/data-warehouse/statistics) documentation for more information.\n\n### Result set caching (Generally Available)\n\nIn 2025, we introduced result set caching (Preview) for Data Warehouse and Lakehouse SQL Analytics Endpoint. This feature expedites repetitive queries by quickly returning cached results instead of recomputing original queries from scratch. Out-of-the-box performance booster is now generally available and enabled by default. No tuning or configurations required. Enjoy the benefits of result set caching today!\n\n![View of the query editor in Microsoft Fabric, where a T-SQL query is being run with result set caching enabled. The query completes quickly in about 1.5 seconds, and the message “Result set cache was used” is visible in the Message Output, to indicate that result set caching was applied.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/view-of-the-query-editor-in-microsoft-fabric-wher.gif)\n\nResult set caching is now enabled by default, boosting eligible queries whenever possible\n\nTo learn more about this feature, see the r[esult set caching](https://learn.microsoft.com/fabric/data-warehouse/result-set-caching) documentation.\n\n### MERGE Transact-SQL (Generally Available)\n\nIn September 2025, the MERGE statement was released for preview in Data Warehouse. This command provides a standardized approach to transforming your data by incorporating conditional logic and DML actions all within a single statement.\n\n![Graphical visualization of multiple Data Modification Language statements happening one by one: Deleting one row from a table, then Inserting two rows to a table, then Updating one row in a table. Then another visual below that showing all 3 operations happening at once with the MERGE command.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/graphical-visualization-of-multiple-data-modificat.gif)\n\nMERGE encapsulates INSERTs, UPDATEs, and DELETEs all within a single statement.\n\nFor more information, see [MERGE (Transact-SQL) – SQL Server](https://learn.microsoft.com/sql/t-sql/statements/merge-transact-sql?view=fabric).\n\n## Real-Time Intelligence\n\n### Integrating real-time data from streaming sources within private networks into RTI Eventstream\n\nReal-Time Intelligence Eventstream is designed to bring real-time data from diverse sources, transform it, and effortlessly route it to various destinations. For sources that run in private network environments such as cloud virtual network or on-premises infrastructures, a secure method is required to allow Eventstream to access the source.\n\nThe streaming connector’s support for virtual networks (vNet) and on-premises environments offers a secure, managed pathway, enabling Eventstream to reliably connect with these private-network streaming sources.\n\nTo enable data transfer from a source within a private network into Eventstream, it is necessary to establish an Azure managed virtual network as an intermediary bridge, as illustrated in the diagram. The Azure virtual network should be connected to the private network hosting the data source using appropriate methods, such as VPN or ExpressRoute for on-premises scenarios, and private endpoints or network peering for Azure sources, etc. Subsequently, the Eventstream streaming connector instance will be injected into this virtual network through SWIFT injection, allowing secure connectivity between the connector and the data source located within the private network.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-22.png)\n\nTo facilitate SWIFT streaming connector injection into an Azure virtual network you’ve created, Fabric provides a centralized location for network or data engineers to manage the references to Azure virtual network resources. The **streaming virtual network data gateway** in Fabric serves this purpose for Eventstream.\n\nUnli**ke Virtual network data gateways** and **On-premises data gateways,** this new option does not require cluster provisioning or additional capacity. However, the user experience across all three gateway types remains largely similar. You create and manage the **streaming virtual network data gateway** in the **Manage Connections and Gateways** page in Fabric. Select it when setting up streaming connections for Eventstream sources using the **Get Events wizard** or **Streaming virtual network.** After that, you can configure your Eventstream data source as usual.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-23.png)\n\nFor more information and a step-by-step guide on getting started, please refer to documentation on [Connect to Streaming Sources in Virtual Network or On Premises with Eventstream](https://aka.ms/ESConnectorPrivateNetworkSupport).\n\n### MQTT v3 support for the Eventstream MQTT Connector\n\nWe now support **MQTT v3 in the Eventstream MQTT Connector**, making it compatible with widely used industry protocols.\n\nWith this update, the connector now supports **MQTT v3.1** and **v3.1.1**, making it easier than ever to stream data from popular IoT platforms and MQTT brokers directly into Eventstream. Once ingested, the data can be immediately leveraged by Fabric Real-Time Intelligence for real-time analytics and alerting, enabling teams to detect patterns and act on IoT events as they happen.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-24.png)\n\nBy bridging MQTT v3 and Eventstream, teams can take full advantage of Eventstream’s **scalability**, **reliability**, and **real-time processing capabilities**—without modifying their existing broker setup. This update significantly lowers the barrier to adoption and empowers organizations to build robust, event-driven streaming pipelines with confidence.\n\nFor more details, visit [Add MQTT source to an eventstream](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/add-source-mqtt).\n\n### Real-Time Weather Connector for Eventstream (Generally Available)\n\nBring production-ready weather data streaming to your Fabric for real-time analytics. As part of this release, we’ve added several key enhancements based on early user feedback. You can now include a location name directly in the event payload, making processing, filtering, and analytics significantly easier. This removes the need for additional enrichment steps and simplifies stream processing logic.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-25.png)\n\nWe’ve also introduced a tenant-level control switch, allowing tenant admin to enable or disable the Weather Connector for the entire organization. This gives teams better governance, cost control, and operational clarity when managing Eventstream with weather data streams.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-26.png)\n\nThese features are now fully supported and ready for mission-critical use cases—from real-time analytics to alerting and dashboard.\n\nWe look forward to seeing your streaming workflow. For more information, please visit [Add a real-time weather source to an eventstream](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/add-source-real-time-weather).\n\n### Eventhouse accelerated OneLake shortcuts now support acceleration based on date-time columns\n\n[Accelerated OneLake shortcuts](https://learn.microsoft.com/fabric/real-time-intelligence/query-acceleration) in Eventhouse indexes and caches data in OneLake, allowing performant queries on top of delta/Iceberg tables in OneLake. By default, the system uses the modificationTime in delta\\_log to determine the scope of data to accelerate.\n\nWe are adding a new property *HotDateTimeColumn* to the query acceleration policy to specify the name of datetime column in the Delta table whose values will be used to determine hot-cache eligibility. When set, data files whose rows have values within the configured Hot period (and/or HotWindows) are selected for caching. You can override the default behavior by changing the query acceleration policy.\n\nFor more information, refer to the [.alter-merge query acceleration policy command](https://learn.microsoft.com/kusto/management/alter-merge-query-acceleration-policy-command?view=microsoft-fabric) documentation.\n\n### Eventhouse accelerated OneLake shortcuts control data freshness latency\n\nMaxAge property in query acceleration policy controls the data freshness. The shortcut returns accelerated data if the last index refresh time is greater than @now – MaxAge. Otherwise, the shortcut operates in non-accelerated mode. The default for MaxAge is 5 minutes.\n\nYou can now override the default MaxAge at query execution. Users can fine-tune acceleration scope dynamically, balancing freshness and performance without altering policy definitions.\n\n**Example**\n\nBelowThis example overrides the MaxAge to 10 sec during querytime.\n\nexternal\\_table( TableName, 10s )\n\nIf you use this property, the external table returns accelerated data if the last index refresh time is greater than @now – MaxAgeOverride. Minimum: 1s\n\nFor more information, refer to the [.alter query acceleration policy command](https://learn.microsoft.com/kusto/management/alter-query-acceleration-policy-command?view=microsoft-fabric) documentation.\n\n### Simplified KQL syntax for querying shortcuts in Eventhouse\n\nEventhouse shortcuts can now be queried just like Tables, eliminating the need for explicit external\\_table() syntax, making queries cleaner and more intuitive. This simplifies external data access by allowing direct querying of external tables using standard table names.\n\nExample: sample 10 rows from shortcut **T** sing the following syntax: **T | take 1**\n\n### Copilot support for querying shortcuts in Eventhouse\n\nCopilot in KQL Queryset and Real-Time Dashboards can now generate KQL for shortcuts.\n\nIn Eventhouse, shortcuts are implemented as external tables. While querying shortcuts was not previously available with Copilot, this functionality is now supported, allowing users to query shortcuts in the same manner as native tables in Eventhouse.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-27.png)\n\n## Data Factory\n\n### More connectors supported for incremental copy in Copy job\n\nWe are expanding Copy job with broader multi‑cloud connectivity and stronger incremental copy support. Incremental copy in Copy job now supports additional connectors, including:\n\n- Google Big Query\n- Google Cloud Storage\n- DB2\n- ODBC\n- Fabric Lakehouse table\n- Folder\n- Azure files\n- SharePoint List\n- Amazon RDS for SQL Server\n- Amazon RDS for Oracle\n- Azure Data Explorer\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2026/01/word-image-32939-31.png) To learn more, please refer to the [Copy job in Data Factory](https://learn.microsoft.com/fabric/data-factory/what-is-copy-job) documentation.",
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "ProcessedDate": "2026-01-27 21:01:52",
  "PubDate": "2026-01-27T09:00:00+00:00",
  "FeedName": "Microsoft Fabric Blog",
  "Author": "Microsoft Fabric Blog"
}
