{
  "Title": "Measuring actual AI Impact for Engineering with Apache DevLake",
  "Tags": [
    "All things Azure",
    "Azure DevOps",
    "Developer Productivity",
    "DevLake",
    "github",
    "GitHub Copilot",
    "ROI",
    "Thought leadership"
  ],
  "PubDate": "2026-02-26T10:06:19+00:00",
  "FeedName": "Microsoft All Things Azure Blog",
  "FeedLevelAuthor": "All things Azure",
  "ProcessedDate": "2026-02-26 10:13:54",
  "FeedUrl": "https://devblogs.microsoft.com/all-things-azure/feed/",
  "OutputDir": "_news",
  "EnhancedContent": "*If you want to skip the explain and get started super quick with adoption + impact insights, use [gh-devlake](https://github.com/DevExpGBB/gh-devlake) to deploy a GitHub Copilot impact dashboard in a few CLI commands.*\n\nSo! You’ve rolled out GitHub Copilot to your engineering teams. You’ve got the built-in dashboards. You know how many seats are assigned, what the acceptance rates look like, which editors your teams prefer. Maybe you’ve even pulled the Copilot Metrics API and built some charts.\n\nBut here’s the question your VP of Engineering or CTO is actually asking:\n\n>\n> “Is GitHub Copilot making us ship faster? Are we more reliable? Is code review getting better?”\n>\n\nAnd the honest answer, if you’re only looking at data from our Copilot Metrics API, is: *you don’t know*. You know adoption. You don’t know impact.\n\nI’ve been working with engineering teams evaluating Copilot for a while now, and this gap — between usage data and delivery outcomes — keeps coming up. The usage metrics live in one place. The deployment data lives in your CI/CD tool. The incident data lives in your issue tracker. The code review data lives in GitHub. They’re all siloed. And the question leadership is asking requires connecting them.\n\nThis post is about a solution that connects them: [Apache DevLake](https://devlake.apache.org/). And more specifically, why it’s *not* just another dashboard — it’s a data warehouse that lets you actually answer the impact question.\n\n[![Apache Devlake](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAACJAQMAAACsFZffAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAHElEQVRYhe3BAQ0AAADCoPdPbQ43oAAAAAAALg0U3wABmccQygAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2026/02/Screenshot-2026-02-20-195752.webp)\n\n## The Observability Gap\n\nGitHub gives you solid Copilot [metrics out of the box](https://docs.github.com/en/enterprise-cloud@latest/copilot/concepts/copilot-usage-metrics/copilot-metrics?apiVersion=2022-11-28&amp;versionId=enterprise-cloud%40latest&amp;category=copilot&amp;subcategory=copilot-metrics):\n\n- **Seats assigned and active** — who has it, who’s using it\n- **Acceptance rates** — how often developers accept suggestions, by language, editor, and model\n- **Feature breakdown** — completions vs. chat vs. PR summaries\n- **Activity trends** — daily/weekly/monthly active users\n\nThis is useful for tracking rollout health. e.g. Is adoption growing? Which languages see the most use?\n\nBut these metrics exist in isolation. They tell you Copilot is being *used*. They don’t tell you what that usage is *doing* to your software delivery. To answer that, you need to correlate Copilot adoption data with the metrics that actually measure delivery performance — and those live in completely different systems.\n\nThe missing link is **correlating AI adoption with actual software delivery outcomes**. Not just “developers are accepting 30% of suggestions” but “during weeks with high Copilot usage, PR cycle times dropped by 33% and deployment frequency doubled.”\n\nThat’s a fundamentally different conversation.\n\n[![GitHub Copilot Adoption vs Impact](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAABTAQMAAAAfsKiWAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAGklEQVRIie3BMQEAAADCoPVPbQ0PoAAAAI4MDKUAARCeuhkAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2026/02/Screenshot-2026-02-20-193857-scaled.webp)\n\n## What Makes DevLake Different: A Data Warehouse, Not a Dashboard\n\n[Apache DevLake](https://devlake.apache.org/) is an open-source dev data platform. It’s not just a pre-built dashboard you plug in and stare at. It’s a data warehouse that ingests from your DevOps tools, normalizes everything into a standard data model, and lets you query it with SQL through Grafana.\n\nIt connects to **20+ data sources**: GitHub, GitLab, Jira, Jenkins, Azure DevOps, SonarQube, PagerDuty, Bitbucket, CircleCI, and more. Each tool’s data gets standardized in one coherent schema. A Jenkins build and a GitHub Actions workflow both become `cicd_deployment_commits` .\n\nThis is deceptively powerful: your Jira issues, GitHub PRs, Jenkins deployments, and Copilot metrics are all sitting in the same SQL database. You can correlate them. You can ask questions that span tools.\n\nOn top of this, you can organize your data at team/repo/org levels, so you can compare data across your developer community if you are in a large Enterprise.\n\n[![Apache Devlake Connections](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAADfAQMAAAB2wnw1AAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAH0lEQVRoge3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAuDQh+QABWh2QYAAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2026/02/Screenshot-2026-02-20-200131-scaled.webp)\n\n### The Configuration Model\n\nThe way it’s setup is actually quite simple. You define:\n\n- **Connections** — authenticated links to each data source (your GitHub org, your Jira instance, your Jenkins server)\n- **Scopes** — which specific repos, boards, or projects to collect from a connection\n- **Project** — groups multiple connections/scopes together and ties the data coherently\n- **Blueprint** — schedules recurring syncs across your connected data sources\n\nOnce configured, DevLake handles the ingestion, normalization, and makes everything queryable. The setup is straightforward — the [`gh-devlake` CLI extension](https://github.com/DevExpGBB/gh-devlake) handles it in a few commands.\n\n## DORA as the Framework\n\nIf you’re going to measure software delivery performance, you need a framework that gets you started. There are several: [GitHub has it’s own](https://assets.ctfassets.net/wfutmusr1t3h/us6AUuwawrtNGTlwlT9Ac/f0fce86712054fc87f10db28b20f303b/GitHub-ESSP.pdf), and [Microsoft adheres to the SPACE Framework](https://developer.microsoft.com/en-us/developer-experience). DORA — the [DevOps Research and Assessment](https://dora.dev/), is what most teams know as a starting point.\n\n>\n> **A note on frameworks**: No single framework is perfect. DORA, SPACE, and others each have blind spots. For your DevEx program, you’ll want to treat them as starting points.. DORA gets you started with a solid foundation; the real work is evolving it to match your organization’s actual delivery context by identifying insights that matter.\n>\n\nDevLake implements DORA with built-in Elite/High/Medium/Low benchmarking. You don’t need to calculate the metrics yourself or figure out how to map your CI/CD data to the framework. Configure your deployment patterns and incident labels, and DevLake does the rest.\n\n[![Apache Devlake DORA Dashboard](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAB4AQMAAABy211jAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAG0lEQVRYhe3BgQAAAADDoPlTH+ECVQEAAADANxJIAAGPZbvpAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2026/02/Screenshot-2026-02-20-200705.webp)\n\n## Adding AI to the Equation: The Copilot Impact Dashboard\n\nOur team at Microsoft has added a working `gh-copilot` plugin that collects Copilot metrics: daily active users, acceptance rates, language and editor breakdowns, PR summary usage, seat utilization — all the data you’d get from the [Copilot Metrics API](https://docs.github.com/en/copilot/concepts/copilot-usage-metrics/copilot-metrics?apiVersion=2022-11-28&amp;versionId=free-pro-team%40latest&amp;category=copilot&amp;subcategory=copilot-metrics), normalized into DevLake’s domain model.\n\nOn top of this, there are two Grafana dashboards:\n\n### The Adoption Dashboard\n\nThis is your Copilot rollout health check. 30 panels tracking:\n\n- **DAU / WAU / MAU** — Active user trends over time\n- **Acceptance rates** — By language, by model, by editor\n- **Feature mix** — Completions vs. chat vs. PR summaries\n- **Seat effectiveness** — Are the seats you’re paying for actually being used?\n- And more…\n\nThink of this as the equivalent of GitHub’s built-in metrics, but living inside the same platform where all your other data sits.\n\n[![Devlake Copilot Adoption Dashboard](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAC8AQMAAAD4ogHBAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAHklEQVRYhe3BMQEAAADCoPVPbQsvoAAAAAAAAAB+BhykAAEIlnm4AAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2026/02/Screenshot-2026-02-20-200959.webp)\n\n### The Impact Dashboard\n\nThis is the one that answers the question leadership is asking. It **correlates Copilot adoption intensity with DORA metrics**.\n\nHere’s how it works:\n\n| Section | What It Measures | | --- | --- | | **Adoption Intensity** | Overall adoption trend, tier distribution, aggregate correlation | | **PR Velocity Impact** | Cycle time, coding time, pickup time, review time, PR throughput | | **Deployment Frequency** | Deploys per week correlated with adoption | | **Change Failure Rate** | CFR % correlated with adoption (negative r = improvement) | | **Recovery Time (MTTR)** | Mean time to recovery by adoption tier | | **Code Review Time** | Review duration trends across adoption levels | | **Code Quality** | Optional — requires SonarQube: complexity, coverage, duplicates |\n\nAnd it correlates your adoption of GitHub Copilot (under 25%, 25–50%, 50–75%, above 75%) with the above\n\n[![impact deployment cfr image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAAB/AQMAAABv3m3bAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAHElEQVRYhe3BMQEAAADCoPVPbQZ/oAAAAACA2wATWQABVcJGbQAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2026/02/impact_deployment_cfr-scaled.webp)\n\nAn example insight you might see: *“Weeks with above 75% Copilot adoption showed 33% faster PR cycle times and 2x deployment frequency compared to weeks below 25%.”* That’s the kind of data point that starts answering the right questions.\n\n## Building a Data Culture, Not Buying a Dashboard\n\nWhat I find compelling about this approach is that every single panel in these dashboards is backed by a SQL query you can inspect. Click on any panel in Grafana, hit “Edit,” and you see the exact query. Don’t like how PR cycle time is calculated? Change it. Want to add a filter for a specific team? Add a WHERE clause. Want to build an entirely new panel that correlates Copilot usage with your custom deployment categories? Write the SQL.\n\n[![grafana panel edit sql image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAACpAQMAAACruZLpAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAHUlEQVRYhe3BMQEAAADCoPVPbQ0PoAAAAAAAADgyGb8AAQCylEMAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2026/02/grafana_panel_edit_sql.webp)\n\nYou own the data and queries, and you can extend it. DevLake has webhook support for tools it doesn’t have plugins for, and the plugin system itself is extensible, which is how we added the GitHub Copilot one!\n\nThe overall goal here is to go from “how many Copilot seats did we buy and who is using it?” to “what changed when our teams started using AI tools?” — and then do something with that answer.\n\n## Getting Started\n\nIf this resonates, the next step is getting DevLake running. I built [`gh-devlake`](https://github.com/DevExpGBB/gh-devlake) — a GitHub CLI extension that takes you from zero to configured DORA + Copilot dashboards in a few commands. The README has a quick-start and full walkthrough.\n\nUseful links:\n\n- [Apache DevLake Documentation](https://devlake.apache.org/)\n- [`gh-devlake`\nCLI Extension](https://github.com/DevExpGBB/gh-devlake)\n- [DORA Research](https://dora.dev/research/)\n- [GitHub Copilot Metrics API](https://docs.github.com/en/rest/copilot/copilot-metrics)\n\n*Have questions or feedback? Open an issue on the [gh-devlake repo](https://github.com/DevExpGBB/gh-devlake/issues).*",
  "Link": "https://devblogs.microsoft.com/all-things-azure/measuring-actual-ai-impact-for-engineering-with-apache-devlake/",
  "Author": "Eldrick Wega",
  "Description": "If you want to skip the explain and get started super quick with adoption + impact insights, use gh-devlake to deploy a GitHub Copilot impact dashboard in a few CLI commands. So! You’ve rolled out GitHub Copilot to your engineering teams. You’ve got the built-in dashboards. You know how many seats are assigned, what the […]\n\nThe post [Measuring actual AI Impact for Engineering with Apache DevLake](https://devblogs.microsoft.com/all-things-azure/measuring-actual-ai-impact-for-engineering-with-apache-devlake/) appeared first on [All things Azure](https://devblogs.microsoft.com/all-things-azure)."
}
