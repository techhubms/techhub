---
layout: "post"
title: "SonarSource Research Highlights Security Risks in LLM-Generated Code"
description: "This article discusses findings from SonarSource showing that large language models (LLMs) like GPT-4o, Claude Sonnet, and Llama-3.2 are capable of producing highly functional code with significant caveats. Despite impressive accuracy and ability, the code generated by these AI models often contains security vulnerabilities, hard-coded credentials, and poor maintainability. The report urges DevOps teams to be aware of these risks, emphasizing the need for careful review and understanding of LLM 'coding personalities' in modern software development."
author: "Mike Vizard"
excerpt_separator: <!--excerpt_end-->
canonical_url: "https://devops.com/sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code/?utm_source=rss&utm_medium=rss&utm_campaign=sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code"
viewing_mode: "external"
feed_name: "DevOps Blog"
feed_url: "https://devops.com/feed/"
date: 2025-08-13 13:43:07 +00:00
permalink: "/2025-08-13-SonarSource-Research-Highlights-Security-Risks-in-LLM-Generated-Code.html"
categories: ["AI", "Coding", "DevOps", "Security"]
tags: ["AI", "AI Coding Tools", "Analysis", "Application Security", "Boilerplate Code", "Business Of DevOps", "Claude Sonnet", "Code Analysis", "Code Smells", "Code Vulnerabilities", "Coding", "DevOps", "GPT 4o", "Hard Coded Credentials", "HumanEval", "Java", "Llama 3.2", "LLM", "Messy Code", "Model Evaluation", "OpenCoder 8B", "Posts", "Security", "Security Risks", "Social Facebook", "Social LinkedIn", "Social X", "SonarSource", "Technical Debt"]
tags_normalized: ["ai", "ai coding tools", "analysis", "application security", "boilerplate code", "business of devops", "claude sonnet", "code analysis", "code smells", "code vulnerabilities", "coding", "devops", "gpt 4o", "hard coded credentials", "humaneval", "java", "llama 3dot2", "llm", "messy code", "model evaluation", "opencoder 8b", "posts", "security", "security risks", "social facebook", "social linkedin", "social x", "sonarsource", "technical debt"]
---

Mike Vizard summarizes SonarSource’s research into AI-generated code, highlighting both the strengths and serious security pitfalls of relying on LLMs such as GPT-4o, Claude Sonnet 4, and others.<!--excerpt_end-->

# SonarSource Research Highlights Security Risks in LLM-Generated Code

**Author:** Mike Vizard

## Overview

A recent SonarSource analysis warns DevOps teams about several key risks when depending on large language models (LLMs) to write code. Although tools like GPT-4o, Claude Sonnet, and Llama-3.2 can generate correct, working code with a high degree of success, their output frequently contains severe vulnerabilities and poor quality patterns that may introduce long-term issues.

## Key Points

- **LLM Evaluation:** SonarSource evaluated LLMs including GPT-4o, Claude Sonnet 4 and 3.7, Llama-3.2-vision:90b, and OpenCoder-8B over 4,400 Java assignments.
- **Functionality vs Risk:** While LLMs such as Claude Sonnet 4 scored 95.57% on HumanEval, indicating strong coding ability, this performance comes with an increased rate of high-severity bugs.
- **Security Weaknesses Detected:** Common issues across LLMs included hard-coded credentials and path-traversal vulnerabilities. Some models, like Llama-3.2-vision:90b, showed over 70% of vulnerabilities classified as 'blocker' level.
- **Tech Debt and Maintainability:** Over 90% of the issues found were "code smells," referencing poor code structure—like dead or redundant code—which leads to technical debt.
- **Variation Among LLMs:** Each model demonstrated its own quirks or "coding personality," with different propensities for code quality and safety issues.
- **Impact on DevOps:** The study suggests developers must rigorously review any code generated by LLMs and consider the trade-offs between productivity and increased risk.

## Developer Takeaways

- **Review Required:** Do not deploy LLM-generated code without manual and/or automated security checks.
- **Understand LLM Bias:** Different LLMs may require different review practices due to their unique characteristics.
- **Technical Debt Awareness:** Watch for “code smells” and pursue code refactoring to maintain long-term health of your codebase.
- **AI in DevOps:** While LLMs boost productivity, unexamined adoption may increase future costs due to security flaws and maintainability problems.

## Resources

- [Full report from SonarSource](https://www.sonarsource.com/company/press-releases/the-coding-personalities-of-leading-llms/)
- Additional news and updates available at [DevOps.com](https://devops.com/)

---

**Quote from the article:**
> “There is no doubt that developers will be more productive, but some of those gains will come at a cost tomorrow that organizations may not realize they are incurring today.”

## Conclusion

SonarSource’s findings serve as a caution for teams eager to adopt LLM-powered development. Security and quality reviews remain critical, regardless of model sophistication.

This post appeared first on "DevOps Blog". [Read the entire article here](https://devops.com/sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code/?utm_source=rss&utm_medium=rss&utm_campaign=sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code)
