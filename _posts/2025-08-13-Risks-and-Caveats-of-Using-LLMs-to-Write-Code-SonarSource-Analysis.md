---
layout: "post"
title: "Risks and Caveats of Using LLMs to Write Code: SonarSource Analysis"
description: "This article by Mike Vizard summarizes new research from SonarSource, which analyzes the security and maintainability risks associated with code generated by leading large language models (LLMs) such as GPT-4o, Claude Sonnet 4, and Llama-3.2. While these LLMs can produce highly functional code, the report highlights significant downsides including frequent vulnerabilities, messy code, and high technical debt. The analysis covers trends in code quality, the prevalence of 'code smells,' and the ongoing challenges DevOps teams face when integrating AI coding tools into their workflows. Key insights include model-by-model comparisons, severity of security flaws, and practical advice for teams evaluating AI-assisted software development."
author: "Mike Vizard"
excerpt_separator: <!--excerpt_end-->
canonical_url: "https://devops.com/sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code/?utm_source=rss&utm_medium=rss&utm_campaign=sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code"
viewing_mode: "external"
feed_name: "DevOps Blog"
feed_url: "https://devops.com/feed/"
date: 2025-08-13 13:43:07 +00:00
permalink: "/2025-08-13-Risks-and-Caveats-of-Using-LLMs-to-Write-Code-SonarSource-Analysis.html"
categories: ["AI", "DevOps", "Security"]
tags: ["AI", "AI Agent", "AI Coding Tools", "Analysis", "Automated Code Review", "Bug Severity", "Business Of DevOps", "Claude Sonnet 4", "Code Quality Analysis", "Code Smells", "Code Translation", "DevOps", "GPT 4o", "Hard Coded Credentials", "HumanEval", "Java", "Llama 3.2", "LLMs", "Messy Code", "Model Bias", "Path Traversal", "Posts", "Security", "Security Vulnerabilities", "Social Facebook", "Social LinkedIn", "Social X", "SonarSource", "Technical Debt"]
tags_normalized: ["ai", "ai agent", "ai coding tools", "analysis", "automated code review", "bug severity", "business of devops", "claude sonnet 4", "code quality analysis", "code smells", "code translation", "devops", "gpt 4o", "hard coded credentials", "humaneval", "java", "llama 3 dot 2", "llms", "messy code", "model bias", "path traversal", "posts", "security", "security vulnerabilities", "social facebook", "social linkedin", "social x", "sonarsource", "technical debt"]
---

Mike Vizard dissects SonarSource's recent report on code written by large language models, breaking down how AI-generated code can introduce security risks and long-term challenges for DevOps teams.<!--excerpt_end-->

# Risks and Caveats of Using LLMs to Write Code: SonarSource Analysis

**Author:** Mike Vizard

## Overview

Recent research by SonarSource investigates the real-world impact of using large language models (LLMs) such as GPT-4o, Claude Sonnet 4, and Llama-3.2 to generate code. While these models are capable of producing highly functional and syntactically correct software, the study uncovers several risks and caveats that developers and DevOps teams must consider.

## Key Findings from the SonarSource Report

- The analysis covered over 4,400 Java programming assignments, using SonarSource's proprietary framework.
- Evaluated LLMs: Anthropic’s Claude Sonnet 4 and 3.7, OpenAI’s GPT-4o, Meta’s Llama-3.2-vision:90b, and OpenCoder-8B.

### Strengths

- High success rates in generating executable, syntactically correct code: Claude Sonnet 4 achieved a 95.57% HumanEval pass rate.
- LLMs show strong understanding of common algorithms, data structures, and can facilitate code translation and framework boilerplate automation.

### Security and Maintainability Concerns

- **High-Severity Vulnerabilities:** All tested models frequently embedded issues like hard-coded credentials and path traversal bugs. Llama-3.2-vision:90b produced 'blocker'-level vulnerabilities in over 70% of cases; GPT-4o and Claude Sonnet 4 also had high rates.
- **Technical Debt:** Over 90% of issues identified were 'code smells'—dead code, redundant code, and poor structure—raising long-term maintainability issues.
- **Risk Trade-Offs:** Improvements in functional correctness often led to more serious bugs. For example, Claude Sonnet 4's benchmark improvement resulted in a 93% rise in high-severity vulnerabilities.

## Implications for DevOps

- Teams should *not* trust LLM outputs blindly. Every model has embedded biases and 'coding personalities' that affect output quality.
- Large volumes of verbose, messy code make debugging and comprehension more difficult, sometimes requiring additional AI tools to review and understand generated code.
- There is uncertainty about DevOps adoption and trust in AI coding tools; productivity may improve, but long-term risks could offset gains.

## Recommendations

- Always **review and audit** code generated by LLMs with automated and manual processes.
- Treat AI coding tools as opinionated resources, not infallible solutions.
- Weigh productivity gains against the increased risk and tech debt from AI-generated code.

---
**References:**

- [SonarSource Report Press Release](https://www.sonarsource.com/company/press-releases/the-coding-personalities-of-leading-llms/)
- [Survey on AI Coding Tools Adoption](https://devops.com/survey-surfaces-varying-levels-of-enthusiasm-for-ai-coding-tools/)

---

This post appeared first on "DevOps Blog". [Read the entire article here](https://devops.com/sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code/?utm_source=rss&utm_medium=rss&utm_campaign=sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code)
