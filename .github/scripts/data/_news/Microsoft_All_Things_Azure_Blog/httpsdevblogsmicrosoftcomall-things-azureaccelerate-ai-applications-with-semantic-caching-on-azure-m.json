{
  "Link": "https://devblogs.microsoft.com/all-things-azure/accelerate-ai-applications-with-semantic-caching-on-azure-managed-redis/",
  "ProcessedDate": "2025-08-24 16:26:02",
  "PubDate": "2025-05-13T21:18:06+00:00",
  "FeedLevelAuthor": "All things Azure",
  "Title": "Accelerate AI Applications with Semantic Caching on Azure Managed Redis",
  "FeedUrl": "https://devblogs.microsoft.com/all-things-azure/feed/",
  "EnhancedContent": "In this blog, we will look at implementing the Semantic caching use case using [Azure Managed Redis](https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/managed-redis/managed-redis-overview). Azure has been at the forefront of providing caching solutions for over a decade with the Azure Cache for Redis enterprise. This service empowered developers with a high-performance, scalable cache that significantly enhanced the responsiveness of their applications. Now, Azure has taken another significant step forward in this area with its latest offering, AMR, which is currently in public preview. AMR runs on the Redis enterprise stack which offers significant advantages over the community edition of Redis. AMR provides advanced features such as Active Geo-Replication, Vector Storage & Search, Semantic Caching, Automatic Zone Redundancy, and Entra ID auth support across all its SKUs and Tiers.\n\n## Why semantic caching?\n\nRedis use cases have been expanding over the years from the traditional Data cache, API Response cache, Session Store, etc. to now include AI Apps use cases like [Vector Store, Vector Search, and Semantic cache](https://learn.microsoft.com/en-us/azure/redis/overview-vector-similarity). Using LLMs often introduces a high amount of latency (due to generation time) and cost (due to per token pricing) to an application. Semantic Caching can help solve these problems by storing the past output of an LLM along with the text as well as vector representation of the query associated with the LLM Output. The subsequent call to LLM can be preceded by cache lookup using the vector representation of the query; this pattern fits very well with the AI Apps implementations.\n\nSome common scenarios to use semantic caching could be in Faster FAQ retrieval in a chat bot by doing vector search of user query in the cache before making a call to the LLM Completion API and Storing previous interactions of users and their context to provide more relevant and personalized responses in a much faster time compared to relying on LLM for generating completion response all the time.\n\nLet’s look at the architecture of an AI App implementing the Semantic caching pattern:\n\n[![redis semantic caching image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAACcQAAAVsAQMAAAAmUp6JAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABvUlEQVR4nO3BMQEAAADCoPVPbQlPoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeBim0gABKruU7AAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/redis-semantic-caching-scaled.jpg)\n\nAs depicted above, the AI App looks up the semantic cache (AMR) before invoking the LLM chat completion API. The cache lookup is not text based alone, the AI App gets a vector representation of the query by invoking the LLM embedding API, this enables semantic search of the user queries. Apart from the AI App, Semantic caching logic can also be implemented in API Management (APIM) when using the AI Gateway pattern. APIM provides built in [semantic caching policies](https://learn.microsoft.com/en-us/azure/api-management/azure-openai-semantic-cache-lookup-policy) that makes implementing this use-case very straightforward.\n\n## Example: Python App with Semantic Cache\n\nLet’s now look at a Python application sample that uses semantic caching pattern and the improvement in response times achieved with this.\n\nIn this section of the app, we import the required libraries and configure API credentials to work with Azure OpenAI and Azure Redis:\n\n[![REDIS blog image 1 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAAJBAQMAAABxswqVAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAWElEQVR4nO3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHwYKBQABD+bDsAAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/REDIS-blog-image-1.png)\n\nNext, we will create a Semantic Cache Index in Azure Redis. Note that [Azure OpenAI Embedding API](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings?tabs=console) is being set as the embedding provider for this Semantic Cache instance. Another important config here is the Semantic Cache Distance Threshold, the lower the value, the more similar the cached and new queries must be to be considered a match in the cache, valid values for this parameter are between 0 and 1.\n\n[![REDIS blog image 2 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAADeAQMAAAAKHL8qAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAMElEQVR4nO3BMQEAAADCoPVPbQ0PoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAB4MWZUAAEuQGuAAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/REDIS-blog-image-2.png)\n\nThe last section of the app includes code to get a prompt from the user, checking for the presence of similar prompt/query from the user in the semantic cache. If there is a match, the response is returned from the Cache and the call to Chat Completion operation in Azure OpenAI is skipped. If there is no match in the cache, then the Chat Completion operation is invoked, and the response is stored in the semantic cache index of Azure Redis.\n\n[![REDIS blog image 3 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAAKAAQMAAACrB8eEAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAYElEQVR4nO3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+DScPAAE96FiNAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/REDIS-blog-image-3.png)\n\nTo see this in action, we run the application and pass the following query to the application:\n\n[![REDIS blog image 4 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAxIAAABbAQMAAADdtSGxAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAIElEQVRoge3BMQEAAADCoPVPbQlPoAAAAAAAAAAAgIcBI4wAAZ9OySoAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/REDIS-blog-image-4.png)\n\nAs expected, the cache lookup resulted in a miss and Azure OpenAI was called to get the response, and it took more than 3 seconds for the execution to complete:\n\n[![REDIS blog image 5 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAl8AAAC1AQMAAABbMQiTAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAI0lEQVRoge3BAQEAAACCIP+vriFAAQAAAAAAAAAAAAAAAMANNnEAAcZNDD8AAAAASUVORK5CYII=)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/REDIS-blog-image-5.png)\n\nWe now run the application again by passing a different query:\n\n[![REDIS blog image 6 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAv4AAABhAQMAAACUBDdFAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAIElEQVRoge3BMQEAAADCoPVPbQwfoAAAAAAAAAAAAE4GJMEAAdjnC/AAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/REDIS-blog-image-6.png)\n\nAlthough the words of this query are different, the cache search led to a hit as the query was semantically like the first query. The response time was less than 200 milliseconds, which is more than 10 times faster than calling LLM.\n\n[![REDIS blog image 7 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAABxAQMAAADiVhwRAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAJElEQVRoge3BAQ0AAADCoPdPbQ43oAAAAAAAAAAAAAAAAIAvAzQWAAGl8bTKAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/all-things-azure/wp-content/uploads/sites/83/2025/05/REDIS-blog-image-7.png)\n\nAs depicted above, using Semantic Search capabilities of Azure Managed Redis where applicable, will lead to better latency and reduced costs for your intelligent applications.\n\n## Resources\n\n- You can learn more about the Vector Embeddings and Vector Search capabilities of Azure Redis [here](https://aka.ms/RedisVectorSimilarity).\n- To run a sample application with Semantic Caching implementation using Azure Redis, check out the samples [here](https://learn.microsoft.com/en-us/azure/redis/tutorial-semantic-cache)\n- The complete code repo associated with the screenshots in the article can be found [here](https://github.com/sdadha/azureredis-semanticcache/tree/main).",
  "Tags": [
    "All things Azure",
    "Azure Managed Redis",
    "Azure OpenAI",
    "Azure Redis",
    "Redis",
    "Semantic caching",
    "Vector embeddings",
    "Vector search"
  ],
  "Description": "In this blog, we will look at implementing the Semantic caching use case using Azure Managed Redis. Azure has been at the forefront of providing caching solutions for over a decade with the Azure Cache for Redis enterprise. This service empowered developers with a high-performance, scalable cache that significantly enhanced the responsiveness of their applications. […]\n\nThe post [Accelerate AI Applications with Semantic Caching on Azure Managed Redis](https://devblogs.microsoft.com/all-things-azure/accelerate-ai-applications-with-semantic-caching-on-azure-managed-redis/) appeared first on [All things Azure](https://devblogs.microsoft.com/all-things-azure).",
  "OutputDir": "_news",
  "FeedName": "Microsoft All Things Azure Blog",
  "Author": "Satish"
}
