{
  "Author": "Luis Quintanilla, Adam Sitnik",
  "FeedLevelAuthor": ".NET Blog",
  "Description": "Announcing the preview of open, modular data ingestion building blocks in .NET, empowering developers to build scalable AI pipelines with seamless integration, extensibility, and easy getting started experiences across the .NET ecosystem.\n\nThe post [Introducing Data Ingestion Building Blocks (Preview)](https://devblogs.microsoft.com/dotnet/introducing-data-ingestion-building-blocks-preview/) appeared first on [.NET Blog](https://devblogs.microsoft.com/dotnet).",
  "FeedName": "Microsoft .NET Blog",
  "OutputDir": "_news",
  "Link": "https://devblogs.microsoft.com/dotnet/introducing-data-ingestion-building-blocks-preview/",
  "Tags": [
    ".NET",
    "AI",
    "Data",
    "dataingestion",
    "rag"
  ],
  "Title": "Introducing Data Ingestion Building Blocks (Preview)",
  "ProcessedDate": "2025-12-03 19:02:53",
  "EnhancedContent": "When building AI applications, context is key.\n\nAI models have a knowledge cutoff and do not have access to your personal or company data by default. To generate high-quality answers, AI apps need two things:\n\n- Access to high-quality data\n- The ability to surface the right information at the right time\n\nBoth are achieved through context engineering, which is the process of designing systems and best practices that provide relevant context to AI models. While context engineering is a broader topic, this post will focus on enabling access to high-quality data through data ingestion pipelines.\n\nToday, .NET developers face a growing challenge: efficiently ingesting, transforming, and retrieving data for intelligent, context-aware experiences.\n\nThat’s why we’re excited to announce the preview release of [data ingestion building blocks for .NET](https://www.nuget.org/packages/Microsoft.Extensions.DataIngestion).\n\nIn this post, we’ll share how these building blocks empower the .NET ecosystem to build composable data ingestion pipelines for their AI applications.\n\n## What is data ingestion?\n\nData ingestion is the process of collecting, reading, and preparing data from different sources such as files, databases, APIs, or cloud services so it can be used in downstream applications. In practice, this is the familiar Extract, Transform, Load (ETL) workflows:\n\n- **Extract** data from its original source, whether that is a PDF, Word document, audio file, or web API.\n- **Transform** the data by cleaning, chunking, enriching, or converting formats.\n- **Load** the data into a destination like a database, vector store, or AI model for retrieval and analysis.\n\nFor AI and machine learning scenarios, especially Retrieval-Augmented Generation (RAG), data ingestion is not just about moving data. It is about making data usable for intelligent applications. This means representing documents in a way that preserves their structure and meaning, splitting them into manageable chunks, enriching them with metadata or embeddings, and storing them so they can be retrieved quickly and accurately.\n\nThe challenge for .NET developers is not just performing ETL, but doing so in a way that is reliable, scalable, and easy to maintain for modern AI scenarios. As applications demand more context and intelligence, the limitations of current data ingestion approaches become more apparent.\n\n## Introducing Data Ingestion Building Blocks in .NET\n\nTo solve the challenges of fragmented workflows, manual effort, and limited extensibility, we are announcing the preview of new data ingestion building blocks for .NET.\n\nThese abstractions are the foundational .NET library for data processing. They enable developers to read, process, and prepare documents for AI and machine learning workflows, especially Retrieval-Augmented Generation (RAG) scenarios.\n\nWith these building blocks, developers can create robust, flexible, and intelligent data pipelines tailored to their application needs. Here’s what’s included in this release:\n\n- **Unified document representation:** Represent any file type (PDF, Image, Microsoft Word, etc.) in a consistent, format that works well with large language models.\n- **Flexible data ingestion:** Read documents from both cloud services and local sources using multiple built-in readers, making it easy to bring in data from wherever it lives.\n- **Built-in AI enhancements:** Automatically enrich content with summaries, sentiment analysis, keyword extraction, and classification, preparing your data for intelligent workflows.\n- **Customizable chunking strategies:** Split documents into chunks using token-based, section-based, or semantic-aware approaches, so you can optimize for your retrieval and analysis needs.\n- **Production-ready storage:** Persist processed chunks in popular vector databases and document stores, with support for embedding generation, making your pipelines ready for real-world scenarios.\n- **End-to-end pipeline composition:** Chain together readers, processors, chunkers, and writers with the `IngestionPipeline`\nAPI, reducing boilerplate and making it easy to build, customize, and extend complete workflows.\n- **Performance and scalability:** Designed for scalable data processing, these components can handle large volumes of data efficiently, making them suitable for enterprise-grade applications.\n\nAll of these components are open and extensible by design. You can add custom logic, new connectors, and extend the system to support emerging AI scenarios. By standardizing how documents are represented, processed, and stored, .NET developers can build reliable, scalable, and maintainable data pipelines without reinventing the wheel for every project.\n\n### Building on stable foundations\n\nThese new data ingestion components are built on top of proven and extensible components in the .NET ecosystem, ensuring reliability, interoperability, and seamless integration with existing AI workflows.\n\n- **Microsoft.ML.Tokenizers:** Tokenizers provide the foundation for chunking documents based on tokens. This enables precise splitting of content, which is essential for preparing data for large language models and optimizing retrieval strategies.\n- **Microsoft.Extensions.AI:** This set of libraries powers enrichment transformations using large language models. It enables features like summarization, sentiment analysis, keyword extraction, and embedding generation, making it easy to enhance your data with intelligent insights.\n- **Microsoft.Extensions.VectorData:** This set of libraries offers a consistent interface for storing processed chunks in a wide variety of vector stores, including [Qdrant](https://www.nuget.org/packages/Microsoft.SemanticKernel.Connectors.Qdrant), [SQL Server](https://www.nuget.org/packages/Microsoft.SemanticKernel.Connectors.SqlServer), [CosmosDB](https://www.nuget.org/packages/Microsoft.SemanticKernel.Connectors.CosmosNoSQL), [MongoDB](https://www.nuget.org/packages/Microsoft.SemanticKernel.Connectors.MongoDB), [ElasticSearch](https://www.nuget.org/packages/Elastic.SemanticKernel.Connectors.Elasticsearch), and many more. This ensures your data pipelines are ready for use and scale across your preferred storage backend.\n\nIn addition to familiar patterns and tools, these building blocks build on already extensible components. Plug-in capability and interoperability are paramount, so as the rest of the .NET AI ecosystem grows, the capabilities of the data ingestion components grow as well. This approach empowers developers to easily integrate new connectors, enrichments, and storage options, keeping their pipelines future-ready and adaptable to evolving AI scenarios.\n\n## Build your first data ingestion pipeline\n\nIn the following example, you will parse markdown documents using [MarkdownReader](https://www.nuget.org/packages/Microsoft.Extensions.DataIngestion.Markdig), apply a set of document and chunk processing techniques, chunk using the Semantic Similarity Chunker, and store chunks in a SQLite vector store.\n\nThe easiest way to get started is to open the sample application in GitHub Codespaces.\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/luisquintanilla/DataIngestion)\n\nAlternatively, the snippets below highlight how the sample was built. For the full code, see the [DataIngestion sample repo](https://github.com/luisquintanilla/DataIngestion).\n\n### Build your application\n\nCreate a new [file-based app](https://learn.microsoft.com/dotnet/csharp/fundamentals/tutorials/file-based-programs).\n\n```console\n# Powershell\nni DataIngestion.cs\n\n# Bash\ntouch DataIngestion.cs ```\n\nInstall the [Microsoft.Extensions.DataIngestion](https://www.nuget.org/packages/Microsoft.Extensions.DataIngestion) and other relevant NuGet packages.\n\n```console // Data Ingestion Building Blocks #:package Microsoft.Extensions.DataIngestion@10.0.1-preview.1.25571.5 #:package Microsoft.Extensions.DataIngestion.Markdig@10.0.1-preview.1.25571.5\n\n// OpenAI ChatClient #:package Microsoft.Extensions.AI.OpenAI@10.0.1-preview.1.25571.5\n\n// Logging #:package Microsoft.Extensions.Logging.Console@10.0.0\n\n// Tokenizer #:package Microsoft.ML.Tokenizers.Data.Cl100kBase@2.0.0\n\n// SQLite Vector Store #:package Microsoft.SemanticKernel.Connectors.SqliteVec@1.67.1-preview ```\n\nAdd using statements\n\n```csharp using System.ClientModel; using Microsoft.Extensions.AI; using Microsoft.Extensions.DataIngestion; using Microsoft.Extensions.DataIngestion.Chunkers; using Microsoft.Extensions.Logging; using Microsoft.ML.Tokenizers; using Microsoft.SemanticKernel.Connectors.SqliteVec; using OpenAI; ```\n\n### Set up your reader\n\nReading the document is the first step in the ingestion pipeline. `IngestionDocumentReader` is an abstraction for reading `IngestionDocument` from file path and/or `Stream` s. In this example, we will use the `MarkDownReader` to read markdown files, as it’s the simplest reader without any external dependencies (like cloud services or MCP servers).\n\n```csharp IngestionDocumentReader reader = new MarkdownReader(); ```\n\n### Set up your document processor(s)\n\nThe second step is to process the `IngestionDocument` s. Document processors operate at the document level and can enrich the content or perform other transformations. Which processors you use depends on your scenario.\n\nThis example uses the built-in `ImageAlternativeTextEnricher` that enriches `IngestionDocumentImage` elements with alternative text using an AI service, so the embeddings generated for text chunks can include the image content information:\n\n```csharp using ILoggerFactory loggerFactory = LoggerFactory.Create(builder => builder.AddSimpleConsole());\n\nOpenAIClient openAIClient = new( new ApiKeyCredential(Environment.GetEnvironmentVariable(\"GITHUB_TOKEN\")!), new OpenAIClientOptions { Endpoint = new Uri(\"https://models.github.ai/inference\") });\n\nIChatClient chatClient = openAIClient.GetChatClient(\"gpt-4.1\").AsIChatClient();\n\nEnricherOptions enricherOptions = new(chatClient) { // Enricher failures should not fail the whole ingestion pipeline, as they are best-effort enhancements. // This logger factory can be used to create loggers to log such failures. LoggerFactory = loggerFactory }\n\nIngestionDocumentProcessor imageAlternativeTextEnricher = new ImageAlternativeTextEnricher(enricherOptions); ```\n\n**Note**\n\nEnricher failures should not fail the whole ingestion pipeline, as they are best-effort enhancements. Use the `LoggerFactory` to create loggers to log such failures.\n\n### Split your data into chunks\n\nEvery `IngestionDocument` needs to be split into `IngestionChunk<T>` s. Today, the data ingestion libraries provide three chunking strategies:\n\n- Header-based chunking to split on headers.\n- Section-based chunking to split on sections (i.e. pages).\n- Semantic-aware chunking to split on semantically similar topics.\n\nAll of them support token-based limits to ensure chunks fit within model context windows.\n\n```csharp IEmbeddingGenerator<string, Embedding<float>> embeddingGenerator = openAIClient.GetEmbeddingClient(\"text-embedding-3-small\").AsIEmbeddingGenerator();\n\nIngestionChunkerOptions chunkerOptions = new(TiktokenTokenizer.CreateForModel(\"gpt-4\")) { MaxTokensPerChunk = 2000, OverlapTokens = 0 };\n\nIngestionChunker<string> chunker = new SemanticSimilarityChunker(embeddingGenerator, chunkerOptions); ```\n\n### Enrich and process your data\n\nThe next step is to process the `IngestionChunk<T>` s. Chunk processors operate at the chunk level and can enrich the content or perform other transformations. Similar to document processors, which processors you choose to use depends on your scenario.\n\nThis example uses the built-in `SummaryEnricher` to enrich each chunk with a summary using an AI service:\n\n```csharp IngestionChunkProcessor<string> summaryEnricher = new SummaryEnricher(enricherOptions); ```\n\n### Store your data\n\nStoring the processed chunks is the final step in the ingestion pipeline. `IngestionChunkWriter<T>` is an abstraction for storing `IngestionChunk<T>` s in any store, but `VectorStoreWriter<T>` is an implementation that uses a vector store. It’s built on top of the [Microsoft.Extensions.VectorData.Abstractions](https://www.nuget.org/packages/Microsoft.Extensions.VectorData.Abstractions) abstractions, so you can use any supported vector store.\n\nIn this example, we will use the `SqliteVectorStore` to store chunks in a local SQLite database:\n\n```csharp using SqliteVectorStore vectorStore = new( \"Data Source=vectors.db;Pooling=false\", new() { EmbeddingGenerator = embeddingGenerator });\n\n// The writer requires the embedding dimension count to be specified. // For OpenAI's `text-embedding-3-small`, the dimension count is 1536. using VectorStoreWriter<string> writer = new(vectorStore, dimensionCount: 1536); ```\n\nThe writer will automatically:\n\n- Create the vector store collection using default schema.\n- Generate embeddings for each chunk using the provided embedding generator.\n- When finished, delete any chunks that were previously stored for the document with the same ID (to support incremental chunking of different versions of the same document).\n\n### Compose and run your pipeline\n\nCreate an `IngestionPipeline` using the previously configured reader, chunker, enrichers, and writer to process all of the files in the current directory.\n\n```csharp using IngestionPipeline<string> pipeline = new(reader, chunker, writer, loggerFactory: loggerFactory) { DocumentProcessors = { imageAlternativeTextEnricher }, ChunkProcessors = { summaryEnricher } };\n\nawait foreach (var result in pipeline.ProcessAsync(new DirectoryInfo(\".\"), searchPattern: \"*.md\")) { Console.WriteLine($\"Completed processing '{result.DocumentId}'. Succeeded: '{result.Succeeded}'.\"); } ```\n\n**Important**\n\nA single document ingestion failure should not fail the whole pipeline. The `IngestionPipeline.ProcessAsync` implements partial success by returning `IAsyncEnumerable<IngestionResult>` . The caller is responsible for handling any failures (for example, by re-trying failed documents or stopping on first error).\n\n### Retrieve data\n\n`VectorStoreWriter<T>` exposes the underlying `VectorStoreCollection` that can be used to perform vector search on the stored chunks. This example prompts the user for a query and returns the top 3 most similar chunks from the vector store:\n\n```csharp var collection = writer.VectorStoreCollection; while (true) { Console.Write(\"Enter your question (or 'exit' to quit): \"); string? searchValue = Console.ReadLine(); if (string.IsNullOrEmpty(searchValue) || searchValue == \"exit\") { break; }\n\nConsole.WriteLine(\"Searching...\\n\"); await foreach (var result in collection.SearchAsync(searchValue, top: 3)) { Console.WriteLine($\"Score: {result.Score}\\n\\tContent: {result.Record[\"content\"]}\"); } } ```\n\n## End-to-End Scenarios\n\nWant to see it in action? Try the new .NET [AI Web Chat Template](https://learn.microsoft.com/dotnet/ai/quickstarts/ai-templates) for a full end-to-end experience. You can parse the documents using [MarkItDown MCP server](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp), chunk them using semantic-aware chunker and store the chunks in a vector database of your choice. The sample app comes with a web chat that uses ingested data for RAG.\n\n```console dotnet new install Microsoft.Extensions.AI.Templates ```\n\n![Choose AI Chat Web App](https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/12/choose-ai-chat-template.webp)\n\n![Provide additional information](https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/12/provide-additional-info.webp)\n\n### Build your distributed application\n\nThe following code snippet from the template shows how to configure the different components of the application in Aspire, including Ollama for hosting the models, Qdrant for vector storage, MarkItDown for document parsing, and the web application itself.\n\n```csharp var builder = DistributedApplication.CreateBuilder(args);\n\nvar ollama = builder.AddOllama(\"ollama\") .WithDataVolume(); var chat = ollama.AddModel(\"chat\", \"llama3.2\"); var embeddings = ollama.AddModel(\"embeddings\", \"all-minilm\");\n\nvar vectorDB = builder.AddQdrant(\"vectordb\") .WithDataVolume() .WithLifetime(ContainerLifetime.Persistent);\n\nvar markitdown = builder.AddContainer(\"markitdown\", \"mcp/markitdown\") .WithArgs(\"--http\", \"--host\", \"0.0.0.0\", \"--port\", \"3001\") .WithHttpEndpoint(targetPort: 3001, name: \"http\");\n\nvar webApp = builder.AddProject<Projects.RagSample_Web>(\"aichatweb-app\"); webApp .WithReference(chat) .WithReference(embeddings) .WaitFor(chat) .WaitFor(embeddings); webApp .WithReference(vectorDB) .WaitFor(vectorDB); webApp .WithEnvironment(\"MARKITDOWN_MCP_URL\", markitdown.GetEndpoint(\"http\"));\n\nbuilder.Build().Run(); ```\n\n![Aspire Resources](https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/12/aspire-resources.webp)\n\n### Observability\n\nWhen using [Aspire](https://learn.microsoft.com/dotnet/aspire/), you get a rich observability experience for your data ingestion pipelines.\n\nThe template already includes [OpenTelemetry tracing](https://learn.microsoft.com/dotnet/core/diagnostics/observability-with-otel#net-implementation-of-opentelemetry) for both the data ingestion process and the web application.\n\n```csharp public static TBuilder ConfigureOpenTelemetry<TBuilder>(this TBuilder builder) where TBuilder : IHostApplicationBuilder { // The rest is omitted for brevity.\n\nbuilder.Services.AddOpenTelemetry() .WithTracing(tracing => { tracing .AddSource(\"Experimental.Microsoft.Extensions.AI\") .AddSource(\"Experimental.Microsoft.Extensions.DataIngestion\"); });\n\nreturn builder; } ```\n\n![Aspire Tracing](https://devblogs.microsoft.com/dotnet/wp-content/uploads/sites/10/2025/12/aspire-tracing.webp)\n\n## Ready to get started?\n\nThe easiest way to get started is by installing the the AI Web Chat Template.\n\n[Install the AI Web Chat Template](https://learn.microsoft.com/dotnet/ai/quickstarts/ai-templates)\n\nOnce you become familiar with the template, try using it with your own files.\n\nIf you’re a library author or ecosystem developer, you can [extend the abstractions](https://www.nuget.org/packages/Microsoft.Extensions.DataIngestion.Abstractions/) to enable your users to seamlessly interop and compose readers, chunkers, processors, and writers from various providers.\n\n[File an issue](https://github.com/dotnet/extensions/issues) with your questions, issues, and suggestions to help us shape the future of these building blocks.",
  "PubDate": "2025-12-03T18:05:00+00:00",
  "FeedUrl": "https://devblogs.microsoft.com/dotnet/feed/"
}
