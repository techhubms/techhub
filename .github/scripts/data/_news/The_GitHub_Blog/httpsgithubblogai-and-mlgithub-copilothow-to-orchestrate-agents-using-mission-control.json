{
  "OutputDir": "_news",
  "EnhancedContent": "We recently shipped [Agent HQ’s mission control](https://github.blog/changelog/2025-10-28-a-mission-control-to-assign-steer-and-track-copilot-coding-agent-tasks/), a unified interface for managing [GitHub Copilot](https://github.com/features/copilot?utm_source=blog-mission-control&amp;utm_medium=blog&amp;utm_campaign=universe25post) coding agent tasks.\n\nNow, you can now assign tasks to Copilot across repos, pick a custom agent, watch real‑time session logs, steer mid-run (pause, refine, or restart), and jump straight into the resulting pull requests—all in one place. Instead of bouncing between pages to see status, rationale, and changes, mission control centralizes assignment, oversight, and review.\n\nHaving the tool is one thing. Knowing how to use it effectively is another. This guide shows you how to orchestrate multiple agents, when to intervene, and how to review their work efficiently. Being great at orchestrating agents means unblocking parallel work in the same timeframe you’d spend on one task, stepping in when logs show drift, tests fail, or scope creeps.\n\n## The mental model shift\n\n### From sequential to parallel\n\nIf you’re already used to working with an agent one at a time, you know it’s inherently sequential. You submit a prompt, wait for a response, review it, make adjustments, and move to the next task.\n\nMission control changes this. You can kick off multiple tasks in minutes—across one repo or many. Previously, you’d navigate to different repos, open issues in each one, and assign Copilot separately. Now you can enter prompts in one place, and Copilot coding agent goes to work across all of them.\n\nThat being said, there is a trade-off to keep in mind: Instead of each task taking30 seconds to a few minutes to complete, your agents might spend a few minutes to an hour on a draft. But you’re no longer just waiting. You’re orchestrating.\n\n### When to stay sequential\n\nNot everything belongs in parallel. Use sequential workflows when:\n\n- Tasks have dependencies\n- You’re exploring unfamiliar territory\n- Complex problems require validating assumptions between steps\n\nWhen assigning multiple tasks from the same repo, consider overlap. Agents working in parallel can create merge conflicts if they touch the same files. Be thoughtful about partitioning work.\n\nTasks that typically run well in parallel:\n\n- Research work (finding feature flags, configuration options)\n- Analysis (log analysis, performance profiling)\n- Documentation generation\n- Security reviews\n- Work in different modules or components\n\n## Tips for getting started\n\nThe shift is simple: you move from waiting on a single run to overseeing multiple progressing in parallel, stepping in for failed tests, scope drift, or correcting unclear intent where guidance will save time.\n\n### Write clear prompts with context\n\nSpecificity matters. Describe the task precisely. Good context remains critical for good results.\n\nHelpful context includes:\n\n- Screenshots showing the problem\n- Code snippets illustrating the pattern you want\n- Links to relevant documentation or examples\n\n**Weak prompt**: “Fix the authentication bug.”\n\n**Strong prompt**: “Users report ‘Invalid token’ errors after 30 minutes of activity. JWT tokens are configured with 1-hour expiration in auth.config.js. Investigate why tokens expire early and fix the validation logic. Create the pull request in the api-gateway repo.”\n\n### Use custom agents for consistency\n\nMission control lets you select custom agents that use agents.md files from your selected repo. These files give your agent a persona and pre-written context, removing the burden of constantly providing the same examples or instructions.\n\nIf you manage repos where your team regularly uses agents, consider creating agents.md files tailored to your common workflows. This ensures consistency across tasks and reduces the cognitive load of crafting detailed prompts each time.\n\nOnce you’ve written your prompt and selected your custom agent (if applicable), kick off the task. Your agent gets to work immediately.\n\n## Tips for active orchestration\n\nYou’re now a conductor of agents. Each task might take a minute or an hour, depending on complexity. You have two choices: watch your agents work so you can intervene if needed, or step away and come back when they’re done.\n\n### Reading the signals\n\nBelow are some common indicators that your agent is not on the right track and needs additional guidance:\n\n- **Failing tests, integrations, or fetches**: The agent can’t fetch dependencies, authentication fails, or unit tests break repeatedly.\n- **Unexpected files being created**: Files outside the scope appear in the diff, or the agent modifies shared configuration.\n- **Scope creep beyond what you requested**: The agent starts refactoring adjacent code or “improving” things you didn’t ask for.\n- **Misunderstanding your intent**: The session log reveals the agent interpreted your prompt differently than you meant.\n- **Circular behavior**: The agent tries the same failing approach multiple times without adjusting.\n\nWhen you spot issues, evaluate their severity. Is that failing test critical? Does that integration point matter for this task? The session log typically shows intent before action, giving you a chance to intervene if you’re monitoring.\n\n### The art of steering\n\nWhen you need to redirect an agent, be specific. Explain why you’re redirecting and how you want it to proceed.\n\n**Bad steering**: “This doesn’t look right.”\n\n**Good steering**: “Don’t modify database.js—that file is shared across services. Instead, add the connection pool configuration in api/config/db-pool.js. This keeps the change isolated to the API layer.”\n\nTiming matters. Catch a problem five minutes in, and you might save an hour of ineffective work. Don’t wait until the agent finishes to provide feedback.\n\nYou can also stop an agent mid-task and give it refined instructions. Restarting with better direction is simple and often faster than letting a misaligned agent continue.\n\n### Why session logs matter\n\nSession logs show reasoning, not just actions. They reveal misunderstandings before they become pull requests, and they improve your future prompts and orchestration practices. When Copilot says “I’m going to refactor the entire authentication system,” that’s your cue to steer.\n\n## Tips for the review phase\n\nWhen your agents finish, you’ll have pull requests to review. Here’s how to do it efficiently. Ensure you review:\n\n1. **Session logs**: Understand what the agent did and why. Look for reasoning errors before they become merged code. Did the agent misinterpret your intent? Did it assume something incorrectly?\n2. **Files changed**: Review the actual code changes. Focus on:\n- Files you didn’t expect to see modified\n- Changes that touch shared, risky, or critical code paths\n- Patterns that don’t match your team’s standards/practices\n- Missing edge case handling\n3. **Checks**: Verify that tests pass (your unit tests, Playwright, CI/CD, etc.). When checks fail, don’t just restart the agent. Investigate why. A failing test might reveal the agent misunderstood requirements, not just wrote buggy code.\n\nThis pattern gives you the full picture: intent, implementation, and validation.\n\n### Ask Copilot to review its own work\n\nAfter an agent completes a task, ask it:\n\n- “What edge cases am I missing?”\n- “What test coverage is incomplete?”\n- “How should I fix this failing test?”\n\nCopilot can often identify gaps in its own work, saving you time and improving the final result. Treat it like a junior developer who’s willing to explain their reasoning.\n\n### Batch similar reviews\n\nGenerating code with agents is straightforward. Reviewing that code—ensuring it meets your standards, does what you want, and that it can be maintained by your team—still requires human judgment.\n\nImprove your review process by grouping similar work together. Review all API changes in one session. Review all documentation changes in another. Your brain context-switches less, and you’ll spot patterns and inconsistencies more easily.\n\n## What’s changed for the better\n\nMission control moves you from babysitting single agent runs to orchestrating a small fleet. You define clear, scoped tasks. You supply just enough context. You launch several agents. The speed gain is not that each task finishes faster; it’s that you unblock more work in the same timeframe.\n\nWhat makes this possible is discipline: specific prompts, not vague requests. Custom agents in agents.md that carry your patterns so you don’t repeat yourself. Early steering when session logs show drift. Treating logs as reasoning artifacts you mine to write a sharper next prompt. And batching reviews so your brain stays in one mental model long enough to spot subtle inconsistencies. Lead your own team of agents to create something great!\n\nReady to start? [Visit mission control](https://github.com/copilot/agents?utm_source=blog-mission-control&amp;utm_medium=blog&amp;utm_campaign=universe25post) or[learn more about GitHub Copilot](https://github.com/features/copilot?utm_source=blog-mission-control&amp;utm_medium=blog&amp;utm_campaign=universe25post) for your organization.",
  "FeedUrl": "https://github.blog/feed/",
  "Title": "How to orchestrate agents using mission control",
  "ProcessedDate": "2025-12-01 17:03:57",
  "Tags": [
    "AI & ML",
    "AI agents",
    "GitHub Copilot",
    "GitHub Copilot coding agent",
    "mission control"
  ],
  "Description": "Run multiple Copilot agents from one place. Learn prompt techniques, how to spot drift early, and how to review agent work efficiently.\n\nThe post [How to orchestrate agents using mission control](https://github.blog/ai-and-ml/github-copilot/how-to-orchestrate-agents-using-mission-control/) appeared first on [The GitHub Blog](https://github.blog).",
  "FeedLevelAuthor": "The GitHub Blog",
  "Author": "Matt Nigh",
  "Link": "https://github.blog/ai-and-ml/github-copilot/how-to-orchestrate-agents-using-mission-control/",
  "FeedName": "The GitHub Blog",
  "PubDate": "2025-12-01T17:00:00+00:00"
}
