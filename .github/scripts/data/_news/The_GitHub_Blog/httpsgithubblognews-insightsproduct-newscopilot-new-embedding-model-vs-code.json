{
  "EnhancedContent": "Finding the right code quickly is critical. After listening to the community’s feedback, we rolled out a new Copilot embedding model that makes code search in VS Code faster, lighter on memory, and far more accurate. This means retrieving the snippets you actually need instead of near misses. It delivers a **37.6% lift in retrieval quality**, about **2x higher throughput**, and an **8x smaller index size**, so GitHub Copilot chat and agentic responses are more accurate, results return faster, and memory use in VS Code is lower.\n\n## Why this matters\n\nGreat AI coding experiences depend on finding the right context: snippets, functions, tests, docs, and bugs in code that match your intent. The “find” step is powered by embeddings, which are vector representations that retrieve semantically relevant code and natural language content, even when the exact words do not match.\n\nBetter embeddings lead to better retrieval quality and result in a better GitHub Copilot experience.\n\n## What we shipped\n\nWe trained and deployed a new embedding model tailored for code and documentation. It now powers context retrieval for GitHub Copilot chat [along with agent, edit, and ask mode](https://github.blog/ai-and-ml/github-copilot/copilot-ask-edit-and-agent-modes-what-they-do-and-when-to-use-them/).\n\n**Impact:**\n\n- **Improved retrieval quality**: **+37.6%** relative lift (average score improved from 0.362 to 0.498) on our multi-benchmark evaluation (Figure 1). For C# developers in VS Code, we saw a **+110.7%** lift in code acceptance ratios — and for Java developers, we saw a **+113.1%** lift in code acceptance ratios.\n\n![A bar graph showing improved code retrieval, agentic code retrieval, and chat retrieval scores.](https://github.blog/wp-content/uploads/2025/09/All_1200x630-1.png?resize=1024%2C538)Figure 1: Model score comparison on multiple code retrieval benchmarks\n- **Greater efficiency: ~2x** higher embedding throughput reduces retrieval latency (Figure 2), and ~8x smaller index memory size improves scaling in both client and server (Figure 2).\n\n![A bar graph showing decreased P50 latency and decreased memory footprint (bits).](https://github.blog/wp-content/uploads/2025/09/All_1200x630.png?resize=1024%2C538) Figure 2: Model efficiency comparison\n\nThe following example shows how the new model is doing better in retrieval quality:\n\nDeveloper prompt: “Which method is invoked to find **a single namespace** by its name within the project?”\n\n**The top code snippet retrieved using the Copilot embedding model contains the findOne function (bolded below), which is correct:**\n\n``` class Namespace extends K8Object { /*...*/ static findOne(params = {}, options = {}) { return Model.findOne(params, options).then((namespace) => { console.log(namespace); if (namespace) { return new Namespace(namespace).setResourceVersion(); } }); } /*...*/ } ```\n\n**The top code snippet retrieved using previous model contains the find function (bolded below), which is incorrect but semantically similar to the findOne function:**\n\n``` class Namespace extends K8Object { /*...*/ static find(params = {}, options = {}) { return Model.find(params, options).then((namespaces) => { if (namespaces) { return Promise.all( namespaces.map((namespace) => new Namespace(namespace).setResourceVersion() ) ); } }); } /*...*/ } ```\n\nThe Copilot embedding model offers improved responses to prompts and instructions. It can also better distinguish between search results that are somewhat relevant and those that are highly relevant.\n\n**Other scenarios where developers benefit include:**\n\n- Searching for a test function in a large monorepo\n- Finding a helper method spread across multiple files\n- Debugging code: *“show me where this error string is handled”*\n\n## How we trained\n\nOur objective was to optimize retrieval quality for real developer workloads while keeping latency and memory within our budgets.\n\nWe optimized retrieval quality using **contrastive learning** with **InfoNCE loss** and **Matryoshka Representation Learning** — an approach that helps embeddings distinguish between nearly identical snippets while supporting multiple embedding sizes for flexibility.\n\nA key ingredient was training with **hard negatives**: code examples that look correct but aren’t. Most failures in code search come from these “near misses,” so teaching the model to separate “almost right” from “actually right” drove the biggest quality gains. We mined hard negatives from large, diverse corpora (public GitHub and Microsoft/GitHub internal repositories), and used LLMs to surface tricky near misses. This helped us reduce shortcut learning and improve generalization.****\n\nThe following example shows how contrastive learning with hard negatives trains the model to distinguish between relevant code and nearly relevant code samples. For a query that asks how a table of stop words is populated, the most relevant code sample shows the function that loads the stop word table from a file. Functions that load words into a table or read stop words from a file are used as hard negatives that do not answer the query. (The query for the following example was *“How is the stop word table populated?”*.)\n\n![An example showing how hard negatives are used for training the embedding model.](https://github.blog/wp-content/uploads/2025/09/Negative_1200x630_0ee98b.png?resize=1024%2C538)\n\nThe top five programming languages in our training data included:\n\n| **Language** | **Data mix ratio** | | --- | --- | | **Python** | 36.7% | | **Java** | 19.0% | | **C++** | 13.8% | | **JavaScript/TypeScript** | 8.9% | | **C#** | 4.6% | | **Other languages** | 17.0% |\n\n## The evaluation suite\n\nWe use a multi-benchmark evaluation, not a single test to cover different aspects of code retrieval. These include:\n\n- **Natural language (NL) to code**: Responding to NL queries with relevant functions/snippets.\n- **Code to NL**: natural language summaries of code.\n- **Code to code**: similar-function search (refactored or translated code).\n- **Problems to code**: turning problem descriptions into suggested code fixes.\n\n## What’s next\n\nThe new Copilot embedding model is one step in making AI coding assistants not only smarter, but more reliable for everyday development. Moving forward, we’re:\n\n- Expanding training and evaluation data to more languages and repositories.\n- Refining hard negative mining pipeline for better quality.\n- Using efficiency gains to deploy larger, more accurate models.\n\n**Want to try our improved agentic search experiences?**\n\n[Try GitHub Copilot in VS Code &gt;](https://github.com/features/copilot)\n\n**Acknowledgments**\n\nHuge thanks to the engineers and researchers across GitHub and Microsoft who built the training pipeline, evaluation suites, and serving stack — and to the GitHub Copilot product and engineering teams for a smooth rollout.",
  "ProcessedDate": "2025-09-24 21:10:20",
  "Tags": [
    "AI models",
    "embedding",
    "generative AI",
    "GitHub Copilot",
    "News & insights",
    "Product",
    "RAG"
  ],
  "FeedUrl": "https://github.blog/feed/",
  "Author": "Shengyu Fu",
  "FeedLevelAuthor": "The GitHub Blog",
  "Link": "https://github.blog/news-insights/product-news/copilot-new-embedding-model-vs-code/",
  "OutputDir": "_news",
  "Title": "GitHub Copilot gets smarter at finding your code: Inside our new embedding model",
  "PubDate": "2025-09-24T20:24:18+00:00",
  "FeedName": "The GitHub Blog",
  "Description": "Learn about a new Copilot embedding model that makes code search in VS Code faster, lighter on memory, and far more accurate.\n\nThe post [GitHub Copilot gets smarter at finding your code: Inside our new embedding model](https://github.blog/news-insights/product-news/copilot-new-embedding-model-vs-code/) appeared first on [The GitHub Blog](https://github.blog)."
}
