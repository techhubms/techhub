{
  "Author": "Isidor Nikolic",
  "FeedName": "Visual Studio Code Releases",
  "Title": "Introducing auto model selection (preview)",
  "EnhancedContent": "Join a [VS Code Dev Days event](/dev-days) near you to learn about AI-assisted development in VS Code.\n\nDismiss this update\n\n# Introducing auto model selection (preview)\n\nSeptember 15th, 2025 by [Isidor Nikolic](https://github.com/isidorn), [@isidorn](https://x.com/isidorn)\n\nFaster responses, a lower chance of rate limiting, and 10% off premium requests for paid users - auto picks the best available model for each request based on current capacity and performance. With auto you can’t choose a specific model, auto handles that for you. Auto model selection in Chat is being rolled out in preview to all GitHub Copilot users in VS Code, starting with the individual plans.\n\n## How auto model selection works\n\nAuto selects the best model to ensure that you get the optimal performance and reduce the likelihood of rate limits. Auto will choose between Claude Sonnet 4, GPT-5, GPT-5 mini and other models, unless your organization has [disabled access to these models](https://docs.github.com/en/copilot/how-tos/use-ai-models/configure-access-to-ai-models). Once auto picks a model, it uses that same model for the entire chat session. As we introduce picking models based on task complexity, this behavior will change over the next iterations.\n\nFor paid users, we currently plan to primarily rely on Claude Sonnet 4 as the model powering auto.\n\n![Screenshot that shows the model picker in the Chat view, showing the auto option.](/assets/blogs/2025/09/15/autoDropdown.png)\n\nWhen using auto model selection, VS Code uses a variable [model multiplier](https://docs.github.com/en/copilot/concepts/billing/copilot-requests#model-multipliers) based on the automatically selected model. If you are a paid user, auto applies a 10% request discount. For example, if auto selects Sonnet 4, it will be counted as 0.9x of a premium request; if auto selects GPT-5-mini, this counts as 0x because the model is included for paid users. You can see which model and model multiplier are used by hovering over the chat response.\n\nIf you are a paid user and run out of premium requests, auto will always choose a 0x model (for example, GPT-5 mini), so you can continue using auto without interruption.\n\n## What’s next\n\nWith VS Code's rapid usage growth, auto helps us manage capacity, so we can handle the millions of agentic requests coming in each day. That's our immediate goal, however this is not our long-term vision for auto. We aim to make auto the best model selection for most users and to achieve this, here's what we plan next:\n\n- Dynamically switch between small and large models based on the task - this flexibility ensures that you get the right balance of performance and efficiency, while saving on requests\n- Add more language models to auto\n- Let users on a free plan take advantage of the latest models through auto\n- Improve the model dropdown to make it more obvious which models and discounts are used\n\nUsing [VS Code Insiders](https://code.visualstudio.com/insiders/) and providing feedback in [our open source repository](https://github.com/microsoft/vscode/issues) is the best way to help us improve auto. For the latest information, check out the [auto model selection documentation](https://code.visualstudio.com/docs/copilot/customization/language-models#_auto-model-selection).\n\nHappy coding!",
  "Description": "Use auto model selection in VS Code to get faster responses, reduced rate limiting, and a 10% discount on premium requests for paid users.\n\n[Read the full article](https://code.visualstudio.com/blogs/2025/09/15/autoModelSelection)",
  "FeedLevelAuthor": "Visual Studio Code - Code Editing. Redefined.",
  "PubDate": "2025-09-15T00:00:00+00:00",
  "Link": "https://code.visualstudio.com/blogs/2025/09/15/autoModelSelection",
  "ProcessedDate": "2025-09-15 16:14:25",
  "Tags": [
    "blog"
  ],
  "OutputDir": "_news",
  "FeedUrl": "https://code.visualstudio.com/feed.xml"
}
