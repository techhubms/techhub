{
  "FeedName": "Microsoft News",
  "FeedLevelAuthor": "Source",
  "Title": "New AI agent understands video and images together",
  "OutputDir": "_news",
  "FeedUrl": "https://news.microsoft.com/source/feed/",
  "ProcessedDate": "2025-11-12 17:03:46",
  "PubDate": "2025-11-12T16:23:01+00:00",
  "Tags": [
    "AI",
    "Company News"
  ],
  "Description": "The post [New AI agent understands video and images together](https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/) appeared first on [Source](https://news.microsoft.com/source).",
  "Link": "https://www.microsoft.com/en-us/research/blog/mmctagent-enabling-multimodal-reasoning-over-large-video-and-image-collections/",
  "EnhancedContent": "![Three white icons on a blue-to-purple gradient background: the first icon shows an image/photo; the second icon depicts a computer monitor with vertical bars; the third icon displays three connected circles with user silhouettes.](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/10/MMCTAgent-BlogHeroFeature-1400x788-1.jpg)\n\nModern multimodal AI models can recognize objects, describe scenes, and answer questions about images and short video clips, but they struggle with long-form and large-scale visual data, where real-world reasoning requires moving beyond object recognition and short-clip analysis.\n\nReal-world reasoning increasingly involves analyzing long-form video content, where context spans minutes or hours, far beyond the context limits of most models. It also entails querying across massive multimodal libraries of videos, images, and transcripts, where finding and integrating relevant evidence requires more than retrieval—it requires strategic reasoning. Existing models typically perform single-pass inference, producing one-shot answers. This limits their ability to handle tasks that require temporal reasoning, cross-modal grounding, and iterative refinement.\n\n## MMCTAgent\n\nTo meet these challenges, we developed the [Multi-modal Critical Thinking Agent](https://www.microsoft.com/en-us/research/publication/mmctagent-multi-modal-critical-thinking-agent-framework-for-complex-visual-reasoning/?msockid=153992cb7df169482b9487167c0968e9), or MMCTAgent, for structured reasoning over long-form video and image data, available on [GitHub (opens in new tab)](https://github.com/microsoft/MMCTAgent) and featured on [Azure AI Foundry Labs (opens in new tab)](https://labs.ai.azure.com/projects/mmct-agent/).\n\nBuilt on [AutoGen](https://www.microsoft.com/en-us/research/project/autogen), Microsoft’s open-source multi-agent system, MMCTAgent provides multimodal question-answering with a Planner–Critic architecture. This design enables planning, reflection, and tool-based reasoning, bridging perception and deliberation in multimodal tasks. It links language, vision, and temporal understanding, transforming static multimodal tasks into dynamic reasoning workflows.\n\nUnlike conventional models that produce one-shot answers, MMCTAgent has modality-specific agents, including ImageAgent and VideoAgent, which include tools like get\\_relevant\\_query\\_frames() or object\\_detection-tool(). These agents perform deliberate, iterative reasoning—selecting the right tools for each modality, evaluating intermediate results, and refining conclusions through a Critic loop. This enables MMCTAgent to analyze complex queries across long videos and large image libraries with explainability, extensibility, and scalability.\n\n[MMCTAgent on Azure AI Foundry Labs](https://labs.ai.azure.com/projects/mmct-agent/)\n\n[!\\[\\](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/06/Azure-AI-Foundry_1600x900.jpg)](https://ai.azure.com/labs)\n\n## Azure AI Foundry Labs\n\nGet a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.\n\n[Azure AI Foundry](https://ai.azure.com/labs)\n\nOpens in a new tab\n\n## How MMCTAgent works\n\nMMCTAgent integrates two coordinated agents, Planner and Critic, orchestrated through AutoGen. The Planner agent decomposes a user query, identifies the appropriate reasoning tools, performs multimodal operations, and drafts a preliminary answer. The Critic agent reviews the Planner’s reasoning chain, validates evidence alignment, and refines or revises the response for factual accuracy and consistency.\n\nThis iterative reasoning loop enables MMCTAgent to improve its answers through structured self-evaluation—bringing reflection into AI reasoning. A key strength of MMCTAgent lies in its modular extensibility. Developers can easily integrate new, domain-specific tools—such as medical image analyzers, industrial inspection models, or specialized retrieval modules—by adding them to ImageQnATools or VideoQnATools. This design makes MMCTAgent adaptable across domains.\n\n### VideoAgent: From ingestion to long-form multimodal reasoning\n\n![MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback. ](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/11/MMCT_UPDATED_FINAL_FINAL.png)Figure 1. MMCTAgent’s Planner–Critic architecture enables multimodal reasoning over long-form video through structured ingestion, retrieval, and iterative feedback\n\nThe VideoAgent extends this architecture to long-form video reasoning. It operates in two connected phases: library creation (ingestion) and query-time reasoning.\n\n#### Phase 1 – Video ingestion and library creation\n\nBefore reasoning, long-form videos undergo an ingestion pipeline that aligns multimodal information for retrieval and understanding:\n\n1. **Transcription** and **translation**: Converts audio to text and, if multilingual, translates transcripts into a consistent language\n2. **Key-frame identification**: Extracts representative frames marking major visual or scene changes\n3. **Semantic chunking** and **chapter generation**: Combines transcript segments and visual summaries into coherent, semantically segmented chapters with associated key frames. Inspired by Microsoft’s [Deep Video Discovery agentic search tool](https://www.microsoft.com/en-us/research/publication/deep-video-discovery-agentic-search-with-tool-use-for-long-form-video-understanding/), this step also extracts detailed descriptions of objects, on-screen text, and characters present within each video segment, integrating these insights directly into the corresponding chapters.\n4. **Multimodal embedding creation**: Generates image embeddings for key frames, linking them to their corresponding transcript and chapter data\n\nAll structured metadata, including transcripts, visual summaries, chapters, and embeddings, is indexed in the Multimodal Knowledgebase using [Azure AI Search (opens in new tab)](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search), which forms the foundation for scalable semantic retrieval and downstream reasoning.\n\n#### Phase 2 – Video question answering and reasoning\n\nWhen a user submits a query, the VideoAgent retrieves, analyzes, and reasons across the indexed video content using specialized planner and critic tools.\n\n##### Planner tools\n\n- **get\\_video\\_analysis**: Finds the most relevant video, provides a summary, and lists detected objects\n- **get\\_context**: Retrieves contextual information and relevant chapters from the Azure AI Search index\n- **get\\_relevant\\_frames**: Selects key frames most relevant to the user query\n- **query\\_frame**: Performs detailed visual and textual reasoning over selected frames\n- **get\\_context** and **get\\_relevant\\_frames** work in tandem to ensure that reasoning begins from the most semantically relevant evidence\n\n##### Critic tool\n\n- **critic\\_tool**: Evaluates the reasoning output for temporal alignment, factual accuracy, and coherence between visual and textual modalities\n\nThis two-phase design, which involves structured ingestion followed by agentic reasoning, enables MMCTAgent to deliver accurate, interpretable insights for long information-dense videos.\n\n### ImageAgent: Structured reasoning for static visuals\n\nWhile the VideoAgent handles temporal reasoning across long-form videos, the ImageAgent applies the same Planner–Critic paradigm to static visual analysis. It performs modular, tool-based reasoning over images, combining perception tools for recognition, detection, and optical character recognition with language-based reasoning for interpretation and explanation.\n\n##### Planner tools\n\n- **vit\\_tool**: Leverages Vision Transformer (ViT) or Vision Languague Model (VLM) for high-level visual understanding and description\n- **recog\\_tool**: Performs scene, face, and object recognition\n- **object\\_detection\\_tool**: Localizes and labels entities within an image\n- **ocr\\_tool**: Extracts embedded text from visual elements\n\n##### Critic tool\n\n- **critic\\_tool**: Validates the Planner’s conclusions for factual alignment and consistency, refining the final response\n\nThis lightweight ImageAgent provides fine-grained, explainable reasoning over image collections—supporting visual question answering, content inspection, and multimodal retrieval—while maintaining architectural symmetry with the VideoAgent.\n\n## Evaluation Results\n\nTo assess the effectiveness of MMCTAgent, we evaluated both the ImageAgent and VideoAgent with multiple base LLM models and a range of benchmark datasets and real-world scenarios. Some key results are presented here.\n\n| Image Datasets | GPT-4V | MMCT with GPT-4V | GPT4o | MMCT with GPT-4o | GPT-5 | MMCT with GPT-5 | | --- | --- | --- | --- | --- | --- | --- | | MM-Vet [1] | 60.20 | 74.24 | 77.98 | 79.36 | 80.51 | 81.65 | | MMMU [2] | 56.80 | 63.57 | 69.10 | 73.00 | 84.20 | 85.44 |\n\n| Video Datasets | GPT4o | MMCT with GPT-4o | | --- | --- | --- | | VideoMME [3] | 72.10 | 76.70 |\n\nMMCTAgent enhances base model performance by augmenting their capabilities with appropriate tools such as object detection and optical character recognition (OCR) for weaker models, or domain-specific tools for stronger models, thereby leading to substantial improvements. For example, integrating these tools raised GPT-4V’s accuracy from 60.20% to 74.24% on MM-Vet dataset. Additionally, the configurable Critic agent provides additional validation, which is especially valuable in critical domains. The additional evaluation results are available [here (opens in new tab)](https://github.com/microsoft/MMCTAgent/blob/main/EVALUATIONS.md).\n\n## Takeaways and next steps\n\nMMCTAgent demonstrates a scalable agentic approach to multimodal reasoning with a Planner–Critic architecture. Its unified multimodal design supports both image and video pipelines, while the extensible toolchain enables rapid integration of domain-specific tools and capabilities. It provides Azure-native deployment and supports configurability within the broader open-source ecosystem.\n\nLooking ahead, we aim to improve efficiency and adaptability in retrieval and reasoning workflows, and to extend MMCTAgent’s applications beyond current agricultural evaluations, exploring new real-world domains through initiatives like [Project Gecko](https://www.microsoft.com/en-us/research/project/project-gecko) to advance the creation of accessible, innovative multimodal applications for people around the globe.\n\n## Acknowledgements\n\nWe would like to thank our team members for their valuable contributions to this work: Aman Patkar, [Ogbemi Ekwejunor-Etchie](https://www.microsoft.com/en-us/research/people/ogbemie), Somnath Kumar, Soumya De, and Yash Gadhia.\n\n**References**\n\n[1] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. “MM-VET: Evaluating large multimodal models for integrated capabilities”, 2023.\n\n[2] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. “MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI”, 2023.\n\n[3] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. “Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis”, 2024.\n\nOpens in a new tab",
  "Author": "stclarke"
}
