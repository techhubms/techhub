{
  "Title": "Microsoft’s Dion boosts optimization method for training new AI models",
  "Description": "The post [Microsoft’s Dion boosts optimization method for training new AI models](https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/) appeared first on [Source](https://news.microsoft.com/source).",
  "FeedName": "Microsoft News",
  "Author": "stclarke",
  "FeedLevelAuthor": "Source",
  "FeedUrl": "https://news.microsoft.com/source/feed/",
  "PubDate": "2025-08-14T15:28:24+00:00",
  "OutputDir": "_news",
  "ProcessedDate": "2025-08-14 16:05:17",
  "EnhancedContent": "![Three white icons on a gradient background transitioning from blue to green. From left to right: a network of interconnected nodes, a speedometer with the needle pointing right, and a flowchart with squares and a diamond shape.](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion-BlogHeroFeature-1400x788_New-scaled.jpg)\n\nTraining AI models requires choosing an optimizer and for nearly a decade, [Adam( (opens in new tab)](https://arxiv.org/pdf/1412.6980)–[W) (opens in new tab)](https://arxiv.org/pdf/1711.05101) has been the optimizer of choice. Given that durability and success, it was fair to doubt that any further improvement was possible. And yet, last December, a new optimizer called [Muon (opens in new tab)](https://kellerjordan.github.io/posts/muon/) showed serious promise by powering a [nanoGPT speedrun (opens in new tab)](https://github.com/KellerJordan/modded-nanogpt/tree/master). This proved out, with multiple AI labs (e.g., [Kimi-AI (opens in new tab)](https://arxiv.org/abs/2502.16982) and [Essential-AI (opens in new tab)](https://arxiv.org/abs/2505.02222)) reporting 2x scale improvements and the release of the 1T parameter [Kimi K2 (opens in new tab)](https://moonshotai.github.io/Kimi-K2/) model. Restated: you can train a model to similar performance with half as many GPUs.\n\nThere’s one fly in the ointment: Muon requires large matrix multiplications in the optimizer, which requires heavy communication in large models at the scale where [FSDP](https://arxiv.org/abs/2304.11277) and [TP](https://arxiv.org/abs/1909.08053) parallelization becomes desirable. Going back to the [inspiration for Muon,](https://jeremybernste.in/writing/deriving-muon) the key idea is an orthonormal update, which sparked the search for more scalable alternative linear algebras realizing the same goal. That’s exactly what [Dion](https://www.microsoft.com/en-us/research/publication/dion-distributed-orthonormalized-updates/) is. We have open-sourced this new optimizer to enable anyone to train large models more efficiently at scale.\n\n## What’s an orthonormal update?\n\n![Illustration of matrix parameters](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure1_Dion.png)Figure1. Illustration of matrix parameters\n\nAt the core of Transformers, a set of input activations is multiplied by a learned weight matrix to produce a new set of output activations. When the weight matrix is updated during training, the resulting change in the output activations generally depends on the direction of the input activations. As a result, the learning rate must be chosen conservatively to accommodate the input direction that induces the largest change. Orthonormalized updates alter this behavior by (approximately) making the change in output activations invariant to the direction of the input. This is achieved by enforcing [orthonormality (opens in new tab)](https://en.wikipedia.org/wiki/Orthonormality) on the update matrix, thereby equalizing its effect across all input directions.\n\n## What is Dion?\n\nWhile Muon has shown strong empirical results, scaling it to very large models poses challenges. As reported by [Essential AI (opens in new tab)](https://www.essential.ai/blog/infra), applying Muon to large architectures like LLaMA-3 becomes *compute-bound*—and potentially *communication-bound*—due to the cost of the [Newton–Schulz orthonormalization steps (opens in new tab)](https://docs.modula.systems/algorithms/newton-schulz/).\n\n![Pseudocode of the centralized version of Dion](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure2_Dion.png)Figure 2. Pseudocode of the centralized version of Dion\n\nThis is where **Dion** enters. At a high level, Dion introduces a new axis for scalability: the **rank**. Specifically, for a given rank r, Dion orthonormalizes only the top r of the singular vector space, reducing communication and compute overhead while preserving performance. Empirically, we observe that the necessary rank for good performance grows much more slowly than the number of parameters in larger models.\n\nDownload [Dion optimizer](https://github.com/microsoft/dion/)\n\nDion implements orthonormalization using [*amortized power iteration* (opens in new tab)](https://arxiv.org/pdf/1905.13727)*.*Power iteration typically pulls out the largest singular value by repeated matrix multiplication. By amortizing this process over optimization steps—applied to the slowly-evolving momentum matrix—we reduce the cost to just two matrix multiplications per step. Incorporating a QR decomposition allows us to extract an approximate orthonormal basis spanning the top singular directions, rather than just the leading one. This amortized power iteration is fully compatible with standard distributed training techniques such as **FSDP** and **tensor parallelism**. Here, we show a simple centralized version, but the technique works for more complex forms of parallelization as presented in the paper. In other words, we can orthogonalize a matrix *without ever seeing a full row or column of it*.\n\nLow-rank approximation would ordinarily introduce error, but Dion overcomes this through an error feedback mechanism. This keeps the residual of low rank approximation in the momentum matrix so that any systematic gradient structure not initially captured accumulates to eventually be applied in a future update.\n\nSpotlight: Event Series\n\n[!\\[Research Forum | abstract background with colorful hexagons\\](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/05/Research-Forum-hero_1400x788.jpg)](https://www.microsoft.com/en-us/research/event/microsoft-research-forum/?OCID=msr_researchforum_MCR_Blog_Promo)\n\n## Microsoft Research Forum\n\nJoin us for a continuous exchange of ideas about research in the era of general AI. Watch the first four episodes on demand.\n\n[Watch on-demand](https://www.microsoft.com/en-us/research/event/microsoft-research-forum/?OCID=msr_researchforum_MCR_Blog_Promo)\n\nOpens in a new tab\n\n## How does it work?\n\nSomething very strange happened in our experiments. Usually, adding an extra constraint on the way an algorithm works can be expected to *decrease* overall performance. And indeed, at the 120M parameter scale of the speedrun, we see Dion’s update taking more time than Muon, while not yielding any significant gains. But at larger scales, we observed a different trend: Dion began to outperform Muon.\n\n![Wall-clock time speedup of Dion for 3B model training](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure-3_Dion.png)Figure 3. Wall-clock time speedup of Dion for 3B model training\n\nWhy would adding a constraint *improve* the update rule? The answer lies in what the constraint enforces. Dion achieves a much closer approximation to true orthonormalization than Muon. This precision, initially subtle, becomes increasingly important as the number of singular vectors grows. Over increasing model scale and training steps, this small advantage accumulates—leading to a measurable improvement in performance.\n\nThis edge further grows with batch size—with larger batches the update quality tends to degrade, but notably more slowly with Dion than Muon (and Muon is already a significant improvement over AdamW).\n\n![Scaling of Dion across different batch sizes](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure4_Dion.png)Figure 4. Scaling of Dion across different batch sizes\n\nHere you can see how the number of steps to reach a pretraining loss compared to AdamW varies as batch size grows with full rank and ¼ rank Dion (in orange) and Muon (in blue).\n\nIn our experiments, these benefits extend to various post-training regimes as well.\n\nWe also experimented with rank, discovering empirically that larger models tolerate smaller rank well.\n\n![Low-rank Dion across different model sizes](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Figure5_Dion.png)Figure 5. Low-rank Dion across different model sizes\n\nProjecting this trend out to the scale of the [LLaMA-3 (opens in new tab)](https://arxiv.org/abs/2407.21783) 405B parameter models suggests that Dion is fully effective even with **rank fractions as low as 1/16 or 1/64** for large dense models like LLaMA-3.\n\nUsing hardware timings of the individual update steps suggests a story that looks this:\n\n![Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon.](https://www.microsoft.com/en-us/research/wp-content/uploads/2025/08/Dion_FIG6.png)Figure 6. Estimated wall-clock time of each optimizer step for Llama 3 405B. Lower is better. Muon is highlighted in orange as our baseline, next to Dion with varying rank fractions. Suggested rank fractions for a 405B parameter model are shown in blue. Using Dion with rank fraction 1/16 or lower offers an order-of-magnitude speedup over Muon.\n\nWe’ve open-sourced a PyTorch FSDP2 + Tensor Parallel (TP) implementation of **Dion**, available via a simple pip install. Our goal is to make faster training with Dion accessible to everyone. As a bonus, the repository also includes a PyTorch FSDP2 implementation of **Muon.**\n\n[Dion optimizer](https://github.com/microsoft/dion/)\n\n## Acknowledgements\n\nWe thank Riashat Islam and Pratyusha Sharma for their helpful feedback on the writing and presentation.\n\nOpens in a new tab",
  "Tags": [
    "AI",
    "Company News"
  ],
  "Link": "https://www.microsoft.com/en-us/research/blog/dion-the-distributed-orthonormal-update-revolution-is-here/"
}
