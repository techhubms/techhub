{
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "ProcessedDate": "2025-08-27 17:10:03",
  "PubDate": "2025-08-27T09:14:00+00:00",
  "OutputDir": "_news",
  "Description": "The August 2025 Fabric Feature Summary showcases several exciting updates designed to streamline workflows and enhance platform capabilities. Notably, users will benefit from the new flat list view in Deployment pipelines, making navigation and management more intuitive. In addition, expanded support for service principals and cross-tenant integration with Azure DevOps reflects Microsoft’s commitment to versatile …\n\n[Continue reading “August 2025 Fabric Feature Summary”](https://blog.fabric.microsoft.com/en-us/blog/august-2025-fabric-feature-summary/)",
  "Tags": [],
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "Author": "Microsoft Fabric Blog",
  "FeedName": "Microsoft Fabric Blog",
  "EnhancedContent": "The August 2025 Fabric Feature Summary showcases several exciting updates designed to streamline workflows and enhance platform capabilities. Notably, users will benefit from the new flat list view in Deployment pipelines, making navigation and management more intuitive. In addition, expanded support for service principals and cross-tenant integration with Azure DevOps reflects Microsoft’s commitment to versatile and secure enterprise solutions.\n\n### **Last chance to join us in Vienna – FabCon is almost sold out!**\n\nFabCon Europe is landing in Vienna from **September 15–18**, and the conference organizers are offering **a last-minute discount on tickets**.\n\nCome connect with other data enthusiast! FabCon features 10+ full-day tutorials, a Partner pre-day, 120+ sessions from Microsoft Product Teams and the Community, the Power BI Dataviz World Championships, our swag-packed Power Hour, and an announcement-filled keynote\n\n**Email** **[info@espc.tech](mailto:info@espc.tech)** **by Friday August 29 to score the discount and secure your spot.**\n\nCan’t make it to Europe this year? FabCon is happening again in the United States in Atlanta. Mark your calendars for March 16-20, 2026.\n\nRegister [here](https://www.fabriccon.com/index.php/tickets) and use code MSCATL for a $200 discount on top of current Super Early Bird pricing!\n\n# Contents\n\n- Fabric Platform\n- New in UI – Flat list view in Deployment pipelines\n- Microsoft Fabric APIs Specification\n- Service Principal and Cross-Tenant Support for Azure DevOps(Preview)\n- Data Engineering\n- Autoscale Billing for Spark (Generally Available)\n- Job Bursting Control for Fabric Data Engineering Workloads\n- Fabric Spark Livy API (Generally Available)\n- JobInsight Diagnostics Library (Preview)\n- Enhanced Monitoring for Spark High Concurrency\n- Use Fabric User Data Functions with Pandas DataFrames and Series in Notebooks\n- Notebook snapshot for running Notebooks\n- OpenAPI spec generation in Fabric User Data Functions (Preview)\n- New test capability for Fabric User Data Functions (Preview)\n- Data Science\n- Serve real-time predictions seamlessly with ML model endpoints (Preview)\n- Discover which query examples influenced the Data Agent response\n- Download Diagnostics in Data Agent\n- Expanded Data Agent Support for Large Data Sources (Preview)\n- Data Warehouse\n- Refresh SQL analytics endpoint Metadata REST API (Generally Available)\n- COPY INTO and OPENROWSET from OneLake (Preview)\n- Visual Experience for SQL Audit Logs in Microsoft Fabric Data Warehouse (Preview)\n- Audit Log (CRUD Operations) Naming Simplification\n- SHOWPLAN_XML set statement (Generally Available)\n- Real-Time Intelligence\n- Additional analytics on Activator activations\n- Tailor Your Data Schema Tree in Queryset\n- Database tree in Edit Tile and AzMon data sources\n- Streamlining Query Sharing with Queryset\n- New Settings for Eventhouse Accelerated Shortcuts: MaxAge and HotWindows\n- Event Schema Registry in Fabric Real-Time Intelligence (Preview)\n- Databases\n- Optimizing Query Management: New Controls in the Editor\n- Use Python Notebooks to Read/Write to Fabric SQL Databases (Preview)\n- Data Factory\n- Easily Manage pipeline Triggers\n- Fabric Simplifies Terminology Data pipelines Now Referred to as pipelines\n- Reset Incremental Copy from Copy job\n- Auto Table Creation on Destination from Copy job\n- JSON format Support in Copy job\n- Create CI/CD-Enabled Dataflow Gen2 from Existing Dataflow Gen2 (Generally Available)\n- Integrated Run History and Validation Feedback in Dataflow Gen2 Editor\n- Improvements to SharePoint as a destination in Dataflow Gen2\n- New Category Filters Added to Template Gallery\n- Streamline Data Source Setup with Copilot’s New ‘Get Data’ Capability\n\n# Fabric Platform\n\n## New in UI – Flat list view in Deployment pipelines\n\nWe’ve rolled out a new ‘Flat list’ view for stage content in the Deployment pipelines! This update enables selecting items across workspace folders and offers greater clarity when using the ‘Select related’ button during deployment.\n\nTo switch between views, use the new toggle located in the top-right corner of the stage content area:\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-2.png)\n\nThe new view will appear as exampled in the screenshot after enablement.\n\n- A single list displaying all workspace items.\n- A new ‘Location’ column showing each item’s full path.\n\nYour selected view remains active even when switching between stages.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-3.png)\n\nTo learn more, refer to the [Deploy content using Deployment pipelines](https://review.learn.microsoft.com/fabric/cicd/deployment-pipelines/deploy-content?branch=main&amp;branchFallbackFrom=pr-en-us-9645&amp;tabs=new-ui#flat-list-view) documentation.\n\n## Microsoft Fabric APIs Specification\n\nI’m excited to share that we’ve successfully published the Microsoft Fabric APIs Specification in the [microsoft/fabric-rest-api-specs](https://github.com/microsoft/fabric-rest-api-specs) GitHub repository!\n\nThis new repository serves as the official source for REST API specifications for Microsoft Fabric. It’s designed to provide developers with a comprehensive, well-organized, and easily accessible collection of public API specifications.\n\nWhether you’re building custom solutions, integrating with Microsoft Fabric, or exploring its capabilities, this resource will help you efficiently understand and leverage the available APIs.\n\nWhat’s Inside?\n\n- OpenAPI specifications for Microsoft Fabric public APIs\n- Up-to-date documentation to support development and integration\n- A central hub for feedback, collaboration, and contributions\n\nWe welcome you to explore the repository and start building with Microsoft Fabric APIs!\n\n## Service Principal and Cross-Tenant Support for Azure DevOps(Preview)\n\nSupport for service principals and cross-tenant integration with Azure DevOps is scheduled to launch in mid-September 2025.\n\nThis highly anticipated feature enables a comprehensive set of automation processes for Fabric customers. For example, Users can now automate workspace setup using tools like the Fabric CLI and Terraform provider and connect the workspace to Azure DevOps repositories—even across tenants—via service principals.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m.jpeg)\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-4.png)\n\nFor more in-depth information, refer to the following documentation.\n\n- [Fabric Git API](https://learn.microsoft.com/rest/api/fabric/core/git)\n- [Automate Git Integration by using API](https://learn.microsoft.com/fabric/cicd/git-integration/git-automation)\n\n# Data Engineering\n\n## Autoscale Billing for Spark (Generally Available)\n\nMicrosoft Fabric has introduced Autoscale Billing for Apache Spark. This is a serverless, pay-as-you-go billing model designed to offer flexibility, transparency, and cost efficiency for Spark workloads.\n\nWith Autoscale Billing, Spark jobs run **independently of your Fabric capacity** and are billed only for execution time. Providing teams the freedom to scale compute without impacting shared workloads.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m.gif)\n\n**Why it matters:**\n\n- **Cost-efficient** – Pay only for job runtime, no idle costs.\n- **Dedicated Spark compute** – No contention with other Fabric workloads.\n- **Quota-aware controls** – Manage and monitor Spark usage with Azure Quota Management.\n- **Subscription-level visibility** – Track CU consumption and request more when needed.\n\nAutoscale Billing is now available in all regions that support Fabric Data Engineering workloads. Enable it today from the **Fabric Capacity Settings page** and start scaling Spark on your terms!\n\nRefer to the documentation to learn how to [configure Autoscale Billing](https://learn.microsoft.com/fabric/data-engineering/autoscale-billing-for-spark-overview).\n\n## Job Bursting Control for Fabric Data Engineering Workloads\n\nMicrosoft Fabric Data Engineering now gives capacity admins more control over **job bursting**. By default, Fabric allows Spark jobs to **burst up to 3 x’s their base CU allocation**, improving throughput for heavy workloads.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-5.png)\n\nWith the new **‘Disable Job-level Bursting’ switch** (Admin Portal → Capacity Settings → Spark Compute), admins can now choose how Spark capacity is consumed:\n\n**How it works**\n\n- **Enabled (default):** A single Spark job can use the full burst limit **(up to 3X the CUs of the base capacity).**\n- **Disabled:** Jobs are capped at base capacity, preserving **concurrency** and preventing one job from monopolizing resources.\n- **Autoscale Billing:** Bursting is not applicable, since compute runs purely on demand.\n\n**When to use it**\n\n- **Keep bursting enabled** → For large ETL pipelines or heavy batch jobs needing maximum throughput.\n- **Disable bursting** → For interactive, multi-user notebook environments where **fair sharing** is more important.\n\nThis new control helps admins balance **performance vs. concurrency** — tailoring Spark behavior to their organization’s workload patterns.\n\nRefer to the [Admin control: Job-level bursting switch](https://learn.microsoft.com/fabric/data-engineering/capacity-settings-management#admin-control-job-level-bursting-switch) documentation to learn more about this setting.\n\n## Fabric Spark Livy API (Generally Available)\n\nThe General Availability (GA) of the Fabric Spark Livy API is now available, enabling automated Spark workloads in Microsoft Fabric. The APIs have been improved since the preview based on user feedback and requirements.\n\nThe Fabric Spark Livy API lets users submit and execute their Spark code on the Spark compute without creating Notebooks or Spark Job Definition artifacts. The Fabric Spark Livy API is integrated within a specific Lakehouse artifact and ensures straightforward access to data stored on OneLake. Additionally, Livy API offers the ability to customize the execution environment through its integration with the Environment artifact.\n\nThe Fabric Spark Livy API supports two different execution modes:\n\n**Session Jobs**:\n\n- A Livy API session job establishes a Spark session that remains active throughout the interaction and supports multiple Spark jobs. This is particularly useful for interactive and iterative workloads.\n- A Spark session starts when a job is submitted and lasts until the user ends it or the system terminates it after 20 minutes of inactivity. Throughout the session, multiple jobs can run, sharing state and cached data between runs.\n\n**Batch Jobs:**\n\n- A Livy API batch job establishes a Spark session for a single execution. In contrast to a Livy session job, a batch job does not sustain an ongoing Spark session.\n- With Livy batch jobs each job initiates a new Spark session, which ends when the job finishes. This approach works well for tasks that don’t rely on previous computations or require maintaining state between jobs.\n\nSince the preview of the Livy API, we’ve added SPN authentication, and the Livy API now supports both SPN and user authentication.\n\nWith the GA release, the **Fabric Spark Livy APIs** are now production-ready—providing a comprehensive solution for running Spark jobs remotely in a scalable, automated fashion.\n\n## JobInsight Diagnostics Library (Preview)\n\nJobInsight is a Java-based diagnostic library that enables developers and data engineers to analyze completed Spark applications directly within Microsoft Fabric Notebooks.\n\n**JobInsight provides two core capabilities:**\n\n- **Interactive Spark Job Analysis**\n\nOffers structured APIs that return execution data – such as Spark queries, jobs, stages, tasks, and executors – as Spark Datasets for deep-dive analysis.\n- **Spark Event Log Access**\n\nAllows users to copy event logs to a OneLake or ADLS Gen2 directory for long-term storage or custom offline diagnostics.\n\nWith its structured APIs, Job Insight makes it easy to investigate performance bottlenecks, debug execution issues, and extract actionable insights programmatically. Users can also save metrics on Lakehouse tables and copy logs for persistent storage. Additionally, Job Insight supports reusing past analyses and provides configuration options for handling large or deeply nested Spark event logs making it a powerful and flexible observability tool for Spark workloads.\n\nTo learn more, refer to the [Gain Deeper Insights into Spark Jobs with JobInsight in Microsoft Fabric](https://blog.fabric.microsoft.com/blog/25817/preview?preview=true) blog post.\n\n## [Enhanced Monitoring for Spark High Concurrency](https://blog.fabric.microsoft.com/en-us/blog/25618/preview?preview=true)\n\nEnhancements to the Spark application detail view now improve monitoring for Notebooks running in high concurrency mode, whether manually or via pipeline. These changes offer better visibility in Spark applications, support efficient debugging, and aid performance tuning across multiple Notebooks.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-1.jpeg)\n\n**Jobs Tab:** Enhanced Job-Level Insights\n\nThe **Jobs** tab now offers more granular visibility into individual Spark jobs within a high concurrency session:\n\n- **Notebook Context:** When multiple Notebooks are running within the same application, the corresponding Notebook name is now displayed alongside each job.\n- **Code Snippet View:** Click the code icon to view and copy the code snippet associated with each job.\n- **Filtering**: Filter Spark jobs by Notebook to focus on one or more Notebooks within the session.\n\n**Logs Tab:** Notebook-Aware Logging\n\nTo simplify debugging in shared high concurrency Spark sessions:\n\n- **REPL ID Prefixing:** Each log entry now includes a REPL ID prefix to help link logs to the appropriate Notebook.\n- **Notebook Filtering:** Filter logs by Notebook to inspect output more precisely – ideal for collaborative or parallel workflows.\n\n**Item Snapshots Tab:** Hierarchical Notebook View\n\nThe Item Snapshots tab now provides a tree view of all Notebooks participating in a shared Spark session:\n\n- **Browse All Notebooks:** View snapshots of both completed and in-progress Notebook runs.\n- **Snapshot Details:**For each Notebook, you can access:\n- Code at the time of submission\n- Execution status per cell\n- Output for each cell\n- Input parameters\n- **Pipeline Integration:** If the application runs within a pipeline, the related pipeline and Spark activity are also displayed for better traceability.\n\nRefer to the [Enhanced Monitoring for Spark High Concurrency Workloads](https://blog.fabric.microsoft.com/en-us/blog/25618/preview?preview=true)blog post for more information.\n\n## Use Fabric User Data Functions with Pandas DataFrames and Series in Notebooks\n\nA major upgrade to Notebook integration with Fabric User Data Functions (UDFs) is now available:\n\nPandas DataFrames and Series can now be used as input and output types—thanks to native integration with Apache Arrow!\n\nThis update brings higher performance, improved efficiency, and greater scalability to your Fabric Notebooks—enabling seamless function reuse for large-scale data processing across Python, PySpark, Scala, and R.\n\nWith this release, Pandas DataFrames and Series are now supported as first-class input and output types for UDFs, enabled by deep integration with Apache Arrow, a highly efficient columnar memory format optimized for analytics workloads.\n\n**Benefits of Arrow Integration:**\n\n- High-performance serialization: Eliminates costly JSON encoding and decoding.\n- Zero-copy data sharing: Reduces overhead during UDF execution.\n- Scalability: Easily handle millions of rows in memory.\n- Seamless compatibility: Works with your existing Pandas logic.\n\nInstead of manually converting large datasets to JSON, developers can now natively pass Pandas DataFrames to UDFs, operate on them with minimal overhead, and return results efficiently – all while enjoying faster execution and reduced memory usage.\n\nRefer to the [Use Fabric User Data Functions with Pandas DataFrames and Series in Notebooks](https://blog.fabric.microsoft.com/blog/25644/preview) blog post for more information.\n\n## Notebook snapshot for running Notebooks\n\nNotebook Snapshots now support running Notebooks, regardless of the method used to trigger them. Whether executed through pipeline activities in a high-concurrency shared session or a standard session, via a direct Notebook schedule, or initiated using NotebookUtils.run() or NotebookUtils.runMultiple(), you can now view the Notebook Snapshot in near real time.\n\n**This snapshot allows you to:**\n\n- View the Notebook code as it existed at the time of submission.\n- Monitor cell-level execution duration, status, and output.\n- See any cell-level errors in near real-time.\n- Inspect the input parameters used for the specific run, if applicable.\n\nThis enhancement gives you deeper visibility into Notebook execution, making it much easier to understand what’s happening behind the scenes. It is especially valuable when diagnosing long-running scheduled or pipeline-triggered Notebook runs.\n\n**New capabilities**\n\n- Pinpoint bottlenecks in execution.\n- Identify long-running cells.\n- View real-time errors alongside the corresponding code and surrounding context—even while the Notebook is still running.\n\nThis new capability is designed to help optimize performance and simplify troubleshooting for Notebook workloads.\n\n## OpenAPI spec generation in Fabric User Data Functions (Preview)\n\nThe Functions portal includes a **Generate invocation code** feature that allows for automatic generation of an Open API specification for Fabric User Data Functions.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-1.gif)\n\nThe [Open API specification](https://swagger.io/docs/specification/v3_0/about/), formerly Swagger Specification, is a widely used, language-agnostic description format for REST APIs. This allows humans and computers alike to discover and understand the capabilities of a service in a standardized format. This is critical for creating integrations with external systems, AI agents and code generators.\n\nTo access this feature, update to the latest version of the fabric-user-data-functions library within the Library Management experience.\n\nRefer to the blog post on [OpenAPI specification code generation now available in Fabric User Data Functions](https://go.microsoft.com/fwlink/?linkid=2332301) for more information.\n\n## New test capability for Fabric User Data Functions (Preview)\n\nA new Test capability is available for Fabric User Data Functions. This feature enables users to test and validate functions in real-time prior to publishing. With the Test capability, you can execute your functions in a dedicated Python runtime and get immediate feedback for all your code changes including libraries and connections.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-2.gif)\n\nTo get started, open the Functions portal and locate the mode switcher on the top right corner to switch to **Develop mode**. In this mode, the controls in the Functions explorer will switch from using the Run capability to the Test capability. After opening the Test panel, you can execute your functions and get their outputs, logs and errors. Once you’ve completed your functions tests, you can publish your functions for other Fabric items and users to run them.\n\nTo learn more, refer to the [Test your User Data Functions in the Fabric portal (preview)](https://go.microsoft.com/fwlink/?linkid=2330551) documentation.\n\n# Data Science\n\n## Serve real-time predictions seamlessly with ML model endpoints (Preview)\n\nFabric now supports real-time inferencing with ML models via secure, scalable, and easy-to-use online endpoints. These endpoints are available as built-in properties of most Fabric models—and they require minimal setup to kick off fully managed deployments.\n\nUsers can activate and customize model endpoints with a [public-facing REST API](https://aka.ms/fabric/model-endpoint-api) or directly from the [Fabric interface](https://aka.ms/fabric/model-endpoints). Endpoints support one-click deployment, auto-scaling out of the box, and other settings to support your custom solutions. A low-code interface enables you to test predictions easily before going live, making it simpler to integrate machine learning into real-time applications.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-6.png)\n\nTo learn more about this integration, check out the [Serve real-time predictions seamlessly with ML model endpoints](https://blog.fabric.microsoft.com/blog/serve-real-time-predictions-seamlessly-with-ml-model-endpoints) blog post or refer to the [Serve real-time predictions with ML model endpoints (Preview)](https://aka.ms/fabric/model-endpoints) documentation.\n\n## Discover which query examples influenced the Data Agent response\n\nWhen the Data Agent responds to a question, it references example queries that have been supplied and utilizes them to inform its reasoning process. With our latest update, creators can now see exactly which example queries were used during a run step to help shape the agent’s response.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/word-image-26643-11.png)\n\nBehind the scenes, the agent searches all configured examples and selects the top 3 most relevant ones based on the user’s question. These examples are passed to the model as reference points when generating a query. Now, in the chat canvas, you’ll be able to view those selected examples, giving you more transparency into how the agent is grounding its response and making it easier to improve your configurations over time.\n\nWant to learn more? Check out the [Data source example queries](https://learn.microsoft.com/fabric/data-science/data-agent-configurations#data-source-example-queries) documentation.\n\n## Download Diagnostics in Data Agent\n\nYou can now download a diagnostics file for any run step in the Data Agent chat canvas—giving you clear visibility into how the agent processed your question behind the scenes. The file includes details like which tools were used, how the question was interpreted, the intermediate reasoning steps, and any errors or fallback logic that occurred. It’s a powerful way to troubleshoot issues or provide rich context when working with Microsoft support.\n\nTo use it, simply select the ‘Download Diagnostics’ button in the chat canvas. The file is fully viewable and editable, so you can review and clean up the contents before sharing as needed.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-7.png)\n\nFor more information, refer to the [Evaluate your data agent (preview)](https://learn.microsoft.com/fabric/data-science/evaluate-data-agent) documentation.\n\n## Expanded Data Agent Support for Large Data Sources (Preview)\n\nData Agent is officially lifting restrictions on adding Data Sources with larger schema sizes. Users can now add Kusto, Semantic Model, Lakehouse, and Warehouse Data sources that contain over 100 Columns + Measures and more than 1000 Tables to the Data Agent. This change allows users to bring larger-scale databases and semantic models into Fabric’s Data Agent, unlocking deeper insights and enhanced capabilities.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-2.jpeg)\n\nFigure : The table named patient\\_medical\\_records contains a total of 103 columns, which exceeds the limit of what Fabric data agent used to support for data sources. Despite the larger schema size, users may add the patient\\_medical\\_records table to Fabric data agent. When selecting the table as an input to the data agent, users may receive a warning stating that the accuracy of results may vary with larger schema sizes. Regardless, users can select this table as an input to the data agent.\n\nPlease refer to the [Expanded Data Agent Support for Large Data Sources](https://blog.fabric.microsoft.com/blog/expanded-data-agent-support-for-large-data-sources) blog post for more details.\n\n# Data Warehouse\n\n## Refresh SQL analytics endpoint Metadata REST API (Generally Available)\n\nIn July of 2025, we introduced the SQL analytics endpoint Metadata Sync REST API in GA. With this API you can programmatically trigger a refresh of your SQL analytics endpoint to keep tables in sync with any changes made in your lakehouse, native and mirrored databases, ensuring that you can keep your data up to date as needed.\n\nTo use this feature, simply pass the workspace ID, SQL analytics endpoint ID, and the API will provide detailed synchronization status for each table, including start and end times, status, last successful sync time and any error messages if applicable.\n\n**Example:** How to refresh a specified SQL analytics endpoint in a workspace.\n\nPOST [https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/sqlEndpoints/{sqlEndpointId}/refreshMetadata](https://api.fabric.microsoft.com/v1/workspaces/%7bworkspaceId%7d/sqlEndpoints/%7bsqlEndpointId%7d/refreshMetadata)\n\nTo learn more about the REST API, checkout the [Refresh SQL analytics endpoint Metadata REST API (Generally Available)](https://blog.fabric.microsoft.com/en-us/blog/25061/preview) blog post, [Fabric REST APIs docs](https://learn.microsoft.com/rest/api/fabric/sqlendpoint/items/refresh-sql-endpoint-metadata) and the [GitHub page](https://github.com/microsoft/fabric-toolbox/tree/main/samples/notebook-refresh-tables-in-sql-endpoint) for a code sample.\n\n## COPY INTO and OPENROWSET from OneLake (Preview)\n\nThe Preview of **COPY INTO** and **OPENROWSET** from **OneLake** in Microsoft Fabric Data Warehouse has been announced, providing secure, workspace-governed data ingestion and file querying without dependence on external storage services.\n\nThis new capability allows users to ingest and query files stored in Lakehouse *Files* folders using familiar SQL syntax — with no need for Spark, pipelines, staging storage, SAS tokens, or complex IAM configuration. With this release, Fabric takes another step toward delivering a **fully SaaS-native, secure, and unified analytics platform**.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-3.jpeg)\n\n**Key highlights:**\n\n- Load data from OneLake using COPY INTO directly into Warehouse tables (supports CSV and Parquet).\n- Query files using OPENROWSET for fast, ad hoc exploration — no load required.\n- Leverages Fabric workspace permissions (secured by Entra ID) — no external storage IAM or firewall configuration needed.\n- Supports cross-workspace scenarios and automation via service principals (SPNs).\n\nWhether you’re automating ingestion pipelines, running SQL-only scenarios, or working within Private Link–enabled environments, these capabilities simplify onboarding and accelerate time-to-insight.\n\nFor example syntax, supported scenarios, and what’s coming next, check out the announcement blog post [OneLake as a Source for COPY INTO and OPENROWSET (Preview)](https://blog.fabric.microsoft.com/blog/24510/preview).\n\nRead JSON Lines format with OPENROWSET(BULK) (Preview)\n\nYou can now use OPENROWSET(BULK) in Microsoft Fabric Data Warehouse to read JSON Lines (JSONL) files directly. JSONL is a widely used format for logs, streaming, and machine learning data. With this enhancement, you can use the **OPENROWSET** function to read JSONL files natively eliminating the need to first import files as plain text and then manually apply T-SQL JSON functions to parse the data.\n\nYou can provide the URL of your JSONL file that is placed in Azure Data Lake or Fabric One Lake and read its content as a set of rows using the **OPENROWSET(BULK)** function:\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-8.png)\n\nThis new capability allows you to directly query JSONL data as if you were querying a regular table, using familiar T-SQL syntax. There’s no need to first import or manually parse the file—the **OPENROWSET(BULK)** function handles the JSON format natively. This enhanced functionality facilitates analytics workflows by enabling more efficient and straightforward ingestion and querying of JSONL data through standard T-SQL syntax.\n\nFor more details about **OPENROWSET(BULK)** support for the JSONL format, please refer to the [JSON Lines Support in OPENROWSET for Fabric Data Warehouse and Lakehouse SQL Analytics Endpoint (Preview)](https://blog.fabric.microsoft.com/blog/public-preview-json-lines-support-in-openrowset-for-fabric-data-warehouse-and-lakehouse-sql-endpoints/) blog post.\n\n## Visual Experience for SQL Audit Logs in Microsoft Fabric Data Warehouse (Preview)\n\nIn April 2025, we introduced the Preview of **SQL Audit Logs** in Microsoft Fabric Data Warehouse—empowering organizations to capture critical audit events for improved transparency, governance, and control.\n\nA major enhancement is now available: a new visual interface for configuring and managing audit logs is accessible within the Fabric Warehouse UI.\n\nThis update simplifies how customers manage auditing policies, with no scripts or advanced setup required. Whether you’re a security administrator, data platform engineer, or compliance lead, this intuitive interface makes configuring audit logging faster, clearer, and more aligned with your organization’s needs….. ![A close-up of a computer screen AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-close-up-of-a-computer-screen-ai-generated-cont.png)**Key Highlights**\n\n- **Effortless Setup**: Easily enable audit logging with a simple toggle and unified configuration pane.\n- **Granular Event Selection**: Choose from categorized event types (authentication, data access, admin actions), or fine-tune by individual action groups.\n- **Flexible Retention**: Configure log retention in OneLake for up to 9 years—validated directly in the UI.\n\nThis release reflects direct customer feedback and brings clarity and control to a critical governance capability.\n\nTo explore the new experience and learn how to get started, refer to the [Experience the New Visual SQL Audit Logs Configuration in Fabric Warehouse](https://blog.fabric.microsoft.com/blog/25270/preview) blog post.\n\n## Audit Log (CRUD Operations) Naming Simplification\n\nMicrosoft Fabric will consolidate audit log operation names beginning after **July 7, 2025,** to facilitate governance processes.\n\nActions like *Create Datamart* or *Delete Warehouse* will now appear as standardized entries—such as CreateArtifact, DeleteArtifact, UpdateArtifact, etc. This change aligns with Fabric’s unified platform model and reduces noise in audit logs.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-9.png)\n\n**There are no changes to functionality; logging has simply been improved for greater clarity and consistency.**\n\nIf you use audit logs for automation or monitoring, review and update any queries or tools using old operation names.\n\nFor details, check out the blog post: [Standardizing Audit Operations for Warehouse, DataMarts and SQL Analytics Endpoint](https://blog.fabric.microsoft.com/blog/24636/preview).\n\n## SHOWPLAN\\_XML set statement (Generally Available)\n\nSHOWPLAN\\_XML is now Generally Available for the Fabric Data Warehouse and Lakehouse SQL analytics endpoint! The SHOWPLAN\\_XML set statement and its ‘Display estimated execution plan’ counterpart in SQL Server Management Studio are staple tools for investigating the details of a T-SQL queryplan: providing information on planned data movements, resource estimates, operator choices, and more.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-10.png)\n\n*View of query output after running SET SHOWPLAN\\_XML ON.*\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-11.png)\n\n*View of graphical plan in SQL Server Management Studio after opening showplan XML (as .sqlplan) or clicking ‘Display Estimated Execution Plan’ button.*\n\nFor additional information, refer to the [SET SHOWPLAN_XML (Transact-SQL)](https://learn.microsoft.com/sql/t-sql/statements/set-showplan-xml-transact-sql?view=fabric) documentation and the [SHOWPLAN_XML in Fabric Data Warehouse (Preview)](https://blog.fabric.microsoft.com/blog/query-plans-in-fabric-data-warehouse/) blog post.\n\n# Real-Time Intelligence\n\n## Additional analytics on Activator activations\n\nWith Activator, users are now able to view additional analytics and KPIs related to their activations. By navigating to the History pivot in the rules section of Activator, these analytics can be accessed.\n\n![Inserting image...](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/inserting-image-.png)\n\nSelect ‘View details’ in your Teams messages and emails will take you to the History tab of your alert in Activator, where you’ll be able to see those analytics listed.\n\n![A black box with white text AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-black-box-with-white-text-ai-generated-content.png)\n\nWhen selecting ‘View details’, the information on the History tab will be filtered to show information that is related to the specific activation you are viewing details on. It will show information related to the specific ID you have selected and the relevant time range.\n\nThis feature is available for both rules that were created on attributes and rules that were created on streams.\n\nTo learn more, refer to the [Create a rule in Fabric Activator](https://learn.microsoft.com/fabric/real-time-intelligence/data-activator/activator-create-activators) documentation.\n\n## Tailor Your Data Schema Tree in Queryset\n\nExploring your data sources in Queryset just got easier. The data tree now comes with a refreshed UI that gives you more control over how your connections are displayed – helping you focus faster and write with context.\n\nYou can now toggle between two views:\n\n- **Flat list** of all connected databases.\n- **Grouped view** by cluster or data source.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-12.png)\n\nMaking it easier to navigate, especially when working across multiple environments.\n\nEach data source can still be expanded to explore its schema, and with a simple double-click on any entity, the full path is instantly copied into your query editor. You can also switch the active context for your query at any time, directly from the tree.\n\nTo learn more about this feature, refer to the [Query data in a KQL queryset](https://learn.microsoft.com/fabric/real-time-intelligence/kusto-query-set?tabs=kql-database) documentation.\n\n## Database tree in Edit Tile and AzMon data sources\n\nWriting queries in Real-Time Dashboards just got significantly easier. The **Edit Tile** experience now includes a **data source tree** – bringing full visibility into your connected data right where you work.\n\nWith this new pane, you no longer must rely solely on memory or IntelliSense. As you build your query, you can **browse your data source in real time –** including tables, columns (with types), functions, materialized views, and more.\n\nThis adjustment is particularly effective when dealing with new schemas or extensive environments.\n\n- Explore the full structure of your connected data.\n- Double-click to insert entity paths directly into your query.\n- Confidently discover and use available resources without context-switching.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/word-image-26643-23.png)\n\nWe’ve also added a **simplified experience for connecting to Azure Monitor data sources**. Previously, this required configuring connection strings manually. Simply enter your Azure resource details into the built-in connection string builder to get started.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/word-image-26643-24.png)\n\nFor more information, refer to the [Add title](https://learn.microsoft.com/fabric/real-time-intelligence/dashboard-real-time-create?tabs=create-manual%2Ckql-database#add-tile) documentation.\n\n## Streamlining Query Sharing with Queryset\n\nA new, streamlined user interface has been introduced for sharing queries in Queryset. This update enhances clarity and usability by providing a preview of the content being copied to the clipboard. Users can conveniently select between copying the raw query, a deep link, results, or any combination thereof.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-13.png)\n\nWhen you share a deep link, recipients can open it directly in Fabric—automatically connected to the right data source, with your query loaded and ready to run. It’s the fastest, most reliable way to share and collaborate.\n\nThis update makes sharing more transparent and helps users adopt best practices that scale across teams.\n\nTo learn more, refer to the [Share queries](https://learn.microsoft.com/fabric/real-time-intelligence/kusto-share-queries#copy-query) documentation.\n\n## New Settings for Eventhouse Accelerated Shortcuts: MaxAge and HotWindows\n\n[Accelerated OneLake Table shortcuts](https://blog.fabric.microsoft.com/blog/eventhouse-accelerated-onelake-table-shortcuts-generally-available?ft=All) caches and indexes data as it lands in OneLake, providing performance comparable to ingesting data in Eventhouse. By using this feature, you can accelerate data landing in OneLake, including existing data and any new updates.\n\nTwo additional settings are now available to facilitate the acceleration of your shortcuts.\n\n**HotWindows**\n\nThis allows for accelerating arbitrary time windows e.g. between (X .. Y) dates and not just last ‘N’ days of data. Delta data files created within these time windows are accelerated.\n\n**MaxAge**\n\nUsers set the data’s latency tolerance, controlling freshness. The shortcut returns accelerated data if the last index refresh is newer than @now – MaxAge. Otherwise, the shortcut table operates in non-accelerated mode.\n\n**Syntax**\n\n*.alter external table MyExternalTable policy query\\_acceleration ‘{“IsEnabled”: true, “Hot”: “1.00:00:00”, “HotWindows”:[{“MinValue”:”2025-07-06 07:53:55.0192810″,”MaxValue”:”2025-07-06 07:53:55.0192814″}], “MaxAge” : “00:05:00”}’*\n\nTo learn more about [.alter query acceleration policy command](https://learn.microsoft.com/kusto/management/alter-query-acceleration-policy-command?view=microsoft-fabric), refer to the documentation.\n\n## Event Schema Registry in Fabric Real-Time Intelligence (Preview)\n\nThis month, Event Schema Registry was introduced as a centralised resource for discovering, storing, and updating schemas for event-driven data flows. With this release, users can create and manage schemas within SchemaSets in Fabric Workspaces.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-14.png)\n\n**Capabilities**\n\n- Auto-discover schemas from Azure SQL CDC sources.\n- Use discovered schemas in transformations.\n- Create destination tables in Eventhouse directly from registered schemas.\n- Validate message formats before ingestion to catch errors early in the data pipeline.\n- Protect data ingested from custom endpoints.\n\nTry creating a Schemaset in the Event schema registry from the Real-Time Hub. Then, import some schemas, and start sending events based on the schemas to an Eventstream.\n\nTo learn more, visit the [Fabric documentation for Event Schema Registry.](https://learn.microsoft.com/fabric/real-time-intelligence/schema-sets/schema-registry-overview) Check out our new blog post for a [deep dive into the schema management concepts](https://blog.fabric.microsoft.com/blog/introducing-schema-registry-creating-type-safe-pipelines-using-schemas-and-eventstreams/) and how to build your first type-safe, schema aware data pipeline for publishing events from a custom endpoint to an Eventstream.\n\nOnce you have explored the feature, provide [your feedback](https://forms.office.com/r/u2iFDYDA9D). We welcome your feedback and aim to make managing data schemas straightforward.\n\n# Databases\n\n## Optimizing Query Management: New Controls in the Editor\n\nAs part of our ongoing commitment to improving the SQL database in Fabric experience based on direct user feedback and workflow needs, we’ve introduced several new features to the query editor designed to streamline common tasks, enhance team collaboration, and offer greater flexibility when working between tools.\n\n**Latest Improvements**\n\n- **Bulk delete query:** This feature enables users to delete multiple saved queries at once, eliminating the need to remove them individually. It was introduced in response to user feedback highlighting the difficulty of managing large lists of saved queries without a multi-select delete option. In the query editor’s ‘Queries’ folders, users can now hold Shift, select multiple queries, and right-click to delete them all in a single action. This streamlines the cleanup process, making it easier to maintain an organized and clutter-free workspace with minimal effort.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-15.png)\n\n- **Open your database in SQL Server Management Studio (SSMS):** This feature integrates the Fabric SQL web-based editor with SSMS, allowing for a smooth transition to the desktop environment. With a single click from the query editor, SSMS launches and automatically fills in the connection details for your Fabric SQL database — no manual copy-paste or setup required. This streamlines the workflow for users who prefer or need the advanced capabilities and richer UI of SSMS, making it faster and easier to switch between tools while working with Fabric SQL.\n\n![A screenshot of a computer program AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-program-ai-generated-c.png)\n\n- **Query sharing within your workspace:** This feature enables collaborative use of SQL queries within a Fabric workspace. You can now save a query and share it with other users in the same workspace, allowing teammates to view, run, or edit it —based on their permissions — directly from the query editor. When a query is marked as shared, it moves into this section and becomes accessible to workspace admins, members, and contributors. This eliminates the need to copy-paste SQL code or share snippets via email, making collaboration faster, easier, and more secure.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-16.png)\n\n## Use Python Notebooks to Read/Write to Fabric SQL Databases (Preview)\n\nYou can now read from and write to SQL databases in Microsoft Fabric using Python Notebooks, thanks to the new integration with the T-SQL magic command. This highly requested feature enables users to run powerful T-SQL queries directly within notebooks—combining scripting, visualizations, and explanatory text in one collaborative workspace. It supports rich, interactive charts, automated workflows, scheduled jobs, and secure sharing, making it easier than ever to analyze and operationalize SQL data seamlessly across the Fabric platform.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-4.jpeg)\n\nTo learn more, refer to the [Connect to your SQL database in Fabric using Python Notebook](https://blog.fabric.microsoft.com/blog/connect-to-your-sql-database-in-fabric-using-python-notebook?ft=All) blog post and check out the [Run T-SQL code in Fabric Python notebooks](https://learn.microsoft.com/fabric/data-engineering/tsql-magic-command-notebook) for more details.\n\n# Data Factory\n\n## Easily Manage pipeline Triggers\n\nTriggers in Fabric Data Factory pipeline allow you to automate your pipelines to run whenever events occur in Fabric such as file arrival, file delete, Lakehouse folder events, and pipeline events. Now to make management of your pipelines easy, we’ve added a panel on your pipeline that opens with the Trigger button from the pipeline canvas to view existing triggers that exist on your pipeline, making trigger management super quick and easy.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-17.png)\n\n![A screenshot of a computer program AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-program-ai-generated-c-1.png)\n\nTo learn more about this feature, refer to the [Data pipelines event triggers in Data Factory](https://learn.microsoft.com/fabric/data-factory/pipeline-storage-event-triggers) documentation.\n\n## Fabric Simplifies Terminology Data pipelines Now Referred to as pipelines\n\nAs we continue to evolve and expand the vision and capabilities of Fabric, we see many new and exciting use cases emerge from our incredible customer base. In Data Factory pipelines in Fabric, we’ve found many new ways to leverage pipelines that aren’t always directly related to data integration scenarios including business workflows and automation scenarios.\n\nTo help provide a clearer understanding of the role of pipelines in Fabric workflows, we are going to drop the ‘data’ term from the ‘data pipelines’ display name in Fabric workspaces. This change will take effect in September. Only the display name is being updated from ‘Data pipeline’ to ‘Pipeline’ in workspace lists and filters, with no impact on APIs or CICD.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-18.png)\n\nFor more information, please refer to the [Concept: Data pipeline Runs](https://learn.microsoft.com/fabric/data-factory/pipeline-runs) documentation.\n\n## Reset Incremental Copy from Copy job\n\nIncremental copy is one of the most loved features in Copy job. It dramatically boosts efficiency by transferring only new or updated data, saving you time, resources, and manual effort. The process is simple: the first run performs a full data load, and every run after that moves just the changes.\n\nNow you have greater flexibility in managing incremental copy, including the ability to reset it back to a full copy on the next run. This is incredibly useful when there’s a data discrepancy between your source and destination—you can simply let Copy Job perform a full copy in the next run to resolve the issue, then continue with incremental updates afterward.\n\nEven better, you can reset incremental copy per table, giving you fine-grained control. For example, you can re-copy smaller tables without impacting larger ones. This means smarter troubleshooting, less disruption, and more efficient data movement.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-19.png)\n\n[What is Copy job in Data Factory for Microsoft Fabric?](https://learn.microsoft.comfabric/data-factory/what-is-copy-job) Find out more in our documentation.\n\n## Auto Table Creation on Destination from Copy job\n\nWe’re on a mission to eliminate every friction point and make your data movement experience as smooth and intuitive as possible. Auto Table Creation is one of the steps in that direction. If the specified table doesn’t exist at your destination, Copy Job will automatically create the table and its schema for you. No manual setup, no interruptions, just effortless data movement from start to finish.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-20.png)\n\nCopy Job can now automatically create tables on the following destination stores:\n\n- SQL Server\n- Azure SQL database\n- Fabric Lakehouse table\n- Snowflake\n- Azure SQL Managed Instance\n- Fabric SQL database\n\n[What is Copy job in Data Factory for Microsoft Fabric?](https://learn.microsoft.comfabric/data-factory/what-is-copy-job) Find out more in our documentation.\n\n## JSON format Support in Copy job\n\nCopy job is the tool for you to move data from any source to any destination in any format. You can always choose binary copy between storage locations for any file format with highly optimized throughput. You can also copy files like CSV or Parquet to and from tables—and now, the same capability is available for JSON file formats.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-21.png)\n\n[What is Copy job in Data Factory for Microsoft Fabric?](https://learn.microsoft.comfabric/data-factory/what-is-copy-job) Find out more in our documentation.\n\n## Create CI/CD-Enabled Dataflow Gen2 from Existing Dataflow Gen2 (Generally Available)\n\nSaving a new Dataflow Gen2 with CI/CD support from a Dataflow Gen2 is now generally available.\n\nCustomers often would like to recreate an existing dataflow as a new dataflow Gen2 (CI/CD), getting all the benefits of the new GIT and CI/CD integration capabilities. Today, to accomplish this, they need to create the new Dataflow Gen2 (CI/CD) item from scratch and copy-paste their existing queries or leverage the Export/Import Power Query template capabilities. This, however, is not only inconvenient due to unnecessary steps, but it also does not carry over additional dataflow settings.\n\nDataflows in Microsoft Fabric now include a ‘Save as’ feature, that in a single click lets you save an existing Dataflow Gen2 as a new Dataflow Gen2 (CI/CD) item.\n\nLearn more about Save As Dataflow Gen2 (CI/CD): [Migrate to Dataflow Gen2 (CI/CD) using Save As](https://learn.microsoft.com/fabric/data-factory/migrate-to-dataflow-gen2-using-save-as)\n\n![Screenshot of the context menu under the ellipsis, showing the Save as Dataflow Gen2 (CI/CD) option.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/screenshot-of-the-context-menu-under-the-ellipsis.png)\n\n## Integrated Run History and Validation Feedback in Dataflow Gen2 Editor\n\nFor Dataflow Gen2 with CI/CD support, one of the most common experiences that you can have today when using a Dataflow is checking the results of its validation and the run operation. A few months ago, we introduced a new way to validate your Dataflow without ever leaving the Dataflow window, but for run operations you still had to go into the monitoring hub or the workspace list to see the ‘Recent runs’. This changes today with the introduction of new and improved embedded experiences inside of the Dataflow editor.\n\n![A screenshot of a computer Description automatically generated](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-description-automatica.png)\n\nIn the home tab of the ribbon, you will now find two new entries in the **Dataflow** group:\n\n- Check validation\n- Recent runs\n\nThe **Check validation** allows you to see the status of your last or ongoing saving operations and the validations that happen against it.\n\n![A screenshot of a computer Description automatically generated](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-description-automatica-1.png)\n\nSelecting the **Check validation** button in the ribbon, rather than Save & Run, allows users to review the status of this operation.\n\n![A screenshot of a computer Description automatically generated](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-description-automatica-2.png)\n\nThe status bar displays the progress of the validation process as it occurs:\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/word-image-26643-41.png)\n\nOnce you trigger a run for your Dataflow, you will also be able to see the progress of the run in the status bar. Once finished, you can see a notification that tells you the timestamp when the last run happened.\n\n![A screenshot of a computer Description automatically generated](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-description-automatica-3.png)\n\nFinally, a new addition to this experience is the explicit button to close your Dataflow and discard any changes that you’ve been working on as well as other operations to simplify your interaction when authoring a Dataflow such as ‘Save, run & close; and ‘Save & close’:\n\n![A screenshot of a computer Description automatically generated](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-description-automatica-4.png)\n\nThese changes are currently rolling out to all production regions.\n\n## Improvements to SharePoint as a destination in Dataflow Gen2\n\nAs part of our ongoing improvements, we’ve removed system folders from the navigator when selecting a folder for your SharePoint data destination. This enhancement streamlines your folder selection experience, ensuring only relevant user folders appear, reducing clutter and making it faster and more intuitive to pinpoint exactly where your data should go.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-22.png)\n\nAs we continue to roll out these enhancements and streamline your data experiences, we encourage you to share your thoughts and let us know how we can further improve. Your feedback is always welcome!\n\nFor more information, refer to the [Dataflow Gen2 data destinations and managed settings](https://learn.microsoft.com/fabric/data-factory/dataflow-gen2-data-destinations-and-managed-settings) documentation.\n\n## New Category Filters Added to Template Gallery\n\nDiscover templates faster and transform your workflow with our latest upgrade to the Template Gallery: category-based filtering! With this new feature, you can now browse templates by category—making it easier than ever to find exactly what you need when you need it. Whether you’re starting a new project, exploring solutions, or just curious about what’s available, categories take the guesswork out of template discovery.\n\nSimply select **Filter**, select a category, and watch your search results instantly become more relevant. Dive in and experience a more organized, efficient, and intuitive way to kick off your next data project!\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-23.png)\n\nTo learn more about templates and pipeline template gallery, read [Templates – Microsoft Fabric | Microsoft Learn.](https://learn.microsoft.com/en-us/fabric/data-factory/templates)\n\n## Streamline Data Source Setup with Copilot’s New ‘Get Data’ Capability\n\nWith the power of new get data Copilot capability supported in Dataflow Gen2, users can quickly connect to existing resources or set up new connections with ease.\n\nWhen you request Copilot to connect to a data resource, it first checks if the resource already exists. If found, dataflow gen2 can access and guide you through navigation and data preview. If not, it launches a filtered get data wizard to help you locate the correct resource efficiently.\n\nFor example, if you know the exact name of an existing SQL database connection, Dataflow Gen2 will enable you to quickly access its server and database navigation with the assistance from Copilot.\n\n![A screenshot of a chat AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-chat-ai-generated-content-may-b.png)\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-24.png)\n\nIf you are unsure whether a SQL connection exists or need to create a new one, Copilot will guide you through the process efficiently.\n\n![A screenshot of a chat AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-chat-ai-generated-content-may-b-1.png)\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-25.png)\n\nIf a connection does not exist, Copilot will share information with Dataflow Gen2 to open a pre-filled setup page to help you create one quickly.\n\n![A screenshot of a chat AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-chat-ai-generated-content-may-b-2.png)\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/08/a-screenshot-of-a-computer-ai-generated-content-m-26.png)\n\nLearn more about Copilot for Data Factory in [Get started with Copilot in Fabric in the Data Factory workload](https://learn.microsoft.com/fabric/data-factory/copilot-fabric-data-factory-get-started).",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/august-2025-fabric-feature-summary/",
  "Title": "August 2025 Fabric Feature Summary"
}
