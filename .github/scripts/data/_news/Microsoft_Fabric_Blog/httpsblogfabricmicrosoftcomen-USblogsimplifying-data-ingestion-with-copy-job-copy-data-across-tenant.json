{
  "FeedName": "Microsoft Fabric Blog",
  "OutputDir": "_news",
  "Description": "Copy job is the go-to solution in Microsoft Fabric Data Factory for simplified data movement, whether you’re moving data across clouds, from on-premises systems, or between services. With native support for multiple delivery styles, including bulk copy, incremental copy, and change data capture (CDC) replication, Copy job offers the flexibility to handle a wide range …\n\n[Continue reading “Simplifying Data Ingestion with Copy job – Copy data across tenants using Copy job in Fabric Data Factory “](https://blog.fabric.microsoft.com/en-us/blog/simplifying-data-ingestion-with-copy-job-copy-data-across-tenants-using-copy-job-in-fabric-data-factory/)",
  "Tags": [],
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/simplifying-data-ingestion-with-copy-job-copy-data-across-tenants-using-copy-job-in-fabric-data-factory/",
  "PubDate": "2025-10-20T12:00:00+00:00",
  "EnhancedContent": "Copy job is the go-to solution in Microsoft Fabric Data Factory for simplified data movement, whether you’re moving data across clouds, from on-premises systems, or between services. With native support for multiple delivery styles, including bulk copy, incremental copy, and change data capture (CDC) replication, Copy job offers the flexibility to handle a wide range of data movement scenarios—all through an intuitive, easy-to-use experience. Learn more in [What is Copy job in Data Factory – Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/en-us/fabric/data-factory/what-is-copy-job).\n\nThis blog provides step-by-step guidance on using Copy job to copy data across different tenants.\n\n# Scenario\n\nIn this scenario, **Tenant A** has a Fabric Data Warehouse and Copy job, while **Tenant B** has an Azure Data Lake Gen2 account. A user from Tenant A will use Copy job to copy data from the Azure Data Lake Gen2 account with service principal authentication (owned by Tenant B) into Fabric Data Warehouse (owned by Tenant A).\n\n# How it works\n\n## **Prerequisites**\n\nAn Azure Data Lake Gen2 account is available in **Tenant B**, with a service principal enabled to allow users to access the account.\n\n### **How to Create an Azure Data Lake Gen2 account with service principal enabled in Tenant B**\n\n1. *Go to* [https://portal.azure.com/](https://portal.azure.com/) and *sign in* with your user account and credentials from Tenant B.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-35.png)\n\n2. After signing in, *create* a **Storage Account** in the Azure Portal.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-36.png)\n\n3. *Open* your Storage Account, *create* a **container**, and ensure that it contains data.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-37.png)\n\n4. In the Azure Portal, *search* for **App registrations**, then *click* **New registration** to create a new app.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-38.png)\n\n5. After registering your app, you’ll see the **Application (client) ID** and **Directory (tenant) ID** on the Overview page. *Copy* these values, as you’ll need them for authentication later.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-39-1024x323.png)\n\n6. In the left menu, go to **Certificates & secrets**, then *click* **New client secret** to add a client secret.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-40.png)\n\n7. *Copy* the **Value** immediately to be used for authentication later— it won’t be shown again.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-41-1024x193.png)\n\n8. In the Azure Portal, go to your Azure Data Lake Gen account. *Click* **Access Control (IAM)**, then *select* **Add role assignment.**\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-42.png)\n\n*9. Choose* a **Role**, for example, **Storage Blob Data Contributor** (read/write)\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-43.png)\n\n10. In **Members**, *select* **User, group, or service principal**, then *choose* the app you just registered.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-44.png)\n\n11. *Click* **Assign**. You now have access to this storage account via the service principal.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-45-1024x889.png)\n\nYou can learn more in [Register a Microsoft Entra app and create a service principal – Microsoft identity platform | Microsoft Learn](https://learn.microsoft.com/en-us/entra/identity-platform/howto-create-service-principal-portal?utm_source=chatgpt.com) & [Access storage using a service principal & Microsoft Entra ID(Azure Active Directory) – Azure Databricks | Microsoft Learn](https://learn.microsoft.com/en-us/azure/databricks/connect/storage/aad-storage-service-principal?utm_source=chatgpt.com)\n\n## **Get started to copy data across tenants using Copy job**\n\n*1. Go to* [Sign in | Microsoft Power BI](https://app.powerbi.com/singleSignOn?ru=https%3A%2F%2Fapp.powerbi.com%2Fhome%3FnoSignUpCheck%3D1) and *log in* to **Fabric** using your user account and credentials from **Tenant A**.\n\n2. *Go to* your workspace and *create* a **Copy job** to move your data.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-56-1024x463.png)\n\n3. After naming the Copy job, *select* **Azure Data Lake Gen2** as the source from which to copy data.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-47.png)\n\n4. *Enter* the URL of your Azure Data Lake Gen2 account, and *select* **service principal** as the authentication type.\n\n*Provide* the **Tenant ID**, **Client ID**, and **Client Secret** that you got from the prerequisite steps, then *click* **Next**.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-48-1024x607.png)\n\n5. After connecting to your source store, you should be able to see your container and its files. *Select* the files you want to copy from your ADLS Gen2 account.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-49.png)\n\n6. *Select* your destination store for the data. In this case, *choose* **Fabric Data Warehouse**.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-50.png)\n\n7. [Optional] *Configure* table mapping or column mapping if you need.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-51.png)\n\n8. You can *choose* either **full copy** or **incremental copy**. If you *select* incremental copy, Copy job will first perform a full copy of all files, and then subsequently copy only the new files from the Azure Data Lake Gen account.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-52.png)\n\n9. *Review* the job summary and *save + run* it.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-53.png)\n\n10. In the job panel, you’ll see that your Copy job has successfully completed the initial full snapshot transfer to the destination.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-54.png)\n\n11. *Open*  your Fabric Data Warehouse, and you will see that the data from the Azure Data Lake Gen account in **Tenant B** has been successfully loaded into the Fabric Data Warehouse.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/image-55.png)\n\n## **Summary**\n\nYou can easily use Copy job to copy data across different tenants. In this scenario, A user from Tenant A can use Copy job to successfully copy data from the Azure Data Lake Gen2 account with service principal authentication (owned by Tenant B) into Fabric Data Warehouse (owned by Tenant A).\n\n## **Additional Resources**\n\nTo learn more, explore [Microsoft Fabric Copy job](https://learn.microsoft.com/fabric/data-factory/what-is-copy-job) documentation.\n\nSubmit your feedback on [Fabric Ideas](https://community.fabric.microsoft.com/t5/Fabric-Ideas/idb-p/fbc_ideas/label-name/data%20factory%20%7C%20copy%20job) and join the conversation in [the Fabric Community](https://community.fabric.microsoft.com/t5/Copy-job/bd-p/db_copyjob).\n\nTo get into the technical details, check out the [Fabric documentation](https://aka.ms/FabricBlog/docs).\n\nIf you have a question or want to share your feedback, please leave us a comment.",
  "Author": "Microsoft Fabric Blog",
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "Title": "Simplifying Data Ingestion with Copy job – Copy data across tenants using Copy job in Fabric Data Factory",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "ProcessedDate": "2025-10-20 19:03:50"
}
