{
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/fabric-october-2025feature-summary/",
  "OutputDir": "_news",
  "Author": "Microsoft Fabric Blog",
  "Title": "Fabric October 2025Feature Summary",
  "FeedName": "Microsoft Fabric Blog",
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "EnhancedContent": "This month’s update delivers key advancements across Microsoft Fabric, including enhanced security with Outbound Access Protection and Workspace-Level Private Link, smarter data engineering features like Adaptive Target File Size, and new integrations such as Data Agent in Lakehouse. Together, these improvements streamline workflows and strengthen data governance for users.\n\n# Contents\n\n- Events & Announcements\n- Fabric Data Days starts next week!\n- FabCon is back from March 16-20, 2026, in Atlanta, GA\n- Fabric Platform\n- Keyboard shortcut support for horizontal tabs and object explorer\n- Use Focus Mode to maximize editing space in Fabric\n- Outbound Access Protection for Spark (Generally Available)\n- Workspace-Level Private Link (Generally Available)\n- OneLake\n- ReadWrite access in OneLake security\n- Data Engineering\n- Introducing Adaptive Target File Size\n- Fabric Connection with Notebook\n- Spark Connector for SQL databases (Preview)\n- Spark Executor Rolling Logs: Easier Access for Large and Long Jobs\n- Table Deep Links in Lakehouse Explorer\n- Data Agent integration in Lakehouse\n- Data Science\n- New authoring experience for Data Agent Creators\n- Introducing the Markdown Editor for Data Agent Creators\n- Data Warehouse\n- Reading and ingesting JSONL files\n- Data source in OPENROWSET function\n- Improving Concurrency in Fabric Data Warehouse – Compaction Preemption\n- Real-time Intelligence\n- Native Graph for Connected Data Insights that Power Intelligent Agents\n- New Eventstream Source: MongoDB CDC\n- Eventstream supports sourcing events with schema from an EventHub Source (Preview)\n- Eventstream Supports Pause/Resume for Derived Stream\n- Data Factory\n- More file formats support in Copy job, including ORC, Excel, Avro, and XML\n- Copy job CSV format now supports quote characters, escape characters, and encoding options\n- Improved experience for Variable libraries in Dataflow Gen2\n- Export Query Results (Preview)\n\n## Events & Announcements\n\n### Fabric Data Days starts next week!\n\nSupercharge your career with 50+ days of data & AI. Join us for 2 months of learning, contests, live sessions, discount exam vouchers and community connection.\n\nDesigned for data professionals at every level, you’ll sharpen your real-world skills, gain a competitive edge in today’s market, and connect with a global network of peers and experts. **It all kicks off on November 4th.**\n\n[Register now](https://aka.ms/fabricdatadays)\n\n[Sign up for live sessions](https://aka.ms/fabricdatadays/live)\n\n### FabCon is back from March 16-20, 2026, in Atlanta, GA\n\nJoin us for the ultimate Power BI, Microsoft Fabric SQL, Real-Time Intelligence, AI, and Databases community-led event. The third annual FabCon Americas will feature sessions from your favorite Microsoft and community speakers, keynotes, more opportunities to Ask the Experts for 1:1 support, an engaging community lounge with opportunities to network and connect with your peers, a dedicated partner pre-day, a packed expo hall, Power Hour, Data Viz World Championship, and a can’t-miss attendee party at the Georgia Aquarium.\n\n[Register](https://aka.ms/fabcon) with code FABCOMM to save $200.\n\n##\n\n## Fabric Platform\n\n### Keyboard shortcut support for horizontal tabs and object explorer\n\nWe’re enhancing the developer experience by adding keyboard shortcut support to the new multitasking experience with Horizontal Tabbing and Object Explorer. This allows developers to switch tabs and items efficiently using keyboard shortcuts, like professional IDEs.\n\nTo learn more, refer to the [Keyboard shortcuts for Object Explorer and Horizontal Tabs](https://learn.microsoft.com/fabric/fundamentals/fabric-home#keyboard-shortcuts-for-object-explorer-and-horizontal-tabs) documentation.\n\n### Use Focus Mode to maximize editing space in Fabric\n\nWhen developers are editing an item, they often need to maximize the available screen space for the editor pane. Focus Mode allows you to hide navigation sidebar and the object explorer, giving you a distraction-free surface to concentrate on editing.\n\nSelect Focus Mode from the global header. The navigation sidebar and the object explorer (if you have it pinned) collapse, leaving only the editor pane maximized.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m.png)\n\nRefer to the [Maximize editing space with focus mode](https://learn.microsoft.com/fabric/fundamentals/fabric-home#maximize-editing-space-with-focus-mode) documentation to learn more.\n\n### Outbound Access Protection for Spark (Generally Available)\n\nWith Outbound Access Protection for Spark, organizations can govern outbound connections from their Spark workloads in Fabric workspaces to external destinations and other Fabric workspaces within the same tenant providing organizations with granular controls to protect against data exfiltration. We are actively working to expand OAP support for additional experiences and plan to add support for Data Factory artifacts soon.\n\n![A diagram of a company's company's company's company's company's company's company's company's company's company's company's company' AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-diagram-of-a-companys-companys-companys-compa.png)\n\nRefer to the [Workspace outbound access protection overview](https://learn.microsoft.com/fabric/security/workspace-outbound-access-protection-overview) documentation to learn more.\n\n### Workspace-Level Private Link (Generally Available)\n\n[Private link for Fabric workspaces](https://learn.microsoft.com/fabric/security/security-workspace-level-private-links-overview) empowers organizations to secure their Fabric workspaces with fine-grained network isolation, allowing private, secure access from their virtual networks — without traversing the public internet.\n\n![A diagram of a fabric test AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-diagram-of-a-fabric-test-ai-generated-content-m.png)\n\nRefer to the [Workspace outbound access protection](https://learn.microsoft.com/fabric/security/workspace-outbound-access-protection-overview) documentation to learn more.\n\n## OneLake\n\n### ReadWrite access in OneLake security\n\nOneLake security now supports ReadWrite access controls, giving data owners the ability to define precise permissions on how users can write data in lakehouses. This enhancement allows teams to assign ReadWrite access to workspace Viewers or users with only Read access. This allows users to write data to tables and folders without having elevated permissions in the workspace to create and manage Fabric items. It’s a critical step toward enabling secure, collaborative workflows without compromising governance.\n\nWith ReadWrite access, all OneLake write operations can be performed through Spark notebooks, the OneLake File Explorer, or OneLake APIs. This allows teams the flexibility of ensuring the principle of least privilege is followed while also enabling key workflows involving uploading pdfs or excel files for further analysis.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-1.png)\n\nTo learn more about how ReadWrite access works in OneLake security, check out [OneLake security access control model (preview)](https://aka.ms/OneLakeSecurityControlModel) documentation.\n\n## Data Engineering\n\n### Introducing Adaptive Target File Size\n\nData teams have long struggled with the File Size Dilemma: the ideal file size for a 10GB table can be disastrous when used for a 10TB table. The new Adaptive Target File Size feature uses Delta table telemetry to estimate the ideal target file size based on heuristics like the table’s current size, and then automatically updates that target as the table grows over time.\n\nYou enable it once: SET spark.microsoft.delta.targetFileSize.adaptive.enabled = True\n\nand the Fabric Spark Runtime handles the rest. For instance, a table that starts at 1GB might get 128MB target files, but as it grows to 10TB over the next five years, the target will automatically and incrementally adapt to be 1GB.\n\n- **Enhanced Data Skipping:** Smaller, right-sized files in smaller tables enable up to 8x more possible file skipping during query execution.\n- **Reduced Update Costs:** For operations like and , smaller target files mean the operations touch and rewrite less data, drastically reducing write amplification.\n- **Optimized Parallelism:** The system ensures your files are not too big (limiting parallelism) or too small (overwhelming the scheduler), maximizing both read and write throughput.\n\nThe performance impact is substantial: a customer benchmark saw a 30% reduction in overall ELT cycle time, and during TPC-DS testing, the compaction phase ran nearly 1.6x faster, leading to the query phase running 1.2x faster.\n\n![A screenshot of a graph AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-graph-ai-generated-content-may.png)\n\nLearn more about this feature in our [Adaptive target file size](https://learn.microsoft.com/fabric/data-engineering/tune-file-size?tabs=sparksql#adaptive-target-file-size) documentation and [Adaptive Target File Size Management in Fabric Spark](https://blog.fabric.microsoft.com/blog/adaptive-target-file-size-management-in-fabric-spark?ft=All) blog post.\n\nFast Optimize & Auto Compaction: End Write Amplification and Automate Maintenance\n\nCompaction is necessary, but the hidden costs of traditional maintenance—like write amplification, constant manual intervention, and performance degradation between maintenance windows—have been a major pain point. We’re tackling these costs with two complementary features.\n\nFast Optimize intelligently analyzes file bins and short-circuits compaction operations that aren’t estimated to provide a meaningful performance improvement.\n\n- **The Check:** It evaluates whether the compaction job is estimated to produce files that meet the minimum target size *and* if there are enough small files to justify the work. If not, the operation is skipped, or its scope is reduced.\n- **The Benefit:** This dramatically improves the idempotency of your jobs, meaning you can run them more frequently without wasting compute resources. In a study, Fast Optimize reduced the time spent doing compaction by 80% by over 200 ELT cycles by simply avoiding the long-term impact of write amplification.\n\n![A graph with a rainbow colored arrow AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-graph-with-a-rainbow-colored-arrow-ai-generated.png)\n\nEnable it with: spark.conf.set(‘spark.microsoft.delta.optimize.fast.enabled’, True)\n\n**Auto Compaction:** Fix Small Files Before They Hurt\n\nWe’ve revamped the OSS Delta implementation of Auto Compaction with critical bug fixes, making it the recommended hands-off approach for table maintenance.\n\n- **Smart Triggering:** It monitors your table’s file distribution as part of every write operation and automatically triggers compaction when small file accumulation crosses a smart threshold.\n- **Why it Matters:** Instead of suffering the ‘sawtooth pattern’ of performance drops between scheduled jobs, your tables maintain optimal performance automatically. This has a direct cost benefit: one customer saw a near ~40% reduction in Spark Capacity Unit (CU) consumption by simply enabling this feature, stabilizing their production environment. In a study measuring the impact of Auto Compaction when running typical ELT pipelines, it resulted in 5x faster performance after 200 iterations:\n\n![A graph of a graph AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-graph-of-a-graph-ai-generated-content-may-be-in.png) Auto Compaction handles the synchronous, immediate small file problem, while Fast Optimize makes your scheduled command efficient and idempotent. Together, these features bring you closer to a truly maintenance-free Lakehouse.\n\nLearn more about these features in the [Compacting Delta tables](https://learn.microsoft.com/fabric/data-engineering/table-compaction?tabs=sparksql) documentation and [Introducing Optimized Compaction in Fabric Spark](https://microsoft.sharepoint.com/sites/AzureDataContentStrategy/Shared%20Documents/Monthly%20Update%20Blogs/Monthly%20Links%20and%20Folders/Fabric_October2025/Fabric%20October%202025%20Feature%20Summary%20-%20Review.docx) blog post.\n\n### Fabric Connection with Notebook\n\nThe integration with Fabric Connection enables consistent and smooth access to external data sources directly from notebooks with Fabric Connection. It allows users to reuse existing connections and credentials, simplifying the process of working with diverse data sources.\n\nAfter the Fabric Connection is set up and bound to the notebook, users can generate code snippets to access the data source directly from within the notebook. These snippets include detailed steps for retrieving credentials from the connection and using the appropriate Python library to access the data source with those credentials.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-2.png)\n\nTo use a fabric connection in notebooks, you need to explicitly enable the connection to be used in notebooks from the Fabric data source management page. There’s a specific toggle to enable the connection to be used in notebooks, named ‘Allow this connection to be used in Code-First Artifact’. This toggle can only be set during the creation of the connection and can’t be modified later. For the connection, which is created with notebook, this flag is turned on by default.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-3.png)\n\nHere are the supported authentication methods for Fabric Connection in notebooks:\n\n- **Basic Authentication**: Supported for Azure SQL Database and other databases that support basic authentication.\n- **Account Key Authentication:** Supported for REST API data sources that require Account key authentication.\n- **Token Authentication:** Supported for data sources that require token-based authentication.\n- **Workspace Identity Authentication:** Supported for Fabric workspace identity authentication.\n\nService Principal Authentication (SPN) authentication support will be added in a future update.\n\n### Spark Connector for SQL databases (Preview)\n\nFabric Spark connector for SQL databases (Azure SQL databases, Azure SQL Managed Instances, Fabric SQL databases and SQL Server in Azure VM) in the Fabric Spark runtime is now available. This connector enables Spark developers and data scientists to access and work with data from SQL database engines using a simplified Spark API. The connector will be included as a default library within the Fabric Runtime, eliminating the need for separate installation.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m.gif)\n\n**Operation Support**\n\nThe Spark connector for Microsoft Fabric Spark Runtime is a high-performance library that enables you to read from and write to SQL databases. The connector offers the following capabilities:\n\n- Use Spark to perform large write and read operations on the following SQL products: Azure SQL Database, Azure SQL Managed Instance, SQL Server on Azure VM, Fabric SQL databases.\n- The connector comes preinstalled within the Fabric runtime, which eliminates the need for separate installations.\n- While you’re accessing a table or a view, the connector upholds security models defined at the SQL engine level. These models include object-level security (OLS), row-level security (RLS), and column-level security (CLS).\n\n**Language Support**\n\nWe are also introducing PySpark support for this connector, in addition to Scala. This means that you no longer need to use a workaround to utilize this connector in PySpark, as it is now available as a native capability.\n\n**Authentication Support**\n\nThis connector supports multiple authentication methods which you can choose based on your need and setup.\n\nTo learn more about the Spark connector for SQL databases, please refer to the [Spark connector for SQL databases](https://learn.microsoft.com/fabric/data-engineering/spark-sql-connector) documentation.\n\n### Spark Executor Rolling Logs: Easier Access for Large and Long Jobs\n\nAs Spark applications continue to grow in scale and duration, efficient log management and analysis have become increasingly critical. To address these evolving needs, we’ve introduced enhancements to the Spark History Server (for completed applications) and Spark UI (for running applications), enabling executor rolling logs for Spark 3.4 and above.\n\nWith this enhancement, when an executor log exceeds 16MB or the Spark job runs for more than one hour, the system automatically splits logs into hourly segments. This makes it easier to navigate, view, and download logs without dealing with extremely large files.\n\n- View logs by hour to quickly pinpoint specific execution windows.\n- Access the latest active logs while the job is still running.\n- Download individual hourly logs or all logs together as needed.\n\nThis feature empowers users to locate and analyze logs from a particular timepoint with ease, while avoiding the hassle of downloading or opening a massive single log file.\n\nExample: Executor Rolling Logs view.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-4.png)\n\nTo learn more, refer to the [Use extended Apache Spark history server to debug and diagnose Apache Spark applications](https://learn.microsoft.com/fabric/data-engineering/apache-spark-history-server) documentation.\n\n### Table Deep Links in Lakehouse Explorer\n\nWhen opening a lakehouse in Lakehouse Explorer, users see a preview of the first table by default. Now, users can generate a unique URL for any table in the lakehouse, allowing them to preview that specific table directly. By copying and sharing this URL with others who have access, recipients can open Lakehouse Explorer with the chosen table already previewed.\n\nSimply click on the ‘…’ beside the table and select ‘Copy URL’. You can use this link to open the lakehouse in Lakehouse Explorer, where you’ll see a preview of the chosen table.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-5.png)\n\nTo learn more, refer to the [Navigate the Fabric Lakehouse explorer](https://learn.microsoft.com/fabric/data-engineering/navigate-lakehouse-explorer) documentation.\n\n### Data Agent integration in Lakehouse\n\nWe’ve created a new integration for Data Agent directly within Lakehouse experiences. This enhancement is designed to streamline and accelerate your journey from raw data to actionable intelligence just with a few clicks.\n\nHow it works:\n\n1. Start in your Lakehouse.\n2. Select the ‘Add to data agent’ button in the ribbon.\n3. Select create a new Data Agent (auto-linked to your data source) or select an existing one.\n4. You’re taken directly to your Data Agent, with your data source ready to query.\n5. Use multitasking navigation to switch between Data Agent and your data source.\n6. Success, errors, and status updates appear in the notification hub.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-6.png)\n\nTry out Data Agent with your Lakehouse data now for quick, actionable, and intuitive AI-driven insights.\n\n## Data Science\n\n### New authoring experience for Data Agent Creators\n\nThe new UI introduces two dedicated tabs—Data and Setup—making it easier than ever to design, test, and refine your agent. Under the Setup tab, you can see and apply all the relevant settings for your data sources, while seamlessly switching back to the Data tab to chat with your agent and test how your configurations affect its behavior—without losing context.\n\nThis side-by-side view lets you explore your schemas while authoring instructions and example queries, improving speed and accuracy. And because all configurations are integrated with CI/CD and Git, tracking changes over time is effortless. This update gives creators the flexibility to multi-task and iterates quickly, setting a new standard for building powerful, tailored Data Agents.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-7.png)\n\nTo learn more about how to configure your Data Agent, refer to the [Data agent configurations](https://learn.microsoft.com/fabric/data-science/data-agent-configurations)documentation.\n\n## Introducing the Markdown Editor for Data Agent Creators\n\nCreators can now write their agent and data source instructions directly in Markdown—a clean, structured, and highly readable format that’s perfect for organizing context. Markdown makes it simple to format instructions with headers, lists, and code blocks, helping you clearly express complex logic or multi-step processes.\n\nIt’s also a natural way to pass context to the Data Agent—LLMs are optimized to understand Markdown, which means your instructions are more likely to be interpreted accurately and consistently. This update not only improves the quality of responses but also enhances your authoring workflow, making it easier to iterate and maintain well-structured instructions over time.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-8.png)\n\nTo learn more about how to author agent and data source instructions, refer to the [Best practices for configuring your data agent](https://learn.microsoft.com/fabric/data-science/data-agent-configuration-best-practices) documentation.\n\n## Data Warehouse\n\n### Reading and ingesting JSONL files\n\nData Warehouse enables you to efficiently read and ingest JSONL (JSON Lines) files by leveraging the OPENROWSET function. With the OPENROWSET function, you can reference one or many JSONL files and read their content as a set of rows.\n\nSELECT \\*\n\nFROM OPENROWSET(BULK ‘/Files/jsonl/year=\\*/month=\\*/tweets-\\*.jsonl’ )\n\nEach property found in the JSON documents is returned as a column, while each value becomes a cell in the resulting dataset. This makes it straightforward to transform semi-structured JSONL data into a tabular format for analysis or further processing.\n\nIn addition to reading flat JSONL files, the OPENROWSET function also enables you to read complex JSON structures that contain nested JSON sub-objects. By using the ‘WITH clause, you can explicitly define all output columns and their data types, mapping each column to the corresponding JSON path within the source files.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-9.png)\n\nThe ‘WITH’ clause allows you to easily extract and transform deeply nested data into a tabular format, making it accessible for further analysis and reporting.\n\nBeyond just reading data, the OPENROWSET function also supports data ingestion. You can ingest the rows returned by OPENROWSET directly into existing tables using the INSERT statement. Alternatively, a new table can be created and populated from the results of the OPENROWSET function used in the CREATE TABLE AS SELECT (CTAS) statement. This streamlined approach simplifies the process of bringing external JSONL data into your data warehouse environment, making it easy to work with large volumes of semi-structured data using T-SQL syntax.\n\nLearn more about the reading and ingesting JSONL files in the [Query and ingest JSONL files in Data Warehouse and SQL Analytics Endpoint for Lakehouse (Generally Available)](https://blog.fabric.microsoft.com/blog/query-and-ingest-jsonl-files-in-data-warehouse-and-sql-endpoint-for-lakehouse-general-availability?ft=All) blog post.\n\n### Data source in OPENROWSET function\n\nThe data\\_source option in the OPENROWSET function allows you to simplify data access to files in the lake by using a relative file path and referencing a data source that points to a root folder location. You can create an external data source with a location that refers to the location of the lakehouse folder:\n\nCREATE EXTERNAL DATA SOURCE MyLakehouse WITH ( LOCATION = ‘https://onelake.dfs.fabric.microsoft.com/{wsid}/{lhid}’ );\n\nAfter creating an external data source and set its location to the root lakehouse URI, you can reference this data source by name in your OPENROWSET queries.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-10.png)\n\nThe data source option enables you to use relative file paths, making your queries cleaner and easier to maintain.\n\nAdditionally, when running OPENROWSET queries on SQL endpoints for Lakehouse, if you don’t specify the data source option, the SQL endpoint automatically defaults to the lakehouse root location. This means you can use shorter relative paths in your queries to access lakehouse files, streamlining your workflow and reducing the need for fully qualified paths.\n\nFind more information about the DATA\\_SOURCE in OPENROWSET in the [Simplifying file access in OPENROWSET using data sources and relative paths (Preview)](https://blog.fabric.microsoft.com/blog/simplifying-file-access-in-openrowset-using-data-sources-and-relative-paths-preview?ft=All) blog post.\n\n### Improving Concurrency in Fabric Data Warehouse – Compaction Preemption\n\nAs part of our ongoing investments to enhance concurrency and reliability in Fabric Data Warehouse, we’re introducing Compaction Preemption, a smart new capability that intelligently prevents write-write conflicts caused by background compaction tasks. These tasks optimize storage by rewriting small, inefficient parquet files into fewer, larger files, but can interfere with user operations like UPDATE, DELETE or MERGE.\n\nWith this enhancement, the system detects active user workloads and preempts compaction tasks before they commit, reducing the likelihood of query failures during concurrent operations. This ensures smoother experiences for workloads with frequent updates and deletes, especially in high-throughput environments.\n\nWe will continue to evolve our conflict avoidance strategies for Compaction Preemption. Explore the latest blog post to gain insights into how compaction preemption works in Microsoft Fabric Data Warehouse, along with best practices to avoid conflicts [Resolving Write Conflicts in Microsoft Fabric Data Warehouse](https://blog.fabric.microsoft.com/blog/concurrency-control-and-conflict-resolution-in-microsoft-fabric-data-warehouse/).\n\n## Real-time Intelligence\n\n### Native Graph for Connected Data Insights that Power Intelligent Agents\n\nFabric Graph is Microsoft’s first horizontally scalable, native graph data management, analytics, and visualization service. Inspired by LinkedIn’s graph engine that efficiently maps over a billion interconnected entities, Fabric Graph helps organizations uncover and act on insights hidden in their most complex, interwoven data. It is a foundation for the next generation of data-rich, reasoning-capable agents.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-11.png)\n\nTo learn more about Explore capabilities, use cases, and getting-started guides, refer to the [Graph in Microsoft Fabric (preview) documentation](https://aka.ms/graphdocs).\n\n### New Eventstream Source: MongoDB CDC\n\nFabric Eventstream now supports MongoDB Change Data Capture (CDC) as a fully managed connector, expanding its rich set of streaming data sources. With this connector, you can stream CDC events from MongoDB to Eventstream for real-time processing and analytics.\n\nHighlights of MongoDB CDC Connector\n\n- Compatible with all MongoDB deployments, including MongoDB Atlas, cloud-hosted, and on-premises.\n- Captures real-time database changes and streams them into Eventstream for immediate processing.\n- Built on Debezium, a widely adopted open-source CDC framework with strong community support, ensuring flexibility and reliability\n\nYou’ll find this new connector in the Real-Time Hub.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-12.png)\n\nThe following example demonstrates how Eventstream using MongoDB Atlas as the source, processing CDC events in real time, and routing them to an Eventhouse destination.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-13.png)\n\nTo learn more, and guidance getting started, refer to the [Add MongoDB CDC source to an eventstream](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/add-source-mongodb-change-data-capture) documentation.\n\n### Eventstream supports sourcing events with schema from an EventHub Source (Preview)\n\nFabric Eventstream now supports sourcing events from an EventHub source while enforcing schema on the payloads. This integration enables data engineers and developers to build governed, type-safe event streaming pipelines using header-based schema validation. This integration leverages Fabric’s Schema Registry to validate events at ingestion, eliminating the need for downstream defensive coding and data quality checks.\n\n- Use Schema Registry to validate events from Azure EventHub across multiple event types flowing through a single EventHub namespace.\n- Support for header-based schema matching, where UTF-8 string headers deterministically map events to registered Avro schemas.\n- While events flow through the Eventstream, the connector validates payload structure, data types, and required fields before forwarding to downstream destinations like Eventhouse.\n\n**Header-Based Routing**\n\nThe integration uses custom EventHub properties (headers) to route events to the correct schema for validation. This approach supports multiple patterns:\n\n- **One-to-one mapping:** A single header key-value pair uniquely identifies one schema.\n- **Multi-schema support:** When multiple types of events flow through a single EventHub, each validated against its registered schema.\n- **Validation at the gate:** Malformed events are rejected at ingestion and logged in Fabric Diagnostics, preventing data quality issues downstream.\n\nTo set up header-based routing, enable schema-mode when adding your EventHub as a source, select ‘Dynamic schema via headers’, and provide unique header-value pairs to enable Fabric Eventstreams to automatically select and apply schemas to the incoming events.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-14.png)\n\nTo learn more about sourcing schema-driven events from EventHub, please refer to the [Add Azure Event Hubs source to an eventstream](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/add-source-azure-event-hubs?branch=pr-en-us-10586&amp;pivots=enhanced-capabilities) documentation.\n\n### Eventstream Supports Pause/Resume for Derived Stream\n\nEventstream customers can now pause/resume their traffic for their [derived stream](https://learn.microsoft.com/fabric/real-time-intelligence/event-streams/add-destination-derived-stream). This capability gives customers greater operational control and flexibility over their event-driven workflows.\n\n- **Cost Optimization:** Pause traffic during maintenance windows or low-demand periods without deleting or recreating streams.\n- **Operational Continuity:** Resume traffic instantly when ready, reducing downtime and avoiding complex reconfigurations.\n- **Resource Efficiency:** Dynamically manage throughput based on business needs, improving overall system efficiencies.\n\nWith pause/resume, customers can maintain agility, reliability, and cost-effectiveness in managing real-time pipelines.\n\n## Data Factory\n\n### More file formats support in Copy job, including ORC, Excel, Avro, and XML\n\nCopy job supports even more file formats—including ORC, Excel, Avro, and XML—so you can move data from any source to any destination with greater flexibility. Whether you’re working with structured tables or semi-structured files, Copy job lets you seamlessly ingest, and distribute your data in the format that best fits your business and analytics needs.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-15.png)\n\nFor more information about Copy job, refer to the [What is Copy job in Data Factory](https://learn.microsoft.com/fabric/data-factory/what-is-copy-job) documentation.\n\n### Copy job CSV format now supports quote characters, escape characters, and encoding options\n\nIt is now possible to set the Quote character, Escape character, and encoding options when transferring data in CSV format through a Copy job.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-16.png)\n\nWith these options, you can:\n\n- Precisely control how text values and delimiters are handled, ensuring accurate data parsing.\n- Escape quotes or delimiters within text fields to avoid broken records.\n- Choose from a wide range of encoding formats—from UTF-8 and UTF-16 to regional encodings like BIG5, GB18030, and SHIFT-JIS—to ensure compatibility across global data sources and destinations.\n\nThis enhancement gives you the flexibility to handle any CSV structure, maintain data fidelity, and enable seamless cross-system integration.\n\nFor more information about Copy job, refer to the [What is Copy job in Data Factory](https://learn.microsoft.com/fabric/data-factory/what-is-copy-job) documentation.\n\n### Improved experience for Variable libraries in Dataflow Gen2\n\nIn September we unveiled the new integration between [Variables libraries and Dataflow Gen2](https://learn.microsoft.com/fabric/data-factory/dataflow-gen2-variable-library-integration). Improvements have since been made to this experience.\n\n- **Gateway support**: evaluations running through a Gateway can now leverage variable libraries during run time. The minimum required gateway version is 3000.282**.**\n- **Power Query editor support**: when creating a solution using the Dataflow editor, your variables will be able to get evaluated in the Power Query editor.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/a-screenshot-of-a-computer-ai-generated-content-m-17.png)\n\nWe’ve heard your feedback about how you wish to get more guidance on how to better take advantage of this integration and to that end we’ve created a [full solution architecture guide](https://learn.microsoft.com/fabric/data-factory/dataflow-gen2-cicd-alm-solution-architecture) and a thorough tutorial on how you could implement your own solution using Variable libraries in Dataflow Gen2.\n\nFeel free to share your comments, inquiries, feedback and suggestions about this experience through our [Data Factory community forum](https://community.fabric.microsoft.com/t5/Dataflow/bd-p/df_dataflows).\n\nCheck out the tutorial on using Fabric variable libraries with Dataflow Gen2: [Variable references in Dataflow](https://learn.microsoft.com/fabric/data-factory/dataflow-gen2-variable-references).\n\n### Export Query Results (Preview)\n\n**Export Query Results in Power BI Desktop unlocks seamless data portability and collaboration across Microsoft Fabric.** This feature empowers analysts to take transformed, cleaned data from Power Query and export it directly to trusted destinations – such as Dataflows Gen2, Lakehouses, and other cloud stores – without relying on complex workarounds or third-party tools. By meeting users where they work (Power BI Desktop) it reduces duplication of effort, accelerates data sharing, and drives interoperability across Fabric workloads.\n\nTo enable this feature, select it in the Preview feature settings in Power BI Desktop.\n\nUsers can select export query results from the PQ ribbon.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/ExportQUery.png)\n\nSelect an online destination.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/EQ2.png)\n\nAnd export their queries to this destination, where they can choose to modify the Dataflow. ![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/10/EQ3.png)\n\nThe result is improved data gravity in OneLake, streamlined refresh and monitoring capabilities, and a minimal-click experience that supports both new and existing destinations. Ultimately, this feature bridges the gap between desktop analytics and enterprise-scale data integration, enabling organizations to maximize the value of their data investments.\n\nTo learn more about how to use this feature, refer to the [Export Query Results in Power BI Desktop](https://go.microsoft.com/fwlink/?linkid=2329827) documentation.",
  "Description": "This month’s update delivers key advancements across Microsoft Fabric, including enhanced security with Outbound Access Protection and Workspace-Level Private Link, smarter data engineering features like Adaptive Target File Size, and new integrations such as Data Agent in Lakehouse. Together, these improvements streamline workflows and strengthen data governance for users. Contents Events & Announcements Fabric Data …\n\n[Continue reading “Fabric October 2025Feature Summary”](https://blog.fabric.microsoft.com/en-us/blog/fabric-october-2025feature-summary/)",
  "ProcessedDate": "2025-10-29 21:02:57",
  "Tags": [],
  "PubDate": "2025-10-29T09:00:00+00:00",
  "FeedLevelAuthor": "Microsoft Fabric Blog"
}
