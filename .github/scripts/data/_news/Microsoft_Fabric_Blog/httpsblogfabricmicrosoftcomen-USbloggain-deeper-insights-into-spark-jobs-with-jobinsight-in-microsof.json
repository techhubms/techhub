{
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/gain-deeper-insights-into-spark-jobs-with-jobinsight-in-microsoft-fabric/",
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "ProcessedDate": "2025-08-27 18:16:03",
  "Description": "JobInsight is a powerful Java-based diagnostic library designed to help developers and data engineers analyze completed Spark applications in Microsoft Fabric. With JobInsight, you can programmatically access Spark execution metrics and logs—all from within a Fabric Notebook. Whether you’re investigating performance bottlenecks, debugging task execution, or conducting post-run analysis across jobs, stages, or executors, JobInsight …\n\n[Continue reading “Gain Deeper Insights into Spark Jobs with JobInsight in Microsoft Fabric”](https://blog.fabric.microsoft.com/en-us/blog/gain-deeper-insights-into-spark-jobs-with-jobinsight-in-microsoft-fabric/)",
  "Author": "Microsoft Fabric Blog",
  "EnhancedContent": "JobInsight is a powerful Java-based diagnostic library designed to help developers and data engineers analyze completed Spark applications in Microsoft Fabric. With JobInsight, you can programmatically access Spark execution metrics and logs—all from within a Fabric Notebook.\n\nWhether you’re investigating performance bottlenecks, debugging task execution, or conducting post-run analysis across jobs, stages, or executors, JobInsight surfaces valuable insights quickly and efficiently.\n\n## What is JobInsight?\n\nJobInsight provides two core capabilities:\n\n- **Interactive Spark Job Analysis**\nOffers structured APIs that return execution data—such as Spark queries, jobs, stages, tasks, and executors—as Spark Datasets for deep-dive analysis.\n- **Spark Event Log Access**\nAllows users to copy Spark event logs to a OneLake or ADLS Gen2 directory for long-term storage or custom offline diagnostics.\n\nWith these tools, you can analyze, debug, and monitor Spark applications—all within your Fabric workspace.\n\n## Key Features and How to Use Them\n\n### Analyze Completed Spark Applications\n\nYou can extract critical execution data with just a few lines of code:\n\n``` import com.microsoft.jobinsight.diagnostic.SparkDiagnostic\n\nval jobInsight = SparkDiagnostic.analyze( workspaceId, artifactId, livyId, jobType, // e.g., \"sessions\" or \"batches\" stateStorePath, // Output path to store analysis results attemptId // Optional; defaults to 1 )\n\nval queries = jobInsight.queries val jobs = jobInsight.jobs val stages = jobInsight.stages val tasks = jobInsight.tasks val executors = jobInsight.executors\n\n```\n\nYou can then apply standard Spark operations to explore trends, detect anomalies, and optimize performance.\n\n### Reuse Past Analyses\n\nNo need to rerun diagnostics on the same job—just reload previously saved results:\n\n``` val jobInsight = SparkDiagnostic.loadJobInsight(stateStorePath)\n\nval queries = jobInsight.queries val jobs = jobInsight.jobs // and so on...\n\n```\n\nThis makes historical analysis and iterative debugging easy and efficient.\n\n### Save Metrics and Logs to a Lakehouse\n\nYou can persist analysis outputs into Lakehouse tables for reporting or integration:\n\n``` val df = jobInsight.queries\n\ndf.write .format(\"delta\") .mode(\"overwrite\") .saveAsTable(\"sparkdiagnostic_lh.Queries\")\n\n```\n\nApply the same logic for other datasets such as jobs, stages, or executors.\n\n## Copy Event Logs to Lakehouse or ADLS Gen2\n\nYou can also copy event logs for deeper inspection or long-term retention:\n\n``` import com.microsoft.jobinsight.diagnostic.LogUtils\n\nval contentLength = LogUtils.copyEventLog( workspaceId, artifactId, livyId, jobType, targetDirectory, asyncMode = true, // Use async mode for best performance attemptId = 1 )\n\n```\n\n#### Example\n\n``` val lakehouseBaseDir = \"abfss://<workspace>@<onelake>/Files/eventlog/0513\" val jobType = \"sessions\"\n\ncopyEventLogs( workspaceId, artifactId, livyId, jobType, attemptId = 1, asyncMode = true, s\"$lakehouseBaseDir/$jobType/async\" )\n\ncopyEventLogs( workspaceId, artifactId, livyId, jobType, attemptId = 1, asyncMode = false, s\"$lakehouseBaseDir/$jobType/sync\" )\n\n```\n\nUse this method when you want to store raw logs outside the UI for further analysis.\n\n## Get Started\n\nJobInsight makes Spark diagnostics in Microsoft Fabric easier, faster, and more powerful. By integrating with familiar Spark APIs and offering deep access to execution logs and metrics, it empowers you to:\n\n- Visualize execution breakdowns.\n- Monitor and tune resource usage.\n- Identify and troubleshoot performance bottlenecks.\n- Reuse and automate reproducible diagnostics.\n\nFor full documentation, check out: [Job insight diagnostics library (Preview)](https://learn.microsoft.com/en-us/fabric/data-engineering/job-insight-library).",
  "PubDate": "2025-08-27T10:20:00+00:00",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "Title": "Gain Deeper Insights into Spark Jobs with JobInsight in Microsoft Fabric",
  "FeedName": "Microsoft Fabric Blog",
  "OutputDir": "_news",
  "Tags": []
}
