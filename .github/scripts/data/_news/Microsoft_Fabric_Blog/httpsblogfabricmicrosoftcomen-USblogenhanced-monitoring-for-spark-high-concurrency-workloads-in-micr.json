{
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "EnhancedContent": "We’ve completed a set of improvements to the monitoring experience for Notebooks running in high concurrency mode, whether triggered manually or as part of a pipeline using the high concurrency execution model. These updates provide deeper visibility into Spark applications, improve observability across multiple Notebooks, and enable more efficient debugging and performance tuning.\n\n## New Enhancements in the Spark Application Monitoring Detail Page\n\nWe’ve introduced several key enhancements to the Spark application detail view to support high concurrency workloads more effectively:\n\n### **Jobs Tab: Detailed Job-Level Insights**\n\nIn the Jobs tab, you can now drill into individual Spark jobs executed under a high concurrency application.\n\n**Key improvements include:**\n\n- **Notebook Context**: For applications running multiple Notebooks, the Notebook name is now shown alongside each job.\n- **Code Snippet View**: Click on the code snippet icon to view and copy the job-related code.\n- **Filtering**: Filter Spark jobs by Notebook to focus on one or more Notebooks within the session.\n\n### **Logs Tab: Notebook-Aware Logging**\n\nTo support easier debugging in high concurrency Spark sessions:\n\n- Notebook ID Prefixing: Each log entry now includes the Notebook ID prefix, making it easier to associate logs with specific Notebooks.\n- Notebook Filtering: Use the filters to view logs by Notebook, allowing more targeted inspection of log output across collaborative or parallel runs.\n\n### **Item Snapshots Tab: Hierarchical Notebook View**\n\nThe Item Snapshots tab introduces a **hierarchical tree view** of all Notebooks participating in a shared high concurrency Spark session:\n\n- Browse All Notebooks: View snapshots of both completed and in-progress Notebook runs within the shared Spark session.\n- Snapshot Details for each Notebook:\n- Code at time of submission.\n- Execution status per cell.\n- Output for each cell.\n- Input Parameters for the Notebook.\n- Pipeline Integration: If the Spark application is part of a pipeline, you’ll also see the related pipeline and Spark activity displayed for easier traceability.\n\n## Start Exploring Today\n\nThese enhancements are designed to help you monitor high concurrency Spark workloads with precision, whether you’re running multiple Notebooks in parallel, debugging shared Spark sessions, or optimizing distributed workloads.\n\nHead over to the Monitoring Hub, open a Spark application running in high concurrency mode, and explore the improved Jobs, Logs, and Item Snapshots tabs—now optimized for multi-Notebook awareness and developer productivity.\n\nAs always, we welcome your feedback to help us keep improving!",
  "FeedName": "Microsoft Fabric Blog",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/enhanced-monitoring-for-spark-high-concurrency-workloads-in-microsoft-fabric/",
  "Description": "We’ve completed a set of improvements to the monitoring experience for Notebooks running in high concurrency mode, whether triggered manually or as part of a pipeline using the high concurrency execution model. These updates provide deeper visibility into Spark applications, improve observability across multiple Notebooks, and enable more efficient debugging and performance tuning. New Enhancements …\n\n[Continue reading “Enhanced Monitoring for Spark High Concurrency Workloads in Microsoft Fabric”](https://blog.fabric.microsoft.com/en-us/blog/enhanced-monitoring-for-spark-high-concurrency-workloads-in-microsoft-fabric/)",
  "Title": "Enhanced Monitoring for Spark High Concurrency Workloads in Microsoft Fabric",
  "PubDate": "2025-08-26T12:04:04+00:00",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "Author": "Microsoft Fabric Blog",
  "Tags": [],
  "OutputDir": "_news",
  "ProcessedDate": "2025-08-26 19:08:48"
}
