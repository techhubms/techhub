{
  "Tags": [],
  "FeedName": "Microsoft Fabric Blog",
  "Author": "Microsoft Fabric Blog",
  "ProcessedDate": "2025-09-16 08:16:55",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "Description": "Microsoft Fabric provides industry leading data integration capabilities that are unique in the market. Fabric is built on OneLake, which provides data integration at the root, with shortcuts and mirroring that enable Zero-ETL approaches to data unification. Fabric Data Factory provides the industry’s largest, most widely adopted data integration capability offered as a single, cohesive …\n\n[Continue reading “Unify your data estate for the era of AI with Fabric Data Factory”](https://blog.fabric.microsoft.com/en-us/blog/unify-your-data-estate-for-the-era-of-ai-with-fabric-data-factory/)",
  "EnhancedContent": "Microsoft Fabric provides industry leading data integration capabilities that are unique in the market. Fabric is built on OneLake, which provides data integration at the root, with shortcuts and mirroring that enable Zero-ETL approaches to data unification. Fabric Data Factory provides the industry’s largest, most widely adopted data integration capability offered as a single, cohesive SaaS led, multi-cloud ready experience. Together, these capabilities enable Fabric customers to break down the silos within data estates to unlock the true value of data no matter where it resides (within Azure and Fabric, across clouds, behind firewalls).\n\nFabric Data Factory is built on tried and tested capability that is unique in the industry and the most widely adopted data integration stack in the industry. Fabric Data Factory is a fusion of standalone experiences that are now brought into a cohesive offering: Azure Data Factory (Pro-grade data integration in Azure) and Power Query (Citizen Data Integration in Power BI, Excel, Dynamics). The metrics below provide some insight into the momentum we are seeing with our data integration capability.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/09/datafactory-momentum.jpg)\n\nWe are always humbled by the scale of Data Factory – with 22 billion orchestration runs per month and 500+ Petabytes of data moved every month. We are also grateful for our large community of Power Query users that have stayed with us and grown to what is the largest self-serve data preparation user community in the world. Data Factory’s unique hybrid architecture enables line of sight to on-prem data sources through our cloud-native experiences via an extremely large footprint of 790,000+ on-premises gateways deployed across our customer base.\n\nAs part of the Microsoft Fabric Community Conference in Vienna, we are eager to bring you the next wave of product innovation that will further customer adoption through new features, substantial updates to pricing and better enterprise-readiness to meet the needs of our largest and most complex deployments.\n\n## Pricing & Performance Updates to Dataflow Gen2\n\nDataflow Gen2 is the most modern, most scalable implementation of Power Query based self-service data preparation. Dataflow Gen2 is built on Fabric OneLake and scaled via Fabric’s compute engines and is Copilot enabled. Together, these capabilities provide for the most broadly approachable high scale self-serve data preparation in the market. However, we also hear you loud and clear in that Dataflow Gen2 is too expensive. We are introducing some meaningful developments with fresh changes on this front.\n\nFirst, we are making a change to the overall pricing model of Dataflow Gen2:\n\n- We are lowering the base rate of Dataflow Gen2 overall. The base rate of Dataflow Gen2 will drop down from 16CU to 12 CU.\n- We are introducing a tiered approach that will make Dataflow Gen2 much more palatable to long running jobs. Jobs that take longer than 10 minutes to complete will see a base rate of 1.5CU for any part of the job that exceeds 10 minutes.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/09/DataflowPricingUpdates.jpg)\n\nSecond, we are introducing dramatic performance improvements to Dataflow Gen2 through a Modern PQ evaluator. And adding the ability to parallelize query runs through partitioning. Together, both these improvements yield dramatic runtime performance that will have an overall positive impact not only on the user experience but will yield further reduction in cost.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/09/dataflowgen2-perf-compare.jpg)\n\nThird, we are further improving the user experience within the design-time experience by adding a ‘Preview only Steps’ feature that will improve feature your design time experience by delivering faster iterations with query editing.\n\nThese pricing and performance enhancements are effective immediately for Dataflow Gen2 (CI/CD) operations. To benefit, upgrade non-CI/CD items using ‘**Save as Dataflow Gen2 (CI/CD)’.**\n\nFor more information on [Dataflow Gen2 pricing for Data Factory in Microsoft Fabric](https://aka.ms/dfg2pricing) and [What is Dataflow Gen](https://aka.ms/FabConEU2025_DataFactory_Transformation)2? refer to the documentation.\n\n## Petabyte scale, cross-cloud data movement with Copy job\n\nData Factory provides connectors to over 170+ data sources and destinations. Pipelines, Copy job and Dataflow Gen2 include data sources and data destinations that are able to move data efficiently and reliably across a multitude of data sources, destinations and clouds. On-Premises Gateway and VNet Gateway enable data access across firewalls and network boundaries.\n\nCopy job in Data Factory enables simple yet powerful data movement at petabyte scale. Copy job is intended to include every business-critical data source and destination across Microsoft and non-Microsoft sources/targets.\n\nThe full set of connectors supported today is illustrated in the following visual example; we’re expanding this area on a weekly basis.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/09/FabConEU2025-DI-copyjob.jpg)\n\nAdditionally, we are announcing the following new capabilities in Copy job:\n\n### Preview\n\n- **Copy job activity** – Enables orchestration of Copy job via pipelines.\n- **Connection Parameterization via variable library for Copy job** – Streamlines CI/CD process for multiple environments without modifying Copy job.\n- **Fabric Lakehouse Change Data Feeds (CDF)** – Enables Fabric Lakehouse change data to be read by Copy job for incremental copy.\n- **Change Data Capture (CDC) can be merged into SQL Database and Snowflake destinations** – Easily merge your change data into SQL Database in Fabric and Snowflake with Copy job.\n- **Copy Assistant in pipeline is now powered by Copy job** – Setting up Copy in pipeline is easier than ever with the updated Copy Assistant powered by Copy job.\n\n### Generally Available\n\n- **VNet Gateway support in Copy job and Copy Activity** – Enable data access across firewalls and network boundaries with Copy job and Copy Activity.\n- **Incremental Copy Reset** – Enables re-set/re-seed of an incremental copy.\n- **Iceberg and JSON file formats support with Copy job** – Copying Iceberg and JSON formats are now supported with Copy job.\n- **Multiple-schedules support with Copy job** – Set multiple-schedules on a single Copy job to optimize your data movement needs.\n- **Database views support in Copy job** – Database views can now be used as the basis for both full and incremental copy.\n- **New enterprise connectors for Copy job and Copy Activity** – Enterprise data sources such as AWS RDS for Oracle, PostgreSQL, Cassendra, Greenplum, HDFS, Informix, Microsoft Access, Presto, Teradata are available for Copy job and pipeline.\n\nTo learn more, refer to the [What is Copy job in Data Factory for Microsoft Fabric?](https://aka.ms/FabConEU2025_DataFactory_DataMovement) documentation.\n\nIn addition to Copy job, Dataflow Gen2 enables new connectors for seamless data movement as part of self-serve data transformation. We are announcing the following new data destinations in Dataflow Gen2.\n\n### Preview\n\n- **Azure Data Lake Storage Gen2:** Write the results of Dataflow into CSV files in ADLS Gen2.\n- **Lakehouse Files:** Write the results of Dataflow into CSV files in Fabric Lakehouse.\n\n### Generally Available\n\n- **SharePoint Files CSV:** Write the results of a Dataflow as a CSV file in a SharePoint Library.\n\n### Coming Soon\n\n- **Snowflake Database**: Write the results of a Dataflow into Snowflake database\n- **SharePoint Files Excel**: Write the results of Dataflow as an Excel file in SharePoint Library.\n\n## Pro-Code & Low-Code Data Orchestration\n\nData Factory provides pipelines for Low-code data orchestration. With its rich library of Activities as well as options for scheduled and triggered execution, pipelines offer highly reliable orchestration to enable a broad range of data engineering scenarios.\n\n### Preview\n\n- **Copy job activity –** Orchestrate Copy jobs via pipeline.\n- **Expression Evaluator –** When writing pipeline expressions, inspect your results in-line during design time.\n- **ADF pipeline Upgrade utility –** Open source Powershell utility to migrate pipelines from ADF to Fabric Data Factory.\n- **Workspace Monitoring for pipelines** – Pipeline execution logs are now available in workspace monitoring providing a real-time observability data store for you to create queries and reports.\n- **Multiple schedules per pipeline –** Automate your pipeline workflows on differing cadences on a given pipeline.\n\n### Generally Available\n\n- **Invoke pipeline activity** – Invoke pipelines across Fabric, Azure Data Factory, Synapse.\n- **Email & Teams Activities –** Send notifications using email and Teams.\n- **Functions Activity –** Execute Fabric functions.\n- **Dataflow activity-** Orchestrate your Dataflows, includes built-in support for Dataflow Gen2 parameters.\n- **Pipeline Variable libraries –** Variable libraries provide global pipeline support for metadata drive pipelines and makes it easy to change values across environments to support your CICD processes.\n- **SPN/Workspace Identity support in activities –** Automate pipelines using SPN or Workspace identity.\n\nData Factory also provides Pro-Code data integration via Managed Airflow. By providing a serverless model for airflow runtimes, Data Factory removes the burden of having to manage and oversee airflow clusters – enabling customers to focus on the core orchestration task at hand.\n\n### Generally Available\n\n- **Airflow SPN UI –** Effortlessly add Airflow SPN connections to your Apache Airflow projects without writing code.\n- **Add Notebook and pipeline execution to your DAG –** Simple, one-click code template that will automatically add the code to your DAGs to call native Fabric components like notebooks and pipelines.\n\n[Learn more](https://aka.ms/FabConEU2025_DataFactory_Orchestration) about all capabilities we are bringing to Orchestration.\n\n## Database Mirroring\n\nMirroring enables simple, zero-copy, zero-ETL based replication of any data into OneLake – no matter the cloud, database, vendor, or the engine serving the data. We are thrilled to announce several new data sources and improvements to existing data sources in our mirroring lineup.\n\n### Preview\n\n- **Google BigQuery –** Mirror your Google BigQuery data alongside other cloud data sources, enabling cross-cloud querying, unified semantic models, and integrated analytics and AI workloads within the Fabric ecosystem.\n- **Oracle** (including Exadata) – Mirror your Oracle data alongside other cloud data sources, enabling cross-cloud querying, unified semantic models, and integrated analytics and AI workloads within the Fabric ecosystem. Oracle Mirroring supports on-prem data as well as data from Oracle Cloud Infrastructure and Oracle Exadata.\n- **Workspace level Private Link –** Use Workspace Private Link to securely access Fabric workspaces (including mirrored database items) with granular network security control\n- **Mirrored Databases available for Data Agent –** Use Data Agent to chat with data in any Mirrored Database for quick insights\n\n### Generally Available\n\n- **Azure SQL Managed Instance –** Mirror your Azure SQL Managed Instance data alongside other cloud data sources, enabling cross-cloud querying, unified semantic models, and integrated analytics and AI workloads within the Fabric ecosystem.\n- **VNet Gateway & On-Prem Gateway for accessing databases behind firewalls –** Mirror data behind a firewall in Snowflake, Azure SQL Database, Azure SQL Managed Instance.\n- **Workspace Identity Authentication for Azure SQL Database –** Use credential-free Workspace Identity authentication to connect to and mirror your Azure SQL Database.\n- **Create Semantic Models in Power BI from Mirrored Database –** Create a semantic model directly from a mirrored database\n\n[Learn more](https://aka.ms/FabConEU2025_DataFactory_Mirroring) about the announcements for Mirroring\n\n## AI-powered Data Integration\n\nData Factory is AI-powered via Copilots. Copilot enables you to author, debug and monitor pipelines and dataflows with minimal effort. Additionally, copilots are available in-line within the experiences to enable targeted scenarios – such as custom columns via NL prompts.\n\nToday, we are announcing some amazing new capabilities that take Data Factory further with AI-led data integration:\n\n### Preview\n\n- **AI-enhanced Modern Get Data experience –** Easily ingest and transform data with natural language as part of the data discovery and connection experience.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/09/word-image-26911-5.png)\n\n### Generally Available\n\n- **Support Natural language to generate custom columns in Dataflow Gen2** -Express custom column definitions using natural language.\n- **Explain Dataflow Gen2 queries and steps:** Derive natural language explanation of any Dataflow query to better understand what a query / step is doing.\n- **Chat with Copilot to create new connections, and connect to existing connection resources.**\n- **Use Copilot-generated summaries to document data pipelines, activities, and Dataflow queries and steps.**\n\n### Coming Soon\n\n- **AI-powered Data Transformation prompts in Dataflow Gen2 –** Soon, you will be able to use natural language prompts to apply transformations like Sentiment Analysis, Summarization, Categorization, and more. It has never been easier to transform & enrich your data with intelligence.\n\nTo learn more about AI-powered Data Integration, refer to [Get started with Copilot in Fabric in the Data Factory](https://aka.ms/FabConEU2025_DataFactory_AI).\n\n## Mission-Critical Data Integration\n\nData Factory is mission critical by design and includes key capability to support the needs of the most demanding enterprise environments. We are thrilled to announce the addition of several new capabilities to provide security, isolation and outbound access protection. The following features are now available to support mission-critical enterprise needs:\n\n### Preview\n\n- **Workspace Identity authentication support for connectors** – Enables secure, seamless connector access using workspace identity, reducing credential sprawl.\n- **Workspace Private Link support for Fabric Data Factory –** Allows using Dataflows Gen2, Pipelines, Copy Job in workspace that are protected by Private Link for inbound access, making data integration more secure.\n- **PowerShell for On-Premises Gateway takeover** – Automates gateway ownership transfer for streamlined administration and disaster recovery.\n- **PowerShell for configuring installation path of On-Premises Data Gateway** – Offers flexibility in gateway deployment by allowing custom installation paths.\n\n### Generally Available\n\n- **Azure Key Vault integration in Connections** – Centralizes secret management for connections by allowing secure credential storage and retrieval via Azure Key Vault.\n- **VNet Gateway support for Data pipeline, Copy job** – Allows secure data movement through virtual networks, ensuring compliance and network isolation.\n- **Connections and Gateways API** – Enables programmatic management of connections and gateways for scalable automation and integration with SPN support.\n- **VNet Gateway shut down and restart controls** – Provides operational control over gateway lifecycle to optimize resource usage and security.\n\n### Coming Soon\n\n- **Snowflake Key-Pair authentication** – Strengthens security and automation for Snowflake connections with key-pair-based authentication.\n\nTo learn more, refer to [Security in Data Factory](https://aka.ms/FabConEU2025_DataFactory_Platform)\n\n## Summary\n\nThis release represents a substantial step for Fabric Data Factory, with a breadth of product capability as well as pricing adjustments that are all based on the feedback from you and our customer community at large. We can’t wait to see what you build on top of these additions.\n\nStay tuned for a series of in-depth blog posts over the next month, getting into the details of many of these features announced here. We hope to see you on our blogs, forums and Ideas channels as you work with Fabric Data Factory!\n\n# Resources\n\nGet started with Fabric Data Factory today!\n\nTo learn more, refer to [Data Factory](https://learn.microsoft.com/fabric/data-factory/data-factory-overview) documentation.",
  "PubDate": "2025-09-16T01:00:00+00:00",
  "OutputDir": "_news",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/unify-your-data-estate-for-the-era-of-ai-with-fabric-data-factory/",
  "Title": "Unify your data estate for the era of AI with Fabric Data Factory",
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/"
}
