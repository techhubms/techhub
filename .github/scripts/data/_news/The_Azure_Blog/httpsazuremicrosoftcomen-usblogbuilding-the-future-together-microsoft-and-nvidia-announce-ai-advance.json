{
  "Title": "Building the future together: Microsoft and NVIDIA announce AI advancements at GTC DC",
  "FeedLevelAuthor": "Microsoft Azure Blog",
  "Tags": [
    "AI",
    "AI + machine learning",
    "Hybrid + multicloud",
    "Internet of things"
  ],
  "Author": "Omar Khan",
  "Description": "New offerings in Azure AI Foundry give businesses an enterprise-grade platform to build, deploy, and scale AI applications and agents.\n\nThe post [Building the future together: Microsoft and NVIDIA announce AI advancements at GTC DC](https://azure.microsoft.com/en-us/blog/building-the-future-together-microsoft-and-nvidia-announce-ai-advancements-at-gtc-dc/) appeared first on [Microsoft Azure Blog](https://azure.microsoft.com/en-us/blog).",
  "ProcessedDate": "2025-10-31 13:10:36",
  "PubDate": "2025-10-28T18:30:00+00:00",
  "FeedName": "The Azure Blog",
  "EnhancedContent": "New offerings in Azure AI Foundry give businesses an enterprise-grade platform to build, deploy, and scale AI applications and agents.\n\nMicrosoft and NVIDIA are deepening our partnership to power the next wave of AI industrial innovation. For years, our companies have helped fuel the AI revolution, bringing the world’s most advanced supercomputing to the cloud, enabling breakthrough frontier models, and making AI more accessible to organizations everywhere. Today, we’re building on that foundation with new advancements that deliver greater performance, capability, and flexibility.\n\nWith added support for [NVIDIA RTX PRO 6000 Blackwell Server Edition](https://www.nvidia.com/en-us/data-center/rtx-pro-6000-blackwell-server-edition/) on [Azure Local](https://azure.microsoft.com/products/local/), customers can deploy AI and visual computing workloads distributed and edge environments with the seamless orchestration and management you use in the cloud. New NVIDIA Nemotron and NVIDIA Cosmos models in [Azure AI Foundry](https://ai.azure.com/) give businesses an enterprise-grade platform to build, deploy, and scale AI applications and agents. With NVIDIA Run:ai on Azure, enterprises can get more from every GPU to streamline operations and accelerate AI. Finally, Microsoft is redefining AI infrastructure with the world’s first deployment of NVIDIA GB300 NVL72.\n\n[Explore our partnership on Azure Local](https://azure.microsoft.com/en-us/products/local/)\n\nToday’s announcements mark the next chapter in our full-stack AI collaboration with NVIDIA, empowering customers to build the future faster.\n\n## Expanding GPU support to Azure Local\n\nMicrosoft and NVIDIA continue to drive advancements in artificial intelligence, offering innovative solutions that span the public and private cloud, the edge, and sovereign environments.\n\nAs highlighted in the [March blog post for NVIDIA GTC](https://azure.microsoft.com/en-us/blog/microsoft-and-nvidia-accelerate-ai-development-and-performance/), Microsoft will offer NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs on Azure. Now, with expanded availability of NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs on Azure Local, organizations can optimize their AI workloads, regardless of location, to provide customers with greater flexibility and more options than ever. Azure Local leverages Azure Arc to empower organizations to run advanced AI workloads on-premises while retaining the management simplicity of the cloud or operating in fully disconnected environments.\n\nNVIDIA RTX PRO 6000 Blackwell GPUs provide the performance and flexibility needed to accelerate a broad range of use cases, from agentic AI, [physical AI](https://www.nvidia.com/en-us/glossary/generative-physical-ai/), and scientific computing to rendering, 3D graphics, digital twins, simulation, and visual computing. This expanded GPU support unlocks a range of edge use cases that fulfill the stringent requirements of critical infrastructure for our healthcare, retail, manufacturing, government, defense, and intelligence customers. This may include real-time video analytics for public safety, predictive maintenance in industrial settings, rapid medical diagnostics, and secure, low-latency inferencing for essential services such as energy production and critical infrastructure. The NVIDIA RTX PRO 6000 Blackwell enables improved virtual desktop support by leveraging [NVIDIA vGPU](https://www.nvidia.com/en-us/data-center/virtual-gpu-technology/) technology and Multi-Instance GPU (MIG) capabilities. This can not only accommodate a higher user density, but also power AI-enhanced graphics and visual compute capabilities, offering an efficient solution for demanding virtual environments.\n\nEarlier this year, Microsoft announced a multitude of AI capabilities at the edge, all enriched with NVIDIA accelerated computing:\n\n- [Edge Retrieval Augmented Generation (RAG)](https://techcommunity.microsoft.com/blog/azurearcblog/transforming-on-premises-data-with-rag-capabilities-on-azure-local/4415217): Empower sovereign AI deployments with fast, secure, and scalable inferencing on local data—supporting mission-critical use cases across government, healthcare, and industrial automation.\n- [Azure AI Video Indexer enabled by Azure Arc](https://techcommunity.microsoft.com/blog/azurearcblog/announcing-the-general-availability-of-azure-ai-video-indexer-enabled-by-azure-a/4399755): Enables real-time and recorded video analytics in disconnected environments—ideal for public safety and critical infrastructure monitoring or post-event analysis.\n\nWith [Azure Local](https://azure.microsoft.com/en-us/products/local/), customers can meet strict regulatory, data residency, and privacy requirements while harnessing the latest AI innovations powered by NVIDIA.\n\nWhether you need ultra-low latency for business continuity, robust local inferencing, or compliance with industry regulations, we’re dedicated to delivering cutting-edge AI performance wherever your data resides. Customers now access the breakthrough performance of the NVIDIA RTX PRO 6000 Blackwell GPUs in new Azure Local solutions—including Dell AX-770, HPE ProLiant DL380 Gen12, and [Lenovo ThinkAgile MX650a V4](https://azurelocalsolutions.azure.microsoft.com/#/catalog/details/c5d7fa5b-b7aa-429e-9cd6-ce453d4e58fb).\n\nTo find out more about upcoming availability and sign up for early ordering, visit:\n\n- [Dell for Azure Local](https://aka.ms/AzureLocalandDell)\n- [HPE for Azure Local](https://www.hpe.com/us/en/alliance/microsoft/azurelocal.html)\n- [Lenovo for Azure Local](https://www.lenovosalesportal.com/azure-local-landing.aspx)\n\n## Powering the future of AI with new models on Azure AI Foundry\n\nAt Microsoft, we’re committed to bringing the most advanced AI capabilities to our customers, wherever they need them. Through our partnership with NVIDIA, [Azure AI Foundry](https://ai.azure.com/) now brings world-class multimodal reasoning models directly to enterprises, deployable anywhere as secure, scalable NVIDIA NIM™ microservices. The portfolio spans a range of different use cases:\n\n### NVIDIA Nemotron Family: High accuracy open models and datasets for agentic AI\n\n- Llama Nemotron Nano VL 8B is available now and is tailored for multimodal vision-language tasks, document intelligence and understanding, and mobile and edge AI agents.\n- NVIDIA Nemotron Nano 9B is available now and supports enterprise agents, scientific reasoning, advanced math, and coding for software engineering and tool calling.\n- NVIDIA Llama 3.3 Nemotron Super 49B 1.5 is coming soon and is designed for enterprise agents, scientific reasoning, advanced math, and coding for software engineering and tool calling.\n\n### NVIDIA Cosmos Family: Open world foundation models for physical AI\n\n- Cosmos Reason-1 7B is available now and supports robotics planning and decision making, training data curation and annotation for autonomous vehicles, and video analytics AI agents extracting insights and performing root-cause analysis from video data.\n- NVIDIA Cosmos Predict 2.5 is coming soon and is a generalist model for world state generation and prediction.\n- NVIDIA Cosmos Transfer 2.5 is coming soon and is designed for structural conditioning and physical AI.\n\n### Microsoft TRELLIS by Microsoft Research: High-quality 3D asset generation\n\n- Microsoft TRELLIS by Microsoft Research is available now and enables digital twins by generating accurate 3D assets from simple prompts, immersive retail experiences with photorealistic product models for AR and virtual try-ons, and game and simulation development by turning creative ideas into production-ready 3D content.\n\nTogether, these open models reflect the depth of the Azure and NVIDIA partnership: combining Microsoft’s adaptive cloud with NVIDIA’s leadership in accelerated computing to power the next generation of agentic AI for every industry. [Learn more about the models here](http://aka.ms/NVIDIAonFoundry).\n\n## Maximizing GPU utilization for enterprise AI with NVIDIA Run:ai on Azure\n\nAs an AI workload and GPU orchestration platform, NVIDIA Run:ai helps organizations make the most of their compute investments, accelerating AI development cycles and driving faster time-to-market for new insights and capabilities. By bringing [NVIDIA Run:ai to Azure](https://marketplace.microsoft.com/en-us/product/azure-applications/nvidia.nvidia-runai?tab=Overview), we’re giving enterprises the ability to dynamically allocate, share, and manage GPU resources across teams and workloads, helping them get more from every GPU.\n\nNVIDIA Run:ai on Azure integrates seamlessly with core Azure services, including Azure NC and ND series instances, Azure Kubernetes Service (AKS), and Azure Identity Management, and offers compatibility with Azure Machine Learning and Azure AI Foundry for unified, enterprise-ready AI orchestration. We’re bringing hybrid scale to life to help customers transform static infrastructure into a flexible, shared resource for AI innovation.\n\nWith smarter orchestration and cloud-ready GPU pooling, teams can drive faster innovation, reduce costs, and unleash the power of AI across their organizations with confidence. [NVIDIA Run:ai on Azure](https://marketplace.microsoft.com/en-us/product/azure-applications/nvidia.nvidia-runai?tab=Overview) enhances AKS with GPU-aware scheduling, helping teams allocate, share, and prioritize GPU resources more efficiently. Operations are streamlined with one-click job submission, automated queueing, and built in governance. This ensures teams spend less time managing infrastructure and more time focused on building what’s next.\n\nThis impact spans industries, supporting the infrastructure and orchestration behind transformative AI workloads at every stage of enterprise growth:\n\n- Healthcare organizations can use NVIDIA Run:ai on Azure to advance medical imaging analysis and drug discovery workloads across hybrid environments.\n- Financial services organizations can orchestrate and scale GPU clusters for complex risk simulations and fraud detection models.\n- Manufacturers can accelerate computer vision training models for improved quality control and predictive maintenance in their factories.\n- Retail companies can power real-time recommendation systems for more personalized experiences through efficient GPU allocation and scaling, ultimately better serving their customers.\n\nPowered by Microsoft Azure and NVIDIA, Run:ai is purpose-built for scale, helping enterprises move from isolated AI experimentation to production-grade innovation.\n\n## Reimagining AI at scale: First to deploy NVIDIA GB300 NVL72 supercomputing cluster\n\nMicrosoft is redefining AI infrastructure with the new NDv6 GB300 VM series, delivering the first [at-scale production cluster](https://azure.microsoft.com/en-us/blog/microsoft-azure-delivers-the-first-large-scale-cluster-with-nvidia-gb300-nvl72-for-openai-workloads/)of NVIDIA GB300 NVL72 systems, featuring over 4600 [NVIDIA Blackwell Ultra GPUs](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/) connected via NVIDIA Quantum-X800 InfiniBand networking. Each NVIDIA GB300 NVL72 rack integrates 72 NVIDIA Blackwell Ultra GPUs and 36 [NVIDIA Grace™ CPUs](https://www.nvidia.com/en-us/data-center/grace-cpu/), delivering over 130 TB/s of NVLink bandwidth and up to 136 kW of compute power in a single cabinet. Designed for the most demanding workloads—reasoning models, agentic systems, and multimodal AI—GB300 NVL72 combines ultra-dense compute, direct liquid cooling, and smart rack-scale management to deliver breakthrough efficiency and performance within a standard datacenter footprint.\n\nAzure’s co-engineered infrastructure enhances GB300 NVL72 with technologies like Azure Boost for accelerated I/O and integrated hardware security modules (HSM) for enterprise-grade protection. Each rack arrives pre-integrated and self-managed, enabling rapid, repeatable deployment across Azure’s global fleet. As the first cloud provider to deploy NVIDIA GB300 NVL72 at scale, Microsoft is setting a new standard for AI supercomputing—empowering organizations to train and deploy frontier models faster, more efficiently, and more securely than ever before. Together, Azure and NVIDIA are powering the future of AI.\n\nLearn more about [Microsoft’s systems approach in delivering GB300 NVL72 on Azure](https://techcommunity.microsoft.com/blog/azureinfrastructureblog/reimagining-ai-at-scale-nvidia-gb300-nvl72-on-azure/4464556).\n\n## Unleashing the performance of ND GB200-v6 VMs with NVIDIA Dynamo\n\nOur collaboration with NVIDIA focuses on optimizing every layer of the computing stack to help customers maximize the value of their existing AI infrastructure investments.\n\nTo deliver high-performance inference for compute-intensive reasoning models at scale, we’re bringing together a solution that combines the open-source [NVIDIA Dynamo](https://www.nvidia.com/en-us/ai/dynamo/) framework, our ND GB200-v6 VMs with [NVIDIA GB200 NVL72](https://www.nvidia.com/en-us/data-center/gb200-nvl72/) and Azure Kubernetes Service(AKS). We’ve demonstrated the performance this combined solution delivers at scale with the gpt-oss 120b model processing 1.2 million tokens per second deployed in a production-ready, managed AKS cluster and have published a deployment guide for developers to get started today.\n\nDynamo is an open-source, distributed inference framework designed for multi-node environments and rack-scale accelerated compute architectures. By enabling disaggregated serving, LLM-aware routing and KV caching, Dynamo significantly boosts performance for reasoning models on Blackwell, unlocking up to 15x more throughput compared to the prior Hopper generation, opening new revenue opportunities for AI service providers.\n\nThese efforts enable AKS production customers to take full advantage of NVIDIA Dynamo’s  inference optimizations when deploying frontier reasoning models at scale. We’re dedicated to bringing the latest open-source software innovations to our customers, helping them fully realize the potential of the NVIDIA Blackwell platform on Azure.\n\nLearn more about Dynamo [on AKS](https://blog.aks.azure.com/2025/10/24/dynamo-on-aks).\n\n## Get more AI resources\n\n- Join us in San Francisco at [Microsoft Ignite](https://ignite.microsoft.com/en-US/home) in November to hear about the latest in enterprise solutions and innovation.\n- Explore [Azure AI Foundry](https://ai.azure.com/) and [Azure Local](https://azure.microsoft.com/en-us/products/local/).",
  "OutputDir": "_news",
  "FeedUrl": "https://azure.microsoft.com/en-us/blog/feed/",
  "Link": "https://azure.microsoft.com/en-us/blog/building-the-future-together-microsoft-and-nvidia-announce-ai-advancements-at-gtc-dc/"
}
