{
  "Tags": [
    "Storage"
  ],
  "FeedLevelAuthor": "Microsoft Azure Blog",
  "Description": "Today in Wisconsin we introduced Fairwater, our newest US AI datacenter, the largest and most sophisticated AI factory we’ve built yet. In addition to our Fairwater datacenter in Wisconsin, we also have multiple identical Fairwater datacenters under construction in other locations across the US.\n\nThe post [Inside the world’s most powerful AI datacenter](https://blogs.microsoft.com/blog/2025/09/18/inside-the-worlds-most-powerful-ai-datacenter/) appeared first on [Microsoft Azure Blog](https://azure.microsoft.com/en-us/blog).",
  "Link": "https://blogs.microsoft.com/blog/2025/09/18/inside-the-worlds-most-powerful-ai-datacenter/",
  "FeedUrl": "https://azure.microsoft.com/en-us/blog/feed/",
  "OutputDir": "_news",
  "PubDate": "2025-09-18T15:45:10+00:00",
  "FeedName": "The Azure Blog",
  "EnhancedContent": "This week we have introduced a wave of purpose-built datacenters and infrastructure investments we are making around the world to support the global adoption of cutting-edge AI workloads and cloud services.\n\nToday in Wisconsin we introduced Fairwater, [our newest US AI datacenter](https://aka.ms/WIdatacenter), the largest and most sophisticated AI factory we’ve built yet. In addition to our Fairwater datacenter in Wisconsin, we also have multiple identical Fairwater datacenters under construction in other locations across the US.\n\nIn Narvik, Norway, [Microsoft announced plans with nScale and Aker JV](https://news.microsoft.com/source/emea/features/the-port-town-in-norway-emerging-as-an-ai-hub/) to develop a new hyperscale AI datacenter.\n\nIn Loughton, UK, we announced [a partnership with nScale to build the UK’s largest supercomputer](https://blogs.microsoft.com/on-the-issues/2025/09/16/microsoft-30-billion-uk-ai-future/) to support services in the UK.\n\nThese AI datacenters are significant capital projects, representing tens of billions of dollars of investments and hundreds of thousands of cutting-edge AI chips, and will seamlessly connect with our global Microsoft Cloud of over 400 datacenters in 70 regions around the world. Through innovation that can enable us to link these AI datacenters in a distributed network, we multiply the efficiency and compute in an exponential way to further democratize access to AI services globally.\n\nSo what is an AI datacenter?\n\n### **The AI datacenter: the new factory of the AI era**\n\n[![Aerial view of Microsoft's new AI datacenter campus in Mt. Pleasant, Wisconsin. ](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-1-Datacenter.jpg)](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-1-Datacenter.jpg)Aerial view of Microsoft’s new AI datacenter campus in Mt Pleasant, Wisconsin.\n\nAn AI datacenter is a unique, purpose-built facility designed specifically for AI training as well as running large-scale artificial intelligence models and applications. Microsoft’s AI datacenters power OpenAI, Microsoft AI, our Copilot capabilities and many more leading AI workloads.\n\nThe new Fairwater AI datacenter in Wisconsin stands as a remarkable feat of engineering, covering 315 acres and housing three massive buildings with a combined 1.2 million square feet under roofs. Constructing this facility required 46.6 miles of deep foundation piles, 26.5 million pounds of structural steel, 120 miles of medium-voltage underground cable and 72.6 miles of mechanical piping.\n\nUnlike typical cloud datacenters, which are optimized to run many smaller, independent workloads such as hosting websites, email or business applications, this datacenter is built to work as one massive AI supercomputer using a single flat networking interconnecting hundreds of thousands of the latest NVIDIA GPUs. In fact, it will deliver 10X the performance of the world’s fastest supercomputer today, enabling AI training and inference workloads at a level never before seen.\n\n### **The role of our AI datacenters – powering frontier AI**\n\nEffective AI models rely on thousands of computers working together, powered by GPUs, or specialized AI accelerators, to process massive concurrent mathematical computations. They’re interconnected with extremely fast networks so they can share results instantly, and all of this is supported by enormous storage systems that hold the data (like text, images or video) broken down into tokens, the small units of information the AI learns from. The goal is to keep these chips busy all the time, because if the data or the network can’t keep up, everything slows down.\n\nThe AI training itself is a cycle: the AI processes tokens in sequence, makes predictions about the next one, checks them against the right answers and adjusts itself. This repeats trillions of times until the system gets better at whatever it’s being trained to do. Think of it like a professional football team’s practice. Each GPU is a player running a drill, the tokens are the plays being executed step by step, and the network is the coaching staff, shouting instructions and keeping everyone in sync. The team repeats plays over and over, correcting mistakes until they can execute them perfectly. By the end, the AI model, like the team, has mastered its strategy and is ready to perform under real game conditions.\n\n### **AI infrastructure at frontier scale**\n\nPurpose-built infrastructure is critical to being able to power AI efficiently. To compute the token math at this trillion-parameter scale of leading AI models, the core of the AI datacenter is made up of dedicated AI accelerators (such as GPUs) mounted on server boards alongside CPUs, memory and storage. A single server hosts multiple GPU accelerators, connected for high-bandwidth communication. These servers are then installed into a rack, with top-of-rack (ToR) switches providing low-latency networking between them. Every rack in the datacenter is interconnected, creating a tightly coupled cluster. From the outside, this architecture looks like many independent servers, but at scale it functions as a single supercomputer where hundreds of thousands of accelerators can train a single model in parallel.\n\nThis datacenter runs a single, massive cluster of interconnected NVIDIA GB200 servers and millions of compute cores and exabytes of storage, all engineered for the most demanding AI workloads. Azure was the first cloud provider to bring online the NVIDIA GB200 server, rack and full datacenter clusters. Each rack packs 72 NVIDIA Blackwell GPUs, tied together in a single NVLink domain that delivers 1.8 terabytes of GPU-to-GPU bandwidth and gives every GPU access to 14 terabytes of pooled memory. Rather than behaving like dozens of separate chips, the rack operates as a single, giant accelerator, capable of processing an astonishing 865,000 tokens per second, the highest throughput of any cloud platform available today. The Norway and UK AI datacenters will use similar clusters, and take advantage of NVIDIAs next AI chip design (GB300) which offers even more pooled memory per rack.\n\nThe challenge in establishing supercomputing scale, particularly as AI training requirements continue to require breakthrough scales of computing, is getting the networking topology just right. To ensure low latency communication across multiple layers in a cloud environment, Microsoft needed to extend performance beyond a single rack. For the latest NVIDIA GB200 and GB300 deployments globally, at the rack level these GPUs communicate over NVLink and NVSwitch at terabytes per second, collapsing memory and bandwidth barriers. Then to connect across multiple racks into a pod, Azure uses both InfiniBand and Ethernet fabrics that deliver 800 Gbps, in a full fat tree non-blocking architecture to ensure that every GPU can talk to every other GPU at full line rate without congestion. And across the datacenter, multiple pods of racks are interconnected to reduce hop counts and enable tens of thousands of GPUs to function as one global-scale supercomputer.\n\nWhen laid out in a traditional datacenter hallway, physical distance between racks introduces latency into the system. To address this, the racks in the Wisconsin AI datacenter are laid out in a two-story datacenter configuration, so in addition to racks networked to adjacent racks, they are networked to additional racks above or below them.\n\nThis layered approach sets Azure apart. Microsoft Azure was not just the first cloud to bring GB200 online at rack and datacenter scale; we’re doing it at massive scale with customers today. By co-engineering the full stack with the best from our industry partners coupled with our own purpose-built systems, Microsoft has built the most powerful, tightly coupled AI supercomputer in the world, purpose-built for frontier models.\n\n[![A high-density cluster of AI infrastructure servers in a Microsoft datacenter.](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-2-Datacenter.jpg)](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-2-Datacenter.jpg)High density cluster of AI infrastructure servers in a Microsoft datacenter.\n\n### **Addressing the environmental impact: closed loop liquid cooling at facility scale**\n\nTraditional air cooling can’t handle the density of modern AI hardware. Our datacenters use advanced liquid cooling systems — integrated pipes circulate cold liquid directly into servers, extracting heat efficiently. The closed-loop recirculation ensures zero water waste, with water only needed to fill up once and then it is continually reused.\n\nBy designing purpose-built AI datacenters, we were able to build liquid cooling infrastructure into the facility directly to get us more rack-density in the datacenter. Fairwater is supported by the second largest water-cooled chiller plant on the planet and will continuously circulate water in its closed loop cooling system. The hot water is then piped out to the cooling “fins” on each side of the datacenter, where 172 20-foot fans chill and recirculate the water back to the datacenter. This system keeps the AI datacenter running efficiently, even at peak loads.\n\n[![An aerial view of part of the closed loop liquid cooling system.](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-3-Datacenter.jpg)](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-3-Datacenter.jpg)Aerial view of part of the closed loop liquid cooling system.\n\nOver 90% of our datacenter capacity uses this system, requiring water only once during construction and continually reusing it with no evaporation losses. The remaining 10% of traditional servers use outdoor air for cooling, switching to water only during the hottest days, a design that dramatically reduces water usage compared to traditional datacenters.\n\nWe’re also using liquid cooling to support AI workloads in many of our existing datacenters; this liquid cooling is accomplished with Heat Exchanger Units (HXUs) that also operate with zero-operational water use.\n\n### **Storage and compute: Built for AI velocity**\n\nModern datacenters can contain exabytes of storage and millions of CPU compute cores. To support the AI infrastructure cluster, an entirely separate datacenter infrastructure is needed to store and process the data used and generated by the AI cluster. To give you an example of the scale — the Wisconsin AI datacenter’s storage systems are five football fields in length!\n\n[![An aerial view of a dedicated storage and compute datacenter used to store and process data for the AI datacenter.](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-4-Datacenter.jpg)](https://blogs.microsoft.com/wp-content/uploads/2025/09/OMB-Image-4-Datacenter.jpg)Aerial view of a dedicated storage and compute datacenter used to store and process data for the AI datacenter.\n\nWe reengineered Azure storage for the most demanding AI workloads, across these massive datacenter deployments for true supercomputing scale. Each Azure Blob Storage account can sustain over 2 million read/write transactions per second, and with millions of accounts available, we can elastically scale to meet virtually any data requirement.\n\nBehind this capability is a fundamentally rearchitected storage foundation that aggregates capacity and bandwidth across thousands of storage nodes and hundreds of thousands of drives. This enables scale to exabyte scale storage, eliminating the need for manual sharding and simplifying operations for even the largest AI and analytics workloads.\n\nKey innovations such as BlobFuse2 deliver high-throughput, low-latency access for GPU node-local training, ensuring that compute resources are never idle and that massive AI training datasets are always available when needed. Multiprotocol support allows seamless integration with diverse data pipelines, while deep integration with analytics engines and AI tools accelerates data preparation and deployment.\n\nAutomatic scaling dynamically allocates resources as demand grows, combined with advanced security, resiliency and cost-effective tiered storage, Azure’s storage platform sets the pace for next-generation workloads, delivering the performance, scalability and reliability required.\n\n### **AI WAN: Connecting multiple datacenters for an even larger AI supercomputer**\n\nThese new AI datacenters are part of a global network of Azure AI datacenters, interconnected via our Wide Area Network (WAN). This isn’t just about one building, it’s about a distributed, resilient and scalable system that operates as a single, powerful AI machine. Our AI WAN is built with growth capabilities in AI-native bandwidth scales to enable large-scale distributed training across multiple, geographically diverse Azure regions, thus allowing customers to harness the power of a giant AI supercomputer.\n\nThis is a fundamental shift in how we think about AI supercomputers. Instead of being limited by the walls of a single facility, we’re building a distributed system where compute, storage and networking resources are seamlessly pooled and orchestrated across datacenter regions. This means greater resiliency, scalability and flexibility for customers.\n\n### **Bringing it all together**\n\nTo meet the critical needs of the largest AI challenges, we needed to redesign every layer of our cloud infrastructure stack. This isn’t just about isolated breakthroughs, but composing multiple new approaches across silicon, servers, networks and datacenters, leading to advancements where software and hardware are optimized as one purpose-built system.\n\nMicrosoft’s Wisconsin datacenter will play a critical role in the future of AI, built on real technology, real investment and real community impact. As we connect this facility with other regional datacenters, and as every layer of our infrastructure is harmonized as a complete system, we’re unleashing a new era of cloud-powered intelligence, secure, adaptive and ready for what’s next.\n\nTo learn more about Microsoft’s datacenter innovations, check out the virtual datacenter tour at [datacenters.microsoft.com](https://datacenters.microsoft.com).\n\n*Scott Guthrie is responsible for hyperscale cloud computing solutions and services including Azure, Microsoft’s cloud computing platform, generative AI solutions, data platforms and information and cybersecurity. These platforms and services help organizations worldwide solve urgent challenges and drive long-term transformation.*\n\nTags: [AI](https://blogs.microsoft.com/blog/tag/ai/), [Azure](https://blogs.microsoft.com/blog/tag/azure/), [Azure Blob Storage](https://blogs.microsoft.com/blog/tag/azure-blob-storage/), [datacenters](https://blogs.microsoft.com/blog/tag/datacenters/), [Microsoft Copilot](https://blogs.microsoft.com/blog/tag/microsoft-copilot/)",
  "Title": "Inside the world’s most powerful AI datacenter",
  "Author": "Scott Guthrie",
  "ProcessedDate": "2025-10-15 16:03:30"
}
