{
  "OutputDir": "_news",
  "PubDate": "2025-07-09T16:00:00+00:00",
  "FeedLevelAuthor": "Microsoft Azure Blog",
  "FeedName": "The Azure Blog",
  "Tags": [
    "AI",
    "AI + machine learning",
    "Copilot",
    "Developer tools",
    "Generative AI"
  ],
  "FeedUrl": "https://azure.microsoft.com/en-us/blog/feed/",
  "EnhancedContent": "Unlock faster, efficient reasoning with Phi-4-mini-flash-reasoning—optimized for edge, mobile, and real-time applications.\n\n## State of the art architecture redefines speed for reasoning models\n\nMicrosoft is excited to unveil a new edition to the Phi model family: **Phi-4-mini-flash-reasoning**. Purpose-built for scenarios where compute, memory, and latency are tightly constrained, this new model is engineered to bring advanced reasoning capabilities to edge devices, mobile applications, and other resource-constrained environments. This new model follows Phi-4-mini, but is built on a new hybrid architecture, that achieves up to 10 times higher throughput and a 2 to 3 times average reduction in latency, enabling significantly faster inference without sacrificing reasoning performance. Ready to power real world solutions that demand efficiency and flexibility, Phi-4-mini-flash-reasoning is available on [Azure AI Foundry](https://ai.azure.com/), [NVIDIA API Catalog](https://build.nvidia.com/microsoft), and [Hugging Face](http://aka.ms/flashreasoning-hf) today.\n\n## Efficiency without compromise\n\nPhi-4-mini-flash-reasoning balances math reasoning ability with efficiency, making it potentially suitable for educational applications, real-time logic-based applications, and more.\n\nSimilar to its predecessor, Phi-4-mini-flash-reasoning is a 3.8 billion parameter open model optimized for advanced math reasoning. It supports a 64K token context length and is fine-tuned on high-quality synthetic data to deliver reliable, logic-intensive performance deployment.\n\n## What’s new?\n\nAt the core of Phi-4-mini-flash-reasoning is the newly introduced decoder-hybrid-decoder architecture, SambaY, whose central innovation is the Gated Memory Unit (GMU), a simple yet effective mechanism for sharing representations between layers.  The architecture includes a self-decoder that combines Mamba (a State Space Model) and Sliding Window Attention (SWA), along with a single layer of full attention. The architecture also involves a cross-decoder that interleaves expensive cross-attention layers with the new, efficient GMUs. This new architecture with GMU modules drastically improves decoding efficiency, boosts long-context retrieval performance and enables the architecture to deliver exceptional performance across a wide range of tasks.\n\nKey benefits of the SambaY architecture include:\n\n- Enhanced decoding efficiency.\n- Preserves linear prefiling time complexity.\n- Increased scalability and enhanced long context performance.\n- Up to 10 times higher throughput.\n\n![A diagram of a computer program](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/07/Decoder-hybrid-decoder-architecture.webp)*Our decoder-hybrid-decoder architecture taking Samba [RLL+25] as the self-decoder. Gated Memory Units (GMUs) are interleaved with the cross-attention layers in the cross-decoder to reduce the decoding computation complexity. As in YOCO [SDZ+24], the full attention layer only computes the KV cache during the prefilling with the self-decoder, leading to linear computation complexity for the prefill stage.*\n\n## Phi-4-mini-flash-reasoning benchmarks\n\nLike all models in the Phi family, Phi-4-mini-flash-reasoning is deployable on a single GPU, making it accessible for a broad range of use cases. However, what sets it apart is its architectural advantage. This new model achieves significantly lower latency and higher throughput compared to Phi-4-mini-reasoning, particularly in long-context generation and latency-sensitive reasoning tasks.\n\nThis makes Phi-4-mini-flash-reasoning a compelling option for developers and enterprises looking to deploy intelligent systems that require fast, scalable, and efficient reasoning—whether on premises or on-device.\n\n![A graph of a number of people](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/07/Generation-Latencies.webp) ![A graph with red and blue dots and numbers](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/07/Prompt-2000.webp)*The top plot shows inference latency as a function of generation length, while the bottom plot illustrates how inference latency varies with throughput. Both experiments were conducted using the vLLM inference framework on a single A100-80GB GPU with tensor parallelism (TP) set to 1.* ![A graph of different colored bars](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/07/Math-benchmarks.webp)*A more accurate evaluation was used where Pass@1 accuracy is averaged over 64 samples for AIME24/25 and 8 samples for Math500 and GPQA Diamond. In this graph, Phi-4-mini-flash-reasoning outperforms Phi-4-mini-reasoning and is better than models twice its size.*\n\n## What are the potential use cases?\n\nThanks to its reduced latency, improved throughput, and focus on math reasoning, the model is ideal for:\n\n- **Adaptive learning platforms**, where real-time feedback loops are essential.\n- **On-device reasoning assistants**, such as mobile study aids or edge-based logic agents.\n- **Interactive tutoring systems** that dynamically adjust content difficulty based on a learner’s performance.\n\nIts strength in math and structured reasoning makes it especially valuable for education technology, lightweight simulations, and automated assessment tools that require reliable logic inference with fast response times.\n\nDevelopers are encouraged to connect with peers and Microsoft engineers through the [Microsoft Developer Discord community](https://aka.ms/foundrydevs) to ask questions, share feedback, and explore real-world use cases together.\n\n## Microsoft’s commitment to trustworthy AI\n\nOrganizations across industries are leveraging Azure AI and [Microsoft 365 Copilot](https://www.microsoft.com/en-us/microsoft-365/copilot) capabilities to drive growth, increase productivity, and create value-added experiences.\n\nWe’re committed to helping organizations use and build [AI that is trustworthy](https://blogs.microsoft.com/blog/2024/09/24/microsoft-trustworthy-ai-unlocking-human-potential-starts-with-trust/), meaning it is secure, private, and safe. We bring best practices and learnings from decades of researching and building AI products at scale to provide industry-leading commitments and capabilities that span our three pillars of security, privacy, and safety. Trustworthy AI is only possible when you combine our commitments, such as our [Secure Future Initiative](https://www.microsoft.com/en-us/trust-center/security/secure-future-initiative) and our [responsible AI principles](https://www.microsoft.com/en-us/ai/responsible-ai), with our product capabilities to unlock AI transformation with confidence.\n\nPhi models are developed in accordance with Microsoft AI principles: accountability, transparency, fairness, reliability and safety, privacy and security, and inclusiveness.\n\nThe Phi model family, including Phi-4-mini-flash-reasoning, employs a robust safety post-training strategy that integrates Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reinforcement Learning from Human Feedback (RLHF). These techniques are applied using a combination of open-source and proprietary datasets, with a strong emphasis on ensuring helpfulness, minimizing harmful outputs, and addressing a broad range of safety categories. Developers are encouraged to apply responsible AI best practices tailored to their specific use cases and cultural contexts.\n\nRead the model card to learn more about any risk and mitigation strategies.\n\n## Learn more about the new model\n\n- Try out the new model on [Azure AI Foundry](https://aka.ms/try-phi).\n- Find code samples and more in the [Phi Cookbook](https://aka.ms/phicookbook).\n- Read the Phi-4-mini-flash-reasoning technical paper on [Arxiv](http://aka.ms/flashreasoning-hf).\n- If you have questions, sign up for the [Microsoft Developer “Ask Me Anything”](https://discord.com/invite/azureaifoundry?event=1382861149288005693).\n\n## Create with Azure AI Foundry\n\n- Get started with [Azure AI Foundry](https://ai.azure.com/), and jump directly into [Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=TeamsDevApp.vscode-ai-foundry).\n- Download the [Azure AI Foundry SDK](https://aka.ms/aifoundrysdk) .\n- Take the [Azure AI Foundry learn courses](https://aka.ms/CreateAgenticAISolutions).\n- Review the [Azure AI Foundry documentation](https://learn.microsoft.com/azure/ai-foundry/).\n- Keep the conversation going in [GitHub](https://aka.ms/azureaifoundry/forum) and [Discord](https://aka.ms/azureaifoundry/discord).",
  "Title": "Reasoning reimagined: Introducing Phi-4-mini-flash-reasoning",
  "ProcessedDate": "2025-08-19 10:12:54",
  "Author": "Weizhu Chen, Jianfeng Gao and Liliang Ren",
  "Description": "Unlock faster, efficient reasoning with Phi-4-mini-flash-reasoning—optimized for edge, mobile, and real-time applications.\n\nThe post [Reasoning reimagined: Introducing Phi-4-mini-flash-reasoning](https://azure.microsoft.com/en-us/blog/reasoning-reimagined-introducing-phi-4-mini-flash-reasoning/) appeared first on [Microsoft Azure Blog](https://azure.microsoft.com/en-us/blog).",
  "Link": "https://azure.microsoft.com/en-us/blog/reasoning-reimagined-introducing-phi-4-mini-flash-reasoning/"
}
