{
  "Tags": [
    "Agent Factory",
    "AI",
    "AI + machine learning",
    "Analytics",
    "Management and governance"
  ],
  "FeedLevelAuthor": "Microsoft Azure Blog",
  "Description": "Azure AI Foundry brings together security, safety, and governance in a layered process enterprises can follow to build trust in their agents.\n\nThe post [Agent Factory: Creating a blueprint for safe and secure AI agents](https://azure.microsoft.com/en-us/blog/agent-factory-creating-a-blueprint-for-safe-and-secure-ai-agents/) appeared first on [Microsoft Azure Blog](https://azure.microsoft.com/en-us/blog).",
  "Link": "https://azure.microsoft.com/en-us/blog/agent-factory-creating-a-blueprint-for-safe-and-secure-ai-agents/",
  "FeedUrl": "https://azure.microsoft.com/en-us/blog/feed/",
  "OutputDir": "_news",
  "PubDate": "2025-09-17T21:30:00+00:00",
  "FeedName": "The Azure Blog",
  "EnhancedContent": "Azure AI Foundry brings together security, safety, and governance in a layered process enterprises can follow to build trust in their agents.\n\n*This blog post is the sixth out of a six-part blog series called [Agent Factory](https://azure.microsoft.com/en-us/blog/tag/agent-factory/) which shares best practices, design patterns, and tools to help guide you through adopting and building agentic AI.*\n\n## Trust as the next frontier\n\nTrust is rapidly becoming the defining challenge for enterprise AI. If observability is about seeing, then security is about steering. As agents move from clever prototypes to core business systems, enterprises are asking a harder question: how do we keep agents safe, secure, and under control as they scale?\n\nThe answer is not a patchwork of point fixes. It is a blueprint. A layered approach that puts trust first by combining identity, guardrails, evaluations, adversarial testing, data protection, monitoring, and governance.\n\n[Learn more about building trust with Azure AI Foundry](https://azure.microsoft.com/products/ai-foundry)\n\n## Why enterprises need to create their blueprint now\n\nAcross industries, we hear the same concerns:\n\n- CISOs worry about agent sprawl and unclear ownership.\n- Security teams need guardrails that connect to their existing workflows.\n- Developers want safety built in from day one, not added at the end.\n\nThese pressures are driving the **shift left phenomenon**. Security, safety, and governance responsibilities are moving earlier into the developer workflow. Teams cannot wait until deployment to secure agents. They need built-in protections, evaluations, and policy integration from the start.\n\nData leakage, prompt injection, and regulatory uncertainty remain the top blockers to AI adoption. For enterprises, trust is now a key deciding factor in whether agents move from pilot to production.\n\n## What safe and secure agents look like\n\nFrom enterprise adoption, five qualities stand out:\n\n- **Unique identity:** Every agent is known and tracked across its lifecycle.\n- **Data protection by design:** Sensitive information is classified and governed to reduce oversharing.\n- **Built-in controls:** Harm and risk filters, threat mitigations, and groundedness checks reduce unsafe outcomes.\n- **Evaluated against threats:** Agents are tested with automated safety evaluations and adversarial prompts before deployment and throughout production.\n- **Continuous oversight:** Telemetry connects to enterprise security and compliance tools for investigation and response.\n\n![A framework defining the cycle of risk evaluation and management.](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/09/Picture1-1024x576.webp)\n\nThese qualities do not guarantee absolute safety, but they are essential for building trustworthy agents that meet enterprise standards. Baking these into our products reflects Microsoft’s approach to trustworthy AI. Protections are layered across the model, system, policy, and user experience levels, continuously improved as agents evolve.\n\n## How Azure AI Foundry supports this blueprint\n\n![A view of security settings and agent controls inside Azure AI Foundry.](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/09/Picture2-1.webp)\n\n[Azure AI Foundry](https://azure.microsoft.com/products/ai-foundry#Security) brings together security, safety, and governance capabilities in a layered process enterprises can follow to build trust in their agents.\n\n- **Entra Agent ID**\nComing soon, every agent created in Foundry will be assigned a unique Entra Agent ID, giving organizations visibility into all active agents across a tenant and helping to reduce shadow agents.\n- **Agent controls**\nFoundry offers industry first [agent controls](https://azure.microsoft.com/products/ai-services/ai-content-safety/) that are both comprehensive and built in. It is the only AI platform with a cross-prompt injection classifier that scans not just prompt documents but also tool responses, email triggers, and other untrusted sources to flag, block, and neutralize malicious instructions. Foundry also provides controls to prevent misaligned tool calls, high risk actions, and sensitive data loss, along with harm and risk filters, groundedness checks, and protected material detection.\n\n![An example of how Azure AI Foundry flags prompts for security risks.](https://azure.microsoft.com/en-us/blog/wp-content/uploads/2025/09/Picture3-1024x824.webp)\n- **Risk and safety evaluations**\nEvaluations provide a feedback loop across the lifecycle. Teams can run harm and risk checks, groundedness scoring, and protected material scans both before deployment and in production. The Azure AI Red Teaming Agent and PyRIT toolkit simulate adversarial prompts at scale to probe behavior, surface vulnerabilities, and strengthen resilience before incidents reach production.\n- **Data control with your own resources**\nStandard agent setup in [Azure AI Foundry Agent Service](https://learn.microsoft.com/azure/ai-foundry/agents/overview) allows enterprises to bring their own Azure resources. This includes file storage, search, and conversation history storage. With this setup, data processed by Foundry agents remains within the tenant’s boundary under the organization’s own security, compliance, and governance controls.\n- **Network isolation**\nFoundry Agent Service supports private network isolation with custom virtual networks and subnet delegation. This configuration ensures that agents operate within a tightly scoped network boundary and interact securely with sensitive customer data under enterprise terms.\n- **Microsoft Purview**\n[Microsoft Purview](https://azure-int.microsoft.com/products/purview/) helps extend data security and compliance to AI workloads. Agents in Foundry can honor Purview sensitivity labels and DLP policies, so protections applied to data carry through into agent outputs. Compliance teams can also use Purview Compliance Manager and related tools to assess alignment with frameworks like the EU AI Act and NIST AI RMF, and securely interact with your sensitive customer data under your terms.\n- **Microsoft Defender**\nFoundry surfaces alerts and recommendations from [Microsoft Defender](https://www.microsoft.com/security/business/cloud-security/microsoft-defender-cloud) directly in the agent environment, giving developers and administrators visibility into issues such as prompt injection attempts, risky tool calls, or unusual behavior. This same telemetry also streams into Microsoft Defender XDR, where security operations center teams can investigate incidents alongside other enterprise alerts using their established workflows.\n- **Governance collaborators**\nFoundry connects with governance collaborators such as Credo AI and Saidot. These integrations allow organizations to map evaluation results to frameworks including the EU AI Act and the NIST AI Risk Management Framework, making it easier to demonstrate responsible AI practices and regulatory alignment.\n\n## Blueprint in action\n\nFrom enterprise adoption, these practices stand out:\n\n1. **Start with identity**. Assign Entra Agent IDs to establish visibility and prevent sprawl.\n2. **Built-in controls**. Use Prompt Shields, harm and risk filters, groundedness checks, and protected material detection.\n3. **Continuously evaluate**. Run harm and risk checks, groundedness scoring, protected material scans, and adversarial testing with the Red Teaming Agent and PyRIT before deployment and throughout production.\n4. **Protect sensitive data**. Apply Purview labels and DLP so protections are honored in agent outputs.\n5. **Monitor with enterprise tools**. Stream telemetry into Defender XDR and use Foundry observability for oversight.\n6. **Connect governance to regulation**. Use governance collaborators to map evaluation data to frameworks like the EU AI Act and NIST AI RMF.\n\n## Proof points from our customers\n\nEnterprises are already creating security blueprints with Azure AI Foundry:\n\n- **[EY](https://azure.microsoft.com/blog/agent-factory-top-5-agent-observability-best-practices-for-reliable-ai/)** uses Azure AI Foundry’s leaderboards and evaluations to compare models by quality, cost, and safety, helping scale solutions with greater confidence.\n- [**Accenture**](https://www.microsoft.com/customers/story/23953-accenture-azure-ai-foundry) is testing the Microsoft AI Red Teaming Agent to simulate adversarial prompts at scale. This allows their teams to validate not just individual responses, but full multi-agent workflows under attack conditions before going live.\n\n## Learn more\n\n- Create with [Azure AI Foundry](https://ai.azure.com/).\n- Join us at [Microsoft Secure on September 30](https://register.secure.microsoft.com/) to learn about our newest capabilities and how Azure AI Foundry integrates with Microsoft Security to help you build safe and secure agents, with speakers including Vasu Jakkal, Sarah Bird, and Herain Oberoi.\n- Implement a [responsible generative AI](https://learn.microsoft.com/training/modules/responsible-ai-studio/) solution in Azure AI Foundry.\n\nDid you miss these posts in the Agent Factory series?\n\n- [The new era of agentic AI—common use cases and design patterns](https://azure.microsoft.com/en-us/blog/agent-factory-the-new-era-of-agentic-ai-common-use-cases-and-design-patterns/)\n- [Building your first AI agent with the tools to deliver real-world outcomes](https://azure.microsoft.com/en-us/blog/agent-factory-building-your-first-ai-agent-with-the-tools-to-deliver-real-world-outcomes/)\n- [Top 5 agent observability best practices for reliable AI](https://azure.microsoft.com/en-us/blog/agent-factory-top-5-agent-observability-best-practices-for-reliable-ai/)\n- [From prototype to production—developer tools and rapid agent development](https://azure.microsoft.com/en-us/blog/agent-factory-from-prototype-to-production-developer-tools-and-rapid-agent-development/)\n- [Connecting agents, apps, and data with new open standards like MCP and A2A](https://azure.microsoft.com/en-us/blog/agent-factory-connecting-agents-apps-and-data-with-new-open-standards-like-mcp-and-a2a/)",
  "Title": "Agent Factory: Creating a blueprint for safe and secure AI agents",
  "Author": "Yina Arenas",
  "ProcessedDate": "2025-10-15 16:03:30"
}
