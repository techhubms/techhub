{
  "EnhancedContent": "The path from prototype to production for AI/ML workloads is rarely straightforward. As data pipelines expand and model complexity grows, teams can find themselves spending more time orchestrating distributed compute than building the intelligence that powers their products. Scaling from a laptop experiment to a production-grade workload still feels like reinventing the wheel. What if scaling AI workloads felt as natural as writing in Python itself? That’s the idea behind [Ray](https://www.ray.io/), the open-source distributed computing framework born at UC Berkeley’s RISELab, and now, it’s coming to Azure in a whole new way.\n\nToday, at Ray Summit, we announced a new partnership between Microsoft and [Anyscale](https://www.anyscale.com/), the company founded by Ray’s creators, to bring Anyscale’s managed Ray service to Azure as an Azure-native offering in private preview. This new managed experience will deliver the simplicity of Anyscale’s developer experience on top of Azure’s enterprise-grade Kubernetes infrastructure, making it possible to run distributed Python workloads with native integrations, unified governance, and streamlined operations, all inside your Azure subscription.\n\n### **Ray: Open-Source Distributed Computing for Python**\n\nRay reimagines distributed systems for the Python ecosystem, making it simple for developers to scale code from a single laptop to a large cluster with minimal changes. Instead of rewriting applications for distributed execution, Ray offers Pythonic APIs that allow functions and classes to be transformed into distributed tasks and actors without altering core logic. Its smart scheduling seamlessly orchestrates workloads across CPUs, GPUs, and heterogeneous environments, ensuring efficient resource utilization.\n\nDevelopers can also build complete AI systems using Ray’s native libraries—Ray Train for distributed training, Ray Data for data processing, Ray Serve for model serving, and Ray Tune for hyperparameter optimization—all fully compatible with frameworks like PyTorch and TensorFlow. By abstracting away infrastructure complexity, Ray lets teams focus on model performance and innovation.\n\n### **Anyscale: Enterprise Ray on Azure**\n\nRay makes distributed computing accessible; Anyscale running on Azure takes it to the next level for enterprise-readiness. At the heart of this offering is Anyscale Runtime, Anyscale’s high-performance runtime for Ray. Anyscale Runtime is designed to maximize cluster efficiency and accelerate Python workloads, enabling teams on Azure to:\n\n- Spin up Ray clusters in minutes, without Kubernetes expertise, directly from the Azure portal or CLI.\n- Dynamically allocate tasks across CPUs, GPUs, and heterogeneous nodes, ensuring efficient resource utilization and minimizing idle time.\n- Easily run large experiments quickly and cost-effectively with elastic scaling, GPU packing, and native support for Azure spot VMs.\n- Run reliably at production scale with automatic fault recovery, zero-downtime upgrades, and integrated observability.\n- Maintain control and governance; clusters run inside your Azure subscription, so data, models, and compute stay secure, with unified billing and compliance under Azure standards.\n\nBy combining Ray’s flexible APIs with Anyscale’s managed platform and runtime performance, Python developers can move from prototype to production faster, with less operational overhead, and at cloud scale on Azure.\n\n### **Kubernetes for Distributed Computing**\n\nUnder the hood, [Azure Kubernetes Service (AKS**)**](https://azure.microsoft.com/en-in/products/kubernetes-service/) powers this new managed offering, providing the infrastructure foundation for running Ray at production scale. AKS handles the complexity of orchestrating distributed workloads while delivering the scalability, resilience, and governance that enterprise AI applications require.\n\nAKS delivers:\n\n- Dynamic resource orchestration: Automatically provision and scale clusters across CPUs, GPUs, and mixed configurations as demand shifts.\n- High availability: Self-healing nodes and failover keep workloads running without interruption.\n- Elastic scaling: scale from development clusters to production deployments spanning hundreds of nodes.\n- Integrated Azure services: Native connections to Azure Monitor, Microsoft Entra ID, Blob Storage, and policy tools streamline governance across IT and data science teams.\n\nAKS gives Ray and Anyscale a strong foundation—one that’s already trusted for enterprise workloads and ready to scale from small experiments to global deployments.\n\n### **Enabling teams with Anyscale running on Azure**\n\nWith this partnership, Microsoft and Anyscale are bringing together the best of open-source Ray, managed cloud infrastructure, and Kubernetes orchestration. By pairing Ray’s distributed computing platform for Python with Anyscale’s management capabilities and AKS’s robust orchestration, Azure customers gain flexibility in how they can scale AI workloads. Whether you want to start small with rapid experimentation or run mission-critical systems at global scale, this offering gives you the choice to adopt distributed computing without the complexity of building and managing infrastructure yourself.\n\nYou can leverage Ray’s open-source ecosystem, integrate with Anyscale’s managed experience, or combine both with Azure-native services, all within your subscription and governance model. This optionality means teams can choose the path that best fits their needs: prototype quickly, optimize for cost and performance, or standardize for enterprise compliance.\n\nTogether, Microsoft and Anyscale are removing operational barriers and giving developers more ways to innovate with Python on Azure, so they can move faster, scale smarter, and focus on delivering breakthroughs. Read the full release [here](https://www.anyscale.com/press/anyscale-collaborates-with-microsoft-to-deliver-ai-native-computing-on-azure).\n\n### Get started\n\nLearn more about the private preview and how to request access at [https://aka.ms/anyscale](https://aka.ms/anyscale) or subscribe to Anyscale in the [Azure Marketplace.](https://marketplace.microsoft.com/en-us/product/saas/anyscale1750870039553.anyscale-2025-1?tab=Overview)",
  "Link": "https://devblogs.microsoft.com/all-things-azure/powering-distributed-aiml-at-scale-with-azure-and-anyscale/",
  "Author": "Brendan Burns",
  "FeedLevelAuthor": "Microsoft Azure Blog",
  "FeedName": "The Azure Blog",
  "FeedUrl": "https://azure.microsoft.com/en-us/blog/feed/",
  "ProcessedDate": "2025-11-21 20:03:27",
  "Tags": [
    "AI",
    "Azure AI",
    "Containers"
  ],
  "Title": "Powering Distributed AI/ML at Scale with Azure and Anyscale",
  "Description": "The path from prototype to production for AI/ML workloads is rarely straightforward. As data pipelines expand and model complexity grows, teams can find themselves spending more time orchestrating distributed compute than building the intelligence that powers their products. Scaling from a laptop experiment to a production-grade workload still feels like reinventing the wheel. What if scaling AI workloads felt as natural as writing in Python itself? That’s the idea behind Ray, the open-source distributed computing framework born at UC Berkeley’s RISELab, and now, it’s coming to Azure in a whole new way.\n\nThe post [Powering Distributed AI/ML at Scale with Azure and Anyscale](https://devblogs.microsoft.com/all-things-azure/powering-distributed-aiml-at-scale-with-azure-and-anyscale/) appeared first on [Microsoft Azure Blog](https://azure.microsoft.com/en-us/blog).",
  "PubDate": "2025-11-04T20:58:41+00:00",
  "OutputDir": "_news"
}
