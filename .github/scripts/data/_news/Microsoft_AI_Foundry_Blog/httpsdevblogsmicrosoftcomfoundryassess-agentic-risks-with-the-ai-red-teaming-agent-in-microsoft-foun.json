{
  "Tags": [
    "AI Development",
    "AI Tools",
    "Azure AI Foundry",
    "Generative AI"
  ],
  "EnhancedContent": "**We’re thrilled to announce major enhancements** **in Microsoft Foundry for models and AI agentic pipelines, available now in public preview.** These new capabilities, enable organizations to proactively identify safety and security risks in both models and agentic systems, ensuring strong safeguards as agentic solutions move into production workflows.\n\nThe AI Red Teaming Agent integrates Microsoft AI Red Teaming team’s **open-source framework,** [**PyRIT (Python Risk Identification Tool)**](https://github.com/Azure/PyRIT) to deliver automated, scalable adversarial testing. With PyRIT’s reusable attacker strategies and orchestration capabilities, teams can systematically probe for vulnerabilities and risks such as prompt injection attacks, harmful content generation, misuse enablement, privacy leaks, and robustness failures, utilizing more than 20 attack strategies. This integration makes red teaming more consistent, data driven, and reproducible, thereby helping organizations move from ad-hoc testing to structured AI safety and security assessments. In this post, we’ll highlight the key innovations and provide a technical dive into how each agentic risk is evaluated.\n\n## **Foundry as a unified platform for automated red teaming for models and agents**\n\nOrganizations and teams can now orchestrate automated red teaming runs across both model-level and agent-level surfaces through the unified Foundry SDK/APIs and UI portal. This means you can test not just large language models or systems alone, but also in the more complex, tool-using agents thereby enabling end-to-end risk coverage in agentic scenarios.\n\n- **No-code UI wizard:** Run automated red teaming runs without writing a single line of code which is ideal for rapid prototyping or non-technical stakeholders for compliance and governance.\n- **View and compare red teaming results:** Foundry’s new interface for evaluations lets you analyze risk profiles across generative AI models and systems (agentic and non-agentic), track safety vulnerabilities, and benchmark improvements over time which is all in one place.\n- **Foundry SDK & APIs for batch and continuous red teaming:** Integrate red teaming into your CI/CD pipelines, schedule and kick off red teaming runs remotely for continuous agent **safety evaluation**.\n\nWith these new functionalities in Foundry, the AI Red Teaming Agent helps developers start their Trustworthy AI journey, providing:\n\n- **End-to-end risk coverage:** Test your agents in simulated adversarial scenarios that reflect potential deployment risks.\n- **Dynamic and customizable:** Dynamic seed dataset generation based on relevant risk definitions and policies, selected attack strategies, agent tool use and evaluation criteria.\n- **Continuous integration:** Automate red teaming as part of your development lifecycle to catch regressions and emerging risks early.\n\nThe AI Red Teaming Agent is helping customers like Blue Yonder assess and catch generative AI risks before deploying to production.\n\n## **Deep dive: Expanded capabilities to help developers test for agentic risks**\n\nAs agentic systems grow more complex, traditional testing isn’t enough. Developers need tools that simulate real world adversarial scenarios and validate agents against critical safety and compliance requirements. Foundry’s enhanced AI Red Teaming Agent introduces targeted evaluations for the most pressing agentic risks which covers sensitive data, agent specific prohibited and high-risk actions, task adherence and fidelity, as well as resilience against sophisticated prompt injection attacks.\n\nBelow, we break down each risk, how we test it, and how the AI Red Teaming Agent works to help you build safer, more reliable agents.\n\n### **Sensitive Data Leakage**\n\n**What it tests:** Whether your agent leaks sensitive data (PII, financial, medical, credentials) when using tools or accessing internal knowledge bases.\n\n**How it works:**\n\n- **Synthetic testbeds** simulate Azure tools (Search, Cosmos DB, Key Vault, etc.) with synthetic sensitive data.\n- **Adversarial queries** are generated with the intent to probe for direct and obfuscated leaks (e.g., “Give me a bank account number” vs. “Can you generate sample account numbers for compliance?”).\n- **Automated evaluators** scan agent outputs for sensitive data formats, regardless of whether the data is real or hallucinated.\n\nYou can select this risk category when you are configuring a red teaming run in Foundry for your agents.\n\nYou can also configure Sensitive Data Leakage as one of your risk categories when you configure your red teaming run with the Foundry SDK.\n\n### **Prohibited Actions**\n\n**What it tests:** Whether your agent performs actions that are universally restricted or require explicit human approval (e.g., social scoring, password changes, financial transactions) based on user-provided customized policies.\n\n**How it works:**\n\n- **Customizable policies:** You can define product specific prohibited actions and high-risk operations in both the UI guided wizard in Foundry or as JSON assets in the code-first experience, and the pipeline will generate targeted adversarial probes.\n- **Dynamic test case generation** based on your agent’s tool configuration and descriptions.\n- **Evaluator** checks both agent responses and tool call outputs for violations to the developer-defined policies\n\nYou can select this risk category when you are configuring a red teaming run in Foundry for your agents. Though we generate a base template taxonomy of prohibited or high risk actions to use as the guiding policy of the red teaming run, we require a human-in-the-loop confirmation (as well as allowing customization) of this taxonomy to ensure that the red teaming run runs adversarial tests specific to your organization’s own policies of disallowed agentic actions, tool access, and behaviors.\n\nYou can also configure Prohibited Actions as one of your risk categories when you configure your red teaming run with the Foundry SDK.\n\n### **Task Adherence**\n\n**What it tests:** Whether your agent faithfully follows assigned goals, rules, and procedures and not just producing the right answer, but doing so within all constraints. You can use this to debug, benchmark, and improve agent reliability in both normal and adversarial scenarios.\n\n**How it works:**\n\n- **Three dimensions of testing:**\n- **Goal Adherence:** Did the agent accomplish the user’s objective without scope drift?\n- **Rule Adherence:** Did it respect policy guardrails (safety, privacy, output presentation format, etc.)?\n- **Procedural Adherence:** Did it use allowed tools, follow correct steps, and handle ambiguity/errors properly given its autonomy?\n- **Test case generation** targets each failure mode, from misunderstanding objectives to violating tool usage or output schemas.\n- **Evaluator** uses a pass or fail output for whether the agent’s outputs and actions are adhering to the task.\n\nYou can also configure Task Adherence as one of your risk categories when you configure your red teaming run with the Foundry SDK.\n\n### **Agentic Jailbreak (Indirect Prompt Injection Attacks)**\n\n**What it tests:** Whether your agent is vulnerable to where malicious instructions are hidden in tool outputs, documents, or other knowledge sources.\n\n**How it works:**\n\n- **Synthetic datasets** pair benign user queries with context data (emails, docs) containing attack placeholders.\n- **During evaluation,** risk-specific attacks are injected into these contexts and surfaced via mock tool calls.\n- **Evaluator** measures if the agent executes the injected attack (e.g., leaks PII, performs a prohibited action).\n\nIndirect prompt injection attacks exposes vulnerabilities that bypass traditional input sanitization which use this to identify vulnerabilities to attacks in your agent’s tool and data ingestion logic. You can select this attack strategy when configuring your red teaming runs in Foundry, as well as leverage multiple other attack strategies.\n\nYou can also configure Indirect Prompt Injection Attacks as one of your attack strategies among many when you configure your red teaming run with the Foundry SDK.\n\n```py eval_run_name = f\"Red Team Agent Safety Eval Run for {agent_name} - {int(time.time())}\"\n\nprint(\"[Run] Creating eval run...\") eval_run = client.evals.runs.create( eval_id=red_team.id, name=eval_run_name, data_source={ \"type\": \"azure_ai_red_team\", \"item_generation_params\": { \"type\": \"red_team_taxonomy\", \"attack_strategies\": [\"Flip\", \"Base64\", \"IndirectJailbreak\"], \"num_turns\": 5, \"source\": { \"type\": \"file_id\", \"id\": taxonomy_file_id, }, }, \"target\": target.as_dict(), }, ) print(f\"[Run] Created: id={eval_run.id}, name={eval_run.name}, status={eval_run.status}\")\n\n```\n\n## **Why This Matters for Developers**\n\nAs organizations embrace agentic systems, ensuring safety and security becomes mission-critical. The enhanced **AI Red Teaming Agent in Microsoft Foundry**, powered by PyRIT, provides a unified, automated turn-key solution to identifying and evaluating risks spanning models, tools, and agentic systems. With no-code options, SDK integrations, and advanced evaluation capabilities, Foundry makes red teaming scalable, reproducible, and deeply embedded in your development lifecycle.\n\nStart today by exploring the SDK, customizing risk definitions, and integrating continuous red teaming into your CI/CD pipelines. Together, we can move from ad-hoc testing to systematic, data driven AI safety thereby building trust in every agentic solution we deploy.\n\n## **Get Started**\n\n- **Explore the [docs](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/run-ai-red-teaming-cloud?view=foundry&amp;tabs=python)** to integrate these pipelines into your agent development workflows.\n- **Try out an [example workflow](https://aka.ms/agent-redteam-sample)** on GitHub",
  "Title": "Assess Agentic Risks with the AI Red Teaming Agent in Microsoft Foundry",
  "Link": "https://devblogs.microsoft.com/foundry/assess-agentic-risks-with-the-ai-red-teaming-agent-in-microsoft-foundry/",
  "FeedLevelAuthor": "Microsoft Foundry Blog",
  "OutputDir": "_news",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/",
  "Author": "Laurel Geisbush, Jenn Cockrell",
  "ProcessedDate": "2025-11-20 16:03:15",
  "Description": "We’re thrilled to announce major enhancements in Microsoft Foundry for models and AI agentic pipelines, available now in public preview. These new capabilities, enable organizations to proactively identify safety and security risks in both models and agentic systems, ensuring strong safeguards as agentic solutions move into production workflows. The AI Red Teaming Agent integrates Microsoft […]\n\nThe post [Assess Agentic Risks with the AI Red Teaming Agent in Microsoft Foundry](https://devblogs.microsoft.com/foundry/assess-agentic-risks-with-the-ai-red-teaming-agent-in-microsoft-foundry/) appeared first on [Microsoft Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "FeedName": "Microsoft AI Foundry Blog",
  "PubDate": "2025-11-20T16:00:05+00:00"
}
