{
  "PubDate": "2025-10-20T07:11:50+00:00",
  "FeedName": "Microsoft AI Foundry Blog",
  "FeedLevelAuthor": "Azure AI Foundry Blog",
  "ProcessedDate": "2025-10-20 08:03:09",
  "EnhancedContent": "Whether you‚Äôre a machine learning practitioner, app developer, or just curious about the latest in AI, this guide shows how you can quickly boost image classification accuracy using cutting-edge Vision-Language Models (VLM) on Azure‚Äîno deep learning expertise required.\n\nIn this walkthrough, you‚Äôll see how to **fine-tune GPT-4o on Azure OpenAI for image classification** using the Stanford Dogs dataset.\n\n[![Picture2 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAvEAAALxAQMAAADv9muiAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAXUlEQVR4nO3BMQEAAADCoPVPbQZ/oAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+AxpvAAEH8SqSAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/Picture2.png)\n\n*Illustrations of some of dogs‚Äô breeds images presented on the dataset.*\n\nWe‚Äôll use the Vision Fine-Tuning API and compare the results to a lightweight CNN baseline, so you can see the impact of modern Vision-Language Models versus traditional approaches.\n\nYou‚Äôll learn how to:\n\n- Prepare your data\n- Run batch inference\n- Fine-tune the model\n- Evaluate metrics\n- Weigh cost and latency trade-offs\n\nWant to try it yourself? All the scripts, notebooks, and experiment details are available in this [GitHub repository](https://github.com/azure-ai-foundry/fine-tuning/tree/main/Demos/Image_Breed_Classification_FT) so you can replicate or extend the project on your own.\n\nLet‚Äôs dive in.\n\n## What Is Image Classification and Why Is It Useful?\n\nComputer Vision has been a key field of Artificial Intelligence / Machine Learning (AI/ML) for decades. It enables many use-cases across various industries with tasks such as Optical Character Recognition (OCR) or image classification.\n\nLet‚Äôs focus on image classification, it can enable filtering, routing, and you might already using it on your daily applications without noticing.\n\nThe backbone of these models has been heavily based on Convolutional Neural Network (CNN) architecture and has been there since 1998 (LeNet). Since 2017 and the arrival of Large Language Models (LLMs), the field of AI/ML has completely evolved, leveraging new capabilities and enabling plenty of exciting use-cases.\n\nOne of the latest capabilities of these models has been the introduction of vision (image / video) input in addition to text. This new type of Vision Language Model (VLM) like latest GPT-5 models from OpenAI, aims to not only generate text from an input but understand vision input to generate text.\n\nThis has democratized access to computer vision while achieving great performance as these models have been trained on a large corpus of data, covering plethora of topics. Now, anyone can access to a VLM via a consumer app (e.g., ChatGPT, Le Chat, Claude) or via an API, upload an image (e.g., a picture of your dog), type the task you want the model to perform (e.g., ‚Äúwhat is the dog‚Äôs breed in the picture?‚Äù) and run it.\n\n## Getting Started: Choosing and Deploying Your Vision-Language Model on Azure\n\n[Azure AI Foundry](https://ai.azure.com/) lets you choose from thousands of models of any type (LLM, Embeddings, Voice) from our partners such as OpenAI, Mistral AI, Meta, Cohere, Hugging Face, etc.\n\nIn this post, we‚Äôll select Azure OpenAI GPT-4o (2024-08-06 model version) as our base model. This is also one of the models which supports both:\n\n1. Azure OpenAI Batch API (batch inference for half the price)\n2. Azure OpenAI Vision Fine-Tuning API (teach base model to perform better or learn a new specific task)\n\nWe‚Äôll first evaluate how behaves GPT-4o (base model) on our test set using the Batch API (for cost efficiency), then fine-tune it with the Vision Fine-Tuning API with the training and validation sets and compare metrics using the same test set.\n\n## Step 1: Run Cost-Effective Batch Inference with Azure OpenAI\n\nLet‚Äôs measure performance of the GPT-4o on the Stanford Dogs Dataset. This dataset contains thousands of dogs‚Äô images across 120 breeds. For the sake of cost management, we‚Äôll down sampled this dataset and only keep 50 images per breed with the following split: 40 train / 5 validation / 5 test.\n\nWith the following ratio, our dataset contains:\n\n- Train set (4,8k images)\n- Validation set (600 images)\n- Test set (600 images)\n\nTo send our requests to Batch API, we must format them in a strict JSONL format. Below is an example of a line within the JSONL:\n\n```json {\"model\": ‚Äúgpt-4o-batch‚Äù, \"messages\": [ {\"role\": \"system\", \"content\":\"Classify the following input image into one of the following categories: [Affenpinscher, Afghan Hound, ... , Yorkshire Terrier].\" }, {\"role\": \"user\", \"content\": [ {\"type\": \"image_url\", \"image_url\": {\"url\": \"b64\", \"detail\": \"low\"}} ]} ]} ```\n\nThis JSONL contains:\n\n- model deployment name (here a gpt-4o 2024-08-06 deployed for batch inference)\n- system message (here is the image classification task described with list of potential breeds)\n- user input (here is the dog‚Äôs image encoded in base64, with low detail resolution to be cost effective)\n\n*This JSONL intentionally doesn‚Äôt include the full list of parameters such as response\\_format. If you want to have more details, please have a look at the [associated repository](https://github.com/azure-ai-foundry/fine-tuning/tree/main/Demos/Image_Breed_Classification_FT).*\n\nWe‚Äôve removed the actual dog‚Äôs breed (the output) as we want the model to run inference on these requests so then we can compare with each actual breed to evaluate its performance.\n\nThe Batch API has the benefit of processing your requests while being 50% cheaper than usual base inference, with the tradeoff of getting the model‚Äôs response within 24 hours. Please note that this is best effort and there is no guarantee from the Azure OpenAI Service that you can retrieve model‚Äôs response within 24 hours in 100% cases.\n\nAfter having waited 15 minutes, the Batch API returned an output JSONL that contains for each line the model‚Äôs response.\n\n![Picture1 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAAJYAQMAAABValnGAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAW0lEQVR4nO3BMQEAAADCoPVPbQsvoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA4GcUnwABlzvAgQAAAABJRU5ErkJggg==)\n\n*Azure AI Foundry Batch job details.*\n\nNow, let‚Äôs extract the Batch API output response and compute the performance of the base model against the test set that will become our baseline.\n\n## Step 2: Fine-Tune GPT-4o for Your Images Using the Vision API\n\nFine-Tuning aims to post-train the model with new data that hasn‚Äôt been used during its initial training to make the model learn new knowledge, improve performance on certain tasks, or emphasis on a specific tone.\n\n[![Picture3 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAAEQAQMAAACHictaAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAANklEQVR4nO3BMQEAAADCoPVPbQsvoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+Bn1gAAFTZONpAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/Picture3.png)\n\nThis process can lead to better performance, decreasing latency and may be cost-effective as you might send less tokens to the fine-tuned model to set its guidelines.\n\nAzure OpenAI enables Fine-Tuning among different models and with different alignment techniques such as Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO) and Reinforced Fine-Tuning (RFT).\n\nIn 2024, we introduced Vision Fine-Tuning, that takes image and text as inputs and passes over following hyperparameters which you can control (epochs, batch size, learning rate multiplier, etc.).\n\nFine-tuning job pricing differs from base model inference due to couple of factors:\n\n- total tokens used during training job (number of tokens in the train/validation datasets multiplied by number of epochs)\n- endpoint hosting (priced per hour)\n- inference (input/outputs tokens)\n\nIn the repo, we‚Äôve taken a gpt-4o (2024-08-06 model version) and have constructed training and validation datasets in JSONL format. It follows supervised fine-tuning (SFT) technique where it uses input/output pairs to fine-tune the model.\n\nLet‚Äôs have a look to a single line of the training set JSONL:\n\n```json {\"messages\": [ {\"role\": \"system\", \"content\": \"Classify the following input image into one of the following categories: [Affenpinscher, Afghan Hound, ... , Yorkshire Terrier].\"}, {\"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": \"data:image/jpeg;base64,<encoded_springer_spaniel_image_in_base64>\", \"detail\": \"low\"}}]}, {\"role\": \"assistant\", \"content\": \"Springer Spaniel\"} ]} ```\n\nHere we‚Äôve selected the following hyper-parameters for fine-tuning:\n\n- **Batch size: 6** (how many examples are processed per training step)\n- **Learning rate: 0.5** (adjusts how quickly the model learns during training)\n- **Epochs: 2** (number of times the model trains on the entire dataset)\n- **Seed: 42** (ensures training results are reproducible when using the same settings)\n\nSee more on hyper-parameters recommendations [here](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tuning?context=%2Fazure%2Fai-foundry%2Fcontext%2Fcontext&amp;tabs=azure-openai&amp;pivots=programming-language-studio#configure-training-parameters-optional).\n\nOnce completed, the fine-tuning job returns a result file that contains data points such as training loss per step. These data points can also be viewed through the Azure AI Foundry portal (see below screenshots).\n\n[![Picture4 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAAMfAQMAAADhQGimAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAcklEQVR4nO3BMQEAAADCoPVPbQsvoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD4GnBZAAH6BNbyAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/Picture4.png)\n\n*Azure AI Foundry Fine-Tuning job details*\n\n![Picture5 image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6gAAAH6AQMAAAAwVvO+AAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAUUlEQVR4nO3BMQEAAADCoPVPbQsvoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAICnAek8AAGMcsK1AAAAAElFTkSuQmCC)\n\n*Azure AI Foundry Fine-Tuning job metrics*\n\n## Step 3: Compare Against a Classic CNN Baseline\n\nTo provide another reference point, we trained a lightweight Convolutional Neural Network (CNN) on the same subset of the Stanford Dogs dataset used for the VLM experiments. This baseline is **not** meant to be state-of-the-art; its role is to show what a conventional, task-specific model can achieve with a relatively small architecture and limited training, compared to a large pre-trained Vision-Language Model.\n\nThis baseline reached a mean accuracy of 61.67% on the test set (see the Comparison section below for numbers alongside GPT-4o base and fine-tuned). It trained in less than 30 minutes and can easily run locally or in Azure Machine Learning.\n\n## Results at a Glance: Accuracy, Latency, and Cost\n\nAfter having run our experimentations, let‚Äôs compare these 3 models (base VLM, fine-tuned VLM, lightweight CNN) on key metrics such as accuracy, latency and cost. Find below a table that consolidates these metrics.\n\n| **Aspect** | **Base gpt-4o (zero-shot)** | **Fine-Tuned gpt-4o** | **CNN Baseline** | | --- | --- | --- | --- | | Mean accuracy (more is better) | 73.67% | **82.67% (+9.0 pp vs base)** | 61.67% (-12.0 pp vs base) | | Mean latency (less is better) | 1665ms | **1506ms (-9.6%)** | ‚Äî (not benchmarked here) | | Cost (less is better) | Inference costs only $$ | Training + Hosting + Inference $$$ | **Local infra $** |\n\n### Accuracy\n\nThe fine-tuned GPT-4o model achieved a substantial boost in mean accuracy, reaching **82.67 %** compared to **73.67 %** for the zero-shot base model and **61.67 %** for the lightweight CNN baseline.\n\nThis shows how even a small amount of domain-specific fine-tuning on Azure OpenAI can significantly close the gap to a specialized classifier.\n\n[![accuracy base vs ft models image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABKUAAAJOAQMAAABcICZzAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAbElEQVR4nO3BAQ0AAADCoPdP7ewBFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABwA1nDAAFbzKUoAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/accuracy_base_vs_ft_models.png)\n\n### Latency\n\nInference with the base GPT-4o took on average **1.67 s per image**, whereas the fine-tuned model reduced that to **1.51 s** (around **9.6 % faster**). This slight improvement reflects better task alignment. The CNN baseline was not benchmarked for latency here, but on commodity hardware it typically returns predictions in **tens of milliseconds** per image, which can be advantageous for ultra-low-latency scenarios.\n\n[![latency histogram image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABLAAAALuAQMAAACEu30xAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAhElEQVR4nO3BAQEAAACCIP+vbkhAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMCjAbpxAAFs28ZhAAAAAElFTkSuQmCC)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/latency_histogram.png)\n\n### Cost\n\nUsing the **Azure OpenAI Batch API** for the base model only incurs inference costs (at a **50 % discount** vs. synchronous calls).\n\nFine-tuning introduces additional costs for **training and hosting the new model** plus inference, but can be more economical in the long run when processing very large batches with higher accuracy. Here, the fine-tuning training job has costed **$152** and inference is **10% more expensive** than the base model across input, cached input, and output.\n\nThe CNN baseline can be hosted locally or on inexpensive Azure Machine Learning compute; it has **low inference cost** but requires more effort to train and maintain (data pipelines, updates, and infrastructure).\n\n## Key takeaways\n\nAcross all three models, the results highlight a clear trade-off between accuracy, latency, and cost.\n\n- Fine-tuning GPT-4o on Azure OpenAI **produced the highest accuracy** while also slightly reducing latency.\n- The zero-shot GPT-4o base model **requires no training and is the fastest path to production,** but with lower accuracy.\n- The lightweight CNN offers a low-cost, low-infrastructure option, yet its accuracy lags far behind and it demands more engineering effort to train and maintain.\n\n## Next Steps: How to Apply This in Your Own Projects\n\nThis walkthrough demonstrated how you can take a pre-trained Vision-Language Model (GPT-4o) on [Azure AI Foundry](https://ai.azure.com/), fine-tune it with your own labeled images using the Vision Fine-Tuning API, and benchmark it against both zero-shot performance and a traditional CNN baseline.\n\nBy combining Batch API inference for cost-efficient evaluation with Vision Fine-Tuning for task-specific adaptation, you can unlock higher accuracy and better latency without building and training large models from scratch.\n\nIf you‚Äôd like to access more insights on the comparison, replicate or extend this experiment, check out the [GitHub repository](https://github.com/azure-ai-foundry/fine-tuning/tree/main/Demos/Image_Breed_Classification_FT) for code, JSONL templates, and evaluation scripts.\n\nFrom here, you can:\n\n- Try other datasets or tasks (classification, OCR, multimodal prompts).\n- Experiment with different fine-tuning parameters.\n- Integrate the resulting model into your own applications or pipelines.\n\nWith Azure AI Foundry, you can move from prototype to production quickly while taking advantage of a diverse model catalog in [Azure AI Foundry Models](https://azure.microsoft.com/en-us/products/ai-foundry/models/?msockid=198f34c7ab4366b01e4122f1aae467c1#Models), managed infrastructure, and enterprise-grade security.\n\n### Learn More\n\n‚ñ∂Ô∏èRegister for Ignite‚Äôs [AI fine-tuning in Azure AI Foundry to make your agents unstoppable](https://azure.microsoft.com/en-us/blog/announcing-new-fine-tuning-models-and-techniques-in-azure-ai-foundry/?msockid=198f34c7ab4366b01e4122f1aae467c1)\n\nüë©‚ÄçüíªExplore‚ÄØ[fine-tuning with Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/fine-tuning-overview) documentation\n\nüëãContinue the conversation [on Discord](https://aka.ms/model-mondays/discord)",
  "Tags": [
    "Azure AI Foundry",
    "azure-openai",
    "Batch API",
    "fine-tuning",
    "Image Classification"
  ],
  "Link": "https://devblogs.microsoft.com/foundry/a-developers-guide-to-fine-tuning-gpt-4o-for-image-classification-on-azure-ai-foundry/",
  "Description": "Whether you‚Äôre a machine learning practitioner, app developer, or just curious about the latest in AI, this guide shows how you can quickly boost image classification accuracy using cutting-edge Vision-Language Models (VLM) on Azure‚Äîno deep learning expertise required. In this walkthrough, you‚Äôll see how to fine-tune GPT-4o on Azure OpenAI for image classification using the [‚Ä¶]\n\nThe post [A Developer‚Äôs Guide to Fine-Tuning GPT-4o for Image Classification on Azure AI Foundry](https://devblogs.microsoft.com/foundry/a-developers-guide-to-fine-tuning-gpt-4o-for-image-classification-on-azure-ai-foundry/) appeared first on [Azure AI Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/",
  "OutputDir": "_news",
  "Title": "A Developer‚Äôs Guide to Fine-Tuning GPT-4o for Image Classification on Azure AI Foundry",
  "Author": "Alexandre Levret"
}
