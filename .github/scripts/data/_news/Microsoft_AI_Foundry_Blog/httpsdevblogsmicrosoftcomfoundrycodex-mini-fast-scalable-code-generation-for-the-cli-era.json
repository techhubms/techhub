{
  "Link": "https://devblogs.microsoft.com/foundry/codex-mini-fast-scalable-code-generation-for-the-cli-era/",
  "ProcessedDate": "2025-08-24 16:29:13",
  "PubDate": "2025-06-20T16:23:00+00:00",
  "FeedLevelAuthor": "Azure AI Foundry Blog",
  "Title": "codex-mini: Fast, Scalable Code Generation for the CLI Era",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/",
  "EnhancedContent": "We‚Äôre announcing the general availability of Azure OpenAI‚Äôs codex-mini in Azure AI Foundry Models.\n\ncodex-mini is a fine-tuned version of the o4-mini model, designed to deliver rapid, instruction-following performance for developers working in CLI workflows. Whether you‚Äôre automating shell commands, editing scripts, or refactoring repositories, codex-mini brings speed, precision, and scalability to your terminal.\n\n[https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/06/Video.webm](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/06/Video.webm)\n\n### Why codex-mini?\n\n- **‚ö°** **Optimized for Speed:**‚ÄØDelivers fast Q&A and code edits with minimal overhead.\n\n- **üß†** **Instruction-Following:**‚ÄØRetains Codex-1‚Äôs strengths in understanding natural language prompts.\n\n- **üñ•Ô∏è** **CLI-Native:**‚ÄØInterprets natural language and returns shell commands or code snippets.\n\n- **üìè** **Long Context:**‚ÄØSupports up to 200k-token inputs‚Äîideal for full repo ingestion and refactoring.\n\n- **üîß** **Lightweight and Scalable:**‚ÄØDesigned for cost-efficient deployment with a small capacity footprint.\n\n- üí∏ **Cost-Efficient**: Approximately 25% more cost-effective than GPT-4.1 when using a similar mix of input and output tokens.\n\n- üîó **Codex CLI Compatible**: Seamlessly integrates with codex-cli for streamlined developer workflows.\n\n[![Pricing per 1M tokens](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAASwAAABfAQMAAABocmjtAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAGklEQVRIie3BMQEAAADCoPVPbQo/oAAAAOBoDnkAATy6o/IAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/06/WhatsApp-Image-2025-06-19-at-9.01.26-PM-1.jpeg)Pricing per 1M tokens\n\nFor full pricing details across all Azure OpenAI models, visit the [Azure OpenAI pricing](https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/).\n\n### How to use Azure OpenAI codex-mini model with OpenAI Codex CLI\n\nFor a hands-on walkthrough of deploying and using Codex Mini in real-world engineering workflows, [check out this detailed guide by Govind Kamtamneni](https://devblogs.microsoft.com/all-things-azure/securely-turbo%E2%80%91charge-your-software-delivery-with-the-codex-coding-agent-on-azure-openai/). His blog post covers everything from CLI setup to GitHub Actions integration‚Äîall within the secure, enterprise-grade Azure environment.\n\nWe‚Äôve been contributing to OpenAI‚Äôs Codex CLI in open source. OpenAI has accepted our pull request, and we are awaiting [final review](https://github.com/openai/codex/pull/1321/).\n\nLearn more and get started at [codex-mini ‚Äì Azure AI Foundry](https://ai.azure.com/resource/models/codex-mini/version/2025-05-16/registry/azure-openai?wsid=/subscriptions/41c843d0-e633-4f0e-8059-0deee6deb387/resourceGroups/erinrg-deleteable/providers/Microsoft.CognitiveServices/accounts/egeaney0527-resource/projects/egeaney0527&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47)\n\n### Get Access\n\nCodex-mini is available in East US2 and Sweden Central, supporting Standard Global deployment. To build with codex-mini, use the [v1 preview API](https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-lifecycle?tabs=key#next-generation-api).\n\n### Key Capabilities\n\ncodex-mini supports features such as streaming, function calling, structured outputs, and image input. With these capabilities in mind, developers can leverage codex-mini for a range of fast, scalable code generation tasks in command-line environments.\n\ncodex-mini is now available via the Azure OpenAI API and Codex CLI. For developers seeking fast, reliable code generation in terminal environments, this purpose-built model offers a powerful new tool in your AI toolkit for fast, low-latency code generation in command-line environments.",
  "Tags": [
    "Azure AI Foundry"
  ],
  "Description": "We‚Äôre announcing the general availability of Azure OpenAI‚Äôs codex-mini in Azure AI Foundry Models. codex-mini is a fine-tuned version of the o4-mini model, designed to deliver rapid, instruction-following performance for developers working in CLI workflows. Whether you‚Äôre automating shell commands, editing scripts, or refactoring repositories, codex-mini brings speed, precision, and scalability to your terminal. Why codex-mini? ‚ö° Optimized for Speed:‚ÄØDelivers fast Q&A and code [‚Ä¶]\n\nThe post [codex-mini: Fast, Scalable Code Generation for the CLI Era](https://devblogs.microsoft.com/foundry/codex-mini-fast-scalable-code-generation-for-the-cli-era/) appeared first on [Azure AI Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "OutputDir": "_news",
  "FeedName": "Microsoft AI Foundry Blog",
  "Author": "Ananya Bishnoi, Anthony Mocny, Govind Kamtamneni"
}
