{
  "PubDate": "2025-09-16T15:00:45+00:00",
  "OutputDir": "_news",
  "Tags": [
    "Azure AI Foundry"
  ],
  "Link": "https://devblogs.microsoft.com/foundry/announcing-computer-use-tool-preview-in-azure-ai-foundry-agent-service/",
  "Author": "Linda Li",
  "Title": "Announcing Computer Use tool (Preview) in Azure AI Foundry Agent Service",
  "FeedLevelAuthor": "Azure AI Foundry Blog",
  "Description": "Overview We are excited to announce Computer Use—are now available in preview in Azure AI Foundry Agent Service. It brings feature parity with the Azure OpenAI Responses API, but with the added advantage of seamless integration into the Foundry agent runtime and enterprise security. With this release, developers can create agents that not only reason […]\n\nThe post [Announcing Computer Use tool (Preview) in Azure AI Foundry Agent Service](https://devblogs.microsoft.com/foundry/announcing-computer-use-tool-preview-in-azure-ai-foundry-agent-service/) appeared first on [Azure AI Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "ProcessedDate": "2025-09-16 15:12:11",
  "EnhancedContent": "## Overview\n\nWe are excited to announce **Computer Use**—are now available in preview in **Azure AI Foundry Agent Service**. It brings feature parity with the Azure OpenAI Responses API, but with the added advantage of seamless integration into the Foundry agent runtime and enterprise security. With this release, developers can create agents that not only reason over text, retrieve knowledge, or call APIs, but also directly interact with computer interfaces through natural language instructions. At launch, it is accessible through **REST API and SDK**, giving developers the flexibility to embed them directly into their applications, pipelines, and automation workflows.\n\n[![computer use preview image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAB4AAAAQ4AQMAAADSHVMAAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABE0lEQVR4nO3BAQ0AAADCoPdPbQ8HFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwKcB+OUAAZkliQ8AAAAASUVORK5CYII=)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/09/computer-use-preview.gif)\n\nBy adding Computer Use, Foundry Agent Service expands the scope of what an agent can achieve in multi-step conversations. If you want to navigate complex enterprise applications, it opens the door to new, creative, and highly practical scenarios for enterprise AI.\n\n## Use cases\n\nComputer Use addresses distinct, high-value scenarios. Computer Use allows agents to interact with applications in the same way a human user would. It enables agents that not only reason and respond but also take meaningful actions across digital environments.\n\n**Computer Use**\n\n- **Web & desktop automation:** Fill out forms, upload/download artifacts, or complete tasks in apps without APIs.\n\n- **Operational copilots:** Help employees triage tickets or manage workflows across multiple enterprise dashboards.\n\n- **Legacy integration:** Interact with older desktop apps by simulating clicks and keystrokes.\n\n- **Human-in-the-loop workflows:** Require users to approve sensitive or high-risk steps before they are executed.\n\n## How it works\n\nComputer Use operates as a continuous loop of suggested actions, execution, and screenshot feedback.\n\n**Computer Use**\n\n- **Action loop:** The agent requests actions (e.g., click, type, screenshot) from the computer-use-preview model.\n\n- **Execution environment:** Your code performs the action in a controlled environment (browser or desktop) and captures a screenshot.\n\n- **Screenshot feedback:** The screenshot is sent back to the model to inform the next step.\n\n- **Pixel-based reasoning:** Unlike Browser Automation, Computer Use interprets **raw pixels**, enabling adaptation to unfamiliar or dynamic UIs.\n\n- **Safety checks:** Malicious instructions, irrelevant domains, or sensitive domains trigger warnings that require human acknowledgment before proceeding.\n\nCompared with Browser Automation, Computer Use offers richer visualization, broader cross-application support, and more human-like interaction patterns. However, because of its power, we strongly recommend using it only on **low-privilege virtual machines** that do not contain sensitive data or credentials.\n\n| | **Browser Automation** | **Computer Use** | | --- | --- | --- | | Model support | Most models supported by Foundry Agent Service | Computer-use-preview model | | Visualize whats happening | No | Yes | | How it understands the screen | Parse the HTML or XML pages into DOM documents | Raw pixel data from screenshots | | How it acts | A list of actions provided by the model | Virtual keyword and mouse | | Multi-step? | Yes | Yes | | Interfaces | Browser | Computer and browser | | Can I bring my own resource | BYO Playwright resource and store keys in connection | No resource required, we recommend using **low-privilege virtual machines** |\n\n## Security and responsible use\n\nWARNING:\n\nThe Computer Use tool comes with significant security and privacy risks, including prompt injection attacks. Learn more about intended uses, capabilities, limitations, risks, and considerations when choosing a use case in the[Azure OpenAI transparency note](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/transparency-note?tabs=image%22).\n\n## Code samples\n\n```py\n# pylint: disable=line-too-long,useless-suppression\n# ------------------------------------\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n# ------------------------------------\n\n\"\"\" DESCRIPTION: This sample demonstrates how to use agent operations with the Computer Use tool (preview) using a synchronous client. This sample uses fake screenshot to demonstrate how output actions work, but the actual implementation would involve mapping the output action types to their corresponding API calls in the user's preferred managed environment framework (e.g. Playwright or Docker).\n\nNOTE: Usage of the computer-use-preview model currently requires approval. Please see https://learn.microsoft.com/azure/ai-foundry/openai/how-to/computer-use for more information.\n\nUSAGE: python sample_agents_computer_use.py\n\nBefore running the sample:\n\npip install azure-ai-agents --pre pip install azure-ai-projects azure-identity\n\nSet these environment variables with your own values: 1) PROJECT_ENDPOINT - The Azure AI Project endpoint, as found in the Overview page of your Azure AI Foundry portal. 2) MODEL_DEPLOYMENT_NAME - The deployment name of the AI model, as found under the \"Name\" column in the \"Models + endpoints\" tab in your Azure AI Foundry project.\n\nOptional:\n- To target a specific environment, set COMPUTER_USE_ENVIRONMENT to one of: windows, mac, linux, browser\nOtherwise defaults to 'browser'. \"\"\"\n\nimport os, time, base64 from typing import List from azure.ai.agents.models._models import ComputerScreenshot, TypeAction from azure.ai.projects import AIProjectClient from azure.ai.agents.models import ( MessageRole, RunStepToolCallDetails, RunStepComputerUseToolCall, ComputerUseTool, ComputerToolOutput, MessageInputContentBlock, MessageImageUrlParam, MessageInputTextBlock, MessageInputImageUrlBlock, RequiredComputerUseToolCall, SubmitToolOutputsAction, ) from azure.identity import DefaultAzureCredential def image_to_base64(image_path: str) -> str: \"\"\" Convert an image file to a Base64-encoded string.\n\n:param image_path: The path to the image file (e.g. 'image_file.png') :return: A Base64-encoded string representing the image. :raises FileNotFoundError: If the provided file path does not exist. :raises OSError: If there's an error reading the file. \"\"\" if not os.path.isfile(image_path): raise FileNotFoundError(f\"File not found at: {image_path}\")\n\ntry: with open(image_path, \"rb\") as image_file: file_data = image_file.read() return base64.b64encode(file_data).decode(\"utf-8\") except Exception as exc: raise OSError(f\"Error reading file '{image_path}'\") from exc asset_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../assets/cua_screenshot.jpg\")) action_result_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../assets/cua_screenshot_next.jpg\")) project_client = AIProjectClient(endpoint=os.environ[\"PROJECT_ENDPOINT\"], credential=DefaultAzureCredential())\n\n# Initialize Computer Use tool with a browser-sized viewport\nenvironment = os.environ.get(\"COMPUTER_USE_ENVIRONMENT\", \"windows\") computer_use = ComputerUseTool(display_width=1026, display_height=769, environment=environment)\n\nwith project_client:\n\nagents_client = project_client.agents\n\n# Create a new Agent that has the Computer Use tool attached.\nagent = agents_client.create_agent( model=os.environ[\"MODEL_DEPLOYMENT_NAME\"], name=\"my-agent-computer-use\", instructions=\"\"\" You are an computer automation assistant. Use the computer_use_preview tool to interact with the screen when needed. \"\"\", tools=computer_use.definitions, )\n\nprint(f\"Created agent, ID: {agent.id}\")\n\n# Create thread for communication\nthread = agents_client.threads.create() print(f\"Created thread, ID: {thread.id}\")\n\ninput_message = ( \"I can see a web browser with bing.com open and the cursor in the search box.\" \"Type 'movies near me' without pressing Enter or any other key. Only type 'movies near me'.\" ) image_base64 = image_to_base64(asset_file_path) img_url = f\"data:image/jpeg;base64,{image_base64}\" url_param = MessageImageUrlParam(url=img_url, detail=\"high\") content_blocks: List[MessageInputContentBlock] = [ MessageInputTextBlock(text=input_message), MessageInputImageUrlBlock(image_url=url_param), ]\n# Create message to thread\nmessage = agents_client.messages.create(thread_id=thread.id, role=MessageRole.USER, content=content_blocks) print(f\"Created message, ID: {message.id}\")\n\nrun = agents_client.runs.create(thread_id=thread.id, agent_id=agent.id) print(f\"Created run, ID: {run.id}\")\n\n# create a fake screenshot showing the text typed in\nresult_image_base64 = image_to_base64(action_result_file_path) result_img_url = f\"data:image/jpeg;base64,{result_image_base64}\" computer_screenshot = ComputerScreenshot(image_url=result_img_url)\n\nwhile run.status in [\"queued\", \"in_progress\", \"requires_action\"]: time.sleep(1) run = agents_client.runs.get(thread_id=thread.id, run_id=run.id)\n\nif run.status == \"requires_action\" and isinstance(run.required_action, SubmitToolOutputsAction): print(\"Run requires action:\") tool_calls = run.required_action.submit_tool_outputs.tool_calls if not tool_calls: print(\"No tool calls provided - cancelling run\") agents_client.runs.cancel(thread_id=thread.id, run_id=run.id) break\n\ntool_outputs = [] for tool_call in tool_calls: if isinstance(tool_call, RequiredComputerUseToolCall): print(tool_call) try: action = tool_call.computer_use_preview.action print(f\"Executing computer use action: {action.type}\") if isinstance(action, TypeAction): print(f\" Text to type: {action.text}\")\n# (add hook to input text in managed environment API here)\n\ntool_outputs.append( ComputerToolOutput(tool_call_id=tool_call.id, output=computer_screenshot) ) if isinstance(action, ComputerScreenshot): print(f\" Screenshot requested\")\n# (add hook to take screenshot in managed environment API here)\n\ntool_outputs.append( ComputerToolOutput(tool_call_id=tool_call.id, output=computer_screenshot) ) except Exception as e: print(f\"Error executing tool_call {tool_call.id}: {e}\")\n\nprint(f\"Tool outputs: {tool_outputs}\") if tool_outputs: agents_client.runs.submit_tool_outputs(thread_id=thread.id, run_id=run.id, tool_outputs=tool_outputs)\n\nprint(f\"Current run status: {run.status}\")\n\nprint(f\"Run completed with status: {run.status}\") if run.status == \"failed\": print(f\"Run failed: {run.last_error}\")\n\n# Fetch run steps to get the details of the agent run\nrun_steps = agents_client.run_steps.list(thread_id=thread.id, run_id=run.id) for step in run_steps: print(f\"Step {step.id} status: {step.status}\") print(step)\n\nif isinstance(step.step_details, RunStepToolCallDetails): print(\" Tool calls:\") run_step_tool_calls = step.step_details.tool_calls\n\nfor call in run_step_tool_calls: print(f\" Tool call ID: {call.id}\") print(f\" Tool call type: {call.type}\")\n\nif isinstance(call, RunStepComputerUseToolCall): details = call.computer_use_preview print(f\" Computer use action type: {details.action.type}\")\n\nprint() # extra newline between tool calls\n\nprint() # extra newline between run steps\n\n# Optional: Delete the agent once the run is finished.\nagents_client.delete_agent(agent.id) print(\"Deleted agent\") ```\n\n## Getting started\n\nTo start using Computer Use in Foundry Agent Service:\n\n- **Request access**\n\n- - [computer-use-preview for Computer Use](https://aka.ms/oai/cuaaccess) (available in **East US 2**, **Sweden Central**, and **South India**)\n\n- **Create deployments** in your Azure OpenAI resource once access is approved.\n\n- **Configure your agent** through REST API or SDK:\n\n- - *Computer Use*: specify environment (browser, windows, mac, ubuntu) → implement the action loop (execute actions, send screenshots, continue until complete).\n\n- **Apply best practices**\n\n- - Run Computer Use agents only on **low-privilege, isolated machines**.\n\n**Learn more**\n\n- 📘[Computer Use tool in Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/computer-use)\n\n- 📘 [Transparency notes for action tools](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/agents/transparency-note)\n\nWith these steps and resources, you can quickly enable agents that combine text reasoning, image creation, and real-world computer interaction.",
  "FeedName": "Microsoft AI Foundry Blog",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/"
}
