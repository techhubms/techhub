{
  "Description": "Azure AI Foundry makes fine-tuning smarter, faster, and more accessible than ever. Whether you’re building agents that reason, tools that adapt, or workflows that scale, this is your launchpad for customizing models to solve real business challenges. Dive in to discover best practices, hands-on resources, and the latest innovations so you can build, test, and […]\n\nThe post [The Developer’s Guide to Smarter Fine-tuning: Unlock custom AI for every business challenge](https://devblogs.microsoft.com/foundry/the-developers-guide-to-smarter-fine-tuning/) appeared first on [Azure AI Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/",
  "OutputDir": "_news",
  "PubDate": "2025-10-14T19:01:20+00:00",
  "EnhancedContent": "[![DevBlog FT Banner image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAB2IAAAQhAQMAAADF090fAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAABCklEQVR4nO3BMQEAAADCoPVP7W0HoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIA31tsAAa1kC4YAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/DevBlog-FT-Banner.png)[Azure AI Foundry](https://ai.azure.com/) makes fine-tuning smarter, faster, and more accessible than ever. Whether you’re building agents that reason, tools that adapt, or workflows that scale, this is your launchpad for customizing models to solve real business challenges. Dive in to discover best practices, hands-on resources, and the latest innovations so you can build, test, and deploy specialized AI with confidence.\n\n#### What is Fine-tuning?\n\nFine-tuning refers to customizing a pre-trained LLM with additional training on a specific task or new dataset for enhanced performance, new skills, or improved accuracy. So instead of building an AI model from scratch, you take a powerful pre-trained model and give it extra training using your own examples, and it learns your style, your needs, and your domain.\n\n[![Data Set Image image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA6AAAAEzAQMAAAAVZvw2AAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAOklEQVR4nO3BMQEAAADCoPVPbQdvoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACA0wCMTwAB6wo8zQAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/Data-Set-Image.png)\n\n#### Fine-tune for Specialized AI Tasks\n\nFoundation models are powerful, but they’re generalists by design. When precision matters, especially in domain-specific applications, **fine-tuning is the unlock.** It lets teams shape models to their exact needs, boosting accuracy, slashing latency, and reducing inference costs. Whether you’re building tools for legal contract analysis, conversational agents that understand nuance, or wealth advisory copilots that deliver tailored insights, fine-tuning turns a capable model into a specialized expert. It’s not just about better answers, it’s about delivering *relevant, reliable, and ready-to-deploy* intelligence.\n\n**o4-mini RFT Wealth Advisory Demo**\n\n##### **Use Case: From Generalist to Expert**\n\nWhen preparing for a trip to Patagonia, one common concern is whether a sleeping bag will be warm enough for the conditions: “w*ill my sleeping bag work for Patagonia?”* becomes a perfect test case for fine-tuning. By combining **user input**, **prompt engineering**, **and RAG, the** model evolves from giving vague travel advice to delivering a precise, context-aware recommendation.\n\nThink of the LLM as a language calculator: user input is the raw data, prompt engineering defines the operation, and fine-tuning adds the specialized logic. The result? A model that factors in regional weather, seasonal temperature ranges, and gear specs to give an answer that was not just helpful but actionable.\n\n[![LLM Language Calculator image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABEkAAAG+AQMAAABVhMgIAAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAAU0lEQVR4nO3BMQEAAADCoPVPbQlPoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAF4G8ioAAQ0G/fcAAAAASUVORK5CYII=)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/LLM-Language-Calculator.png)\n\n#### Best Practices in the Agentic Era\n\nDone right, fine-tuning is a superpower. Done wrong, it’s a liability. A robust, iterative approach, grounded in clear objectives, high-quality data, and continuous feedback, is essential for safe, effective, and innovative agent deployments.\n\n- **Start with Clear Objectives**: Define the specific use case, tasks and outcomes you want your model to achieve. Common use cases include reducing prompt length, teaching new skills, improving tool use, and domain adaptation. Consider vertical use cases such as improving dialect/tone responses, customer specific knowledge inclusion, or natural language to code applications.\n- **Invest in Data Quality**: High-quality training data is the backbone of effective fine-tuning. The better your training data, the more effective your fine-tuned model will be.\n- **Establish a Signals Loop:** Agents evolve and so should your model. Continuously evaluate model performance and retrain as needed using observed reasoning, tool use, and performance metrics to ensure alignment, safety, and accuracy.\n- **Build Fine-tuning into the Agent-building Process:** Organizations can enhance the performance of pre-trained models and enable agents to meet unique business requirements without starting from scratch.\n- **Leverage Azure’s Ecosystem**: Azure offers a full-stack toolkit for building and deploying fine-tuned agents. Take advantage of integrated tools, documentation, and community resources for support and innovation.\n\n#### Fine-tuning Innovations in Azure AI Foundry\n\nRecent Azure updates have made fine-tuning more flexible, affordable, and developer-friendly, enabling teams to build agents that are not just reactive.\n\n[![FT Methods image](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA4gAAAHGAQMAAAAMsze4AAAAA1BMVEXW1taWrGEgAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAASUlEQVR4nO3BMQEAAADCoPVPbQ0PoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAfg3KLAABXFgOJQAAAABJRU5ErkJggg==)](https://devblogs.microsoft.com/foundry/wp-content/uploads/sites/89/2025/10/FT-Methods.png)\n\n- **Reinforcement Fine-Tuning (RFT) Upgrades**: Promoted [o4-mini RFT](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/o4-mini-reinforcement-fine-tuning-rft-now-generally-available-on-azure-ai-foundr/4452597)to general availability, o4-mini RFT enables models to learn from reward signals and optimize for complex objectives which is essential for agentic workflows. RFT is now fully API and Swagger ready, enabling users to fine-tune models using reinforcement learning techniques.\n- **More Global Training Options:** Expanded support for Azure OpenAI model fine-tuning across any of the 26 supported regions, making it easier to deploy customized models worldwide. We’ve also promoted Global Training to generally available and added support for gpt-4o and gpt-4o-mini.\n- **Developer Tier for hosting/inferencing:**Experiment faster with this cost-effective way to evaluate and test fine-tuned models before scaling to production, now promoted to generally available. Key features include:\n- Deploy fine-tuned GPT-4.1 and GPT-4.1-mini models from any training region, including Global Training.\n- Free hosting for 24 hours per deployment.\n- Pay per token at the same rate as Global Standard, helping you budget your testing costs.\n- Simultaneously evaluate multiple models to choose the best candidate for production.\n- **Evaluations Enhancements**: Added [several new capabilities:](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/what%E2%80%99s-new-in-azure-ai-foundry-finetuning-july-2025/4438850) RFT Observability (Auto-Evals), Quick Evals, and Python Grader streamlining evals and debugging during fine-tuning, now in public preview.\n- **More Granular Control:** Added the ability to [copy fine-tuned models](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/what%E2%80%99s-new-in-azure-ai-foundry-fine-tuning-august-2025/4445176) across Azure regions via REST API, enabling flexible multi-region deployment and added [Pause & Resume capabilities](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/what%E2%80%99s-new-in-azure-ai-foundry-fine-tuning-august-2025/4445176), giving customers control to halt and resume fine-tuning jobs without losing progress.\n\n#### Getting Started with Fine-Tuning in Azure AI Foundry\n\nAzure AI Foundry makes it easy to begin fine-tuning advanced language models for your specific use case. Once you have your use case(s) figured out you can dive into:\n\n1. **Data Preparation**: Gather and curate high-quality, domain-specific datasets.\n2. **Model Selection**: Choose the right foundation model (e.g., GPT-4o, 4.1-nano).\n3. **Training & Optimization**: Use advanced techniques like Direct Preference Optimization (DPO), RFT, and distillation to enhance model performance.\n4. **Deployment**: Seamlessly deploy fine-tuned models with automatic scaling and monitoring.\n5. **Iterate and evaluate:** Fine-tuning is an iterative process—start with a baseline, measure performance, and refine your approach based on results to build a reliable signals loop.\n\n[Create with Azure AI Foundry](https://ai.azure.com)\n\n#### Featured Hands-on Technical Content & Community Resources\n\nWhether you’re just getting started or scaling production-grade agents, Azure offers a rich ecosystem of developer-first resources to support every stage of your fine-tuning workflow:\n\n##### Code Samples\n\n| **Name** | **Description** | **Links** | | --- | --- | --- | | O4-mini RFT Code Sample | Dive into a hands-on example of fine-tuning a reasoning model (o4-mini) using RFT on the Countdown dataset. | [GitHub Repo](https://github.com/azure-ai-foundry/fine-tuning/blob/main/Demos/RFT_Countdown/demo_with_python_grader.ipynb) | | RAFT Fine-tuning on Azure AI Foundry Code Sample | Explore a recipe for using Meta Llama 3.1 405B or OpenAI GPT-4o deployed on Azure AI to generate synthetic datasets with the RAFT method. | [GitHub Repo](https://github.com/Azure-Samples/azureai-foundry-finetuning-raft) | | Fine-tuning gpt-oss-20B Code Sample | Fine-tune gpt‑oss‑20b using Managed Compute on Azure — available in preview and accessible via notebook. | [GitHub Repo](https://github.com/Azure/azureml-examples/blob/4df413cccaef14bd6f6c7efc6f41fdad0cf0533d/sdk/python/jobs/finetuning/standalone/model-as-a-platform/chat-completion/gpt-oss-20b/gpt-oss-20b-chat-completion.ipynb) | | Distill DeepSeek V3 into Phi-4-mini Code Sample | Distill knowledge from DeepSeek V3 into Phi-4-mini using Azure’s powerful AI stack. | [GitHub Repo](https://github.com/microsoft/Build25-LAB329) | | AI Tour 2025: Efficient Model Customization Code Sample | Zava retail demo shows how Azure AI Foundry transforms retail with intelligence fine-tuning agents to deliver truly personalized experiences, precise answers, and cost-efficient innovation. This presentation and repo focuses on Distillation, RFT and RAFT approaches using Azure AI Foundry. | [GitHub Repo](https://github.com/microsoft/Build25-LAB329)<br><br><br>[Slides](https://speakerdeck.com/nitya/aitour-26-efficient-model-customization-with-azure-ai-foundry) | | Models for Beginners Course | Free course on GitHub coming soon! Checkout the preview | [GitHub Repo Preview](https://aka.ms/models-for-beginners) |\n\n##### Videos with Demos\n\n**Azure AI Show: There’s no reason not to fine-tune ([YouTube Video](https://aka.ms/AIShow/FineTuning))**\n\nLearn how to fine-tune foundation models in Azure AI Foundry for improved performance, lower costs, and agentic scenarios. Watch Alicia and Omkar break it down in this episode.\n\n**Model Mondays: Fine-tuning & Distillation ([YouTube Video](https://youtu.be/VSNGzBB20aw)).**\n\nDave Voutila shows how Azure AI Foundry makes it easy to fine-tune existing models and use distillation techniques without needing deep ML expertise.\n\n##### Helpful Docs\n\n- [Fine-tune models with Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/fine-tuning-overview) (Comprehensive Guide)\n- [Reinforcement Fine-Tuning (RFT) Overview](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reinforcement-fine-tuning)\n- [Developer Tier Details](https://azure.microsoft.com/en-us/products/ai-foundry/)\n- [Evaluations Enhancements & Auto-Evals](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-generative-ai-app)\n\n##### Community\n\n👋 Continue the conversation on [Discord](https://aka.ms/model-mondays/discord)\n\n**About the Authors**\n\nHi! I’m Jacques “Jack”, Microsoft Technical Trainer at Microsoft. I help learners and organizations adopt intelligent automation through Microsoft technologies. This blog reflects my experience guiding teams in building agentic AI solutions that are not only powerful but also secure, ethical, and scalable.\n\nI’m Malena, I lead product marketing for fine-tuning at Microsoft. I help developers make Azure AI real by activating hands-on content and building community online and offline.\n\n**#SkilledByMTT**",
  "Title": "The Developer’s Guide to Smarter Fine-tuning: Unlock custom AI for every business challenge",
  "Link": "https://devblogs.microsoft.com/foundry/the-developers-guide-to-smarter-fine-tuning/",
  "FeedName": "Microsoft AI Foundry Blog",
  "FeedLevelAuthor": "Azure AI Foundry Blog",
  "Tags": [
    "Azure AI Foundry",
    "fine-tuning",
    "MicrosoftLearn",
    "Model customization",
    "SkilledByMTT",
    "What's New"
  ],
  "Author": "Malena Lopez-Sotelo, Jacques Guibert de Bruet",
  "ProcessedDate": "2025-10-14 19:02:59"
}
