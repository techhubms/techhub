{
  "FeedName": "Microsoft Azure SDK Blog",
  "OutputDir": "_news",
  "Link": "https://devblogs.microsoft.com/azure-sdk/easily-connect-ai-workloads-to-azure-blob-storage-with-adlfs/",
  "Tags": [
    "adlfs",
    "AI",
    "Azure SDK",
    "blob storage",
    "data-lake",
    "fsspec",
    "ml",
    "python",
    "storage"
  ],
  "EnhancedContent": "# Easily connect AI workloads to Azure Blob Storage with adlfs\n\nWe’re excited to announce the newest release of [adlfs](https://pypi.org/project/adlfs/2025.8.0/)—a unified Pythonic file system interface to Azure Blob Storage and Azure Data Lake Storage. Microsoft works closely with the [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) open-source community to enhance this package. We continue to focus on optimizing performance, security, and authentication. This collaboration ensures that adlfs can be a robust and reliable solution for connecting AI workloads to Azure Blob and Data Lake Storage.\n\n## Who is this for?\n\nData professionals face a constant challenge: bridging code and cloud storage no matter the scale. Python’s [fsspec](https://filesystem-spec.readthedocs.io/en/latest/) standard is the universal “file adapter,” and adlfs is its specialized, high-performance gateway to Azure’s Blob and Data Lake Storage. While any Python code can utilize adlfs, what sets it apart is the native integration into frameworks like Dask, Pandas, Ray, PyTorch, PyIceberg, and more. adlfs is a clear choice for Azure-centric ML, data science, and extract-transform-load (ETL) workloads. For example, developers can use adlfs to load datasets and save model checkpoints to Azure using PyTorch and PyTorch Lightning.\n\nThese improvements in adlfs benefit the broad range of AI/ML tools at once. By making Azure Storage faster and more reliable in adlfs, every tool from PyTorch to pandas that uses fsspec gets a boost on Azure. There’s no custom integration required. Switching from local files or other cloud files to Azure is often as simple as changing a file path (for example, from file:// or s3:// to az://) or a configuration setting.\n\n## What’s new?\n\nThe 2025.8.0 release of adlfs (now [available on PyPI](https://pypi.org/project/adlfs/2025.8.0/)) brings several key enhancements focused on performance, resiliency, and ease-of-use. In summary, this update delivers faster file operations (via parallel uploads) and improved reliability (smaller default block sizes to reduce timeouts, plus fixes for geo-redundant storage scenarios).\n\n- Writing large files is two to five times faster due to support for concurrent block uploads.\n- Decreased default block size from 1 GiB to 50-MiB addresses timeout and connection issues for large file uploads.\n\n## Installation\n\nadlfs is listed on PyPI, and you can install it using your favorite package manager. This example utilizes `pip` :\n\n```shell pip install adlfs==2025.8.0 ```\n\n## Example usage in Ray\n\nadlfs can be used directly with Ray to enable distributed access to Azure Blob Storage within Ray data pipelines. You can pass an adlfs `AzureBlobFileSystem` as the `filesystem` argument in Ray’s data loading functions. Doing so allows data to be read from Azure storage in parallel across the Ray cluster. You can configure authentication through various methods, including Azure CLI credentials, environment variables, managed identity, or explicit parameters. This flexibility makes it easy to switch from one development environment to another.\n\n```python import ray from adlfs import AzureBlobFileSystem\n\nray.init()\n# Configure authentication - set anon=False to use credentials\nabfs = AzureBlobFileSystem(account_name=\"myaccount\", anon=False) ds = ray.data.read_parquet(\"az://mycontainer/data/\", filesystem=abfs) print(ds.count()) ray.shutdown() ```\n\nThis pattern allows seamless, scalable data access from cloud storage within Ray jobs using standard Azure authentication and storage paths.\n\n## What’s next?\n\nMicrosoft is actively contributing to the adlfs package to ensure that customers can have the best experience when interacting with their data in Azure Blob Storage. If you work on Python or AI/ML frameworks, upgrade to adlfs 2025.8.0 and try it out. If you’re already using these frameworks, the improvements we made are automatically available to your applications and require no code changes.\n\nWe’d love to hear your feedback. If you have feature requests or run into issues, let us know on the [adlfs GitHub repo](https://github.com/fsspec/adlfs). Community input will directly influence our next round of contributions. We’re excited to continuously improve how AI workloads can utilize Azure Storage.\n\n## Summary\n\nThe 2025.8.0 release of adlfs brings significant performance improvements and enhanced reliability for AI workloads using Azure Blob Storage and Data Lake Storage. With 2-5 times faster large file writes and improved timeout handling, adlfs makes it easier than ever to connect Python AI/ML frameworks to Azure storage services.",
  "Title": "Easily connect AI workloads to Azure Blob Storage with adlfs",
  "ProcessedDate": "2025-10-15 18:03:18",
  "FeedLevelAuthor": "Azure SDK Blog",
  "Author": "Vishnu Charan TJ",
  "FeedUrl": "https://devblogs.microsoft.com/azure-sdk/feed/",
  "Description": "Microsoft works with the fsspec open-source community to enhance adlfs. This update delivers faster file operations and improved reliability for AI workloads that connect to Azure Blob and Data Lake Storage.\n\nThe post [Easily connect AI workloads to Azure Blob Storage with adlfs](https://devblogs.microsoft.com/azure-sdk/easily-connect-ai-workloads-to-azure-blob-storage-with-adlfs/) appeared first on [Azure SDK Blog](https://devblogs.microsoft.com/azure-sdk).",
  "PubDate": "2025-10-15T17:17:17+00:00"
}
