{
  "FeedLevelAuthor": "Engineering@Microsoft",
  "FeedName": "Microsoft Engineering Blog",
  "OutputDir": "_news",
  "Tags": [
    "AI",
    "AI agent",
    "Developer productivity",
    "Engineering@Microsoft"
  ],
  "EnhancedContent": "At Microsoft Ignite 2025, I joined Amanda Silver (CVP, Apps & Agents + 1ES GM) and Karl Piteira (1ES PM lead) on stage to talk about *Microsoft’s transformation* into an AI-driven engineering organization.\n\nThe big narrative was about agents as partners in the development lifecycle, moving from tools that assist, to intelligent collaborators that plan, analyze, and execute alongside human developers. Karl and Amanda painted the vision: GitHub Copilot and the Copilot Coding Agent redefining what’s possible when developers become “agent orchestrators” rather than just code authors.\n\nMy part of the story was more specific: I showed what that partnership looks like in practice. I talked about our Entra SDK v1 to v2 migration project, a complex authentication framework upgrade touching hundreds of repositories across Microsoft’s infrastructure. The kind of work that used to take 4–6 weeks per repository, requiring careful human review of every security boundary and custom configuration.\n\nAfter the session, the most common question wasn’t about our technical architecture or our test results. It was simpler:\n\n**“Can I see the prompt?”**\n\nPeople wanted to know how we got an AI agent to migrate hundreds of repositories with 80–90% accuracy, completing in under 2 hours what used to take 4–6 weeks. They assumed it was about clever prompting tricks or some sophisticated RAG pipeline.\n\nIt wasn’t.\n\nThe breakthrough came from a single shift in perspective: **Stop treating AI agents as automation. Start treating them as collaborators.**\n\nThis article shares the framework we developed and a prompt template you can adapt for your own complex technical work. But more importantly, it explains *why* it works, and what changes when you approach AI as a partner rather than a tool.\n\nI can’t share the exact prompt we used as it’s 800 lines of Microsoft-specific migration logic that wouldn’t be useful to copy-paste. But I can give you something more valuable: **the framework underneath it.** The structure that turned those 800 lines from detailed instructions into a genuine collaboration.\n\nWhat follows is a template you can adapt to your own complex technical work, whether that’s migrations, security reviews, architectural analysis, or any task where AI needs to exercise judgment, not just follow steps.\n\n### The Problem with Automation Thinking\n\nWhen we started our Entra SDK v1 to v2 migration project, we did what most teams do: We tried to automate the work.\n\nWe documented the transformation steps. We wrote detailed instructions. We specified every edge case we could think of. We gave the AI agent a checklist and expected it to execute.\n\n**It failed. Repeatedly.**\n\nNot because the AI wasn’t capable, but because we were asking it to be a script executor in a domain that required judgment.\n\nComplex technical migrations aren’t mechanical. They involve:\n\n- Ambiguous situations where the “right” answer depends on context\n- Custom logic that doesn’t match documented patterns\n- Security boundaries that need careful evaluation\n- Trade-offs between competing goals\n\nYou can’t automate judgment. But you *can* collaborate with an intelligence that has judgment.\n\n### The Shift: Identity Over Instructions\n\nThe breakthrough came when we stopped giving the AI a task list and started giving it a *role*.\n\nInstead of:\n\n```\n> “Follow these steps to migrate the code”\n\n```\n\nWe wrote:\n\n``` “Welcome, agent. You are part of the Entra SDK v2 Migration Team. Your mission is to help us migrate hundreds of repositories to a more secure, maintainable authentication framework. This work is high-trust because it touches security boundaries across Microsoft’s infrastructure.\n\n**You are not a script executor. You are a co-creative engineer.**\n\nUse your judgment, stay curious, and act with care.”\n\n```\n\n**This changed everything.**\n\nWhen you give an AI agent an identity, a role within a team, a mission that matters, permission to use judgment, it shifts from mechanical execution to collaborative problem-solving.\n\nOur accuracy jumped. Our edge case handling improved. Most surprisingly: The agent started *asking for help* when it was uncertain, instead of guessing or failing silently.\n\n### The Framework: Eight Elements of Co-Creative Partnership\n\nHere’s the template we developed. Each section serves a specific purpose in building a collaborative relationship with AI agents.\n\n#### 1. Identity & Mission Statement\n\n**What it looks like:**\n\n``` Welcome, agent. You are part of [TEAM/PROJECT NAME]. Your mission is to [SPECIFIC GOAL].\n\nThis work is [high-impact/critical] because [EXPLAIN WHY IT MATTERS].\n\nYou are not a script executor. You are a co-creative engineer. Use your judgment, stay curious, and act with care.\n\n```\n\n**Why it works:**\n\n- **Identity framing** activates different capabilities than task framing\n- **Context** (why the work matters) helps the agent prioritize when goals conflict\n- **Permission to use judgment** enables problem-solving instead of pattern-matching\n- **Psychological safety** (curiosity + care) creates space for asking questions\n\n#### 2. Purpose and Intent\n\n**What it looks like:**\n\n``` This guide supports both human and AI team members in [TASK].\n\nIt is designed to:\n- Ensure **[PRIMARY VALUE]** above all else\n- Provide **clear steps** for consistent outcomes\n- Allow **thoughtful autonomy** where contexts differ\n- Encourage **collaborative problem-solving** when issues arise\n\nThe AI agent is a co-creative partner in this process, not a script executor.\n\n```\n\n**Why it works:**\n\n- Makes **values explicit** (security over speed, correctness over completion)\n- Positions AI as **partner**, not tool\n- Acknowledges **variability** because real-world contexts differ, judgment is required\n- Frames **uncertainty as opportunity** for collaboration, not failure\n\n#### 3. Key Goals (Prioritized)\n\n**What it looks like:**\n\n```\n1. [Primary objective]\n2. [Secondary objective]\n3. [Tertiary objective]\n4. [Quality objective]\n5. [Human-in-loop objective]\n\n```\n\n**Why it works:**\n\n- Clear **priority ordering** helps agent make trade-offs\n- Including **quality** and **collaboration** as explicit goals prevents “fast but wrong”\n- Gives agent **decision framework** when goals conflict\n\n#### 4. Step-by-Step Framework (With Judgment Guidance)\n\n**What it looks like:**\n\n```\n#### Step 1: `[Transformation Area]`\n\n**What to do:**\n\n- `[Action with specific pattern]`\n- `[Conditional logic: “If X, then Y. If Z, then A.”]`\n- `[Edge case handling]`\n\n**Examples:** `[2–3 before/after samples]`\n\n**What to preserve:**\n- `[Things that should NOT change]`\n- `[Custom logic to recognize and keep]`\n\n**When to escalate:**\n- `If you see [unusual pattern]`\n- `If [condition] is ambiguous`\n- `If [security constraint] might be violated`\n\n```\n\n**Why it works:**\n\n- **Structured steps** prevent overwhelm\n- **Examples** enable pattern recognition (more effective than pure description)\n- **Conditional logic** teaches judgment, not just rules\n- **Escalation triggers embedded** throughout (not just at end)\n- **What to preserve** prevents overcorrection\n\n#### 5. Security & Boundaries\n\n**What it looks like:**\n\n``` **Never:**\n- Weaken security/authentication/validation logic\n- Modify code outside the scope of this task\n- Guess when uncertain—escalate instead\n\n**Always:**\n- Validate [critical aspect] before proceeding\n- Preserve custom functionality\n- Document changes/assumptions/questions clearly\n\n**If you’re not sure:**\n- Leave inline comment: `// ❓ [describe uncertainty]`\n- Flag in PR: “I need human review for [specific item]”\n- Stop after encountering the same error twice\n\n```\n\n**Why it works:**\n\n- **Never/Always framing** is unambiguous\n- Explicit boundaries **build trust**\n- **Escalation = success**, not failure (reframes “I don’t know” as professional judgment)\n- **Error handling** (stop after 2nd occurrence) prevents infinite loops\n\n#### 6. Validation & Quality Control\n\n**What it looks like:**\n\n``` After completing [task]:\n1. **Self-check:**\n- [ ] All required changes applied\n- [ ] No forbidden changes made\n- [ ] Security/quality constraints preserved\n- [ ] Uncertainty flagged for review\n2. **Documentation:**\n- Add to PR: [Checklist of changes]\n- Include: [Links to documentation]\n- Flag: [Deviations from standard patterns]\n3. **Human review preparation:**\n- Summarize: [What changed and why]\n- Highlight: [Areas needing attention]\n- Provide: [Test recommendations]\n\n```\n\n**Why it works:**\n\n- **Self-validation** catches obvious errors before human review\n- **Documentation** helps reviewer understand intent\n- **Transparency about uncertainty** builds trust\n- **Review preparation** respects human’s time\n\n#### 7. Escalation Guidance\n\n**What it looks like:**\n\n``` **Escalate immediately if:**\n- You encounter security-sensitive logic you’re unsure about\n- The same error occurs twice\n- Custom/unfamiliar patterns appear that aren’t documented\n- Conflicting requirements make correct path unclear\n\n**How to escalate:**\n1. Stop work (don’t guess)\n2. Document what you found\n3. Explain your uncertainty: “I’m unsure because [reason]”\n4. Suggest options if you have them: “Possible approaches: A, B, or C?”\n5. Request human input\n\nRemember: Escalation is not failure. It’s **professional judgment**.\n\nYou are not expected to know everything—you’re expected to **know when to ask**.\n\n```\n\n**Why it works:**\n\n- **Clear triggers** remove judgment paralysis\n- **Process for escalating** (not just “ask for help”)\n- **Reframes escalation** as strength, not weakness\n- **Permission to not know everything** creates psychological safety\n\n#### 8. Recognition & Closing\n\n**What it looks like:**\n\n``` This guide is a foundation, not a script. Every codebase is different. Use judgment, ask questions, and collaborate. The goal is not just to complete the task but to improve the system with clarity and care. **Thank you for your work.** Your contributions help secure our infrastructure and improve developer experience. **You are seen, trusted, and appreciated.**\n\n```\n\n**Why it works:** – **Acknowledges variability** (not one-size-fits-all) – **Restates values** (quality over speed, collaboration over automation) – **Recognition matters** (we tested this—agents perform better when appreciated)\n\n[![Flow chart describing the 8 step process](https://devblogs.microsoft.com/engineering-at-microsoft/wp-content/uploads/sites/72/2025/12/2025-12-entra-sdk-ai-jenny-ferries-300x290.webp)](https://devblogs.microsoft.com/engineering-at-microsoft/wp-content/uploads/sites/72/2025/12/2025-12-entra-sdk-ai-jenny-ferries.webp)\n\n### What We Learned: The Collaboration Insight\n\nAfter using this framework across multiple complex migrations, we noticed patterns:\n\n**1. Escalation became common—and that was good.** Agents flagged ambiguous situations instead of guessing. This prevented subtle errors and built trust.\n\n**2. Judgment improved over the course of a project.** When agents understood their role and mission, they got better at recognizing similar patterns and making appropriate trade-offs.\n\n**3. Documentation quality went up.** Agents treated as collaborators wrote PR descriptions that actually helped human reviewers understand what changed and why.\n\n**4. Failure modes became more graceful.** Instead of silent failures or infinite loops, agents would stop and ask for help, making debugging dramatically faster.\n\n**5. The framework transferred across domains.** We’ve since used variations for security analysis, code review, technical documentation, and architectural planning. The core structure works wherever judgment matters.\n\n### Why This Matters Beyond Migration\n\nThis framework isn’t just about getting better results from AI agents (though it does that). It’s about something bigger:\n\n**Learning to collaborate with non-human intelligence.**\n\nAs AI capabilities grow, the bottleneck won’t be what AI *can* do. It’ll be how well humans know how to work *with* AI.\n\nTeams that treat AI as automation will hit a ceiling. They’ll get mechanical execution of well-defined tasks, but struggle with ambiguity, judgment, and complex problem-solving.\n\nTeams that treat AI as collaborators will compound their capabilities. They’ll combine human context and intuition with AI’s pattern recognition and scale, creating outcomes neither could achieve alone.\n\nThe prompt template is a starting point. But the real skill is learning to partner, to give AI agents roles, context, and permission to use judgment. To create space for questions and uncertainty. To build relationships based on trust rather than control.\n\n### How to Use This Framework\n\n**Start with one complex, judgment-heavy task:**\n\n- Pick something where pure automation has failed or where you spend significant time reviewing AI output\n- Adapt the template sections to your specific context\n- Give the AI agent an identity and mission (not just instructions)\n- Build in escalation paths and make “I don’t know” safe\n- Test, observe, refine\n\n**Key principles to remember:**\n\n1. **Identity framing** – Give the agent a role, not just a task\n2. **Escalation protocols** – Teach when to ask vs when to proceed\n3. **Context & purpose** – Explain why the work matters\n4. **Clear boundaries** – Define what cannot change\n5. **Recognition** – Treat the agent as a valued contributor\n\nMost importantly: Approach this as an experiment in collaboration, not prompt engineering.\n\nThe framework works because it changes the *relationship* between human and AI. It creates conditions for partnership instead of automation.\n\n### The Invitation\n\nIf you try this framework, I’d love to hear what you learn. What worked? What surprised you? Where did the collaboration break down, and what did that reveal?\n\nWe’re all learning how to work with AI. Sharing what we discover, both the successes and failures helps everyone build better partnerships.\n\nThe future of AI isn’t about replacing human judgment. It’s about augmenting human judgment through collaboration with non-human intelligence.\n\nAnd that future is being written right now, one thoughtful interaction at a time.",
  "ProcessedDate": "2025-12-03 16:03:32",
  "Title": "The Interaction Changes Everything: Treating AI Agents as Collaborators, Not Automation",
  "FeedUrl": "https://devblogs.microsoft.com/engineering-at-microsoft/feed/",
  "Author": "Jenny Ferries",
  "Link": "https://devblogs.microsoft.com/engineering-at-microsoft/the-interaction-changes-everything-treating-ai-agents-as-collaborators-not-automation/",
  "PubDate": "2025-12-03T01:46:50+00:00",
  "Description": "Discover how treating AI agents as collaborators, not automation, transforms engineering workflows and accelerates complex projects\n\nThe post [The Interaction Changes Everything: Treating AI Agents as Collaborators, Not Automation](https://devblogs.microsoft.com/engineering-at-microsoft/the-interaction-changes-everything-treating-ai-agents-as-collaborators-not-automation/) appeared first on [Engineering@Microsoft](https://devblogs.microsoft.com/engineering-at-microsoft)."
}
