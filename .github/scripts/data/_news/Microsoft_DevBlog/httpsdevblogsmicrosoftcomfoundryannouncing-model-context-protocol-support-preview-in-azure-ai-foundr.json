{
  "OutputDir": "_news",
  "Title": "Announcing Model Context Protocol Support (preview) in Azure AI Foundry Agent Service",
  "FeedUrl": "https://devblogs.microsoft.com/foundry/feed/",
  "Link": "https://devblogs.microsoft.com/foundry/announcing-model-context-protocol-support-preview-in-azure-ai-foundry-agent-service/",
  "Author": "Linda Li",
  "ProcessedDate": "2025-08-05 07:56:35",
  "EnhancedContent": "Generative-AI agents only become useful when they can *do* things—query systems of record, trigger workflows, or look up specialized knowledge. Until now that meant hand-rolling Azure Functions, managing OpenAPI specs, or writing custom plug-ins for every backend you own.\n\n**MCP changes the economics**: it is an open, JSON-RPC–based protocol—originally proposed by Anthropic—that lets a “server” publish *tools* (functions) and *resources* (context) once and have any compliant “client” (your agent runtime) discover and call them automatically. Think “USB-C for AI integrations.”\n\nWith today’s preview, Foundry Agent Service become first-class MCP clients. Bring any remote MCP server—self-hosted or SaaS—and Azure AI Foundry will import its capabilities in seconds, keep them updated, and route calls through the service’s enterprise envelope.\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) allows developers, organizations, and service providers to host services and APIs on MCP servers and easily expose and connect tools to MCP-compatible clients, such as the Foundry Agent Service. MCP is an open standard that defines how services provide various functions and context to AI models and agents. With Foundry Agent Service’s support of MCP, users can bring an existing MCP server endpoint and add it as a tool to Foundry agents. When connecting to an MCP server, actions and knowledge are automatically added to the agent and updated as functionality evolves. This streamlines the process of building agents and reduces the time required for maintaining them.\n\nWith MCP support in Foundry Agent Service, it empowers you to:\n\n- **Easily integrate with services and APIs.** Whether you want to connect with your internal services or APIs from external providers, MCP provides an easy way to integrate with Foundry Agent Service without writing and managing custom functions.\n- **Enhance your AI agent with enterprise features in Foundry Agent Service.** With Foundry Agent Service, you can enable enterprise-ready features such as Bring Your Own thread storage.\n\nYour browser does not support the video tag.\n\n### Code Samples\n\n**Step 1:** Import the needed packages. Please make sure you use the latest packages for [azure-ai-projects](https://pypi.org/project/azure-ai-projects/1.0.0b12/) and [azure-ai-agents](https://pypi.org/project/azure-ai-agents/1.1.0b2/) and the supported region.\n\n``` import time import json\n\nfrom azure.ai.agents.models import MessageTextContent, ListSortOrder, McpTool from azure.ai.projects import AIProjectClient from azure.identity import DefaultAzureCredential\n\n```\n\n**Step 2: Create AI Project Client and an Azure AI Foundry Agent** Use `server_label` to provide a unique name for the MCP server within the same Foundry Agent. `server_url` is the URL of the MCP server. `allowed_tools` is optional to specify which tools are enabled for the agent.\n\n```python mcp_tool = McpTool( server_label=mcp_server_label, server_url=mcp_server_url, allowed_tools=[], # Optional )\n\nproject_client = AIProjectClient( endpoint=PROJECT_ENDPOINT, credential=DefaultAzureCredential() )\n\nwith project_client: agent = project_client.agents.create_agent( model=MODEL_DEPLOYMENT_NAME, name=\"my-mcp-agent\", instructions=( \"You are a helpful assistant. Use the tools provided to answer the user's \" \"questions. Be sure to cite your sources.\" ), tools=mcp_tool.definitions ) print(f\"Created agent, agent ID: {agent.id}\")\n\n```\n\n**Step 3: Create a Thread, Message and Run** Within the Run, you can pass custom headers and use the `server_label` you provided to map to the specific MCP server.\n\n```python thread = project_client.agents.threads.create() print(f\"Created thread, thread ID: {thread.id}\")\n\nmessage = project_client.agents.messages.create( thread_id=thread.id, role=\"user\", content=\"<a question for your MCP server>\" ) print(f\"Created message, message ID: {message.id}\")\n\n```\n\nPlease note headers are only valid for the run. You can provide headers in this way:\n\n```python mcp_tool.update_headers(\"SuperSecret\", \"123456\") ```\n\n```python run = project_client.agents.runs.create( thread_id=thread.id, agent_id=agent.id )\n\n```\n\n**Step 4: Execute the Run and retrieve Message** You can use `run_step` to get more details on tool inputs and outputs. By default, every tool calling to a MCP server needs approval.\n\n```python while run.status in [\"queued\", \"in_progress\", \"requires_action\"]: time.sleep(1) run = agents_client.runs.get(thread_id=thread.id, run_id=run.id)\n\nif run.status == \"requires_action\" and isinstance(run.required_action, SubmitToolApprovalAction): tool_calls = run.required_action.submit_tool_approval.tool_calls if not tool_calls: print(\"No tool calls provided - cancelling run\") agents_client.runs.cancel(thread_id=thread.id, run_id=run.id) break\n\ntool_approvals = [] for tool_call in tool_calls: if isinstance(tool_call, RequiredMcpToolCall): try: print(f\"Approving tool call: {tool_call}\") tool_approvals.append( ToolApproval( tool_call_id=tool_call.id, approve=True, headers=mcp_tool.headers, ) ) except Exception as e: print(f\"Error approving tool_call {tool_call.id}: {e}\")\n\nprint(f\"tool_approvals: {tool_approvals}\") if tool_approvals: agents_client.runs.submit_tool_outputs( thread_id=thread.id, run_id=run.id, tool_approvals=tool_approvals )\n\nprint(f\"Current run status: {run.status}\")\n\n# Retrieve the generated response:\nmessages = agents_client.messages.list(thread_id=thread.id) print(\"\\nConversation:\") print(\"-\" * 50)\n\nfor msg in messages: if msg.text_messages: last_text = msg.text_messages[-1] print(f\"{msg.role.upper()}: {last_text.text.value}\") print(\"-\" * 50)\n\n```\n\n**Step 5: Clean up**\n\n```python project_client.agents.delete_agent(agent.id) print(f\"Deleted agent, agent ID: {agent.id}\")\n\n```\n\nAt **Microsoft Build 2025**, Satya Nadella shared our vision for an *open-by-design* AI ecosystem and announced a partnership with **Anthropic** to make the **Model Context Protocol (MCP)** a first-class standard across Windows 11, GitHub, Copilot Studio, and **Azure AI Foundry**.\n\nToday’s **preview support for MCP in Azure AI Foundry Agent Service** is the next step in that journey. It brings the same *“connect once, integrate anywhere”* promise to cloud-hosted agents—letting you plug any MCP server directly into Foundry with **zero custom code**.\n\n### Create with Azure AI Foundry\n\n- Get started with\n[Azure AI Foundry](https://ai.azure.com/) and jump directly into [Visual Studio Code](https://marketplace.visualstudio.com/items?itemName=TeamsDevApp.vscode-ai-foundry).\n- Download the\n[Azure AI Foundry SDK](https://aka.ms/aifoundrysdk).\n- Read the\n[documentation](https://aka.ms/FoundryAgentMCPDoc) to learn more about the feature.\n- Take the\n[Azure AI Foundry Learn courses](https://aka.ms/CreateAgenticAISolutions).\n- Keep the conversation going in\n[GitHub](https://aka.ms/azureaifoundry/forum) and [Discord](https://aka.ms/azureaifoundry/discord).",
  "Tags": [
    "AIAgent",
    "Azure AI Foundry",
    "MCP"
  ],
  "PubDate": "2025-06-27T22:30:03+00:00",
  "Description": "Generative-AI agents only become useful when they can do things—query systems of record, trigger workflows, or look up specialized knowledge. Until now that meant hand-rolling Azure Functions, managing OpenAPI specs, or writing custom plug-ins for every backend you own. MCP changes the economics: it is an open, JSON-RPC–based protocol—originally proposed by Anthropic—that lets a “server” publish tools (functions) and resources (context) once and have any compliant […]\n\nThe post [Announcing Model Context Protocol Support (preview) in Azure AI Foundry Agent Service](https://devblogs.microsoft.com/foundry/announcing-model-context-protocol-support-preview-in-azure-ai-foundry-agent-service/) appeared first on [Azure AI Foundry Blog](https://devblogs.microsoft.com/foundry).",
  "FeedLevelAuthor": "Azure AI Foundry Blog",
  "FeedName": "Microsoft DevBlog"
}
