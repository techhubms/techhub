{
  "Tags": [
    "Azure SDK",
    "python",
    "pytorch",
    "release",
    "storage"
  ],
  "OutputDir": "_news",
  "FeedLevelAuthor": "Azure SDK Blog",
  "ProcessedDate": "2025-08-05 14:10:14",
  "FeedUrl": "https://devblogs.microsoft.com/azure-sdk/feed/",
  "Title": "Introducing the Azure Storage Connector for PyTorch",
  "Description": "This post announces the Azure Storage Connector for PyTorch (azstoragetorch), integrating files from Azure Blob Storage into your PyTorch training pipeline.\n\nThe post [Introducing the Azure Storage Connector for PyTorch](https://devblogs.microsoft.com/azure-sdk/introducing-the-azure-storage-connector-for-pytorch/) appeared first on [Azure SDK Blog](https://devblogs.microsoft.com/azure-sdk).",
  "Link": "https://devblogs.microsoft.com/azure-sdk/introducing-the-azure-storage-connector-for-pytorch/",
  "FeedName": "Microsoft DevBlog",
  "Author": "Rohit Ganguly",
  "EnhancedContent": "We’re excited to introduce the Azure Storage Connector for PyTorch (`azstoragetorch` ), a new library that brings seamless, performance-optimized integration between Azure Storage and PyTorch. The library makes it easy to access and store data in Azure Blob Storage directly within your training workflows.\n\n## What is PyTorch?\n\n[PyTorch](https://pytorch.org/) is a widely used open-source machine learning framework, known for its flexibility and strong support for both research and production deployments. Training models with PyTorch often involves handling large volumes of data. This can include loading massive datasets, saving and restoring model checkpoints, and managing data pipelines. These pipelines can live across environments like local machines, cloud virtual machines, and distributed compute clusters, adding to complexity.\n\n## How can the Azure Storage Connector for PyTorch help?\n\nThe Azure Storage Connector for PyTorch (`azstoragetorch` ) bridges the powerful storage capabilities of Azure Storage with PyTorch, enabling seamless integrations with key PyTorch APIs for your model training workflows.\n\nThe package supports model checkpointing directly with Azure Storage with `torch.save()` and `torch.load()` and directly loading data from Azure Storage to PyTorch `Dataset` classes.\n\n## Use the Azure Storage Connector for PyTorch\n\n### Installation\n\nThe Azure Storage Connector for PyTorch is listed on PyPI, and you can install it using your favorite package manager. This example utilizes `pip` :\n\n```bash pip install azstoragetorch ```\n\nThis installs the `azstoragetorch` library and other dependencies such as `torch` and `azure-storage-blob` .\n\n### Authentication\n\nThe Azure Storage Connector for PyTorch package’s interfaces default to using the Azure Identity library’s [`DefaultAzureCredential`](https://learn.microsoft.com/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python), which automatically retrieves Microsoft Entra ID tokens based on your current environment. For more information, see [DefaultAzureCredential overview](http://aka.ms/azsdk/python/identity/credential-chains#defaultazurecredential-overview).\n\nThis means that as long as there exists a credential on your machine in the chained credential list for `DefaultAzureCredential` , your credentials are securely handled automatically and you’re ready to start using the library.\n\n### Save and Load a Model Checkpoint\n\nThe Azure Storage Connector for PyTorch provides the [`azstoragetorch.io.BlobIO`](https://azure.github.io/azure-storage-for-pytorch/api.html#azstoragetorch.io.BlobIO) class to save and load models directly to and from Azure Blob Storage. This class adheres to the file-like object accepted when using `torch.save()` and `torch.load()` for model checkpointing in PyTorch.\n\nThe `BlobIO` class takes an Azure Storage Blob URL and the mode you’d like to operate in – either write (`\"wb\"` ) for saving or read (`\"rb\"` ) for loading. Because the `BlobIO` class is file-like in Python, it can be safely handled when using the `with` statement.\n\n```python import torch import torchvision.models # Install this separately, e.g. ``pip install torchvision`` from azstoragetorch.io import BlobIO\n\n# The URL to your container\nCONTAINER_URL = \"https://<my-storage-account-name>.blob.core.windows.net/<my-container-name>\"\n\n# Your model of choice, in this case ResNet18 for image recognition\nmodel = torchvision.models.resnet18(weights=\"DEFAULT\")\n\n# Save model weights to an Azure Storage Blob named \"model_weights.pth\" in the container\nwith BlobIO(f\"{CONTAINER_URL}/model_weights.pth\", \"wb\") as f: torch.save(model.state_dict(), f)\n\n# Load model weights from model_weights.pth in the Azure Storage Container\nwith BlobIO(f\"{CONTAINER_URL}/model_weights.pth\", \"rb\") as f: model.load_state_dict(torch.load(f)) ```\n\n### Sample: Use PyTorch Datasets on Azure Storage\n\nPyTorch has two primitives for loading samples, [`DataSet` and `DataLoader`](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html#datasets-dataloaders). The Azure Storage Connector for PyTorch has implementations for both of PyTorch datasets, [map-style](https://docs.pytorch.org/docs/stable/data.html#map-style-datasets) and [iterable-style](https://docs.pytorch.org/docs/stable/data.html#map-style-datasets).\n\nThe [`azstoragetorch.datasets.BlobDataset`](https://azure.github.io/azure-storage-for-pytorch/api.html#azstoragetorch.datasets.BlobDataset) class is a map-style dataset, enabling random access to data samples. The [`azstoragetorch.datasets.IterableBlobDataset`](https://azure.github.io/azure-storage-for-pytorch/api.html#azstoragetorch.datasets.IterableBlobDataset) class is an iterable dataset, which should be used when working on large datasets that may not fit in memory.\n\nBoth classes support two methods: `from_container_url()` and `from_blob_urls()` . The `from_container_url()` method instantiates a dataset by listing blobs in a container, and the `from_blob_urls()` method takes a list of blob URLs when creating a dataset.\n\nThese `Dataset` integrations fit naturally into a PyTorch workflow. Let’s dive into an image example using the [caltech101](https://data.caltech.edu/records/mzrjq-6wc02) dataset and the `resnet18` model.\n\nThe caltech101 dataset contains about 9,000 images across various categories and the `resnet18` model is a residual neural network introduced in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385).\n\n#### Prerequisites and Setup\n\nFirst, install prerequisite packages `azstoragetorch` , `pillow` , and `torchvision` .\n\n```bash pip install azstoragetorch pillow torchvision ```\n\nOnce you have the packages installed, ensure you have the caltech101 dataset in your Azure Storage Account. To ease this setup process, clone the [GitHub repository](https://github.com/Azure/azure-storage-for-pytorch/tree/main) and run the provided [setup file](https://github.com/Azure/azure-storage-for-pytorch/blob/main/samples/intro_notebook/bootstrap.py).\n\nLastly, it’s helpful to understand the importance of transforming data for PyTorch operations. The default output of dataset samples in the `azstoragetorch` package are dictionaries representing a blob in the dataset. It’s often necessary to transform this data into the shape needed for your PyTorch workflows.\n\nTo override the default output, we can provide a `transform` callable in the `from_blob_urls` or `from_container_url` methods that accept an argument of type [`azstoragetorch.datasets.Blob`](https://azure.github.io/azure-storage-for-pytorch/api.html#azstoragetorch.datasets.Blob). The transform callable in the following code sample is based on the [PyTorch documentation](https://pytorch.org/hub/pytorch_vision_resnet/). To learn more about how to use the `transform` callable in the `azstoragetorch` library, visit the [documentation](https://azure.github.io/azure-storage-for-pytorch/user-guide.html#transforming-dataset-output).\n\n```python from torch.utils.data import DataLoader import torchvision.transforms from PIL import Image\n\nfrom azstoragetorch.datasets import BlobDataset\n\n# Method to transform blob data to a torch.Tensor\n# To learn more about why these particular transforms were chosen,\n# Visit the documentation site: https://pytorch.org/hub/pytorch_vision_resnet/\ndef blob_to_category_and_tensor(blob): with blob.reader() as f: img = Image.open(f).convert(\"RGB\") img_transform = torchvision.transforms.Compose([ torchvision.transforms.Resize(256), torchvision.transforms.CenterCrop(224), torchvision.transforms.ToTensor(), torchvision.transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] ), ]) img_tensor = img_transform(img)\n\n# Get second to last component of blob name which will be the image category\n# Example: blob.blob_name -> datasets/caltech101/dalmatian/image_0001.jpg\n# category -> dalmatian\ncategory = blob.blob_name.split(\"/\")[-2] return category, img_tensor\n\n# The URL to your container\nCONTAINER_URL = \"https://<my-storage-account-name>.blob.core.windows.net/<my-container-name>\"\n\n# Initialize dataset with the azstoragetorch library\ndataset = BlobDataset.from_container_url( CONTAINER_URL, prefix=\"datasets/caltech101\", transform=blob_to_category_and_tensor, )\n\n# Set up data loader\nloader = DataLoader(dataset, batch_size=32)\n\nfor categories, img_tensors in loader: print(categories, img_tensors.size()) ```\n\n## Conclusion\n\nThe Azure Storage Connector for PyTorch is designed around the principle that cloud storage integration for your ML workflows shouldn’t require learning new paradigms. Key features include:\n\n- Zero configuration: Automatic credential discovery means no setup code\n- Familiar patterns: If you know `open()`\nand PyTorch datasets, you already know this library\n- Framework integration: Direct compatibility with `torch.save()`\n, `torch.load()` , and `DataLoader`\n- Flexible access: Support for both container-wide and specific blob/object access patterns for reads and writes\n- Debugging friendly: Clear error messages and standard Python exceptions\n\nInstall azstoragetorch today to enable several use cases with your Machine Learning workflows using your data stored in Azure Blob Storage:\n\n- Distributed Training: Save and load model checkpoints across multiple nodes without shared file systems\n- Model Sharing: Easily share trained models across teams and environments\n- Dataset Management: Access large datasets stored in Azure Blob Storage without local storage constraints\n- Experimentation: Quickly iterate on different models and datasets without data movement overhead\n\nThe Azure Storage Connector for PyTorch is in Public Preview and we’re actively seeking feedback from the community. The project is open source and available on GitHub where we’d love to get your feedback, feature requests, and future integrations we should include.\n\n## Resources\n\n- [Azure Storage Connector for PyTorch (azstoragetorch) (Preview)](https://aka.ms/azstoragetorch)\n- [azstoragetorch Samples and Quickstart](https://aka.ms/azstoragetorch/samples)\n- [Data-intensive AI Training and Inferencing with Azure Blob Storage](https://build.microsoft.com/sessions/BRK192)\n- [Quickstart: Azure Blob Storage client library for Python](https://learn.microsoft.com/azure/storage/blobs/storage-quickstart-blobs-python)\n\nFor feature requests, bug reports, or general support, [open an issue](https://github.com/Azure/azure-storage-for-pytorch/issues) in the repository on GitHub.",
  "PubDate": "2025-06-26T15:00:12+00:00"
}
