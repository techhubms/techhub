<<<<<<< HEAD
{
  "ProcessedDate": "2025-08-15 14:31:54",
  "PubDate": "2025-08-13T13:43:07+00:00",
  "Link": "https://devops.com/sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code/?utm_source=rss&utm_medium=rss&utm_campaign=sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code",
  "FeedUrl": "https://devops.com/feed/",
  "Author": "Mike Vizard",
  "FeedName": "DevOps Blog",
  "EnhancedContent": "[![](https://devops.com/wp-content/uploads/2025/02/cropped-devops-logo.png)](https://devops.com/)\n\n# Sign up for our newsletter! Stay informed on the latest DevOps news\n\nSonar Surfaces Multiple Caveats When Relying on LLMs to Write Code\n\nAn [analysis](https://www.sonarsource.com/company/press-releases/the-coding-personalities-of-leading-llms/) of the code created by large language models (LLMs) finds that in addition to having a tendency toward creating messy code that introduces security weaknesses and vulnerabilities into the output generated.\n\nSonar today published a report based on its proprietary analysis framework for assessing LLM-generated code spanning more than 4,400 Java programming assignments. The LLMs evaluated included Anthropic’s Claude Sonnet 4 and 3.7, OpenAI’s GPT-4o, Meta’s Llama-3.2-vision:90b and OpenCoder-8B.\n\nEach model has its own “coding personality” and possesses a strong ability to generate syntactically correct code and boilerplate for common frameworks and functions. For example, Claude Sonnet 4’s success rate of 95.57% on HumanEval demonstrates a very high capability to produce valid, executable code. The models possess a strong foundational understanding of common algorithms and data structures and can create viable solutions for well-defined problems. Additionally, the models are highly effective at translating code concepts and snippets from one programming language to another, which makes them a powerful tool for developers who work with different technology stacks, according to the report.\n\nHowever, critical flaws such as hard-coded credentials and path-traversal injections were common across all models. While the exact prevalence varies between models, all evaluated LLMs produced a high percentage of vulnerabilities with high severity ratings. For Llama-3.2-vision:90b, over 70% of its vulnerabilities are considered ‘blocker’ level of severity; for GPT-4o, it’s 62.5%; and for Claude Sonnet 4, it is nearly 60%, according to the report.\n\nIn fact, the study finds that improved functional performance was often accompanied by much higher levels of risk. While Claude Sonnet 4 improved its performance benchmark pass rate by 6.3% over Claude 3.7 Sonnet, meaning it solved problems more correctly, this performance gain came at a price: The percentage of high-severity bugs rose by 93%.\n\nAll models tested also showed a bias toward messy code, with more than 90% of the issues found being so-called “code smells” issues that are indicators of poor structure and low maintainability. Those smells, which include dead and redundant code, increase the long-term technical debt that might be accrued.\n\n[![Techstrong Gang Youtube](https://securityboulevard.com/wp-content/uploads/2024/12/Techstrong-Gang-Youtube-PodcastV2-770.png)](https://youtu.be/Fojn5NFwaw8)\n\n[![Techstrong Gang Youtube](https://devops.com/wp-content/uploads/2025/03/770X330-DevOps-AppDev-Mar2025D.jpg)](https://info.futurumgroup.com/devops_appdev?utm_source=referral&utm_medium=in-article&utm_campaign=DevOps-Application-Report)\n\n[![Techstrong Gang Youtube](https://devops.com/wp-content/uploads/2025/03/770X330-DevOps-AppDev-Mar2025-2D.jpg)](https://info.futurumgroup.com/devops_appdev?utm_source=referral&utm_medium=in-article&utm_campaign=DevOps-Application-Report)\n\nPrasenjit Sarkar, solutions marketing manager for Sonar, said the report makes it clear that DevOps teams need to review the code generated by LLMs but also realize that each LLM is essentially an opinionated resource that has been programmed to behave in a specific manner. As such, each developer should take into account some of the inherent biases that have been embedded into each model, he added.\n\nThe verbosity of the code generated can also be challenging to debug and fix, noted Sarkar. Many developers often find it challenging to understand code that they did not write. In fact, they may need a separate LLM and an associated set of AI agents to review code created by an LLM.\n\nIt’s not clear to what degree [DevOps teams have adopted AI coding tools](https://devops.com/survey-surfaces-varying-levels-of-enthusiasm-for-ai-coding-tools/) and just how much they trust the output generated. There is no doubt that developers will be more productive, but some of those gains will come at a cost tomorrow that organizations may not realize they are incurring today.\n\nhttps://www.addtoany.com/add_to/x?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/share\n\nhttps://www.addtoany.com/add_to/x?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F%3Futm_source%3Drss%26utm_medium%3Drss%26utm_campaign%3Dsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F%3Futm_source%3Drss%26utm_medium%3Drss%26utm_campaign%3Dsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F%3Futm_source%3Drss%26utm_medium%3Drss%26utm_campaign%3Dsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F%3Futm_source%3Drss%26utm_medium%3Drss%26utm_campaign%3Dsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F%3Futm_source%3Drss%26utm_medium%3Drss%26utm_campaign%3Dsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/share\n\n[« From Firefighting to Forward-Thinking: My Real-World Lessons in DevOps and Cloud Engineering](https://devops.com/from-firefighting-to-forward-thinking-my-real-world-lessons-in-devops-and-cloud-engineering/)\n\n[The Right Kind of AI for Infrastructure as Code »](https://devops.com/the-right-kind-of-ai-for-infrastructure-as-code/)\n\n[![](https://devops.com/wp-content/uploads/2024/11/Copy-of-DO-Banners-1540x660-1.png)](https://webinars.devops.com/in-article-newsletter-popup)\n\n×",
  "Title": "Sonar Surfaces Multiple Caveats When Relying on LLMs to Write Code",
  "FeedLevelAuthor": "DevOps.com",
  "OutputDir": "_posts",
  "Description": "![SonarSource, LLMs, code data, agentic, JFrog, security, devsecops, Digma, code, Go, code, kernel, eBPF, Veracode GitKraken JFrog GitGuardian organizations, quality fear unknown software app Rust Contrast Security Adds API Support to Application Security Platform](https://devops.com/wp-content/uploads/2022/04/securitytool.jpg)\n\n![SonarSource, LLMs, code data, agentic, JFrog, security, devsecops, Digma, code, Go, code, kernel, eBPF, Veracode GitKraken JFrog GitGuardian organizations, quality fear unknown software app Rust Contrast Security Adds API Support to Application Security Platform](https://devops.com/wp-content/uploads/2022/04/securitytool-150x150.jpg)New SonarSource research shows LLMs like GPT-4o, Claude Sonnet 4, and Llama-3.2 produce highly functional yet risky code — with frequent high-severity vulnerabilities, hard-coded credentials, and messy “code smells” that raise long-term tech debt.",
  "Tags": [
    "AI",
    "AI coding tools",
    "analysis",
    "Blogs",
    "Business of DevOps",
    "code smells",
    "Features",
    "messy code",
    "News",
    "report",
    "Social - Facebook",
    "Social - LinkedIn",
    "Social - X"
  ]
}
=======
{
  "Link": "https://devops.com/sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code/?utm_source=rss&utm_medium=rss&utm_campaign=sonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code",
  "OutputDir": "_posts",
  "FeedName": "DevOps Blog",
  "EnhancedContent": "[![](https://devops.com/wp-content/uploads/2025/02/cropped-devops-logo.png)](https://devops.com/)\n\n# Sign up for our newsletter! Stay informed on the latest DevOps news\n\nSonar Surfaces Multiple Caveats When Relying on LLMs to Write Code\n\nAn [analysis](https://www.sonarsource.com/company/press-releases/the-coding-personalities-of-leading-llms/) of the code created by large language models (LLMs) finds that in addition to having a tendency toward creating messy code that introduces security weaknesses and vulnerabilities into the output generated.\n\nSonar today published a report based on its proprietary analysis framework for assessing LLM-generated code spanning more than 4,400 Java programming assignments. The LLMs evaluated included Anthropic’s Claude Sonnet 4 and 3.7, OpenAI’s GPT-4o, Meta’s Llama-3.2-vision:90b and OpenCoder-8B.\n\nEach model has its own “coding personality” and possesses a strong ability to generate syntactically correct code and boilerplate for common frameworks and functions. For example, Claude Sonnet 4’s success rate of 95.57% on HumanEval demonstrates a very high capability to produce valid, executable code. The models possess a strong foundational understanding of common algorithms and data structures and can create viable solutions for well-defined problems. Additionally, the models are highly effective at translating code concepts and snippets from one programming language to another, which makes them a powerful tool for developers who work with different technology stacks, according to the report.\n\nHowever, critical flaws such as hard-coded credentials and path-traversal injections were common across all models. While the exact prevalence varies between models, all evaluated LLMs produced a high percentage of vulnerabilities with high severity ratings. For Llama-3.2-vision:90b, over 70% of its vulnerabilities are considered ‘blocker’ level of severity; for GPT-4o, it’s 62.5%; and for Claude Sonnet 4, it is nearly 60%, according to the report.\n\nIn fact, the study finds that improved functional performance was often accompanied by much higher levels of risk. While Claude Sonnet 4 improved its performance benchmark pass rate by 6.3% over Claude 3.7 Sonnet, meaning it solved problems more correctly, this performance gain came at a price: The percentage of high-severity bugs rose by 93%.\n\nAll models tested also showed a bias toward messy code, with more than 90% of the issues found being so-called “code smells” issues that are indicators of poor structure and low maintainability. Those smells, which include dead and redundant code, increase the long-term technical debt that might be accrued.\n\n[![Techstrong Gang Youtube](https://securityboulevard.com/wp-content/uploads/2024/12/Techstrong-Gang-Youtube-PodcastV2-770.png)](https://youtu.be/Fojn5NFwaw8)\n\n[![Techstrong Gang Youtube](https://devops.com/wp-content/uploads/2025/03/770X330-DevOps-AppDev-Mar2025D.jpg)](https://info.futurumgroup.com/devops_appdev?utm_source=referral&utm_medium=in-article&utm_campaign=DevOps-Application-Report)\n\n[![Techstrong Gang Youtube](https://devops.com/wp-content/uploads/2025/03/770X330-DevOps-AppDev-Mar2025-2D.jpg)](https://info.futurumgroup.com/devops_appdev?utm_source=referral&utm_medium=in-article&utm_campaign=DevOps-Application-Report)\n\nPrasenjit Sarkar, solutions marketing manager for Sonar, said the report makes it clear that DevOps teams need to review the code generated by LLMs but also realize that each LLM is essentially an opinionated resource that has been programmed to behave in a specific manner. As such, each developer should take into account some of the inherent biases that have been embedded into each model, he added.\n\nThe verbosity of the code generated can also be challenging to debug and fix, noted Sarkar. Many developers often find it challenging to understand code that they did not write. In fact, they may need a separate LLM and an associated set of AI agents to review code created by an LLM.\n\nIt’s not clear to what degree [DevOps teams have adopted AI coding tools](https://devops.com/survey-surfaces-varying-levels-of-enthusiasm-for-ai-coding-tools/) and just how much they trust the output generated. There is no doubt that developers will be more productive, but some of those gains will come at a cost tomorrow that organizations may not realize they are incurring today.\n\nhttps://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0https://www.addtoany.com/share\n\nhttps://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevops.com%2Fsonar-surfaces-multiple-caveats-when-relying-on-llms-to-write-code%2F&amp;linkname=Sonar%20Surfaces%20Multiple%20Caveats%20When%20Relying%20on%20LLMs%20to%20Write%20Code%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/share\n\n[« From Firefighting to Forward-Thinking: My Real-World Lessons in DevOps and Cloud Engineering](https://devops.com/from-firefighting-to-forward-thinking-my-real-world-lessons-in-devops-and-cloud-engineering/)\n\n[![](https://devops.com/wp-content/uploads/2024/11/Copy-of-DO-Banners-1540x660-1.png)](https://webinars.devops.com/in-article-newsletter-popup)\n\n×",
  "PubDate": "2025-08-13T13:43:07+00:00",
  "FeedLevelAuthor": "DevOps.com",
  "Tags": [
    "AI",
    "AI coding tools",
    "analysis",
    "Blogs",
    "Business of DevOps",
    "code smells",
    "Features",
    "messy code",
    "News",
    "report",
    "Social - Facebook",
    "Social - LinkedIn",
    "Social - X"
  ],
  "Description": "![SonarSource, LLMs, code data, agentic, JFrog, security, devsecops, Digma, code, Go, code, kernel, eBPF, Veracode GitKraken JFrog GitGuardian organizations, quality fear unknown software app Rust Contrast Security Adds API Support to Application Security Platform](https://devops.com/wp-content/uploads/2022/04/securitytool.jpg)\n\n![SonarSource, LLMs, code data, agentic, JFrog, security, devsecops, Digma, code, Go, code, kernel, eBPF, Veracode GitKraken JFrog GitGuardian organizations, quality fear unknown software app Rust Contrast Security Adds API Support to Application Security Platform](https://devops.com/wp-content/uploads/2022/04/securitytool-150x150.jpg)New SonarSource research shows LLMs like GPT-4o, Claude Sonnet 4, and Llama-3.2 produce highly functional yet risky code — with frequent high-severity vulnerabilities, hard-coded credentials, and messy “code smells” that raise long-term tech debt.",
  "Title": "Sonar Surfaces Multiple Caveats When Relying on LLMs to Write Code",
  "ProcessedDate": "2025-08-13 15:05:03",
  "FeedUrl": "https://devops.com/feed/",
  "Author": "Mike Vizard"
}
>>>>>>> f38d706 (Articles from old repo, styling and some other fixes)
