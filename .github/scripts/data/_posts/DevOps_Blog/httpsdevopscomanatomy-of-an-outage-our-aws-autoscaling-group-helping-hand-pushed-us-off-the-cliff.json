{
  "FeedUrl": "https://devops.com/feed/",
  "PubDate": "2025-10-30T10:32:08+00:00",
  "Tags": [
    "automation failure",
    "autoscaling group loop",
    "AWS outage",
    "Blogs",
    "Business of DevOps",
    "cloud disaster recovery",
    "cloud operations",
    "cloud outage recovery",
    "cloud reliability",
    "cloud resilience",
    "Contributed Content",
    "DevOps incident response",
    "EC2 instances pending",
    "pinning ASG",
    "scaling policy troubleshooting",
    "Social - Facebook",
    "Social - LinkedIn",
    "Social - X",
    "us-east-1 outage"
  ],
  "ProcessedDate": "2025-10-31 01:31:07",
  "OutputDir": "_posts",
  "Author": "Muhammad Yawar Malik",
  "Description": "![service, services, outage, failure, fail, system fail,](https://devops.com/wp-content/uploads/2025/10/fail-1776388_1280-2.png)\n\n![service, services, outage, failure, fail, system fail,](https://devops.com/wp-content/uploads/2025/10/fail-1776388_1280-2-150x150.png)An AWS us-east-1 outage exposed how automation can backfire. Learn why autoscaling failed, how pinning ASGs saved uptime, and what to do in future outages.",
  "FeedName": "DevOps Blog",
  "FeedLevelAuthor": "DevOps.com",
  "EnhancedContent": "[![](https://devops.com/wp-content/uploads/2025/02/cropped-devops-logo.png)](https://devops.com/)\n\n# Sign up for our newsletter! Stay informed on the latest DevOps news\n\nAnatomy of an Outage: Our AWS AutoScaling Group “Helping” Hand Pushed us off the Cliff\n\nLike many of you, our week was defined by the us-east-1 outage. When the alerts fired, we all piled into the virtual war room. The first thing we noticed was that we were flying blind. The AWS console was barely loading, and core services like CloudTrail were completely unreachable. We had no logs, no telemetry. All we could do was watch our dashboards and the EC2 instance list itself.\n\nWe quickly saw a terrifying pattern. Most of our services were degraded, but online. Their instance counts were stable.\n\nBut one of our most critical, customer-facing services was completely dark.\n\n### **The Self-Inflicted Wound**\n\nWhen we finally got the EC2 console to load for that service, we saw a nightmare. The instance list was in a constant state of churn. We weren’t seeing “unhealthy” instances; we were seeing instances that never even had a chance to live. They were stuck in a pending state for ages, and then would flip to terminating.\n\n### **The Problem was our Autoscaling Policy**\n\nThe regional network issues were likely causing just enough lag or [CPU pressure](https://devops.com/how-to-troubleshoot-cpu-problems/) on our existing instances to trigger our scale-up policy. The Autoscaling Group (ASG) did exactly what we told it to: “*We are under load! Add more instances*!”\n\nBut here’s the fatal part: The broken us-east-1 control plane meant every single one of those “add instance” requests failed. The ASG would try to launch a new instance, it would get stuck in pending, and then the ASG would, after a timeout, terminate it and… try again.\n\n[![Techstrong Gang Youtube](https://securityboulevard.com/wp-content/uploads/2024/12/Techstrong-Gang-Youtube-PodcastV2-770.png)](https://youtu.be/Fojn5NFwaw8)\n\n[![Techstrong Gang Youtube](https://devops.com/wp-content/uploads/2025/03/770X330-DevOps-AppDev-Mar2025D.jpg)](https://info.futurumgroup.com/devops_appdev?utm_source=referral&utm_medium=in-article&utm_campaign=DevOps-Application-Report)\n\n[![Techstrong Gang Youtube](https://devops.com/wp-content/uploads/2025/03/770X330-DevOps-AppDev-Mar2025-2D.jpg)](https://info.futurumgroup.com/devops_appdev?utm_source=referral&utm_medium=in-article&utm_campaign=DevOps-Application-Report)\n\nIt was a relentless, automated feedback loop of failure.\n\nOur own automation was effectively DDOS-ing our service’s ability to stabilize. The services that survived were the ones where we hadn’t implemented a scaling policy.\n\n### **The “Aha!” Moment: The Power of Pinning**\n\nThat’s when it hit us. The solution wasn’t to “fix” anything. The solution was to do nothing.\n\nIf that service was supposed to be running on 10 instances, our immediate “break-glass” procedure should have been to pin the ASG.\n\nWe should have immediately set our configuration:\n\n\\* min: 10\n\n\\* max: 10\n\n\\* desired: 10\n\nBy setting min, max, and desired to the same number, we would have instantly disabled the scaling policy. The ASG would have stopped trying to add new instances. Because it wasn’t trying to add, it wouldn’t be stuck in that pending/terminating loop. It wouldn’t have terminated anything.\n\nIt would have just… stopped.\n\nOur service would have kept functioning on its existing 10 instances. They might have been slow. They might have been degraded. But they would have been running. We traded 100% uptime for 0% uptime, all because our automation was trying to “help.”\n\n### **Our New Playbook: Automation is for Peacetime**\n\nThis incident taught us a painful lesson: Automation is for application-level failures, not platform-level meltdowns. When the cloud itself is breaking, your automation’s assumptions are all wrong.\n\nWe are now building a “Red Button” script. The moment we confirm a major regional outage, our first action isn’t to fail over. It’s to run a script that identifies all critical ASGs and pins them, setting min, max, and desired to their current values.\n\nIn a total platform failure, you have to be the one to stop your own systems from “helping” themselves to death.\n\nhttps://www.addtoany.com/add_to/x?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%E2%80%9CHelping%E2%80%9D%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%E2%80%9CHelping%E2%80%9D%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%E2%80%9CHelping%E2%80%9D%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%E2%80%9CHelping%E2%80%9D%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%E2%80%9CHelping%E2%80%9D%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0https://www.addtoany.com/share\n\nhttps://www.addtoany.com/add_to/x?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%26quot%3BHelping%26quot%3B%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%26quot%3BHelping%26quot%3B%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%26quot%3BHelping%26quot%3B%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%26quot%3BHelping%26quot%3B%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevops.com%2Fanatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff%2F&amp;linkname=Anatomy%20of%20an%20Outage%3A%20Our%20AWS%20AutoScaling%20Group%20%26quot%3BHelping%26quot%3B%20Hand%20Pushed%20us%20off%20the%20Cliff%C2%A0%20-%20DevOps.comhttps://www.addtoany.com/share\n\n[« Exploring Cloud Key Management Options](https://devops.com/exploring-cloud-key-management-options/)\n\n[How to Integrate Quantum-Safe Security into Your DevOps Workflow »](https://devops.com/how-to-integrate-quantum-safe-security-into-your-devops-workflow/)\n\n[![](https://devops.com/wp-content/uploads/2024/11/Copy-of-DO-Banners-1540x660-1.png)](https://webinars.devops.com/in-article-newsletter-popup)\n\n×",
  "Link": "https://devops.com/anatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff/",
  "Title": "Anatomy of an Outage: Our AWS AutoScaling Group “Helping” Hand Pushed us off the Cliff"
}
