{
  "Tags": [
    ".NET Core",
    "AI",
    "ASP.NET Core"
  ],
  "OutputDir": "_posts",
  "FeedLevelAuthor": "Andrew Lock | .NET Escapades",
  "ProcessedDate": "2025-08-05 14:29:47",
  "FeedUrl": "https://andrewlock.net/rss.xml",
  "Title": "Exploring the new AI chat template",
  "Description": "In this post I explore the new .NET AI Chat Web App template (currently in preview) and take a brief look at the implementation it provides",
  "Link": "https://andrewlock.net/exploring-the-new-ai-chat-template/",
  "FeedName": "Andrew Lock's Blog",
  "Author": "Andrew Lock",
  "EnhancedContent": "In this post I explore [the new .NET AI Chat Web App template (currently in preview)](https://devblogs.microsoft.com/dotnet/announcing-dotnet-ai-template-preview2/) to create a chat application and take a brief look at everything it provides. In the next post I then customize the app so that instead of ingesting PDFs, it ingests the contents of a website and uses that data to answer questions in the chat.\n\n## Getting started with the new .NET AI Chat Web App template\n\nThe .NET AI Chat Web App is a new template that shows how to get started building a chat style application backed by a large language model (LLM). Chat apps are one of the most prolific use cases for AI (obviously heavily popularised by ChatGPT), and while they're not always the best way to \"add AI\" to your app, they can have their uses.\n\nTo install the AI template, you can run the following command\n\n```bash dotnet new install Microsoft.Extensions.AI.Templates\n\n```\n\nThis installs the template, making the AI Chat Web App template available using name `aichatweb` :\n\n```bash\n> dotnet new install Microsoft.Extensions.AI.Templates\nThe following template packages will be installed: Microsoft.Extensions.AI.Templates\n\nSuccess: Microsoft.Extensions.AI.Templates::9.4.0-preview.2.25216.9 installed the following templates: Template Name Short Name Language Tags --------------- ---------- -------- -------------------------------- AI Chat Web App aichatweb [C#] Common/AI/Web/Blazor/.NET Aspire\n\n```\n\nThe template includes various options to control how it works, but there are three main aspects to consider:\n\n- Do you want the app configured to use Aspire? Yes, of course you do ðŸ˜‰\n- Which LLM provider do you want to use to power the chat interface:\n- [**GitHub Models**](https://docs.github.com/en/github-models). This is a great getting-started option, as it's free for developers, and is what I use in this post.\n- **OpenAI**. Uses the [OpenAI API Platform](https://openai.com/api/).\n- **Azure OpenAI**. Uses the [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service).\n- **Ollama**. Runs locally on your machine, using [Ollama](https://ollama.com/) with the llama3.2 and all-minilm models.\n- Which vector embedding store do you want to use for ingesting the data:\n- **Local**. Uses a JSON file on disk, which is great for prototyping.\n- **Azure AI Search**. Uses Azure AI Search, which manages the data ingestion automatically.\n- **Qdrant**. Runs the [Qdrant](https://qdrant.tech/) vector database in a docker container.\n\nIf you're new to LLMs and AI then that might all be a bit overwhelming, but there's basically two different concepts to understand here:\n\n- The LLM provider is what provides the AI interface that powers the chat.\n- The Vector embedding is how the LLM model \"ingests\" data. For each document you provide it, the LLM provides an array of numbers that you store in a database (or in a JSON file in the `local`\ncase). The LLM can then run queries against the database and find \"similar\" data.\n\nFor this post, I chose to use GitHub Models for the LLM provider, as it's free and very easy to get up and running (as I'll show shortly). For the vector store I chose to store the data locally.\n\n>\n> These are the default values for the template for good reason, as they're pretty much the quickest way to get up and running. You wouldn't choose these options for production, but they're ideal for prototyping.\n> >\n\nTo install the template, you can either use your IDE, or you can use the .NET CLI like so:\n\n```bash dotnet new aichatweb \\ -- output ModernDotNetShowChat --provider githubmodels \\ --vector-store local \\ --aspire true\n\n```\n\nThis creates a full solution consisting of:\n\n- A Web project containing the chat app\n- A \"Service Defaults\" projectâ€”A suggested best practice for creating Aspire applications these days\n- An \"App Host\" projectâ€”The Aspire host project, that wires up the dependencies\n\nThere's also a *.sln* file you can open in your IDE:\n\n![The solution layout](/content/images/2025/githubmodels_01.png)\n\nInside the solution folder is a *README.md* file that describes the remaining configuration. For our setup, there's just one step we need to take: configuring GitHub Models.\n\n## Using GitHub Models\n\nThe *README.md* file contains instructions for getting started with GitHub Models:\n\n>\n> To use models hosted by GitHub Models, you will need to create a GitHub personal access token. The token should not have any scopes or permissions. See [Managing your personal access tokens](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens).\n> >\n\n[GitHub Models](https://docs.github.com/en/github-models/prototyping-with-ai-models) is a service from GitHub that provides developers an easy way to prototype with LLMs and Generative AI. All that you need is a GitHub account, and you can be running against all the latest models from OpenAI and others *without* having to sign up to those services directly.\n\n>\n> GitHub Models is strictly for \"prototyping\" so it comes with some hefty usage limits and [content filters](https://azure.microsoft.com/en-us/products/ai-services/ai-content-safety).\n> >\n\nTo get started with GitHub models you just need to choose a model and retrieve a token:\n\n1. Go to [github.com/marketplace/models](https://github.com/marketplace/models).\n2. Click **Model: Select a Model** at the top left of the page.\n3. Choose a model from the dropdown menu.\n\nAfter selecting a model, you'll see the screen below. As you can see, you can get SDK details and see various other configuration options:\n\n![The getting started page from GitHub models](/content/images/2025/githubmodels.png)\n\nWe don't need any to worry about any of that SDK information, as that's already handled by the .NET NuGet packages and the template. All you need is a personal access token (PAT):\n\n- Click **Get developer key** on the above screen\n- This redirects to GitHub's token management page: [https://github.com/settings/tokens](https://github.com/settings/tokens)\n- Click **Generate new token**, and generate a fine grained token (not classic)\n- Enter a name for the token, and an expiry ~~but do not add any permissions or roles.~~ Note that [as of May 15 2025](https://github.blog/changelog/2025-05-15-modelsread-now-required-for-github-models-access/), your fine-grained GitHub Models tokens must now have the `model:read`\npermission.\n\nAfter creating the token, you can add it as a secret to your application. You need to add the token as a connection string inside the Aspire AppHost project. You can do that using the IDE editor integration in Visual Studio or Rider, or you can use the command line. For example, for my app, I ran the following (replacing `YOUR-API-KEY` with the token value):\n\n```bash cd ModernDotNetShowChat.AppHost dotnet user-secrets set ConnectionStrings:openai \"Endpoint=https://models.inference.ai.azure.com;Key=YOUR-API-KEY\"\n\n```\n\nAs you can probably tell from the above setting, GitHub Models runs using [Azure OpenAI Service](https://azure.microsoft.com/en-us/products/ai-services/openai-service), hence the reference to Azure in the connection string. If you choose a different LLM provider then these settings will be different.\n\nWith the secret added, everything is ready to take the template for a spin.\n\n## Briefly trying out the template\n\nBefore we dig in further, we'll take the standard template for a spin.\n\n>\n> You can read more about getting started in [the Preview 2 announcement post](https://devblogs.microsoft.com/dotnet/announcing-dotnet-ai-template-preview2/) for the template.\n> >\n\nYou run the app by running the Aspire AppHost project. This starts the web app (and passes in all the required connection strings). The web app then runs an \"ingestion\" process against 2 pdf files (about watches) that are available in the content folder. More on this later.\n\nThe web app is a \"traditional\" chat application, just like you've seen with ChatGPT or GitHub Copilot Chat. This interface lets you ask questions about the PDFs that were ingested. In the example below I asked the question \"Which watches are available\":\n\n![Trying out the default template](/content/images/2025/githubmodels_02.png)\n\nThe chat assistant interprets your question and decides what phrases to search for in the documents. It then answers your question based on the details it finds in the documents, and even provides a link to the file that contains the answer.\n\n>\n> This general technique of providing \"sources\" for the LLM to use, instead of relying on the built-in knowledge is called retrieval-augmented generation (RAG), and is one way to try to ensure that the LLM provides answers grounded in facts. It involves ingesting source data, encoding it as vectors in a vector store, and making this store available to the LLM.\n> >\n\nThat's pretty much all there is to the app, but it provides a powerful template that you can extend and modify to work for your own application. For the remainder of the post I look at a couple of points of interest about the template.\n\n## The Aspire App Host\n\nWe'll start by looking at the Aspire App Host. This is where the general architecture of the app is defined, and which reveals that there are essentially three components:\n\n- The OpenAI (via GitHub models) connection\n- The Blazor web app\n- A SQLite database as a cache for the generated embeddings\n\nYou can see all this configured in the *Program.cs* file:\n\n```csharp var builder = DistributedApplication.CreateBuilder(args);\n\nvar openai = builder.AddConnectionString(\"openai\");\n\nvar ingestionCache = builder.AddSqlite(\"ingestionCache\");\n\nvar webApp = builder.AddProject<Projects.ModernDotNetShowChat_Web>(\"aichatweb-app\"); webApp.WithReference(openai); webApp .WithReference(ingestionCache) .WaitFor(ingestionCache);\n\nbuilder.Build().Run();\n\n```\n\nWhen you run the AppHost, Aspire initializes the SQLite database and starts the web app, passing in the connection strings.\n\n## The web app\n\nThe main application is a Blazor Server app. In addition to the standard Blazor and ASP.NET Core services, it contains three main components:\n\n- The GitHub Models/OpenAI chat client. This is the core infrastructure for interacting with the OpenAI service using the OpenAI NuGet package and Microsoft.Extensions.AI abstractions. This also registers an embedding generator for OpenAI for converting the ingested documents into vectors that can later be serialised.\n- The vector store is responsible for persisting the generated embeddings, and for providing the mechanism for OpenAI to search the store as required.\n- The ingestion cache is an EF Core `DbContext`\nthat keeps track of which documents have been ingested into the vector store. This allows the app to avoid ingesting the same documents multiple times as well as to remove documents that are no longer available.\n\nThe configuration of these services all happens in the *Program.cs* file of the web app, as shown below. I haven't reproduced the whole file here, just the configuration related to the above components:\n\n```csharp var builder = WebApplication.CreateBuilder(args);\n\n// Add the OpenAI chat client to the container var openai = builder.AddAzureOpenAIClient(\"openai\"); openai.AddChatClient(\"gpt-4o-mini\") // Use the ChatGPT 4o mini model .UseFunctionInvocation() // Allow the LLM to call local functions in your app .UseOpenTelemetry(configure: c => // Configure OTel for c.EnableSensitiveData = builder.Environment.IsDevelopment()); openai.AddEmbeddingGenerator(\"text-embedding-3-small\"); // Allow generating text embeddings\n\n// Add an IVectorStore implementation that stores the embeddings in a JSON file var vectorStore = new JsonVectorStore(Path.Combine(AppContext.BaseDirectory, \"vector-store\")); builder.Services.AddSingleton<IVectorStore>(vectorStore); builder.Services.AddScoped<DataIngestor>(); // Used to ingest embeddings builder.Services.AddSingleton<SemanticSearch>(); // Used to search embeddings\n\n// Add the EF Core DbContext for tracking which files have been ingested builder.AddSqliteDbContext<IngestionCacheDbContext>(\"ingestionCache\");\n\n```\n\nWhen the app starts, it ensures the SQLite database has been created, starts the web app, and then starts the data ingestion:\n\n```csharp await DataIngestor.IngestDataAsync( app.Services, new PDFDirectorySource(Path.Combine(builder.Environment.WebRootPath, \"Data\")));\n\n```\n\nMuch of the chat application uses standard NuGet packages for interacting with the LLM, however the `DataIngestor` and `PdfDirectorySource` implementations are specific to the template, and show a general approach to generating text embeddings.\n\n## Ingesting data and generating embeddings\n\nThe `DataIngestor` implementation defined in the template manages the storing of text embedding vectors in an `IVectorStore` based on the implementation in an `IIngestionSource` , using the `IngestionCacheDbContext` to track which documents have been previously ingested.\n\nThe implementation, reproduced below, is pretty self-explanatory, but I've added a few extra comments for clarity:\n\n```csharp public class DataIngestor( ILogger<DataIngestor> logger, IEmbeddingGenerator<string, Embedding<float>> embeddingGenerator, IVectorStore vectorStore, IngestionCacheDbContext ingestionCacheDb) { public async Task IngestDataAsync(IIngestionSource source) { // Get or create the \"collection\" for holding the embeddings in the vector store var vectorCollection = vectorStore.GetCollection<string, SemanticSearchRecord>(\"data-moderndotnetshowchat-ingested\"); await vectorCollection.CreateCollectionIfNotExistsAsync();\n\n// Read which documents have already been ingested from the SQLite cache var documentsForSource = ingestionCacheDb.Documents .Where(d => d.SourceId == source.SourceId) .Include(d => d.Records);\n\n// Ask the IIngestionSource for a list of files to delete var deletedFiles = await source.GetDeletedDocumentsAsync(documentsForSource);\n\n// Delete the removed files from the IVectorStore and the SQLite cache foreach (var deletedFile in deletedFiles) { logger.LogInformation(\"Removing ingested data for {file}\", deletedFile.Id); await vectorCollection.DeleteBatchAsync(deletedFile.Records.Select(r => r.Id)); ingestionCacheDb.Documents.Remove(deletedFile); } await ingestionCacheDb.SaveChangesAsync();\n\n// Ask the IIngestionSource for a list of new or modified files to ingest var modifiedDocs = await source.GetNewOrModifiedDocumentsAsync(documentsForSource);\n\n// For each new/modified document: // - Delete the embeddings if they already exist (changed files) // - Generate the embeddings for the document // - Save the embeddings in the IVectorStore // - Record the updated status in the SQLite cache foreach (var modifiedDoc in modifiedDocs) { logger.LogInformation(\"Processing {file}\", modifiedDoc.Id);\n\nif (modifiedDoc.Records.Count > 0) { await vectorCollection.DeleteBatchAsync(modifiedDoc.Records.Select(r => r.Id)); }\n\nvar newRecords = await source.CreateRecordsForDocumentAsync(embeddingGenerator, modifiedDoc.Id); await foreach (var id in vectorCollection.UpsertBatchAsync(newRecords)) { }\n\nmodifiedDoc.Records.Clear(); modifiedDoc.Records.AddRange(newRecords.Select(r => new IngestedRecord { Id = r.Key, DocumentId = modifiedDoc.Id }));\n\nif (ingestionCacheDb.Entry(modifiedDoc).State == EntityState.Detached) { ingestionCacheDb.Documents.Add(modifiedDoc); } }\n\nawait ingestionCacheDb.SaveChangesAsync(); logger.LogInformation(\"Ingestion is up-to-date\"); } }\n\n```\n\nI won't dive into the `IIngestionSource` implementation in this post, as I'll take a closer look at an alternative implementation in the next post. At a high level, the `PDFDirectorySource` :\n\n- Reads the list of *.pdf* files available in the source directory.\n- Uses the last write time for the file to determine whether it has been modified or not.\n- Uses [the PdfPig library](https://github.com/UglyToad/PdfPig) to read the text of the PDF document.\n- Generates an embedding for each paragraph, of each file.\n\n## The chat flow and embeddings\n\nSo how does this all come together?\n\nThe core of the implementation is in the `Chat.razor` component. This component configures the `IChatClient` with a system prompt and a tool/function invocator which the LLM can use to search the local embeddings by invoking `SearchAsync()` .\n\nThe system prompt and tool are provided as follows:\n\n```csharp private const string SystemPrompt = @\" You are an assistant who answers questions about information you retrieve. Do not answer questions about anything else. Use only simple markdown to format your responses.\n\nUse the search tool to find relevant information. When you do this, end your reply with citations in the special XML format:\n\n<citation filename='string' page_number='number'>exact quote here</citation>\n\nAlways include the citation in your response if there are results.\n\nThe quote must be max 5 words, taken word-for-word from the search result, and is the basis for why the citation is relevant. Don't refer to the presence of citations; just emit these tags right at the end, with no surrounding text. \";\n\nprivate readonly ChatOptions chatOptions = new(); private readonly List<ChatMessage> messages = new();\n\nprotected override void OnInitialized() { messages.Add(new(ChatRole.System, SystemPrompt)); chatOptions.Tools = [AIFunctionFactory.Create(SearchAsync)]; }\n\n[Description(\"Searches for information using a phrase or keyword\")] private async Task<IEnumerable<string>> SearchAsync( [Description(\"The phrase to search for.\")] string searchPhrase, [Description(\"If possible, specify the filename to search that file only. If not provided or empty, the search includes all files.\")] string? filenameFilter = null) { await InvokeAsync(StateHasChanged); IReadOnlyList<SemanticSearchRecord> results = await Search.SearchAsync(searchPhrase, filenameFilter, maxResults: 5); return results.Select(result => $\"<result filename=\\\"{result.FileName}\\\" page_number=\\\"{result.PageNumber}\\\">{result.Text}</result>\"); }\n\n```\n\nThe system prompt here is interesting; it shows how the prompt tries very hard to restrict the LLM to *only* producing facts based on the files provided rather than the inherent \"knowledge\" it has. From what I've seen from my testing, this seems to work pretty well!\n\nThat's as far as I'm going to go in this post. In the next post I describe an experiment which starts from this template and modifies it to ingest data from a website instead, so that you can chat and ask questions about the website instead.\n\n## Summary\n\nIn this post I introduced [the new .NET AI Chat Web App template (currently in preview)](https://devblogs.microsoft.com/dotnet/announcing-dotnet-ai-template-preview2/) and showed the default experience of chatting about PDF files. I then described the basic mechanics of the template, and showed some of the code and services around the core features of data ingestion and embedding generation. In the next post I show how you can modify the template to ingest data from a website instead.\n\n## Tags\n\nAndrew Lock | .Net Escapades\n\n![](/assets/img/icons/apple/apple-touch-icon-180x180.png) Want an email when there's new posts?\n\nStay up to the date with the latest posts!\n\nOops! Check your details and try again.\n\nThanks! Check your email for confirmation.",
  "PubDate": "2025-05-06T09:00:00+00:00"
}
