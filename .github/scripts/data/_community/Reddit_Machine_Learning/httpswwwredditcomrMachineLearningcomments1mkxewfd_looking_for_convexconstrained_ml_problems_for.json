{
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1mkxewf/d_looking_for_convexconstrained_ml_problems_for/",
  "Tags": [
    "MachineLearning"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Machine Learning",
  "ProcessedDate": "2025-08-08 16:08:38",
  "Title": "[D] Looking for convex-constrained ML problems for benchmarks",
  "FeedLevelAuthor": "Machine Learning",
  "EnhancedContent": "Hello,\n\nI am looking for Machine Learning (ML) use cases to try out a class of optimization algorithms, namely Frank Wolfe (FW) algorithms. Those are *gradient-based* and *projection-free* algorithms for optimizing a cost function (convex or non-convex) over a *convex* set of constraints. Usually, such problems are tackled by Projected Gradient Descent (PGD), where each iteration consists of a descent in the direction of the gradient, then a projection onto the set of constraints to ensure that the new solution is feasible. However, depending on the set of constraints, this projection step can be very costly and thus prohibitive. FW algorithms avoid this projection step, which leads to less compute-intensive iterations.\n\nI am turning toward [r/machinelearning](/r/machinelearning/) communities for ideas of problems that satisfy those conditions: optimization over a convex set of constraints (original or relaxed version of a problem), ideally that can be large-scale so I can push the FW algorithms to their limits.\n\nFor the moment, I found those following problems:\n\n- **Adversarial attack** : modifying an image in a imperceptible way for a human so that a classifier misclassifies it. The modification ùõø can be constrained in the ùúÄ-ball so that it remains small, which is a convex set so it fits the description.\n- **Polynomial Regression**/**Compressed Sensing**: when we need a sparse represention, we can set the constraint that the coefficients live in the L1-norm ball that is sparsity-inducing.\n- **Matrix Completion**: not the original formulation that constrain that the rank of the matrix *X* denoted rank(*X*) is low, but setting a constraint of the nuclear-norm value of the matrix *X*, which is a convex constraint.\n\nI am also looking for optimization over the set of Doubly Stochastic Matrices (also called the Birkhoff polytope, which is the convex hull of permutation matrices), but I've been looking for a few hours on Google and I haven't found any concrete application, so if you have any ideas I will gladly take them. I've heard that they are useful in matching/assignment problems.\n\nThanks for reading",
  "Author": "Ttghtg",
  "PubDate": "2025-08-08T14:55:45+00:00",
  "Description": "Hello,\n\nI am looking for Machine Learning (ML) use cases to try out a class of optimization algorithms, namely Frank Wolfe (FW) algorithms. Those are *gradient-based* and *projection-free* algorithms for optimizing a cost function (convex or non-convex) over a *convex* set of constraints. Usually, such problems are tackled by Projected Gradient Descent (PGD), where each iteration consists of a descent in the direction of the gradient, then a projection onto the set of constraints to ensure that the new solution is feasible. However, depending on the set of constraints, this projection step can be very costly and thus prohibitive. FW algorithms avoid this projection step, which leads to less compute-intensive iterations.\n\nI am turning toward [r/machinelearning](/r/machinelearning) communities for ideas of problems that satisfy those conditions: optimization over a convex set of constraints (original or relaxed version of a problem), ideally that can be large-scale so I can push the FW algorithms to their limits.\n\nFor the moment, I found those following problems:\n\n- **Adversarial attack** : modifying an image in a imperceptible way for a human so that a classifier misclassifies it. The modification ùõø can be constrained in the ùúÄ-ball so that it remains small, which is a convex set so it fits the description.\n- **Polynomial Regression**/**Compressed Sensing**: when we need a sparse represention, we can set the constraint that the coefficients live in the L1-norm ball that is sparsity-inducing.\n- **Matrix Completion**: not the original formulation that constrain that the rank of the matrix *X* denoted rank(*X*) is low, but setting a constraint of the nuclear-norm value of the matrix *X*, which is a convex constraint.\n\nI am also looking for optimization over the set of Doubly Stochastic Matrices (also called the Birkhoff polytope, which is the convex hull of permutation matrices), but I've been looking for a few hours on Google and I haven't found any concrete application, so if you have any ideas I will gladly take them. I've heard that they are useful in matching/assignment problems.\n\nThanks for reading"
}
