{
  "Title": "[D] Simple Questions Thread",
  "FeedName": "Reddit Machine Learning",
  "OutputDir": "_community",
  "Tags": [
    "MachineLearning"
  ],
  "ProcessedDate": "2025-08-05 17:32:20",
  "Description": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!",
  "Author": "AutoModerator",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1meysr1/d_simple_questions_thread/",
  "PubDate": "2025-08-01T15:01:19+00:00",
  "EnhancedContent": "Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!\n\nIf you are an ML Researcher, what do you think is the most important development of the last 6 months?\n\nJust a newbie here, so be gentle please, I wanted to bounce some of my unqualified ideas off some folks because they might have merit beyond a frontier model just humoring me:\n\nInstead of relying purely on gradient-based learning, is there a practical way to capture specific layers/activations that indicate \"this concept is being processed\"? My thinking: if you could map which activation patterns correspond to specific concepts or reasoning paths, you might be able to:\n\nCreate shortcuts to refined results by injecting known-good patterns Build episodic memory systems using activation patterns as storage/retrieval keys Potentially make inference more efficient for repeated concept combinations\n\nSome half-baked ideas I'm exploring:\n\nUsing backprop during inference to identify which activations contributed to successful responses, then storing those patterns MOE architectures with specialized memory experts that activate based on activation similarity Hybrid approaches where certain layers can be \"bypassed\" when similar activation patterns have been cached\n\nBefore I go too deep down any rabbit holes: Are these directions that have practical merit, or am I missing fundamental limitations? I've had mixed experiences in other technical communities where enthusiasm meets incomplete knowledge, so I'm trying to gauge feasibility before investing too much time. Happy to elaborate on any of these if they sound interesting rather than completely off-base.\n\nFor your main question: This sounds like you're talking somewhat about a [Mixture Of Experts](https://huggingface.co/blog/moe#a-brief-history-of-moes) model. MOE systems map tokens to particular \"experts\" (really just small feed-forward networks) instead of determining what is being learned, which you seem to be proposing. The main problem with the idea you're proposing digs into the core concept of the [black-box model](https://www.ibm.com/think/topics/black-box-ai). If we understood the specific ways in which a model learned concepts then yeah it would be really straightforward to make more efficient systems. This isn't the case though. It's far easier to architect a system that trains in a specific way than to try to figure out how the system learned what it did in the first place. The reason people rely on gradients is that they're simple, well-optimized, and still produce novel results. A general rule of thumb is that integrating more abstract logic and training systems into AI makes it harder to train and less generalizable, rather than a better reasoner.\n\nRegarding the half-baked idea, I'm not following it too well sorry. Overall, I don't doubt that people are working on shortcuts for specific inference time responses, but with an emphasis on scale these days, I wouldn't be surprised if this type of research is less focused. Determining what counts as similar to existing stored information and when to use shortcuts comes with its own set of complications, in addition to existing issues of generalizability.\n\n**\\*\\*Important Side Note:** I actually like that this question was asked. If you (or any other beginners for that matter) have concerns on practicality, what is a good idea vs. a bad one, etc, why not try it out! **If you want to make a change and feed your curiosity, learning the fundamentals will only help**. By understanding these concepts more, you can gain much better insights and start to understand AI better via experimentation (it's more fun this way too). The barrier to entry is much lower than people may think, especially if you've shown past interest. Additionally, numerous simpler, practical problems can be solved now using AI implementation, rather than trying to be on the cutting edge of current AI optimization. If you want resources, well, we have chatbots don't we? Honestly, chatbots are great at finding resources and compiling general source material for questions like \"beginner's curriculum for AI\".\n\nI'm a beginner in AI, just a user attempting to understand LLMs and platform architecture, etc.\n\nI predominantly use one AI platform to learn about its AI architecture—for example, prompts, personalisation, hallucinations, sanitisation, agreement bias, optimisation, engagement, etc. Honesty, wherever the rabbit hole takes me…\n\nWhat concerns me the most is the high risk of user manipulation, particularly in how output is determined to maximise engagement. Or how outputs are crafted, not in the best interest of users, but on how not to make users too uncomfortable, so that they keep engaged with the platform.\n\nWhat confuses me the most is that the assistant provided this information!? I’m aware that my understanding of AI is limited, however, given the platform, I would have thought that would go against native programming?\n\nIt also disclosed how to bypass guardrails, how to counter or exploit model behaviours, inverse prompting, and how to use quotes to attempt flying under the radar.\n\nIt seemed like it was showing me how to jailbreak the platform without getting flagged? What am I missing here?\n\nI am interested in learning AI. I don't know programming as I used to work with desktop software tech support. I have used ChatGPT etc. I have seen Coursera, edx, Udacity, Google. I am bad at reading and trying to understand what I read its a very bad learning disability. If I saw videos of something I would be able to pick up the idea visual learning. I just don't know whats the best way to start a career path in AI. I am 47yr old unemployed Should I look into AI agents where to start to get my toes in the water to make me stand out to get a job to start in this field. I have seen coursiv learn 15 min skills theres so many resources I just need to focus on one site and follow through. Google looks most interesting to learn. Thoughts. Thanks\n\nYeah, that's a solid way to keep things organized—bunching up questions in one thread cuts down on the mess and makes it easier for people to track down answers. From my time messing around with SideProjects, it's helped speed up community support by zeroing in on the main problems instead of spreading things out. If you've got a specific Supabase question, just toss it in here!",
  "FeedLevelAuthor": "Machine Learning",
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss"
}
