{
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1mjh0cp/d_fp4_training_methods_request_for_paper/",
  "Tags": [
    "MachineLearning"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Machine Learning",
  "ProcessedDate": "2025-08-08 16:08:39",
  "Title": "[D] FP4 training methods (request for paper recommendations)",
  "FeedLevelAuthor": "Machine Learning",
  "EnhancedContent": "The new OSS models by OpenAI have low precision weights (MXFP4). Does anyone know:\n\n- Is it likely that they were trained with MXFP4?\n- Could anyone recommend papers on how to train models in such a low precision? Is it possible to train with SGD in such a low range, i.e. FP4, has just 16 values?\n- Is it possible to go even lower? I.e. FP3 or FP2?\n\nI don’t think FP3 or FP2 could exist, you need 2(?) bits in the exponent for a float to make any sense.\n\n> > >\n> Is it likely that they were trained with MXFP4?\n> > > >\n\nI could see MXFP4 being used in the forward pass, I am less sure about the backward pass, I don’t think it was used for the master weights.\n\nSee [https://youtu.be/5TFDG-y-EHs](https://youtu.be/5TFDG-y-EHs) for use of FP3\n\n1.57 bits models exist (ternary: -1, 0 ,1). IIRC, the gradients are in higher resolution but the weights remain in these 3 states.\n\nHere's a recent method of ours. You still need to keep a high-precision master copy of weights, but otherwise it's normal optimizer and hyper-parameters. We also quantized the backward pass and show that it's worth it from the real convergence speed perspective. [https://arxiv.org/abs/2505.14669](https://arxiv.org/abs/2505.14669)\n\nThe [NVIDIA post](https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss/) says that they were trained with H100s, which dont have MXFP4 support, the [vLLM](https://blog.vllm.ai/2025/08/05/gpt-oss.html) blog here says that that they used the linked triton kernel(under the MoE section). If you go into the Triton code, you can see that it goes to tl.scaled\\_dot() for the Hopper arch path, which is a Triton function that maps differently depending on the underlying hardware. Going to the Triton backend, the function computes in fp16 for H100s. So they emulated MXFP4 on FP16 computation hardware.\n\n2)\n\nQAT is usually the way to go in these situations, meaning that you clamp your values into the range of buckets. For FP4 its 16 buckets. You can imagine that the error is quite large scales of (0.05), but the magic here is that this is fully-connected layers, so you have dot products with large vectors. When you have a large dot product, lets say between vectors A and B, you are doing this: A1\\*B1 + A2\\*B2 ... A8000 \\* B8000, etc. In each multiplication, you have the chance to cancel the noise introduced by the previous quantized multiplication since the noise is normally distributed around 0. This means that the noise grows in SQRT(N) when the signal grows in N.\n\n3)\n\nYou can go down all the way to 1 bit\n\nThink a bit about 2 things here:\n- What hardware support do we have\n- What does a computation represent\n\n1-bit networks for example fail to represent magnitude, you can only do logical OR/AND as there is not enough information to do magnitudes. This is not to say that the technique is useless, there can be parts of your neural network that don't need magnitudes.\n\nFP3 represents 8 buckets, FP2 would represent 4 buckets.\n\nThe point here is that if you know that some parts of your network do not need fine-grained information representation, you can go down in precision.\n\nThanks for your answer, it's very detailed and helpful!",
  "Author": "ArtisticHamster",
  "PubDate": "2025-08-06T21:17:56+00:00",
  "Description": "The new OSS models by OpenAI have low precision weights (MXFP4). Does anyone know:\n\n- Is it likely that they were trained with MXFP4?\n- Could anyone recommend papers on how to train models in such a low precision? Is it possible to train with SGD in such a low range, i.e. FP4, has just 16 values?\n- Is it possible to go even lower? I.e. FP3 or FP2?"
}
