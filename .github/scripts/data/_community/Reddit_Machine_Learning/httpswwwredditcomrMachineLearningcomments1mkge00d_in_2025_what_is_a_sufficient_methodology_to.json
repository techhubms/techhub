{
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1mkge00/d_in_2025_what_is_a_sufficient_methodology_to/",
  "Tags": [
    "MachineLearning"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Machine Learning",
  "ProcessedDate": "2025-08-08 16:08:38",
  "Title": "[D] In 2025, what is a sufficient methodology to analyze document summaries generated by LLMs? BERTScore, G-Eval, Rogue, etc",
  "FeedLevelAuthor": "Machine Learning",
  "EnhancedContent": "Greetings,\n\nAt work, I am currently building a very simple document summarization platform that takes in source documents, produces small and concise summaries of the documents, and storing them in a database.\n\nThe project plans to expand to a lot of other functionalities later on, but for the moment I've been asked to determine a way to \"grade\" or \"analyze\" the generated summaries against the original source text and give it a score, as an aid for some of our human reviewers.\n\nI've been working on this for about a week, and have tried various methods like BERTScore, MoverScore, G-eval, ROGUE, BLEU and the like. And I've come to the conclusion that the scores themselves don't tell me a lot, at least personally (which could simply be due in part to me misunderstanding or overlooking details). For example I understand cosine similarity to a degree, but it's hard to put into context of \"grade this summary.\" I've also tried out an idea about sending the summary to another decoder-only model (such as Qwen or even Phi-4), asking it to extract key facts or questions, then running each of those through a BERT NLI model against chunks of the source material (checking \"faithfulness\" I believe). I also thought about maybe doing some kind of \"miniature RAG\" against a single document and seeing how that relates to the summary itself, as in to find gaps in coverage.\n\nFor the most part, I wasn't disappointed in the results but I also was not thrilled by them either. Usually I'd get a score that felt \"middle of the road\" and would be difficult to determine whether or not the summary itself was good.\n\nSo my question is: Does anyone here have any experience with this and have any suggestions for things to try out or experiment with? I feel like this might be a large area of ongoing research as is, but at this point we (where I work) might actually just be striving for something simple.\n\nThanks!\n\n1. Generate \"counterfactuals\" by having an LLM create questions you should be able to answer from the original.\n2. Use G-Eval/RAGAS to judge if the questions are still answerable from the summary.\n\nYou can probably get cheaper to generate and \"shallower to inspect\" metrics comparing originals, summaries, and generated Q&A using RAGAS, too.\n\nThese are cheap/easy ways. High quality would involve skilled annotators. A large quantity of lower quality LLM as judge metrics has a quality of its own, though.\n\nYou might look at LLMLingua from Microsoft to start from a terse \"compression\" of the documents, too. If nothing else, it might save you some time and API or compute billing.\n\nAwesome, thanks for the input :) I'll definitely be checking some of those out.\n\nI forgot to mention that we're hosting everything in house, on H100 boxes. Models are generally through vLLM, and we not against building something if vLLM cannot handle it for us. The only thing we have to plan out are models to GPUs and internal access sorta stuff, but no consideration for API/compute billing.\n\nMaybe get more focused feedback in [r/LanguageTechnology](/r/LanguageTechnology/)",
  "Author": "IThrowShoes",
  "PubDate": "2025-08-08T00:07:35+00:00",
  "Description": "Greetings,\n\nAt work, I am currently building a very simple document summarization platform that takes in source documents, produces small and concise summaries of the documents, and storing them in a database.\n\nThe project plans to expand to a lot of other functionalities later on, but for the moment I've been asked to determine a way to \"grade\" or \"analyze\" the generated summaries against the original source text and give it a score, as an aid for some of our human reviewers.\n\nI've been working on this for about a week, and have tried various methods like BERTScore, MoverScore, G-eval, ROGUE, BLEU and the like. And I've come to the conclusion that the scores themselves don't tell me a lot, at least personally (which could simply be due in part to me misunderstanding or overlooking details). For example I understand cosine similarity to a degree, but it's hard to put into context of \"grade this summary.\" I've also tried out an idea about sending the summary to another decoder-only model (such as Qwen or even Phi-4), asking it to extract key facts or questions, then running each of those through a BERT NLI model against chunks of the source material (checking \"faithfulness\" I believe). I also thought about maybe doing some kind of \"miniature RAG\" against a single document and seeing how that relates to the summary itself, as in to find gaps in coverage.\n\nFor the most part, I wasn't disappointed in the results but I also was not thrilled by them either. Usually I'd get a score that felt \"middle of the road\" and would be difficult to determine whether or not the summary itself was good.\n\nSo my question is: Does anyone here have any experience with this and have any suggestions for things to try out or experiment with? I feel like this might be a large area of ongoing research as is, but at this point we (where I work) might actually just be striving for something simple.\n\nThanks!"
}
