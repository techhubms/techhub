{
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1mkelg5/d_can_llms_have_accurate_world_models/",
  "Tags": [
    "MachineLearning"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Machine Learning",
  "ProcessedDate": "2025-08-08 16:08:38",
  "Title": "[D] Can LLMs Have Accurate World Models?",
  "FeedLevelAuthor": "Machine Learning",
  "EnhancedContent": "I have seen many articles (one example [https://aiguide.substack.com/p/llms-and-world-models-part-1](https://aiguide.substack.com/p/llms-and-world-models-part-1)) stating that LLMs have no coherent/effective world models and because of this their accuracy is inherently limited. Can this obstacle be overcome, and if not why?\n\nThis is much debated and there are basically two positions:\n\n1. Obviously yes it has an internal world model, because it can answer questions like 'can a pair of scissors cut through a Boeing 747?' that do not appear in the training data.\n2. Obviously no it does not have an internal world model, because it hallucinates and doesn't generalize well out-of-domain.\n\nIt seems like the obvious resolution is that it’s possible to have elements of a world model without a fully consistent or complete one\n\nI would argue that 1 is the case and hallucinations are more a problem of the training objective then the internal representations.\n\nIt certainly fair to say that LLMs have internal models of the abstract world, things like shapes and language. But that’s really different than being able to understand causality in the physical world, do counter-factual work, model real physics, etc.\n\nYeah, the latter is a physics model. All language vector spaces are world models. Whether they are good world models or bad is another question, but they are world models. A good world model should also encompass a good physics model.\n\nI think the OthelloGPT research, up through the Centaur 70B research seems to indicate that they have sophisticated world (and self) models.\n\nI think this post summarizes it pretty well:\n\n[https://www.reddit.com/r/artificial/s/U2PrAfkDHC](https://www.reddit.com/r/artificial/s/U2PrAfkDHC)\n\nHuman has human world models, octupus has octupus world models. Obviously LLM can have LLM world models.\n\nThe better question is can LLMs have human world models.\n\nAs another data point, Keyon Vafa and Sendhil Mullainathan published two papers on the ability of transformers to learn accurate world models that seem to broadly suggest the answer is \"not currently\": [https://neurips.cc/virtual/2024/poster/94550](https://neurips.cc/virtual/2024/poster/94550) and [https://arxiv.org/abs/2507.06952](https://arxiv.org/abs/2507.06952)\n\nObviously LLMs are much larger models, but they also have much more to learn, so the insights from these papers seem transferable to LLMs.\n\nI don’t think they have a stance on whether learning an accurate world model is in principle impossible, though.\n\nApproximate yes, accurate no.",
  "Author": "NandoGando",
  "PubDate": "2025-08-07T22:48:30+00:00",
  "Description": "I have seen many articles (one example [https://aiguide.substack.com/p/llms-and-world-models-part-1](https://aiguide.substack.com/p/llms-and-world-models-part-1)) stating that LLMs have no coherent/effective world models and because of this their accuracy is inherently limited. Can this obstacle be overcome, and if not why?"
}
