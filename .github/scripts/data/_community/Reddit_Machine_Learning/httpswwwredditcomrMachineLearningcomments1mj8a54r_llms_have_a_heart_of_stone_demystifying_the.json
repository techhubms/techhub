{
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1mj8a54/r_llms_have_a_heart_of_stone_demystifying_the/",
  "Tags": [
    "MachineLearning"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Machine Learning",
  "ProcessedDate": "2025-08-08 16:08:39",
  "Title": "[R] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models",
  "FeedLevelAuthor": "Machine Learning",
  "EnhancedContent": "**TL;DR**: [Soft tokens](https://www.arxiv.org/abs/2505.15778) (probabilities-weighted sum over vocab) actually underperform traditional \"hard\" tokens. But a Gumbel-Softmax trick can salvage this issue.\n\n**Paper:** [https://www.arxiv.org/pdf/2508.03440](https://www.arxiv.org/pdf/2508.03440)\n\n**Abstract:**\n\n> > >\n> Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the `Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.\n> > > >\n\n**Visual Highlights:**\n\n[!\\[r/MachineLearning - \\\\[R\\\\] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models\\](https://preview.redd.it/r-llms-have-a-heart-of-stone-demystifying-the-soft-thinking-v0-zza3t8r17fhf1.png?width=1099&amp;format=png&amp;auto=webp&amp;s=46b89ae73f814dd70c921790d739929348ccd899)](https://preview.redd.it/r-llms-have-a-heart-of-stone-demystifying-the-soft-thinking-v0-zza3t8r17fhf1.png?width=1099&amp;format=png&amp;auto=webp&amp;s=46b89ae73f814dd70c921790d739929348ccd899)\n\n[!\\[r/MachineLearning - \\\\[R\\\\] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models\\](https://preview.redd.it/r-llms-have-a-heart-of-stone-demystifying-the-soft-thinking-v0-lulzrar27fhf1.png?width=1109&amp;format=png&amp;auto=webp&amp;s=9b1ba1d3e54e9e580b7d4f7c1931a6a576ea86f6)](https://preview.redd.it/r-llms-have-a-heart-of-stone-demystifying-the-soft-thinking-v0-lulzrar27fhf1.png?width=1109&amp;format=png&amp;auto=webp&amp;s=9b1ba1d3e54e9e580b7d4f7c1931a6a576ea86f6)",
  "Author": "StartledWatermelon",
  "PubDate": "2025-08-06T15:48:20+00:00",
  "Description": "| [!\\[\\\\[R\\\\] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models\\](https://b.thumbs.redditmedia.com/4Bhn8GNeXEscPmJtntIn68Svp-bbuRCi23Gt_zNELsw.jpg \"\\[R\\] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models\")](https://www.reddit.com/r/MachineLearning/comments/1mj8a54/r_llms_have_a_heart_of_stone_demystifying_the/) | **TL;DR**: [Soft tokens](https://www.arxiv.org/abs/2505.15778) (probabilities-weighted sum over vocab) actually underperform traditional \"hard\" tokens. But a Gumbel-Softmax trick can salvage this issue.<br><br> <br>**Paper:** [https://www.arxiv.org/pdf/2508.03440](https://www.arxiv.org/pdf/2508.03440)<br><br> <br>**Abstract:**<br><br> <br><br>> <br>> Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the `Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.<br>> <br>> <br><br><br><br> <br>**Visual Highlights:**<br><br> <br>[https://preview.redd.it/zza3t8r17fhf1.png?width=1099&format=png&auto=webp&s=e12815cb0774bce2a2614b2c3ad0df47b071d8c8](https://preview.redd.it/zza3t8r17fhf1.png?width=1099&format=png&auto=webp&s=e12815cb0774bce2a2614b2c3ad0df47b071d8c8)<br><br> <br>[https://preview.redd.it/lulzrar27fhf1.png?width=1109&format=png&auto=webp&s=0fd5cd8dc90a9c09afb46dbd8e0412a72800dbe3](https://preview.redd.it/lulzrar27fhf1.png?width=1109&format=png&auto=webp&s=0fd5cd8dc90a9c09afb46dbd8e0412a72800dbe3)<br>"
}
