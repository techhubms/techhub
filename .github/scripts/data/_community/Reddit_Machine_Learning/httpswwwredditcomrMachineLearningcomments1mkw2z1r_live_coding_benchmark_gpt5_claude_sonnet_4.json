{
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1mkw2z1/r_live_coding_benchmark_gpt5_claude_sonnet_4/",
  "Tags": [
    "MachineLearning"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Machine Learning",
  "ProcessedDate": "2025-08-08 16:08:38",
  "Title": "[R] Live coding benchmark: GPT-5, Claude Sonnet 4, Gemini 2.5 Pro, GLM45 ‚Äî same prompt, varying difficulty",
  "FeedLevelAuthor": "Machine Learning",
  "EnhancedContent": "We‚Äôre running a live comparative test today to see how four leading LLMs handle coding tasks in a natural-language coding environment.\n\n**Models tested:**\n\n- GPT-5\n- Claude Sonnet 4\n- Gemini 2.5 Pro\n- GLM45 (open-source)\n\n**Format:**\n\n- All models receive **the exact same prompt**\n- Multiple runs at different complexity levels:\n\n- Simple builds\n- Bug-fix tasks\n- Multi-step complex builds\n- Possible planning flows\n\nWe‚Äôll compare:\n\n- Output quality\n- Build speed\n- Debugging performance\n\n**When:** Today, 16:00 UTC (19:00 EEST)\n\n**Where:** [https://live.biela.dev](https://live.biela.dev)\n\nHop in with questions, curiosities, prompt suggestions and whatever comes in mind to make the test even better! :)\n\nBotted or paid comments.\n\nI've reported them for Spam. We're not trying anything here it's a honest Live. Some of our post on instagram were botted as well without us requesting, maybe someone else eyed us on Reddit as well and just wants to do harm. Not our comments!\n\nThis is \"research\"? üòÇ\n\nMight not be the most appropriate flair, what do you suggest instead?\n\nWhy not qwen which as the top of coding benchmarks for open weight models? [https://lmarena.ai/leaderboard/text/coding-no-style-control](https://lmarena.ai/leaderboard/text/coding-no-style-control)\n\nYeah, Qwen‚Äôs been crushing coding benchmarks lately,, no doubt about that.\n\nFor this run we had to keep the roster to four so we could give each model enough time (multiple prompts, retries, debugging). We picked a mix that covered open-source (GLM4.5), closed-source (GPT-5, Claude, Gemini), and different reasoning styles.\n\nQwen‚Äôs definitely on the shortlist for a follow-up test especially for a more ‚Äúopen weights only‚Äù focused session.\n\nThat‚Äôs nice, can I ask you what to prompt?\n\nive tried already the gpt 5 for web app and its so wack.\n\nLive? I'm in!\n\nCan't w8 to see it",
  "Author": "darkageofme",
  "PubDate": "2025-08-08T14:03:12+00:00",
  "Description": "We‚Äôre running a live comparative test today to see how four leading LLMs handle coding tasks in a natural-language coding environment.\n\n**Models tested:**\n\n- GPT-5\n- Claude Sonnet 4\n- Gemini 2.5 Pro\n- GLM45 (open-source)\n\n**Format:**\n\n- All models receive **the exact same prompt**\n- Multiple runs at different complexity levels:\n- Simple builds\n- Bug-fix tasks\n- Multi-step complex builds\n- Possible planning flows\n\nWe‚Äôll compare:\n\n- Output quality\n- Build speed\n- Debugging performance\n\n**When:** Today, 16:00 UTC (19:00 EEST)\n\n**Where:** [https://live.biela.dev](https://live.biela.dev)\n\nHop in with questions, curiosities, prompt suggestions and whatever comes in mind to make the test even better! :)"
}
