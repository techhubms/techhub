{
  "FeedUrl": "https://www.reddit.com/r/MachineLearning/.rss",
  "Link": "https://www.reddit.com/r/MachineLearning/comments/1mkyrrw/p_explaining_gnn_predictions_on_linear_dfgs_gnn/",
  "Tags": [
    "MachineLearning"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Machine Learning",
  "ProcessedDate": "2025-08-08 16:08:38",
  "Title": "[P] Explaining GNN Predictions on \"\"linear\"\" DFGs - GNN experts I need your help <3",
  "FeedLevelAuthor": "Machine Learning",
  "EnhancedContent": "I’m working on a research project where, starting from an event log, I build for each trace a Direct Follows Graph (DFG) representing that trace, where each node corresponds to an activity.\n\nMy goals are:\n\n1. From the obtained DFGs, derive Prefix graphs (i.e., DFGs with the final nodes removed) and apply a GNN for **next activity prediction at the node level**. This way, if I feed the model a list of activities during inference, it should return the next activity.\n2. Given the prediction, I want to apply **GNN explainability techniques**, specifically *Perturbation-based methods*and *Surrogate-based methods*, to explain the model’s decision.\n\nMy question is mainly about point 2: since the DFGs are mostly linear (with at most some self-loops or a few normal loops), does it make sense to search for subgraphs that explain the result (e.g., with GNNExplainer or SubgraphX)? For example, if I use a 3-layer GNN, wouldn’t the prediction already be fully explained by the 3-hop neighborhood? These are not very large graphs with huge numbers of edges... maybe I’m missing something.\n\nP.S.: I’m new in the world of GNNs.",
  "Author": "Street_Car_1297",
  "PubDate": "2025-08-08T15:47:29+00:00",
  "Description": "I’m working on a research project where, starting from an event log, I build for each trace a Direct Follows Graph (DFG) representing that trace, where each node corresponds to an activity.\n\nMy goals are:\n\n1. From the obtained DFGs, derive Prefix graphs (i.e., DFGs with the final nodes removed) and apply a GNN for **next activity prediction at the node level**. This way, if I feed the model a list of activities during inference, it should return the next activity.\n2. Given the prediction, I want to apply **GNN explainability techniques**, specifically *Perturbation-based methods*and *Surrogate-based methods*, to explain the model’s decision.\n\nMy question is mainly about point 2: since the DFGs are mostly linear (with at most some self-loops or a few normal loops), does it make sense to search for subgraphs that explain the result (e.g., with GNNExplainer or SubgraphX)? For example, if I use a 3-layer GNN, wouldn’t the prediction already be fully explained by the 3-hop neighborhood? These are not very large graphs with huge numbers of edges... maybe I’m missing something.\n\nP.S.: I’m new in the world of GNNs."
}
