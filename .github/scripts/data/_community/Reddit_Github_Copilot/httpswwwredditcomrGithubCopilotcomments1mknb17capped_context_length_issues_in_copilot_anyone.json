{
  "FeedUrl": "https://www.reddit.com/r/GithubCopilot.rss",
  "Link": "https://www.reddit.com/r/GithubCopilot/comments/1mknb17/capped_context_length_issues_in_copilot_anyone/",
  "Tags": [
    "GithubCopilot"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit Github Copilot",
  "ProcessedDate": "2025-08-08 16:34:46",
  "Title": "Capped Context Length Issues in Copilot - Anyone Else Experiencing This?",
  "FeedLevelAuthor": "GitHubCopilot",
  "EnhancedContent": "Let's fly with Copilot\n\nI've been testing various models in Copilot and noticed they're all capping out at around 128k context length (Found this out with some debugging), even though some models like GPT-5 are supposed to handle 400k. This is causing conversations to get summarized way too early and breaking continuity. Same observation with Sonnet-4, gemini-2.5-pro, gpt-4.1.\n\nHas anyone else run into this? Is this a known limitation right now, or am I missing something in the settings?\n\nReally hoping this gets bumped up to the full supported lengths soon — would make such a difference for longer conversations and complex tasks. Also wasting our Premium requests as part of shorter agent context lengths.\n\nScreenshots attached to which tells what is the actual context length of the model.\n\nAnyone from Copilot team noticing this, Plz restore to full context length.\n\nCopilot would be so much more useful if there was a 1M context for Gemini\n\nI'm interested to hear the reasons behind this, also.\n\nPlease increase the Copilot context window, according to the \"real\" model parameters.\n\nProbably money\n\nLet's fly with Copilot",
  "Author": "EfficientApartment52",
  "PubDate": "2025-08-08T05:57:31+00:00",
  "Description": "| [!\\[Capped Context Length Issues in Copilot - Anyone Else Experiencing This?\\](https://a.thumbs.redditmedia.com/dIYtarECQldWD2NGZ4crvZMFDjtBTqxjNh4jxSy6J70.jpg \"Capped Context Length Issues in Copilot - Anyone Else Experiencing This?\")](https://www.reddit.com/r/GithubCopilot/comments/1mknb17/capped_context_length_issues_in_copilot_anyone/) | I've been testing various models in Copilot and noticed they're all capping out at around 128k context length (Found this out with some debugging), even though some models like GPT-5 are supposed to handle 400k. This is causing conversations to get summarized way too early and breaking continuity.<br> Same observation with Sonnet-4, gemini-2.5-pro, gpt-4.1.<br><br> <br>Has anyone else run into this? Is this a known limitation right now, or am I missing something in the settings?<br><br> <br>Really hoping this gets bumped up to the full supported lengths soon — would make such a difference for longer conversations and complex tasks. Also wasting our Premium requests as part of shorter agent context lengths.<br><br> <br>Screenshots attached to which tells what is the actual context length of the model.<br><br> <br>Anyone from Copilot team noticing this, Plz restore to full context length.<br>"
}
