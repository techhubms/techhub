{
  "ProcessedDate": "2025-08-15 14:40:33",
  "PubDate": "2025-08-13T13:06:16+00:00",
  "Link": "https://techcommunity.microsoft.com/t5/apps-on-azure-blog/build-lightweight-ai-apps-on-azure-app-service-with-gpt-oss-20b/ba-p/4442885",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Author": "TulikaC",
  "FeedName": "Microsoft Tech Community",
  "EnhancedContent": "OpenAI recently introduced gpt-oss as an open-weight language model that delivers strong real-world performance at low cost. Available under the flexible Apache 2.0 license, these models outperform similarly sized open models on reasoning tasks, demonstrate strong tool use capabilities, and are optimized for efficient deployment on consumer hardware; see the announcement:Â [https://openai.com/index/introducing-gpt-oss/](https://openai.com/index/introducing-gpt-oss/).\n\nItâ€™s an excellent choice for scenarios where you want the security and efficiency of a smaller model running on your application instance â€” while still getting impressive reasoning capabilities.\n\nBy hosting it on **Azure App Service**, you can take advantage of **enterprise-grade features** without worrying about managing infrastructure:\n\n- Built-in autoscaling\n- Integration with VNet\n- Enterprise-grade security and compliance\n- Easy CI/CD integration\n- Choice of deployment methods\n\nIn this post, weâ€™ll walk through a complete sample that uses gpt-oss-20b as a **sidecar container** running alongside a Python Flask app on Azure App Service.\n\nAll the source code and Bicep templates are available here: ðŸ“‚ [Azure-Samples/appservice-ai-samples/gpt-oss-20b-sample](https://github.com/Azure-Samples/appservice-ai-samples/tree/main/gpt-oss-20b-sample)\n\n#### **Architecture of our sample at a glance**\n\n- Web app (Flask) runs as a code-based App Service.\n- Model runs in a sidecar container (Ollama) in the same App Service.\n- The Flask app calls the model over localhost:11434.\n- Bicep provisions the Web App and an Azure Container Registry (ACR). You push your model image to ACR and attach it as a sidecar in the Portal.\n\n##### **1. Wrapping gpt-oss-20b in a Container**\n\nWeâ€™ve packaged the model into a container image so it can be easily run as a sidecar.\n\nDockerfile:\n\n``` FROM ollama/ollama EXPOSE 11434 COPY startup.sh / RUN chmod +x /startup.sh ENTRYPOINT [\"./startup.sh\"]\n\n```\n\nstartup.sh\n\n```\n# Start Ollama in the background\nollama serve & sleep 5\n\n# Pull and run gpt-oss:20b\nollama pull gpt-oss:20b\n\n# Restart ollama and run it in the foreground\npkill -f \"ollama\" ollama serve\n\n```\n\n##### **2. The Flask Application**\n\nOur main app is a simple Python Flask service that connects to the model running in the sidecar.\n\nSince the sidecar shares the same network namespace as the main app, we can call it at **http://localhost:11434**.\n\n``` OLLAMA_HOST = \"http://localhost:11434\" MODEL_NAME = \"gpt-oss:20b\"\n\n@app.route(\"/chat\", methods=[\"POST\"]) def chat(): data = request.get_json() prompt = data.get(\"prompt\", \"\") payload = { \"model\": MODEL_NAME, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"stream\": True }\n\ndef generate(): with requests.post(f\"{OLLAMA_HOST}/api/chat\", json=payload, stream=True) as r: for line in r.iter_lines(decode_unicode=True): if line: event = json.loads(line) if \"message\" in event: yield event[\"message\"][\"content\"]\n\nreturn Response(generate(), mimetype=\"text/plain\")\n\n```\n\nThis allows your app to **stream responses back to the browser in real-time** â€” giving a chat-like experience.\n\n##### **3. Deploying to Azure App Service**\n\nYou can deploy the app using your preferred method â€” VS Code, GitHub Actions, az webapp up, or via **Bicep**.\n\nWeâ€™ve included a **Bicep template** that sets up:\n\n- An Azure Container Registry for your sidecar image\n- An Azure Web App running on **Premium V4** for best performance and cost efficiency\nðŸ”— [Azure App Service Premium V4 now in Public Preview](https://techcommunity.microsoft.com/blog/appsonazureblog/azure-app-service-premium-v4-plan-is-now-in-public-preview/4413461)\n\nIf you want to use the azd template, pull down the repo and run these commands from the folder.\n\n``` azd init azd up\n\n```\n\nOpen the Web App in Azure Portal and add a sidecar:\n\n- How-to:Â [https://learn.microsoft.com/azure/app-service/configure-sidecar](https://learn.microsoft.com/azure/app-service/configure-sidecar)\n- Choose your ACR image (the Ollama container), set port to 11434\n\n*First startup note: the sidecar downloads the gpt-oss-20b model on first run; cold start will take time. Subsequent restarts will be faster because the model layers will not need to be pulled down.*\n\nTry it, then open your siteâ€”itâ€™s a chat UI backed by gpt-oss-20b running locally as a sidecar on Azure App Service.\n\n#### **Conclusion**\n\nWith GPT-OSS-20B running as a sidecar on Azure App Service, you get the best of both worlds â€” the flexibility of open-source models and the reliability, scalability, and security of a fully managed platform. This setup makes it easy to integrate AI capabilities into your applications without having to provision or manage custom infrastructure.\n\nWhether youâ€™re building a lightweight chat experience, prototyping a new AI-powered feature, or experimenting with domain-specific fine-tuning, this approach provides a robust foundation. You can scale your application based on demand, swap out models as needed, and take advantage of the full Azure ecosystem for networking, observability, and deployment automation.\n\n**Next Steps & Resources**\n\nHere are some useful resources to help you go further:\n\n- ðŸ“‚ **Sample Code & Templates** â€“ [gpt-oss-20b Sample Repository](https://github.com/Azure-Samples/appservice-ai-samples/tree/main/gpt-oss-20b-sample)\n- ðŸ“– **About GPT-OSS** â€“ [Introducing gpt-oss (OpenAI blog)](https://openai.com/index/introducing-gpt-oss/)\n- ðŸ›  **Deploying Sidecars** â€“ [Configure Sidecars in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/configure-sidecar)\n- ðŸš€ **Premium V4 Plan** â€“ [Azure App Service Premium V4 announcement](https://techcommunity.microsoft.com/blog/appsonazureblog/azure-app-service-premium-v4-plan-is-now-in-public-preview/4413461)\n- ðŸ“¦ **Pushing Images to ACR** â€“ [Push and pull container images in Azure Container Registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-get-started-docker-cli?tabs=azure-cli)\n- ðŸ’¡ **Advanced AI Patterns** â€“ [Build RAG solutions with Azure AI Search](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview?tabs=docs)\n\nUpdated Aug 13, 2025\n\nVersion 1.0\n\n[azure app service](/tag/azure%20app%20service?nodeId=board%3AAppsonAzureBlog)\n\n[python](/tag/python?nodeId=board%3AAppsonAzureBlog)\n\n[!\\[TulikaC&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-1.svg?image-dimensions=50x50)](/users/tulikac/1661700) [TulikaC](/users/tulikac/1661700) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined December 21, 2022\n\n[View Profile](/users/tulikac/1661700)\n\n/category/azure/blog/appsonazureblog [Apps on Azure Blog](/category/azure/blog/appsonazureblog) Follow this blog board to get notified when there's new activity",
  "Title": "Build lightweight AI Apps on Azure App Service with gpt-oss-20b",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "OutputDir": "_community",
  "Description": "OpenAI recently introduced gpt-oss as an open-weight language model that delivers strong real-world performance at low cost. Available under the flexible Apache 2.0 license, these models outperform similarly sized open models on reasoning tasks, demonstrate strong tool use capabilities, and are optimized for efficient deployment on consumer hardware; see the announcement: [https://openai.com/index/introducing-gpt-oss/](https://openai.com/index/introducing-gpt-oss/).\n\nItâ€™s an excellent choice for scenarios where you want the security and efficiency of a smaller model running on your application instance â€” while still getting impressive reasoning capabilities.\n\nBy hosting it on **Azure App Service**, you can take advantage of **enterprise-grade features** without worrying about managing infrastructure:\n\n- Built-in autoscaling\n- Integration with VNet\n- Enterprise-grade security and compliance\n- Easy CI/CD integration\n- Choice of deployment methods\n\nIn this post, weâ€™ll walk through a complete sample that uses gpt-oss-20b as a **sidecar container** running alongside a Python Flask app on Azure App Service.\n\nAll the source code and Bicep templates are available here: ðŸ“‚ [Azure-Samples/appservice-ai-samples/gpt-oss-20b-sample](https://github.com/Azure-Samples/appservice-ai-samples/tree/main/gpt-oss-20b-sample)\n\n#### **Architecture of our sample at a glance**\n\n- Web app (Flask) runs as a code-based App Service.\n- Model runs in a sidecar container (Ollama) in the same App Service.\n- The Flask app calls the model over localhost:11434.\n- Bicep provisions the Web App and an Azure Container Registry (ACR). You push your model image to ACR and attach it as a sidecar in the Portal.\n\n##### **1. Wrapping gpt-oss-20b in a Container**\n\nWeâ€™ve packaged the model into a container image so it can be easily run as a sidecar.\n\nDockerfile:\n\n- FROM ollama/ollama\nEXPOSE 11434 COPY startup.sh / RUN chmod +x /startup.sh ENTRYPOINT [\"./startup.sh\"]\n\nstartup.sh\n- # Start Ollama in the background\nollama serve & sleep 5\n\n# Pull and run gpt-oss:20b\nollama pull gpt-oss:20b\n\n# Restart ollama and run it in the foreground\npkill -f \"ollama\" ollama serve\n\n##### **2. The Flask Application**\n\nOur main app is a simple Python Flask service that connects to the model running in the sidecar.\n\nSince the sidecar shares the same network namespace as the main app, we can call it at **http://localhost:11434**.\n- OLLAMA\\_HOST = \"http://localhost:11434\"\nMODEL\\_NAME = \"gpt-oss:20b\"\n\n@app.route(\"/chat\", methods=[\"POST\"]) def chat(): data = request.get\\_json() prompt = data.get(\"prompt\", \"\") payload = { \"model\": MODEL\\_NAME, \"messages\": [{\"role\": \"user\", \"content\": prompt}], \"stream\": True }\n\ndef generate(): with requests.post(f\"{OLLAMA\\_HOST}/api/chat\", json=payload, stream=True) as r: for line in r.iter\\_lines(decode\\_unicode=True): if line: event = json.loads(line) if \"message\" in event: yield event[\"message\"][\"content\"]\n\nreturn Response(generate(), mimetype=\"text/plain\")\n\nThis allows your app to **stream responses back to the browser in real-time** â€” giving a chat-like experience.\n\n##### **3. Deploying to Azure App Service**\n\nYou can deploy the app using your preferred method â€” VS Code, GitHub Actions, az webapp up, or via **Bicep**.\n\nWeâ€™ve included a **Bicep template** that sets up:\n\n- An Azure Container Registry for your sidecar image\n- An Azure Web App running on **Premium V4** for best performance and cost efficiency\nðŸ”— [Azure App Service Premium V4 now in Public Preview](https://techcommunity.microsoft.com/blog/appsonazureblog/azure-app-service-premium-v4-plan-is-now-in-public-preview/4413461)\n\nIf you want to use the azd template, pull down the repo and run these commands from the folder.\n- azd init\nazd up\n\nOpen the Web App in Azure Portal and add a sidecar:\n\n- How-to: [https://learn.microsoft.com/azure/app-service/configure-sidecar](https://learn.microsoft.com/azure/app-service/configure-sidecar)\n- Choose your ACR image (the Ollama container), set port to 11434\n\n*First startup note: the sidecar downloads the gpt-oss-20b model on first run; cold start will take time. Subsequent restarts will be faster because the model layers will not need to be pulled down.*\n\nTry it, then open your siteâ€”itâ€™s a chat UI backed by gpt-oss-20b running locally as a sidecar on Azure App Service.\n\n![]()\n\n#### **Conclusion**\n\nWith GPT-OSS-20B running as a sidecar on Azure App Service, you get the best of both worlds â€” the flexibility of open-source models and the reliability, scalability, and security of a fully managed platform. This setup makes it easy to integrate AI capabilities into your applications without having to provision or manage custom infrastructure.\n\nWhether youâ€™re building a lightweight chat experience, prototyping a new AI-powered feature, or experimenting with domain-specific fine-tuning, this approach provides a robust foundation. You can scale your application based on demand, swap out models as needed, and take advantage of the full Azure ecosystem for networking, observability, and deployment automation.\n\n**Next Steps & Resources**\n\nHere are some useful resources to help you go further:\n\n- ðŸ“‚ **Sample Code & Templates** â€“ [gpt-oss-20b Sample Repository](https://github.com/Azure-Samples/appservice-ai-samples/tree/main/gpt-oss-20b-sample)\n- ðŸ“– **About GPT-OSS** â€“ [Introducing gpt-oss (OpenAI blog)](https://openai.com/index/introducing-gpt-oss/)\n- ðŸ›  **Deploying Sidecars** â€“ [Configure Sidecars in Azure App Service](https://learn.microsoft.com/en-us/azure/app-service/configure-sidecar)\n- ðŸš€ **Premium V4 Plan** â€“ [Azure App Service Premium V4 announcement](https://techcommunity.microsoft.com/blog/appsonazureblog/azure-app-service-premium-v4-plan-is-now-in-public-preview/4413461)\n- ðŸ“¦ **Pushing Images to ACR** â€“ [Push and pull container images in Azure Container Registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-get-started-docker-cli?tabs=azure-cli)\n- ðŸ’¡ **Advanced AI Patterns** â€“ [Build RAG solutions with Azure AI Search](https://learn.microsoft.com/en-us/azure/search/retrieval-augmented-generation-overview?tabs=docs)",
  "Tags": []
}
