{
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/the-importance-of-streaming-for-llm-powered-chat-applications/ba-p/4459574",
  "Tags": [],
  "Description": "Thanks to the popularity of chat-based interfaces like ChatGPT and GitHub Copilot, users have grown accustomed to getting answers conversationally. As a result, thousands of developers are now deploying chat applications on Azure for their own specialized domains.\n\nTo help developers understand how to build LLM-powered chat apps, we have open-sourced many chat app templates, like [a super simple chat app](https://github.com/Azure-Samples/openai-chat-app-quickstart) and the very popular and sophisticated [RAG chat app](https://github.com/Azure-Samples/azure-search-openai-demo/). All our templates support an important feature: **streaming**.\n\nAt first glance, streaming might not seem essential. But users have come to *expect* it from modern chat experiences. Beyond meeting expectations, streaming can dramatically improve the **time to first token** — letting your frontend display words as soon as they’re generated, instead of making users wait seconds for a complete answer.\n\n![Animated GIF of GitHub CoPilot answering a question about bash](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMEqW6XThud5KTSTp5jXJGdpnmwUaxuXABBFOu2D83D9I5Xd2GPgyoqSvW1GcCVVtoFuS0HYuAys0jFTxFJVBdyFUX9b79zoYzUBYtqCLefP376J9onwypSUa9EqaVtj8lWOCj8peIarxnXcHRbAoxe2vDmVO6lYJIuuBCPsaU8bJavfNmv06hXoYyOA/s1600/stream_copilot.gif)\n\n### How to stream from the APIs\n\n**Most modern LLM APIs and wrapper libraries now support streaming responses** — usually through a simple boolean flag or a dedicated streaming method.\n\nLet’s look at an example using the official OpenAI Python SDK. The [openai](https://pypi.org/project/openai/) package makes it easy to stream responses by passing a `stream=True` argument:\n\n- completion\\_stream = openai\\_client.chat.completions.create(\nmodel=\"gpt-5-mini\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What does a product manager do?\"}, ], stream=True, )\n\nWhen `stream` is true, the return type is an iterable, so we can use a for loop to process each of the [ChatCompletion chunk objects](https://platform.openai.com/docs/api-reference/chat-streaming/streaming):\n- for chunk in await completion\\_stream:\ncontent = event.choices[0].delta.content\n\n### Sending stream from backend to frontend\n\nWhen building a web app, we need a way to **stream data from the backend to the browser**. A normal HTTP response won’t work here — it sends all the data at once, then closes the connection. Instead, we need a protocol that allows data to arrive progressively.\n\nThe most common options are:\n\n- [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API): A bidirectional channel where both client and server can send data at any time.\n- [Server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events): A one-way channel where the server continuously pushes events to the client over HTTP.\n- [Readable streams](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Using_readable_streams): An HTTP response with a `Transfer-encoding`\nheader of \"chunked\", allowing the client to process chunks as they arrive.\n\nAll of these could potentially be used for a chat app, and I myself have experimented with both [server-sent events](http://blog.pamelafox.org/2023/05/streaming-chatgpt-with-server-sent.html) and [readable streams](http://blog.pamelafox.org/2023/08/fetching-json-over-streaming-http.html). Behind the scenes, the ChatGPT API actually uses server-sent events, so you'll find code in the openai package for parsing that protocol. However, I now prefer using readable streams for my frontend to backend communication. It's the simplest code setup on both the frontend and backend, and it supports the POST requests that our apps are already sending.\n\nThe key is to send chunks from the backend in NDJSON (newline-delimited JSON) format and parse them incrementally on the frontend. See my [blog post on fetching JSON over streaming HTTP](http://blog.pamelafox.org/2023/08/fetching-json-over-streaming-http.html) for Python and JavaScript example code.\n\n### Achieving a word-by-word effect\n\nWith all of that in place, we now have a frontend that **reveals the model’s answer gradually** — almost like watching it type in real time.\n\n![Animated GIF of answer appearing gradually](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHPyrUp5uYqABe9qbSzud81HvBnWvOSM31xg4AYPCvN9twGQig2mDcohDrX1RNyLkOG0EOXqrabyLi35bZWBldeEfceh_xTHlLywRZ89r5aBWzusDptbCmeOtVUdeOMlJTdUfBSvzO-vnXut7xO6ypY3SXAVsappi3M_B_aS6IFJ0JdNUG6RYw71T_7w/s1600/stream_before.gif)\n\nBut something still feels off! Despite our frontend receiving chunks of just a few tokens at a time, that UI tends to reveal entire sentences at once. Why does that happen?\n\nIt turns out the browser is batching repaints. Instead of immediately re-rendering after each DOM update, it waits until it’s more efficient to repaint — a smart optimization in most cases, but not ideal for a streaming text effect.\n\nMy colleague Steve Steiner explored several techniques to make the browser repaint more frequently. The most effective approach uses `window.setTimeout()` with a delay of 33 milliseconds for each chunk. While this adds a small overall delay, it stays well within a natural reading pace and produces a smooth, word-by-word reveal. [See his PR for implementation details for a React codebase](https://github.com/Azure-Samples/azure-search-openai-demo/pull/659).\n\nWith that change, our frontend now displays responses at the **same granularity as the chat completions API** itself — chunk by chunk:\n\n![Animated GIF of answer appearing word by word](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhptsIsoF6jyJejFtY3Htqb0UerM645BDkDGYlB4fnUtZ1WFRwezOtPFmohH9oqeC-k8LQaqAFoWJ_bVfIzFOFMjWwSVbIQ9l5GQoKqhWGcWEA3m6yDUbXgfT5KTmOxIzvjd7vElTA7j_ZgVi1YNShetou8-aF1Boi9QuqtlYJe0D0CMVvH5-LWGQfOxQ/s1600/stream_after.gif)\n\n### Streaming more of the process\n\nMany of our sample apps use RAG (Retrieval-Augmented Generation) pipelines that **chain together multiple operations** — querying data stores (like Azure AI Search), generating embeddings, and finally calling the chat completions API. Naturally, that chain takes longer than a single LLM call, so users may wait several seconds before seeing a response.\n\nOne way to improve the experience is to **stream more of the process itself**. Instead of holding back everything until the final answer, the backend can emit progress updates as each step completes — keeping users informed and engaged.\n\nFor example, your app might display messages like this sequence:\n\n- *Processing your question: \"Can you suggest a pizza recipe that incorporates both mushroom and pineapples?\"*\n- *Generated search query \"pineapple mushroom pizza recipes\"*\n- *Found three related results from our cookbooks: 1) Mushroom calzone 2) Pineapple ham pizza 3) Mushroom loaf*\n- *Generating answer to your question...*\n- *Sure! Here's a recipe for a mushroom pineapple pizza...*\n\nAdding streamed progress like this makes your app feel responsive and alive, even while the backend is doing complex work. Consider experimenting with progress events in your own chat apps — a few simple updates can greatly improve user trust and engagement.\n\n### Making it optional\n\nAfter all this talk about streaming, here’s one final recommendation: **make streaming optional.**\n\nProvide a setting in your frontend to disable streaming, and a corresponding non-streaming endpoint in your backend. This flexibility helps both your users *and* your developers:\n\n- **For users:** Some may prefer (or require) a non-streamed experience for accessibility reasons, or simply to receive the full response at once.\n- **For developers:** There are times when you’ll want to interact with the app programmatically — for example, using curl, requests, or automated tests — and a standard, non-streaming HTTP endpoint makes that much easier.\n\nDesigning your app to gracefully support both modes ensures it’s inclusive, debuggable, and production-ready.\n\n### Sample applications\n\nWe’ve already linked to several of our sample apps that support streaming, but here’s a complete list so you can explore the one that best fits your tech stack:\n\n| **Repository** | **App purpose** | **Backend** | **Frontend** | | --- | --- | --- | --- | | [azure-search-openai-demo](https://github.com/Azure-Samples/azure-search-openai-demo) | RAG with AI Search | Python + Quart | React | | [rag-postgres-openai-python](https://github.com/Azure-Samples/rag-postgres-openai-python/) | RAG with PostgreSQL | Python + FastAPI | React | | [openai-chat-app-quickstart](https://github.com/Azure-Samples/openai-chat-app-quickstart) | Simple chat with Azure OpenAI models | Python + Quart | plain JS | | [openai-chat-backend-fastapi](https://github.com/Azure-Samples/openai-chat-backend-fastapi) | Simple chat with Azure OpenAI models | Python + FastAPI | plain JS | | [deepseek-python](https://github.com/Azure-Samples/deepseek-python) | Simple chat with Azure AI Foundry models | Python + Quart | plain JS |\n\nEach of these repositories includes streaming support out of the box, so you can inspect real implementation details in both the frontend and backend. They’re a great starting point for learning how to structure your own LLM chat application — or for extending one of the samples to match your specific use case.",
  "Author": "Pamela_Fox",
  "OutputDir": "_community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "PubDate": "2025-10-07T05:57:30+00:00",
  "Title": "The importance of streaming for LLM-powered chat applications",
  "ProcessedDate": "2025-10-07 06:04:21",
  "EnhancedContent": "Thanks to the popularity of chat-based interfaces like ChatGPT and GitHub Copilot, users have grown accustomed to getting answers conversationally. As a result, thousands of developers are now deploying chat applications on Azure for their own specialized domains.\n\nTo help developers understand how to build LLM-powered chat apps, we have open-sourced many chat app templates, like [a super simple chat app](https://github.com/Azure-Samples/openai-chat-app-quickstart) and the very popular and sophisticated [RAG chat app](https://github.com/Azure-Samples/azure-search-openai-demo/). All our templates support an important feature: **streaming**.\n\nAt first glance, streaming might not seem essential. But users have come to *expect* it from modern chat experiences. Beyond meeting expectations, streaming can dramatically improve the **time to first token** — letting your frontend display words as soon as they’re generated, instead of making users wait seconds for a complete answer.\n\n![Animated GIF of GitHub CoPilot answering a question about bash](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgMEqW6XThud5KTSTp5jXJGdpnmwUaxuXABBFOu2D83D9I5Xd2GPgyoqSvW1GcCVVtoFuS0HYuAys0jFTxFJVBdyFUX9b79zoYzUBYtqCLefP376J9onwypSUa9EqaVtj8lWOCj8peIarxnXcHRbAoxe2vDmVO6lYJIuuBCPsaU8bJavfNmv06hXoYyOA/s1600/stream_copilot.gif)\n\n### How to stream from the APIs\n\n**Most modern LLM APIs and wrapper libraries now support streaming responses** — usually through a simple boolean flag or a dedicated streaming method.\n\nLet’s look at an example using the official OpenAI Python SDK. The [openai](https://pypi.org/project/openai/) package makes it easy to stream responses by passing a `stream=True` argument:\n\n``` completion_stream = openai_client.chat.completions.create( model=\"gpt-5-mini\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What does a product manager do?\"}, ], stream=True, )\n\n```\n\nWhen `stream` is true, the return type is an iterable, so we can use a for loop to process each of the [ChatCompletion chunk objects](https://platform.openai.com/docs/api-reference/chat-streaming/streaming):\n\n``` for chunk in await completion_stream: content = event.choices[0].delta.content\n\n```\n\n### Sending stream from backend to frontend\n\nWhen building a web app, we need a way to **stream data from the backend to the browser**. A normal HTTP response won’t work here — it sends all the data at once, then closes the connection. Instead, we need a protocol that allows data to arrive progressively.\n\nThe most common options are:\n\n- [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API): A bidirectional channel where both client and server can send data at any time.\n- [Server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events): A one-way channel where the server continuously pushes events to the client over HTTP.\n- [Readable streams](https://developer.mozilla.org/en-US/docs/Web/API/Streams_API/Using_readable_streams): An HTTP response with a `Transfer-encoding`\nheader of \"chunked\", allowing the client to process chunks as they arrive.\n\nAll of these could potentially be used for a chat app, and I myself have experimented with both [server-sent events](http://blog.pamelafox.org/2023/05/streaming-chatgpt-with-server-sent.html) and [readable streams](http://blog.pamelafox.org/2023/08/fetching-json-over-streaming-http.html). Behind the scenes, the ChatGPT API actually uses server-sent events, so you'll find code in the openai package for parsing that protocol. However, I now prefer using readable streams for my frontend to backend communication. It's the simplest code setup on both the frontend and backend, and it supports the POST requests that our apps are already sending.\n\nThe key is to send chunks from the backend in NDJSON (newline-delimited JSON) format and parse them incrementally on the frontend. See my [blog post on fetching JSON over streaming HTTP](http://blog.pamelafox.org/2023/08/fetching-json-over-streaming-http.html) for Python and JavaScript example code.\n\n### Achieving a word-by-word effect\n\nWith all of that in place, we now have a frontend that **reveals the model’s answer gradually** — almost like watching it type in real time.\n\n![Animated GIF of answer appearing gradually](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhHPyrUp5uYqABe9qbSzud81HvBnWvOSM31xg4AYPCvN9twGQig2mDcohDrX1RNyLkOG0EOXqrabyLi35bZWBldeEfceh_xTHlLywRZ89r5aBWzusDptbCmeOtVUdeOMlJTdUfBSvzO-vnXut7xO6ypY3SXAVsappi3M_B_aS6IFJ0JdNUG6RYw71T_7w/s1600/stream_before.gif)\n\nBut something still feels off! Despite our frontend receiving chunks of just a few tokens at a time, that UI tends to reveal entire sentences at once. Why does that happen?\n\nIt turns out the browser is batching repaints. Instead of immediately re-rendering after each DOM update, it waits until it’s more efficient to repaint — a smart optimization in most cases, but not ideal for a streaming text effect.\n\nMy colleague Steve Steiner explored several techniques to make the browser repaint more frequently. The most effective approach uses `window.setTimeout()` with a delay of 33 milliseconds for each chunk. While this adds a small overall delay, it stays well within a natural reading pace and produces a smooth, word-by-word reveal. [See his PR for implementation details for a React codebase](https://github.com/Azure-Samples/azure-search-openai-demo/pull/659).\n\nWith that change, our frontend now displays responses at the **same granularity as the chat completions API** itself — chunk by chunk:\n\n![Animated GIF of answer appearing word by word](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhptsIsoF6jyJejFtY3Htqb0UerM645BDkDGYlB4fnUtZ1WFRwezOtPFmohH9oqeC-k8LQaqAFoWJ_bVfIzFOFMjWwSVbIQ9l5GQoKqhWGcWEA3m6yDUbXgfT5KTmOxIzvjd7vElTA7j_ZgVi1YNShetou8-aF1Boi9QuqtlYJe0D0CMVvH5-LWGQfOxQ/s1600/stream_after.gif)\n\n### Streaming more of the process\n\nMany of our sample apps use RAG (Retrieval-Augmented Generation) pipelines that **chain together multiple operations** — querying data stores (like Azure AI Search), generating embeddings, and finally calling the chat completions API. Naturally, that chain takes longer than a single LLM call, so users may wait several seconds before seeing a response.\n\nOne way to improve the experience is to **stream more of the process itself**. Instead of holding back everything until the final answer, the backend can emit progress updates as each step completes — keeping users informed and engaged.\n\nFor example, your app might display messages like this sequence:\n\n- *Processing your question: \"Can you suggest a pizza recipe that incorporates both mushroom and pineapples?\"*\n- *Generated search query \"pineapple mushroom pizza recipes\"*\n- *Found three related results from our cookbooks: 1) Mushroom calzone 2) Pineapple ham pizza 3) Mushroom loaf*\n- *Generating answer to your question...*\n- *Sure! Here's a recipe for a mushroom pineapple pizza...*\n\nAdding streamed progress like this makes your app feel responsive and alive, even while the backend is doing complex work. Consider experimenting with progress events in your own chat apps — a few simple updates can greatly improve user trust and engagement.\n\n### Making it optional\n\nAfter all this talk about streaming, here’s one final recommendation: **make streaming optional.**\n\nProvide a setting in your frontend to disable streaming, and a corresponding non-streaming endpoint in your backend. This flexibility helps both your users *and* your developers:\n\n- **For users:** Some may prefer (or require) a non-streamed experience for accessibility reasons, or simply to receive the full response at once.\n- **For developers:** There are times when you’ll want to interact with the app programmatically — for example, using curl, requests, or automated tests — and a standard, non-streaming HTTP endpoint makes that much easier.\n\nDesigning your app to gracefully support both modes ensures it’s inclusive, debuggable, and production-ready.\n\n### Sample applications\n\nWe’ve already linked to several of our sample apps that support streaming, but here’s a complete list so you can explore the one that best fits your tech stack:\n\n| **Repository** | **App purpose** | **Backend** | **Frontend** | | --- | --- | --- | --- | | [azure-search-openai-demo](https://github.com/Azure-Samples/azure-search-openai-demo) | RAG with AI Search | Python + Quart | React | | [rag-postgres-openai-python](https://github.com/Azure-Samples/rag-postgres-openai-python/) | RAG with PostgreSQL | Python + FastAPI | React | | [openai-chat-app-quickstart](https://github.com/Azure-Samples/openai-chat-app-quickstart) | Simple chat with Azure OpenAI models | Python + Quart | plain JS | | [openai-chat-backend-fastapi](https://github.com/Azure-Samples/openai-chat-backend-fastapi) | Simple chat with Azure OpenAI models | Python + FastAPI | plain JS | | [deepseek-python](https://github.com/Azure-Samples/deepseek-python) | Simple chat with Azure AI Foundry models | Python + Quart | plain JS |\n\nEach of these repositories includes streaming support out of the box, so you can inspect real implementation details in both the frontend and backend. They’re a great starting point for learning how to structure your own LLM chat application — or for extending one of the samples to match your specific use case.\n\nPublished Oct 07, 2025\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[genai](/tag/genai?nodeId=board%3AAzureDevCommunityBlog)\n\n[pamela fox](/tag/pamela%20fox?nodeId=board%3AAzureDevCommunityBlog)\n\n[python](/tag/python?nodeId=board%3AAzureDevCommunityBlog)\n\n[tips and tricks](/tag/tips%20and%20tricks?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[Pamela_Fox&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xNjA0MDc4LTQxODI4MWk5MjkyQjFBMEVGOUE5NkM5?image-dimensions=50x50)](/users/pamela_fox/1604078) [Pamela_Fox](/users/pamela_fox/1604078) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined November 08, 2022\n\n[View Profile](/users/pamela_fox/1604078)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "FeedName": "Microsoft Tech Community",
  "FeedLevelAuthor": "rss.livelink.threads-in-node"
}
