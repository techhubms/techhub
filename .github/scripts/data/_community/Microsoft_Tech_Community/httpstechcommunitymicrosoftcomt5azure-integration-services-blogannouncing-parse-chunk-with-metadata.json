{
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "EnhancedContent": "The new **Parse document with metadata** and **Chunk text with metadata** actions in Logic Apps bring powerful improvements to how you handle documents. Unlike the previously released parse and chunk actions, these new versions provide **rich metadata** alongside the text:\n\n- **pageNumber** â€” the page a chunk came from\n- **totalPages** â€” the total number of pages in the document\n- **sentencesAreComplete** â€” ensures chunks end on full sentences, avoiding broken fragments\n\nThis means you donâ€™t just get raw textâ€”you also get the context you need for citations, navigation, and downstream processing. You can also adjust your chunking strategy based on these metadata fields\n\nOnce parsed and chunked with metadata, you can embed and index documents in Azure AI Search, and then use an **Agent Loop** in Logic Apps that calls **Vector Search as a Tool** to answer questions with precise, page-level references\n\nIn this blog, weâ€™ll walk through a scenario where we index two enterprise contracts (a Master Service Agreement and a Procurement Agreement) and then use an Agent Loop to answer natural language questions with citations.\n\n## Pre-requisites\n\n- Azure Blob Storage for your documents\n- Azure AI Search with an index\n- Azure OpenAI deployment (embeddings + chat model)\n- Logic App (Standard) with the new AI actions\n\nHere is a [sample demo on GitHub](https://github.com/Azure/logicapps/tree/shahparth-lab-patch-1/ws-vscode) you can provision to follow along.\n\n## Step 1: Ingestion flow\n\n**Goal:** Convert raw PDFs into sentence-aware chunks with metadata, then index them.\n\nðŸ“¸ *Workflow overview*\n\n- **When a blob is added or modified** (container with your contracts).\n\nðŸ“¸Â *Blob trigger*\n- **Read blob contentðŸ“¸***Read blob content action*\n- **Parse document with Metadata**\n- **Input:**File content from previous step\n**ðŸ“¸***Parse document with metadata action*\n- **Chunk text with metadata**\n- Input: **entire** parsed text items array\n**ðŸ“¸***Chunk text with metadata action*\n- **Get multiple embeddings**\n- Input: Embedding model and text chunks for which vector impressions will be generated\n**ðŸ“¸** *Get multiple embeddings action*\n- **Select index objects**\n- Input: Raw text content, embeddings, documentName and uniqueID to be passed into the index**** **ðŸ“¸**Â *Select array action*\n- **Index multiple documents**\n- Input: Array object output from previous Select step\n**ðŸ“¸** *Index documents action*\n\n## Step 2: Agent flow with Vector Search as a tool\n\n**Goal:** Let the agent answer natural language questions grounded in your indexed contract\n\n- **Conversational workflow** **creation**: From the portal, create a new Conversational workflow type\n**ðŸ“¸** *Conversational flow creation*\n- **Agent action**\n- Model: **gpt-4.1** (Note: please use this model instead of gpt-4 or gpt-4o)\n- System instructions\n\n``` You are a helpful assistant, answering questions about specific documents. When a question is asked, follow these steps in order: Use the agent parameter body prompt to pass in the user's questions to the Document search tool. Use this tool to do a vector search of the user's question, the output of the vector search tool will have the related information to answer the question. The output will be in the form of a json array. Each array object will have a \"content\" property, use the \"content\" property to generate an answer. Use only information to answer the user's question and cite the source using the page number you found it on. No other data or information should be used to answer the question. ```\n\nðŸ’¡One of the coolest parts is how you can create an **Agent Parameter** that automatically carries the chat input into the tool call. In this case, our body prompt parameter brings the userâ€™s question straight into the tool.\n\nðŸ’¡Second of the coolest parts is because the toolâ€™s response comes back in content, the agent automatically extracts itâ€”no expressions required. Itâ€™s declarative and effortless.\n\n**ðŸ“¸** *Agent action*\n- **Tool: Overview**\n- Input description: Details on what the tool achieves\n- Agent parameter: Body prompt to pass in context from the chat prompt to the tool\n**ðŸ“¸** *Tool action*\n- **Tool: Search vectors with natural language**\n- Input index name: this is the name of AI Search index\n- Search text: Body prompt parameter containing query from prompt\n- Nearest neighbors: Number of matches to be returned\n**ðŸ“¸** *Tool: Search vector action*\n\n## Step 3: Try it out (example end-to-end)\n\n**Indexing** is automatic whenever a file is added to your storage container.\n\nThe **Storage trigger** fires the document is read, parsed, chunked, embedded, and indexed into AI Search. You can confirm this end-to-end in run history for the indexing Logic App as well, where Parse and Chunk outputs clearly show pageNumber, totalPages, and sentencesAreComplete values.\n\nðŸ“¸Â *Screenshot: Indexing flow run history with Parse/Chunk metadata outputs*\n\nNow let's see it in action using theÂ **Chat** experience to validate the retrieval flow\n\nExample question: *\"What is the standard payment timeline\"*\n\n**ðŸ“¸***Answer*\n\nThe answer contains detailed information along with citation of page numbers which is leveraged from the new actions that contain such metadata information.\n\n**ðŸ“¸***Run history view of Agent*\n\nOne can also trace the path agent followed with the inputs and outputs to streamline debugging and ensure agent responds reliably.\n\n## Conclusion\n\nWith **Parse & Chunk with Metadata**, you donâ€™t just split textâ€”you gain page numbers, total pages, and sentence completeness that make answers trustworthy and easy to cite. Combined with **Agent Loop + Vector Search as a Tool**, this unlocks production-ready contract Q&A in just a few steps.\n\nUpdated Oct 01, 2025\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AIntegrationsonAzureBlog)\n\n[!\\[shahparth&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-2.svg?image-dimensions=50x50)](/users/shahparth/1051277) [shahparth](/users/shahparth/1051277) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined May 11, 2021\n\n[View Profile](/users/shahparth/1051277)\n\n/category/azure/blog/integrationsonazureblog [Azure Integration Services Blog](/category/azure/blog/integrationsonazureblog) Follow this blog board to get notified when there's new activity",
  "ProcessedDate": "2025-10-02 00:08:29",
  "Link": "https://techcommunity.microsoft.com/t5/azure-integration-services-blog/announcing-parse-chunk-with-metadata-in-logic-apps-build-context/ba-p/4458438",
  "Title": "Announcing Parse & Chunk with Metadata in Logic Apps: Build Context-Aware RAG Agents",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Tags": [],
  "Author": "shahparth",
  "Description": "The new **Parse document with metadata** and **Chunk text with metadata** actions in Logic Apps bring powerful improvements to how you handle documents. Unlike the previously released parse and chunk actions, these new versions provide **rich metadata** alongside the text:\n\n- **pageNumber** â€” the page a chunk came from\n- **totalPages** â€” the total number of pages in the document\n- **sentencesAreComplete** â€” ensures chunks end on full sentences, avoiding broken fragments\n\nThis means you donâ€™t just get raw textâ€”you also get the context you need for citations, navigation, and downstream processing. You can also adjust your chunking strategy based on these metadata fields\n\nOnce parsed and chunked with metadata, you can embed and index documents in Azure AI Search, and then use an **Agent Loop** in Logic Apps that calls **Vector Search as a Tool** to answer questions with precise, page-level references\n\nIn this blog, weâ€™ll walk through a scenario where we index two enterprise contracts (a Master Service Agreement and a Procurement Agreement) and then use an Agent Loop to answer natural language questions with citations.\n\n## Pre-requisites\n\n- Azure Blob Storage for your documents\n- Azure AI Search with an index\n- Azure OpenAI deployment (embeddings + chat model)\n- Logic App (Standard) with the new AI actions\n\nHere is a [sample demo on GitHub](https://github.com/Azure/logicapps/tree/shahparth-lab-patch-1/ws-vscode) you can provision to follow along.\n\n## Step 1: Ingestion flow\n\n**Goal:** Convert raw PDFs into sentence-aware chunks with metadata, then index them.\n\nðŸ“¸ *Workflow overview*\n\n![]()\n\n- **When a blob is added or modified** (container with your contracts).\n\nðŸ“¸ *Blob trigger*\n\n![]()\n- **Read blob contentðŸ“¸** *Read blob content action*![]()\n- **Parse document with Metadata**\n- **Input:** File content from previous step\n**ðŸ“¸** *Parse document with metadata action*![]()\n- **Chunk text with metadata**\n- Input: **entire** parsed text items array\n**ðŸ“¸** *Chunk text with metadata action*![]()\n- **Get multiple embeddings**\n- Input: Embedding model and text chunks for which vector impressions will be generated\n**ðŸ“¸** *Get multiple embeddings action*![]()\n- **Select index objects**\n- Input: Raw text content, embeddings, documentName and uniqueID to be passed into the index **** **ðŸ“¸** *Select array action*![]()\n- **Index multiple documents**\n- Input: Array object output from previous Select step\n**ðŸ“¸** *Index documents action*![]()\n\n## Step 2: Agent flow with Vector Search as a tool\n\n**Goal:** Let the agent answer natural language questions grounded in your indexed contract\n\n- **Conversational workflow** **creation**: From the portal, create a new Conversational workflow type\n**ðŸ“¸** *Conversational flow creation* ![]()\n- **Agent action**\n- Model: **gpt-4.1** (Note: please use this model instead of gpt-4 or gpt-4o)\n- System instructions\n- You are a helpful assistant, answering questions about specific documents. When a question is asked, follow these steps in order:\nUse the agent parameter body prompt to pass in the user's questions to the Document search tool. Use this tool to do a vector search of the user's question, the output of the vector search tool will have the related information to answer the question. The output will be in the form of a json array. Each array object will have a \"content\" property, use the \"content\" property to generate an answer. Use only information to answer the user's question and cite the source using the page number you found it on. No other data or information should be used to answer the question.\n\nðŸ’¡One of the coolest parts is how you can create an **Agent Parameter** that automatically carries the chat input into the tool call. In this case, our body prompt parameter brings the userâ€™s question straight into the tool.\n\nðŸ’¡Second of the coolest parts is because the toolâ€™s response comes back in content, the agent automatically extracts itâ€”no expressions required. Itâ€™s declarative and effortless.\n\n**ðŸ“¸** *Agent action* ![]()\n- **Tool: Overview**\n- Input description: Details on what the tool achieves\n- Agent parameter: Body prompt to pass in context from the chat prompt to the tool\n**ðŸ“¸** *Tool action*![]()\n- **Tool: Search vectors with natural language**\n- Input index name: this is the name of AI Search index\n- Search text: Body prompt parameter containing query from prompt\n- Nearest neighbors: Number of matches to be returned\n**ðŸ“¸** *Tool: Search vector action* ![]()\n\n## Step 3: Try it out (example end-to-end)\n\n**Indexing** is automatic whenever a file is added to your storage container.\n\nThe **Storage trigger** fires the document is read, parsed, chunked, embedded, and indexed into AI Search. You can confirm this end-to-end in run history for the indexing Logic App as well, where Parse and Chunk outputs clearly show pageNumber, totalPages, and sentencesAreComplete values.\n\nðŸ“¸ *Screenshot: Indexing flow run history with Parse/Chunk metadata outputs*\n\n![]()\n\nNow let's see it in action using the **Chat** experience to validate the retrieval flow\n\nExample question: *\"What is the standard payment timeline\"*\n\n**ðŸ“¸** *Answer*\n\n![]()\n\nThe answer contains detailed information along with citation of page numbers which is leveraged from the new actions that contain such metadata information.\n\n**ðŸ“¸** *Run history view of Agent*\n\n![]()\n\nOne can also trace the path agent followed with the inputs and outputs to streamline debugging and ensure agent responds reliably.\n\n## Conclusion\n\nWith **Parse & Chunk with Metadata**, you donâ€™t just split textâ€”you gain page numbers, total pages, and sentence completeness that make answers trustworthy and easy to cite. Combined with **Agent Loop + Vector Search as a Tool**, this unlocks production-ready contract Q&A in just a few steps.",
  "PubDate": "2025-10-01T23:25:57+00:00",
  "OutputDir": "_community",
  "FeedName": "Microsoft Tech Community"
}
