{
  "Description": "Co-Authored by [Sanjeev Nair​](javascript:void%280%29)\n\nThis guide walks through a proven approach to Databricks cost optimization, structured in three phases: Discovery, Cluster/Data/Code Best Practices, and Team Alignment & Next Steps.\n\n#### **Phase 1: Discovery**\n\n##### **Assessing Your Current State**\n\nThe following questions are designed to guide your initial assessment and help you identify areas for improvement. Documenting answers to each will provide a baseline for optimization and inform the next phases of your cost management strategy.\n\n| **Environment & Organization** | **Cluster Management**<br><br><br><br>**** | **Cost Optimization**<br><br><br><br>**** | **Data Management**<br><br><br><br>**** | **Performance Monitoring**<br><br><br><br>**** | **Future Planning**<br><br><br><br>**** | | --- | --- | --- | --- | --- | --- | | What is the current scale of your Databricks environment?<br><br><br><br><br><br><br><br>How many workspaces do you have?<br><br><br><br><br><br><br><br>How are your workspaces organized (e.g., by environment type, region, use case)?<br><br><br><br><br><br><br><br>How many clusters are deployed?<br><br><br><br>How many users are active?<br><br><br><br><br><br><br><br>What are the primary use cases for Databricks in your organization?<br><br><br><ul><br><li><span>Data engineering</span><span> </span></li><br><br><li><span>Data science</span><span> </span></li><br><br><li><span>Machine learning</span><span> </span></li><br><br><li><span>Business intelligence</span></li><br><br></ul> | How are clusters currently managed?<br><br><br><ul><br><li><span>Manual configuration</span><span> </span></li><br><br><li><span>Automated scripts</span><span> </span></li><br><br><li><span>Databricks REST API</span><span> </span></li><br><br><li><span>Cluster policies</span><span> </span></li><br><br></ul><br><br><br><br><br><br>What is the average cluster uptime?<br><br><br><ul><br><li><span>Hours per day</span><span> </span></li><br><br><li><span>Days per week</span><span> </span></li><br><br></ul><br><br><br><br><br><br>What is the average cluster utilization rate?<br><br><br><ul><br><li><span>CPU usage</span><span> </span></li><br><br><li><span>Memory usage</span><span> </span></li><br><br></ul> | What is the current monthly spend on Databricks?<br><br><br><ul><br><li><span>Total cost</span><span> </span></li><br><br><li><span>Breakdown by workspace</span><span> </span></li><br><br><li><span>Breakdown by cluster</span><span> </span></li><br><br></ul><br><br>What cost management tools are currently in use?<br><br><br><ul><br><li><span>Azure Cost Management</span><span> </span></li><br><br><li><span>Third-party tools</span><span> </span></li><br><br></ul><br><br>Are there any existing cost optimization strategies in place?<br><br><br><ul><br><li><span>Reserved instances</span><span> </span></li><br><br><li><span>Spot instances</span><span> </span></li><br><br><li><span>Cluster auto-scaling</span><span> </span></li><br><br></ul> | What is the current data storage strategy?<br><br><br><ul><br><li><span>Data lake</span><span> </span></li><br><br><li><span>Data warehouse</span><span> </span></li><br><br><li><span>Hybrid</span><span> </span></li><br><br></ul><br><br>What is the average data ingestion rate?<br><br><br><ul><br><li><span>GB per day</span><span> </span></li><br><br><li><span>Number of files</span><span> </span></li><br><br></ul><br><br>What is the average data processing time?<br><br><br><ul><br><li><span>ETL jobs</span><span> </span></li><br><br><li><span>Machine learning models</span><span> </span></li><br><br></ul><br><br>What types of data formats are used in your environment?<br><br><br><ul><br><li><span>Delta Lake</span><span> </span></li><br><br><li><span>Parquet</span><span> </span></li><br><br><li><span>JSON</span><span> </span></li><br><br><li><span>CSV</span><span> </span></li><br><br><li><span>Other formats relevant to your workloads</span><span> </span></li><br><br></ul> | What performance monitoring tools are currently in use?<br><br><br><ul><br><li><span>Databricks Ganglia</span><span> </span></li><br><br><li><span>Azure Monitor</span><span> </span></li><br><br><li><span>Third-party tools</span><span> </span></li><br><br></ul><br><br>What are the key performance metrics tracked?<br><br><br><ul><br><li><span>Job execution time</span><span> </span></li><br><br><li><span>Cluster performance</span><span> </span></li><br><br><li><span>Data processing speed</span><span> </span></li><br><br></ul> | Are there any planned expansions or changes to the Databricks environment?<br><br><br><ul><br><li><span>New use cases</span><span> </span></li><br><br><li><span>Increased data volume</span><span> </span></li><br><br><li><span>Additional users</span><span> </span></li><br><br></ul><br><br>What are the long-term goals for Databricks cost optimization?<br><br><br><ul><br><li><span>Reducing overall spend</span><span> </span></li><br><br><li><span>Improving resource utilization & cost attribution</span><span> </span></li><br><br><li><span>Enhancing performance</span><span> </span></li><br><br></ul> |\n\n##### **Understanding Databricks Cost Structure**\n\n**Total Cost = Cloud Cost + DBU Cost**\n\n- - Cloud Cost: Compute (VMs, networking, IP addresses), storage (ADLS, MLflow artifacts), other services (firewalls), cluster type (serverless compute, classic compute)\n\n- - DBU Cost: Workload size, cluster/warehouse size, photon acceleration, compute runtime, workspace tier, SKU type (Jobs, Delta Live Tables, All Purpose Clusters, Serverless), model serving, queries per second, model execution time\n\n##### **Diagnose Cost and Issues**\n\nEffectively diagnosing cost and performance issues in Databricks requires a structured approach. Use the following steps and metrics to gain visibility into your environment and uncover actionable insights.\n\n1. ###### **Review Cluster Metrics**\n\n- - CPU Utilization: Track guest, iowait, idle, irq, nice, softirq, steal, system, and user times to understand how compute resources are being used.\n- Memory Utilization: Monitor used, free, buffer, and cached memory to identify over- or under-utilization.\n\n![]()\n- - Key Question: Is your cluster over- or under-utilized? Are resources being wasted or stretched too thin?\n\n1. ###### **Review SQL Warehouse Metrics**\n\n![]()\n1. 1. **Live Statistics****:** Monitor warehouse status, running/queued queries, and current cluster count.\n2. **Time Scale Filter****:** Analyze query and cluster activity over different time frames (8 hours, 24 hours, 7 days, 14 days).\n3. **Peak Query Count Chart****:** Identify periods of high concurrency.\n4. **Completed Query Count Chart****:** Track throughput and query success/failure rates.\n5. **Running Clusters Chart****:** Observe cluster allocation and recycling events.\n6. **Query History Table****:** Filter and analyze queries by user, duration, status, and statement type.\n2.\n\n###### **3. Review Spark UI**\n\n![]()\n- - **Stages Tab****:** Look for skewed data, high input/output, and shuffle times. Uneven task durations may indicate data skew or inefficient data handling.\n\n- - **Jobs Timeline****:** Identify long-running jobs or stages that consume excessive resources.\n\n- - **Stage Analysis****:** Determine if stages are I/O bound or suffering from data skew/spill.\n\n- - **Executor Metrics****:** Monitor memory usage, CPU utilization, and disk I/O. Frequent garbage collection or high memory usage may signal the need for better resource allocation.\n\n###### **3.1. Storage & Jobs Tab**\n\n- - **Storage Level****:** Check if data is stored in memory, on disk, or both.\n\n- - **Size****:** Assess the size of cached data.\n\n- - **Job Analysis****:** Investigate jobs that dominate the timeline or have unusually long durations. Look for gaps caused by complex execution plans, non-Spark code, driver overload, or cluster malfunction.\n\n###### **3.2. Executor Tab**\n\n- - **Storage Memory****:** Compare used vs. available memory.\n\n- - **Task Time (Garbage Collection)****:** Review long tasks and garbage collection times.\n\n- - **Shuffle Read/Write****:** Measure data transferred between stages.\n\n#### **Phase 2: Cluster/Code/Data Best Practices Alignment**\n\n##### **Cluster UI Configuration and Cost Attribution**\n\nEffectively configuring clusters/workloads in Databricks is essential for balancing performance, scalability, and cost. Tunning settings and features when used strategically can help organizations maximize resource efficiency and minimize unnecessary spending.\n\n![]()\n\n##### **Key Configuration Strategies**\n\n**1. Reduce Idle Time:** Clusters to incur costs even when not actively processing workloads. To avoid paying for unused resources:\n\n- - **Enable Auto-Terminate**: Set clusters automatically shut down after a period of inactivity. This simple setting can significantly reduce wasted spending.\n\n![]()\n1. **Enable Autoscaling:** Workloads fluctuate in size and complexity. Autoscaling allows clusters to dynamically adjust the number of nodes based on demand:\n\n- - **Automatic Resource Adjustment:** Scale up for heavy jobs and scale down for lighter loads, ensuring you only pay for what you use.\n\n1. **Use Spot Instances:** For batch processing and non-critical workloads, spot instances offer substantial cost savings:\n\n- - **Lower VM Costs**: Spot instances are typically much cheaper than standard VMs. However, they are not recommended for jobs requiring constant uptime due to potential interruptions.\n\n![]()\n1. **Leverage Photon Engine:** Photon is Databricks’ high-performance, vectorized query engine:\n\n- - **Accelerate Large Workloads**: Photon can dramatically reduce runtime for compute-intensive tasks, improving both speed and cost efficiency.\n\n1. **Keep Runtimes Up to Date:** Using the latest Databricks runtime ensures optimal performance and security:\n\n- - **Benefit from Improvements:** **** Regular updates include performance enhancements, bug fixes, and new features.\n\n1. **Apply Cluster Policies:** Cluster policies help standardize configurations and enforce cost controls across teams:\n\n- - **Governance and Consistency:** **** Policies can restrict certain settings, enforce tagging, and ensure clusters are created with cost-effective defaults.\n\n1. **Optimize Storage:** type impacts both performance and cost:\n\n- - **Switch from HDDs to SSDs**: SSDs provide faster caching and shuffle operations, which can improve job efficiency and reduce runtime.\n\n1. **Tag Clusters for Cost Attribution:** Tagging clusters enables granular tracking and reporting:\n\n- - **Visibility and Accountability:** Use tags to attribute costs to specific teams, projects, or environments, supporting better budgeting and chargeback processes.\n\n1. **Select the Right Cluster Type:** Different workloads require different cluster types:\n\n- - **Job Clusters:** **** Ideal for scheduled jobs and Delta Live Tables.\n\n- - **All-Purpose Clusters:** **** Suited for ad-hoc analysis and collaborative work.\n\n- - **Single-Node Clusters:** Efficient for simple exploratory data analysis or pure Python tasks.\n\n- - **Serverless Compute:** **** Scalable, managed workloads with automatic resource management.\n\n1. **Monitor and Adjust Regularly:** review cluster metrics and query history:\n\n- - **Continuous Optimization:** Use built-in dashboards to monitor usage, identify bottlenecks, and adjust cluster size or configuration as needed.\n\n##### **Code Best Practices**\n\n1. **Avoid Reprocessing Large Tables**\n\n- - Use a CDC (Change Data Capture) architecture with Delta Live Tables (DLT) to process only new or changed data, minimizing unnecessary computation.\n\n1. **Ensure Code Parallelizes Well**\n\n- - Write Spark code that leverages parallel processing. Avoid loops, deeply nested structures, and inefficient user-defined functions (UDFs) that can hinder scalability.\n\n1. **Reduce Memory Consumption**\n\n- - Tweak Spark configurations to minimize memory overhead. Clean out legacy or unnecessary settings that may have carried over from previous Spark versions.\n\n1. **Prefer SQL Over Complex Python**\n\n- - Use SQL (declarative language) for Spark jobs whenever possible. SQL queries are typically more efficient and easier to optimize than complex Python logic.\n\n1. **Modularize Notebooks**\n\n- - Use %run to split large notebooks into smaller, reusable modules. This improves maintainability.\n\n1. **Use LIMIT in Exploratory Queries**\n\n- - When exploring data, always use the LIMIT clause to avoid scanning large datasets unnecessarily.\n\n1. **Monitor Job Performance**\n\n- - Regularly review Spark UI to detect inefficiencies such as high shuffle, input, or output. Optimize join strategies and data layout accordingly.\n\n![]()\n\n##### **Databricks Code Performance Enhancements & Data Engineering Best Practices**\n\nBy enabling the below features and applying best practices, you can significantly lower costs, accelerate job execution, and build Databricks pipelines that are both scalable and highly reliable. For more guidance review: [Comprehensive Guide to Optimize Data Workloads | Databricks](https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro).\n\n| **Feature / Technique** | **Purpose / Benefit** | **How to Use / Enable / Key Notes** | | --- | --- | --- | | **[Disk Caching](https://learn.microsoft.com/en-us/azure/databricks/optimizations/disk-cache)** | Accelerates repeated reads of Parquet files | Set spark.databricks.io.cache.enabled = true | | **[Dynamic File Pruning (DFP)](https://learn.microsoft.com/en-us/azure/databricks/optimizations/dynamic-file-pruning)** | Skips irrelevant data files during queries, improves query performance | Enabled by default in Databricks | | **[Low Shuffle Merge](https://learn.microsoft.com/en-us/azure/databricks/optimizations/low-shuffle-merge)** | Reduces data rewriting during MERGE operations, less need to recalculate ZORDER | Use Databricks runtime with feature enabled | | **[Adaptive Query Execution (AQE)](https://learn.microsoft.com/en-us/azure/databricks/optimizations/aqe)** | Dynamically optimizes query plans based on runtime statistics | Available in Spark 3.0+, enabled by default | | **[Deletion Vectors](https://learn.microsoft.com/en-us/azure/databricks/delta/deletion-vectors)** | Efficient row removal/change without rewriting entire Parquet file | Enable in workspace settings, use with Delta Lake | | **[Materialized Views](https://www.databricks.com/blog/introducing-materialized-views-and-streaming-tables-databricks-sql)** | Faster BI queries, reduced compute for frequently accessed data | Create in Databricks SQL | | **[Optimize](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-optimize)** | Compacts Delta Lake files, improves query performance | Run regularly, combine with ZORDER on high-cardinality columns | | **ZORDER** | Physically sorts/co-locates data by chosen columns for faster queries | Use with OPTIMIZE, select columns frequently used in filters/joins | | **[Auto Optimize](https://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size#auto-optimize)** | Automatically compacts small files during writes | Enable optimizeWrite and autoCompact table properties | | **[Liquid Clustering](https://learn.microsoft.com/en-us/azure/databricks/delta/clustering)** | Simplifies data layout, replaces partitioning/ZORDER, flexible clustering keys | Recommended for new Delta tables, enables easy redefinition of clustering keys | | **[File Size Tuning](https://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size)** | Achieve optimal file size for performance and cost | Set delta.targetFileSize table property | | **[Broadcast Hash Join](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-syntax-qry-select-hints)** | Optimizes joins by broadcasting smaller tables | Adjust spark.sql.autoBroadcastJoinThreshold and spark.databricks.adaptive.autoBroadcastJoinThreshold | | **Shuffle Hash Join** | Faster join alternative to sort-merge join | Prefer over sort-merge join when broadcasting isn’t possible, Photon engine can help | | **[Cost-Based Optimizer (CBO)](https://learn.microsoft.com/en-us/azure/databricks/optimizations/cbo)** | Improves query plans for complex joins | Enabled by default, collect column/table statistics with ANALYZE TABLE | | **[Data Spilling & Skew](https://learn.microsoft.com/en-us/azure/databricks/optimizations/aqe)** | Handles uneven data distribution and excessive shuffle | Use AQE, set spark.sql.shuffle.partitions=auto, optimize partitioning | | **Data Explosion Management** | Controls partition sizes after transformations (e.g., explode, join) | Adjust spark.sql.files.maxPartitionBytes, use repartition() after reads | | **[Delta Merge](https://learn.microsoft.com/en-us/azure/databricks/delta/merge)** | Efficient upserts and CDC (Change Data Capture) | Use MERGE operation in Delta Lake, combine with CDC architecture | | **[Data Purging (Vacuum)](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-vacuum)** | Removes stale data files, maintains storage efficiency | Run VACUUM regularly based on transaction frequency |\n\n#### **Phase 3: Team Alignment and Next Steps**\n\n##### **Implementing Cost Observability and Taking Action**\n\nEffective cost management in Databricks goes beyond configuration and code—it requires robust observability, granular tracking, and proactive measures. Below outlines how your teams can achieve this using system tables, tagging, dashboards, and actionable scripts.\n\n1. ###### **Cost Observability with System Tables**\n\nDatabricks Unity Catalog provides system tables that store operational data for your account. These tables enable historical cost observability and empower FinOps teams to analyze spend independently.\n\n- - **System Tables Location:** **** Found inside the Unity Catalog under the “system” schema.\n- **Key Benefits**: Structured data for querying, historical analysis, and cost attribution.\n- **Action**: Assign permissions to FinOps teams so they can access and analyze dedicated cost tables.\n\n![]()\n1. **Enable Tags for Granular Tracking**\n\nTagging is a powerful feature for tracking, reporting, and budgeting at a granular level.\n\n- - **Classic Compute:** **** Manually add key/value pairs when creating clusters, jobs, SQL Warehouses, or Model Serving endpoints. Use cluster policies to enforce custom tags.\n\n![]()\n- - **Serverless Compute:** Create budget policies and assign permissions to teams or members for serverless workloads.\n\n![]()\n- - **Action**: Tag all compute resources to enable detailed cost attribution and reporting.\n\n1. **Track Costs with Dashboards and Alerts**\n\nDatabricks offers prebuilt dashboards and queries for cost forecasting and usage analysis.\n\n- - [Dashboards:](https://www.databricks.com/resources/demos/tutorials/governance/system-tables) Visualize spend, usage trends, and forecast future costs.\n- [Prebuilt Queries](https://community.databricks.com/t5/technical-blog/top-10-queries-to-use-with-system-tables/ba-p/82331): Use top queries with system tables to answer meaningful cost questions.\n- [Budget Alerts:](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/budgets) Set up alerts in the Account Console (Usage > Budget) to receive notifications when spend approaches defined thresholds.\n\n![]()\n1. **Review Bottlenecks and Optimization Opportunities**\n\nWith observability in place, regularly review system tables, dashboards, and tagged resources to identify:\n\n- - **Cost Bottlenecks:** Clusters or jobs with unusually high spend.\n\n- - **Optimization Opportunities:** **** Underutilized resources, inefficient jobs, or misconfigured clusters.\n\n- - **Team Alignment:** Share insights with engineering and FinOps teams to drive collaborative optimization.\n\n#### **Summary Table: Cost Observability & Action Steps**\n\n| **Area** | **Best Practice / Action** | | --- | --- | | **System Tables** | Use for historical cost analysis and attribution | | **Tagging** | Apply to all compute resources for granular tracking | | **Dashboards** | Visualize spend, usage, and forecasts | | **Alerts** | Set budget alerts for proactive cost management | | **Scripts/Queries** | Build custom analysis tools for deep insights | | **Cluster/Data/Code Review & Align** | Regularly review best practices, share findings, and align teams on optimization |",
  "Author": "Rafia_Aqil",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "EnhancedContent": "Co-Authored by [Sanjeev Nair​](javascript:void%280%29)\n\nThis guide walks through a proven approach to Databricks cost optimization, structured in three phases: Discovery, Cluster/Data/Code Best Practices, and Team Alignment & Next Steps.\n\n#### **Phase 1: Discovery**\n\n##### **Assessing Your Current State**\n\nThe following questions are designed to guide your initial assessment and help you identify areas for improvement. Documenting answers to each will provide a baseline for optimization and inform the next phases of your cost management strategy.\n\n| **Environment & Organization** | **Cluster Management**<br><br><br><br>**** | **Cost Optimization**<br><br><br><br>**** | **Data Management**<br><br><br><br>**** | **Performance Monitoring**<br><br><br><br>**** | **Future Planning**<br><br><br><br>**** | | --- | --- | --- | --- | --- | --- | | What is the current scale of your Databricks environment?<br><br><br><br><br><br><br><br>How many workspaces do you have?<br><br><br><br><br><br><br><br>How are your workspaces organized (e.g., by environment type, region, use case)?<br><br><br><br><br><br><br><br>How many clusters are deployed?<br><br><br><br>How many users are active?<br><br><br><br><br><br><br><br>What are the primary use cases for Databricks in your organization?<br><br><br><ul><br><li><span>Data engineering</span><span>&nbsp;</span></li><br><br><li><span>Data science</span><span>&nbsp;</span></li><br><br><li><span>Machine learning</span><span>&nbsp;</span></li><br><br><li><span>Business intelligence</span></li><br><br></ul> | How are clusters currently managed?<br><br><br><ul><br><li><span>Manual configuration</span><span>&nbsp;</span></li><br><br><li><span>Automated scripts</span><span>&nbsp;</span></li><br><br><li><span>Databricks REST API</span><span>&nbsp;</span></li><br><br><li><span>Cluster policies</span><span>&nbsp;</span></li><br><br></ul><br><br><br><br><br><br>What is the average cluster uptime?<br><br><br><ul><br><li><span>Hours per day</span><span>&nbsp;</span></li><br><br><li><span>Days per week</span><span>&nbsp;</span></li><br><br></ul><br><br><br><br><br><br>What is the average cluster utilization rate?<br><br><br><ul><br><li><span>CPU usage</span><span>&nbsp;</span></li><br><br><li><span>Memory usage</span><span>&nbsp;</span></li><br><br></ul> | What is the current monthly spend on Databricks?<br><br><br><ul><br><li><span>Total cost</span><span>&nbsp;</span></li><br><br><li><span>Breakdown by workspace</span><span>&nbsp;</span></li><br><br><li><span>Breakdown by cluster</span><span>&nbsp;</span></li><br><br></ul><br><br>What cost management tools are currently in use?<br><br><br><ul><br><li><span>Azure Cost Management</span><span>&nbsp;</span></li><br><br><li><span>Third-party tools</span><span>&nbsp;</span></li><br><br></ul><br><br>Are there any existing cost optimization strategies in place?<br><br><br><ul><br><li><span>Reserved instances</span><span>&nbsp;</span></li><br><br><li><span>Spot instances</span><span>&nbsp;</span></li><br><br><li><span>Cluster auto-scaling</span><span>&nbsp;</span></li><br><br></ul> | What is the current data storage strategy?<br><br><br><ul><br><li><span>Data lake</span><span>&nbsp;</span></li><br><br><li><span>Data warehouse</span><span>&nbsp;</span></li><br><br><li><span>Hybrid</span><span>&nbsp;</span></li><br><br></ul><br><br>What is the average data ingestion rate?<br><br><br><ul><br><li><span>GB per day</span><span>&nbsp;</span></li><br><br><li><span>Number of files</span><span>&nbsp;</span></li><br><br></ul><br><br>What is the average data processing time?<br><br><br><ul><br><li><span>ETL jobs</span><span>&nbsp;</span></li><br><br><li><span>Machine learning models</span><span>&nbsp;</span></li><br><br></ul><br><br>What types of data formats are used in your environment?<br><br><br><ul><br><li><span>Delta Lake</span><span>&nbsp;</span></li><br><br><li><span>Parquet</span><span>&nbsp;</span></li><br><br><li><span>JSON</span><span>&nbsp;</span></li><br><br><li><span>CSV</span><span>&nbsp;</span></li><br><br><li><span>Other formats relevant to your workloads</span><span>&nbsp;</span></li><br><br></ul> | What performance monitoring tools are currently in use?<br><br><br><ul><br><li><span>Databricks Ganglia</span><span>&nbsp;</span></li><br><br><li><span>Azure Monitor</span><span>&nbsp;</span></li><br><br><li><span>Third-party tools</span><span>&nbsp;</span></li><br><br></ul><br><br>What are the key performance metrics tracked?<br><br><br><ul><br><li><span>Job execution time</span><span>&nbsp;</span></li><br><br><li><span>Cluster performance</span><span>&nbsp;</span></li><br><br><li><span>Data processing speed</span><span>&nbsp;</span></li><br><br></ul> | Are there any planned expansions or changes to the Databricks environment?<br><br><br><ul><br><li><span>New use cases</span><span>&nbsp;</span></li><br><br><li><span>Increased data volume</span><span>&nbsp;</span></li><br><br><li><span>Additional&nbsp;users</span><span>&nbsp;</span></li><br><br></ul><br><br>What are the long-term goals for Databricks cost optimization?<br><br><br><ul><br><li><span>Reducing overall spend</span><span>&nbsp;</span></li><br><br><li><span>Improving resource utilization &amp; cost attribution</span><span>&nbsp;</span></li><br><br><li><span>Enhancing performance</span><span>&nbsp;</span></li><br><br></ul> |\n\n##### **Understanding Databricks Cost Structure**\n\n**Total Cost = Cloud Cost + DBU Cost**\n\n- - Cloud Cost: Compute (VMs, networking, IP addresses), storage (ADLS, MLflow artifacts), other services (firewalls), cluster type (serverless compute, classic compute)\n\n- - DBU Cost: Workload size, cluster/warehouse size, photon acceleration, compute runtime, workspace tier, SKU type (Jobs, Delta Live Tables, All Purpose Clusters, Serverless), model serving, queries per second, model execution time\n\n##### **Diagnose Cost and Issues**\n\nEffectively diagnosing cost and performance issues in Databricks requires a structured approach. Use the following steps and metrics to gain visibility into your environment and uncover actionable insights.\n\n1. ###### **Review Cluster Metrics**\n\n- - CPU Utilization: Track guest, iowait, idle, irq, nice, softirq, steal, system, and user times to understand how compute resources are being used.\n- Memory Utilization: Monitor used, free, buffer, and cached memory to identify over- or under-utilization.\n\n- - Key Question: Is your cluster over- or under-utilized? Are resources being wasted or stretched too thin?\n\n1. ###### **Review SQL Warehouse Metrics**\n\n1. 1. **Live Statistics****:**Monitor warehouse status, running/queued queries, and current cluster count.\n2. **Time Scale Filter****:** Analyze query and cluster activity over different time frames (8 hours, 24 hours, 7 days, 14 days).\n3. **Peak Query Count Chart****:** Identify periods of high concurrency.\n4. **Completed Query Count Chart****:** Track throughput and query success/failure rates.\n5. **Running Clusters Chart****:**Observe cluster allocation and recycling events.\n6. **Query History Table****:** Filter and analyze queries by user, duration, status, and statement type.\n2.\n\n###### **3. Review Spark UI**\n\n- - **Stages Tab****:** Look for skewed data, high input/output, and shuffle times. Uneven task durations may indicate data skew or inefficient data handling.\n\n- - **Jobs Timeline****:** Identify long-running jobs or stages that consume excessive resources.\n\n- - **Stage Analysis****:** Determine if stages are I/O bound or suffering from data skew/spill.\n\n- - **Executor Metrics****:** Monitor memory usage, CPU utilization, and disk I/O. Frequent garbage collection or high memory usage may signal the need for better resource allocation.\n\n###### **3.1. Storage & Jobs Tab**\n\n- - **Storage Level****:** Check if data is stored in memory, on disk, or both.\n\n- - **Size****:** Assess the size of cached data.\n\n- - **Job Analysis****:** Investigate jobs that dominate the timeline or have unusually long durations. Look for gaps caused by complex execution plans, non-Spark code, driver overload, or cluster malfunction.\n\n###### **3.2. Executor Tab**\n\n- - **Storage Memory****:** Compare used vs. available memory.\n\n- - **Task Time (Garbage Collection)****:** Review long tasks and garbage collection times.\n\n- - **Shuffle Read/Write****:** Measure data transferred between stages.\n\n#### **Phase 2: Cluster/Code/Data Best Practices Alignment**\n\n##### **Cluster UI Configuration and Cost Attribution**\n\nEffectively configuring clusters/workloads in Databricks is essential for balancing performance, scalability, and cost. Tunning settings and features when used strategically can help organizations maximize resource efficiency and minimize unnecessary spending.\n\n##### **Key Configuration Strategies**\n\n**1. Reduce Idle Time:**Clusters to incur costs even when not actively processing workloads. To avoid paying for unused resources:\n\n- - **Enable Auto-Terminate**: Set clusters automatically shut down after a period of inactivity. This simple setting can significantly reduce wasted spending.\n\n1. **Enable Autoscaling:** Workloads fluctuate in size and complexity. Autoscaling allows clusters to dynamically adjust the number of nodes based on demand:\n\n- - **Automatic Resource Adjustment:** Scale up for heavy jobs and scale down for lighter loads, ensuring you only pay for what you use.\n\n1. **Use Spot Instances:** For batch processing and non-critical workloads, spot instances offer substantial cost savings:\n\n- - **Lower VM Costs**: Spot instances are typically much cheaper than standard VMs. However, they are not recommended for jobs requiring constant uptime due to potential interruptions.\n\n1. **Leverage Photon Engine:** Photon is Databricks’ high-performance, vectorized query engine:\n\n- - **Accelerate Large Workloads**: Photon can dramatically reduce runtime for compute-intensive tasks, improving both speed and cost efficiency.\n\n1. **Keep Runtimes Up to Date:** Using the latest Databricks runtime ensures optimal performance and security:\n\n- - **Benefit from Improvements:******Regular updates include performance enhancements, bug fixes, and new features.\n\n1. **Apply Cluster Policies:** Cluster policies help standardize configurations and enforce cost controls across teams:\n\n- - **Governance and Consistency:******Policies can restrict certain settings, enforce tagging, and ensure clusters are created with cost-effective defaults.\n\n1. **Optimize Storage:**type impacts both performance and cost:\n\n- - **Switch from HDDs to SSDs**: SSDs provide faster caching and shuffle operations, which can improve job efficiency and reduce runtime.\n\n1. **Tag Clusters for Cost Attribution:** Tagging clusters enables granular tracking and reporting:\n\n- - **Visibility and Accountability:** Use tags to attribute costs to specific teams, projects, or environments, supporting better budgeting and chargeback processes.\n\n1. **Select the Right Cluster Type:** Different workloads require different cluster types:\n\n- - **Job Clusters:******Ideal for scheduled jobs and Delta Live Tables.\n\n- - **All-Purpose Clusters:******Suited for ad-hoc analysis and collaborative work.\n\n- - **Single-Node Clusters:** Efficient for simple exploratory data analysis or pure Python tasks.\n\n- - **Serverless Compute:******Scalable, managed workloads with automatic resource management.\n\n1. **Monitor and Adjust Regularly:** review cluster metrics and query history:\n\n- - **Continuous Optimization:** Use built-in dashboards to monitor usage, identify bottlenecks, and adjust cluster size or configuration as needed.\n\n##### **Code Best Practices**\n\n1. **Avoid Reprocessing Large Tables**\n\n- - Use a CDC (Change Data Capture) architecture with Delta Live Tables (DLT) to process only new or changed data, minimizing unnecessary computation.\n\n1. **Ensure Code Parallelizes Well**\n\n- - Write Spark code that leverages parallel processing. Avoid loops, deeply nested structures, and inefficient user-defined functions (UDFs) that can hinder scalability.\n\n1. **Reduce Memory Consumption**\n\n- - Tweak Spark configurations to minimize memory overhead. Clean out legacy or unnecessary settings that may have carried over from previous Spark versions.\n\n1. **Prefer SQL Over Complex Python**\n\n- - Use SQL (declarative language) for Spark jobs whenever possible. SQL queries are typically more efficient and easier to optimize than complex Python logic.\n\n1. **Modularize Notebooks**\n\n- - Use %run to split large notebooks into smaller, reusable modules. This improves maintainability.\n\n1. **Use LIMIT in Exploratory Queries**\n\n- - When exploring data, always use the LIMIT clause to avoid scanning large datasets unnecessarily.\n\n1. **Monitor Job Performance**\n\n- - Regularly review Spark UI to detect inefficiencies such as high shuffle, input, or output. Optimize join strategies and data layout accordingly.\n\n##### **Databricks Code Performance Enhancements & Data Engineering Best Practices**\n\nBy enabling the below features and applying best practices, you can significantly lower costs, accelerate job execution, and build Databricks pipelines that are both scalable and highly reliable. For more guidance review: [Comprehensive Guide to Optimize Data Workloads | Databricks](https://www.databricks.com/discover/pages/optimize-data-workloads-guide#intro).\n\n| **Feature / Technique** | **Purpose / Benefit** | **How to Use / Enable / Key Notes** | | --- | --- | --- | | **[Disk Caching](https://learn.microsoft.com/en-us/azure/databricks/optimizations/disk-cache)** | Accelerates repeated reads of Parquet files | Set spark.databricks.io.cache.enabled = true | | **[Dynamic File Pruning (DFP)](https://learn.microsoft.com/en-us/azure/databricks/optimizations/dynamic-file-pruning)** | Skips irrelevant data files during queries, improves query performance | Enabled by default in Databricks | | **[Low Shuffle Merge](https://learn.microsoft.com/en-us/azure/databricks/optimizations/low-shuffle-merge)** | Reduces data rewriting during MERGE operations, less need to recalculate ZORDER | Use Databricks runtime with feature enabled | | **[Adaptive Query Execution (AQE)](https://learn.microsoft.com/en-us/azure/databricks/optimizations/aqe)** | Dynamically optimizes query plans based on runtime statistics | Available in Spark 3.0+, enabled by default | | **[Deletion Vectors](https://learn.microsoft.com/en-us/azure/databricks/delta/deletion-vectors)** | Efficient row removal/change without rewriting entire Parquet file | Enable in workspace settings, use with Delta Lake | | **[Materialized Views](https://www.databricks.com/blog/introducing-materialized-views-and-streaming-tables-databricks-sql)** | Faster BI queries, reduced compute for frequently accessed data | Create in Databricks SQL | | **[Optimize](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-optimize)** | Compacts Delta Lake files, improves query performance | Run regularly, combine with ZORDER on high-cardinality columns | | **ZORDER** | Physically sorts/co-locates data by chosen columns for faster queries | Use with OPTIMIZE, select columns frequently used in filters/joins | | **[Auto Optimize](https://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size#auto-optimize)** | Automatically compacts small files during writes | Enable optimizeWrite and autoCompact table properties | | **[Liquid Clustering](https://learn.microsoft.com/en-us/azure/databricks/delta/clustering)** | Simplifies data layout, replaces partitioning/ZORDER, flexible clustering keys | Recommended for new Delta tables, enables easy redefinition of clustering keys | | **[File Size Tuning](https://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size)** | Achieve optimal file size for performance and cost | Set delta.targetFileSize table property | | **[Broadcast Hash Join](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-syntax-qry-select-hints)** | Optimizes joins by broadcasting smaller tables | Adjust spark.sql.autoBroadcastJoinThreshold and spark.databricks.adaptive.autoBroadcastJoinThreshold | | **Shuffle Hash Join** | Faster join alternative to sort-merge join | Prefer over sort-merge join when broadcasting isn’t possible, Photon engine can help | | **[Cost-Based Optimizer (CBO)](https://learn.microsoft.com/en-us/azure/databricks/optimizations/cbo)** | Improves query plans for complex joins | Enabled by default, collect column/table statistics with ANALYZE TABLE | | **[Data Spilling & Skew](https://learn.microsoft.com/en-us/azure/databricks/optimizations/aqe)** | Handles uneven data distribution and excessive shuffle | Use AQE, set spark.sql.shuffle.partitions=auto, optimize partitioning | | **Data Explosion Management** | Controls partition sizes after transformations (e.g., explode, join) | Adjust spark.sql.files.maxPartitionBytes, use repartition() after reads | | **[Delta Merge](https://learn.microsoft.com/en-us/azure/databricks/delta/merge)** | Efficient upserts and CDC (Change Data Capture) | Use MERGE operation in Delta Lake, combine with CDC architecture | | **[Data Purging (Vacuum)](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/delta-vacuum)** | Removes stale data files, maintains storage efficiency | Run VACUUM regularly based on transaction frequency |\n\n#### **Phase 3: Team Alignment and Next Steps**\n\n##### **Implementing Cost Observability and Taking Action**\n\nEffective cost management in Databricks goes beyond configuration and code—it requires robust observability, granular tracking, and proactive measures. Below outlines how your teams can achieve this using system tables, tagging, dashboards, and actionable scripts.\n\n1. ###### **Cost Observability with System Tables**\n\nDatabricks Unity Catalog provides system tables that store operational data for your account. These tables enable historical cost observability and empower FinOps teams to analyze spend independently.\n\n- - **System Tables Location:******Found inside the Unity Catalog under the “system” schema.\n- **Key Benefits**: Structured data for querying, historical analysis, and cost attribution.\n- **Action**: Assign permissions to FinOps teams so they can access and analyze dedicated cost tables.\n\n1. **Enable Tags for Granular Tracking**\n\nTagging is a powerful feature for tracking, reporting, and budgeting at a granular level.\n\n- - **Classic Compute:******Manually add key/value pairs when creating clusters, jobs, SQL Warehouses, or Model Serving endpoints. Use cluster policies to enforce custom tags.\n\n- - **Serverless Compute:** Create budget policies and assign permissions to teams or members for serverless workloads.\n\n- - **Action**: Tag all compute resources to enable detailed cost attribution and reporting.\n\n1. **Track Costs with Dashboards and Alerts**\n\nDatabricks offers prebuilt dashboards and queries for cost forecasting and usage analysis.\n\n- - [Dashboards:](https://www.databricks.com/resources/demos/tutorials/governance/system-tables) Visualize spend, usage trends, and forecast future costs.\n- [Prebuilt Queries](https://community.databricks.com/t5/technical-blog/top-10-queries-to-use-with-system-tables/ba-p/82331): Use top queries with system tables to answer meaningful cost questions.\n- [Budget Alerts:](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/budgets) Set up alerts in the Account Console (Usage &gt; Budget) to receive notifications when spend approaches defined thresholds.\n\n1. **Review Bottlenecks and Optimization Opportunities**\n\nWith observability in place, regularly review system tables, dashboards, and tagged resources to identify:\n\n- - **Cost Bottlenecks:** Clusters or jobs with unusually high spend.\n\n- - **Optimization Opportunities:******Underutilized resources, inefficient jobs, or misconfigured clusters.\n\n- - **Team Alignment:** Share insights with engineering and FinOps teams to drive collaborative optimization.\n\n#### **Summary Table: Cost Observability & Action Steps**\n\n| **Area** | **Best Practice / Action** | | --- | --- | | **System Tables** | Use for historical cost analysis and attribution | | **Tagging** | Apply to all compute resources for granular tracking | | **Dashboards** | Visualize spend, usage, and forecasts | | **Alerts** | Set budget alerts for proactive cost management | | **Scripts/Queries** | Build custom analysis tools for deep insights | | **Cluster/Data/Code Review & Align** | Regularly review best practices, share findings, and align teams on optimization |\n\nUpdated Nov 15, 2025\n\nVersion 12.0\n\n[analytics](/tag/analytics?nodeId=board%3AAnalyticsonAzure)\n\n[azure](/tag/azure?nodeId=board%3AAnalyticsonAzure)\n\n[azure databricks](/tag/azure%20databricks?nodeId=board%3AAnalyticsonAzure)\n\n[delta lake](/tag/delta%20lake?nodeId=board%3AAnalyticsonAzure)\n\n[spark](/tag/spark?nodeId=board%3AAnalyticsonAzure)\n\n[!\\[Rafia_Aqil&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0zMDcyNDQwLUZEQmYzMQ?image-coordinates=60%2C75%2C544%2C559&amp;image-dimensions=50x50)](/users/rafia_aqil/3072440) [Rafia_Aqil](/users/rafia_aqil/3072440) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined June 13, 2025\n\n[View Profile](/users/rafia_aqil/3072440)\n\n/category/azure/blog/analyticsonazure [Analytics on Azure Blog](/category/azure/blog/analyticsonazure) Follow this blog board to get notified when there's new activity",
  "PubDate": "2025-11-15T08:01:17+00:00",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Link": "https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/azure-databricks-cost-optimization-a-practical-guide/ba-p/4470235",
  "OutputDir": "_community",
  "Tags": [],
  "ProcessedDate": "2025-11-15 08:03:35",
  "FeedName": "Microsoft Tech Community",
  "Title": "Azure Databricks Cost Optimization: A Practical Guide"
}
