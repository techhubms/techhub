{
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Description": "**From Physics and Information Theory to Practical Reliability in the Cloud**\n\nTwo years ago I wrote about [mitigating downtime and increasing reliability by managing complexity in cloud‑native systems](https://techcommunity.microsoft.com/blog/azurearchitectureblog/mitigating-downtime-and-increasing-reliability-strategies-for-managing-complexit/3810399). I used ideas from physics and chaos theory as a lens to explain why cloud architectures feel fragile as they grow: more moving parts, more states, more surprising failure modes.\n\nSince then, I’ve gone deeper, especially into three concepts:\n\n- Thermodynamic entropy: energy spreading out, order decaying.\n- Shannon entropy: uncertainty and information in signals.\n- Chaos theory: deterministic systems behaving unpredictably due to sensitivity to initial conditions.\n\nWhat is striking is how naturally these ideas line up with what we experience every day in Azure estates:\n\n- The system's that we design and deploy through their published SLA's that look good on paper but still experience weird outages.\n- Architectures that are theoretically resilient but operationally brittle.\n- Systems that only seem understandable right up until the moment they fail.\n\nThis post is the “Part 2” I did not write in 2023. I will start with entropy and information, map them onto cloud and distributed systems, and then land in concrete patterns for designing and operating highly reliable systems on Azure. Along the way, I’ll use a small SLA example (revisited, corrected, and extended to multi‑region) to connect the theory to real design decisions.\n\n**1. Entropy: from physics to information**\n\nYou do not need full statistical mechanics or information theory to make these concepts useful as an architect. You just need a few core ideas.\n\n1.1 Thermodynamic entropy: energy vs usefulness\n\nAt the physical level, two things are true:\n\n1. Energy is conserved. It does not disappear.\n2. Entropy tends to increase in a closed system.\n\nEntropy, in thermodynamics, measures how spread-out energy is and how many microscopic arrangements (microstates) correspond to the same macroscopic appearance (macrostate).\n\n- When energy is concentrated, for example, in a small number of particles with high energy, entropy is low and it’s easy to extract useful work.\n- When energy is dispersed, many particles with a little energy each, entropy is high and the energy is harder to harness.\n\nSame total energy, remarkably different usefulness.\n\nA classic example: the Earth and the Sun.\n\n- Earth receives low‑entropy energy: relatively few high‑energy photons.\n- Earth radiates back high‑entropy energy: many lower‑energy photons.\n\nEverything interesting that happens in between: weather, chemistry, life, is the process of converting concentrated energy into dispersed heat.\n\n1.2 Shannon entropy: uncertainty and information\n\nClaude Shannon came at entropy from a completely different direction: communication.\n\nHe wanted to quantify:\n\n- How unpredictable a source of messages is.\n- How much information each message carries.\n- How much we can compress data or correct errors over a noisy channel.\n\nThe key idea:\n\n- Information is linked to surprise.\n- A highly predictable event carries little information.\n- A rare, surprising event carries a lot.\n\nShannon entropy is the average surprise of a source. It is high when:\n\n- Many outcomes are possible, and\n- They are all likely.\n\nIt turns out the formula Shannon derived for entropy in information theory has the same mathematical form as entropy in statistical physics. The interpretations differ:\n\n- Thermodynamic entropy: missing information about the exact microscopic state of a physical system.\n- Shannon entropy: missing information (uncertainty) about the outcome of a random variable or source.\n\nBut they are connected by the same idea:\n\nEntropy is about uncertainty and the number of possible states.\n\nThat is going to matter a lot when we talk about observability and incident response.\n\n1.3 Chaos: when deterministic systems feel random\n\nChaos theory completes the picture.\n\nChaotic systems are:\n\n- Deterministic: future states are fully determined by the current state and rules.\n- But highly sensitive to initial conditions: tiny differences in the starting point grow into large differences in behaviour over time.\n\nWeather is the classic example:\n\n- The equations governing fluid flow are known.\n- But a tiny uncertainty in measurements today can grow into a completely different storm pattern in a week.\n\nFrom an information point of view:\n\n- Chaos amplifies small unknowns into large uncertainties about the future.\n- Shannon entropy of “where the system might be” grows quickly as you look further ahead.\n\nKeep those three in your head:\n\n- Thermodynamic entropy: the number of microstates and how energy spreads.\n- Shannon entropy: how uncertain we are about state or signals.\n- Chaos: how uncertainty evolves over time.\n\nIf we were look at a cloud system.\n\n**2. Cloud systems as entropy machines**\n\nIn the first post, I argued that cloud‑native architectures are highly ordered structures sitting on top of an unreliable substrate. That intuition still holds, but we can add more clarity to it.\n\n2.1 Resources vs structured capability\n\nOn Azure, we rarely run out of raw resources:\n\n- Compute, storage, and network are elastic.\n- We can usually scale up and out.\n\nWhat we run out of is structured capability:\n\n- Clear domain boundaries and ownership.\n- Clean, stable APIs and schemas.\n- Well‑understood patterns for retries, timeouts, and idempotency.\n- Bounded configuration and feature flag complexity.\n- Telemetry that tells us what’s happening, not just that “something is wrong”.\n- Human attention to support and evolve all the above.\n\nEvery time we:\n\n- Add a new microservice or function,\n- Introduce a new data store,\n- Add another “temporary” feature flag,\n- Create a special‑case code path for one tenant or one region,\n- Wire in another third‑party dependency,\n\nwe increase the number of possible states the system can be in.\n\nMany of those states are benign. Some are edge cases waiting to be discovered at 4 a.m.\n\nThis is architectural entropy: the space of states your design permits.\n\n2.2 Architectural entropy vs epistemic entropy\n\nIt helps to split entropy into two types:\n\n1. Architectural entropy : how many states the system can be in.\n- Number of services and instances.\n- Number of data stores, schemas, and caches.\n- Number of configuration combinations and feature modes.\n- Number of possible call paths and feedback loops.\n2. Epistemic (Shannon) entropy: how much uncertainty we have about which state it’s actually in right now.\n- What do we know from logs, metrics, and traces?\n- How many different hypotheses are still plausible when something goes wrong?\n\nYou can have:\n\n- Low architectural entropy but high epistemic entropy\n(simple system, almost no telemetry), or\n- High architectural entropy but low epistemic entropy\n(complex system, but excellent observability, automation, and discipline).\n\nMost real systems are somewhere in between.\n\nChaos matters because:\n\n- As systems grow, small unknowns (tiny config difference, slight timing change) can lead to very different behaviours.\n- The “distance” between your architecture diagram and real runtime behaviour grows.\n\nIn other words: Diagrams are macrostates; incidents happen in the microstates.\n\nOur job is to make sure the microstate space is constrained, observable, and survivable.\n\n**3. Four kinds of entropy in cloud architectures**\n\nTo make this actionable in design reviews, I want to talk about four dimensions of entropy.\n\n3.1 State entropy: the many ways reality can differ from the diagram\n\nQuestions:\n\n- How many data stores are involved (SQL, NoSQL, queues, caches, blobs, search, etc.)?\n- For each business concept (customer, order, balance):\n- Is there exactly one source of truth?\n- Or multiple systems that can write it?\n- How many schema versions or representations exist at once?\n- Do we have dual‑writes or “temporary” copies that became permanent?\n\nHigher state entropy means more ways the system can be ”mostly right but subtly wrong”.\n\n3.2 Configuration entropy: the number of behavioural knobs\n\nQuestions:\n\n- How many feature flags are active in production?\n- How many environment, region, or tenant‑specific settings are there?\n- Is there a lifecycle for config and flags (creation > rollout >retirement)?\n- Who can change config, and How (pipeline, portal, manual script)?\n\nHigher config entropy increases the number of “if these three flags are on, in that region, for that tenant, after that deploy…” states.\n\n3.3 Interaction entropy: tangles in the dependency graph\n\nQuestions:\n\n- For each critical user journey:\n- How many services and external dependencies are in the hot path?\n- What does the call graph look like?\n- Fan‑in, fan‑out, cycles.\n- How many asynchronous links:\n- Queues, topics, streams, background jobs?\n- Are there shared components multiple domains depend on?\n\nHigher interaction entropy means more ways failures can cascade and more opportunities for chaotic feedback (retries, timeouts, autoscaling) to bite.\n\n3.4 Organisational entropy: how many mental models are involved\n\nQuestions:\n\n- How many teams are involved in a single critical journey?\n- For each component, is there a clear accountable owner?\n- Are docs and runbooks up to date and used?\n- How many teams typically end up on incident calls?\n\nHigher organisational entropy means larger gaps between:\n\n- What people think the system does, and\n- What it actually does under pressure.\n\nThese four dimensions give you a way to talk about an entropy budget: Where are we comfortable adding more possible states and interactions, and where are we not?\n\nBefore we go into patterns, it’s worth doing a short SLA calculation with some Azure numbers, because it illustrates both the power of multi‑region and Availability Zones and where the maths stops being the true limiting factor.\n\n**4. Sidebar: SLAs, regions, zones: and where math stops helping**\n\nLet’s revisit a simple example using three common services:\n\n- Azure App Service: 99.95%\n- Azure SQL Database: 99.99%\n- Azure Storage (Blob): 99.9%\n\nAssume a request needs all three to succeed. If any one is unavailable, the scenario fails.\n\n4.1 Single region, no zones\n\nIn one region (say, UK South), convert the SLAs to decimals:\n\n- Web App SLA = 99.95% = 0.9995\n- SQL DB SLA = 99.99% = 0.9999\n- Blob SLA = 99.9% = 0.9990\n\nComposite regional SLA (all three must be up):\n\n- SLA\\_region = 0.9995 × 0.9999 × 0.9990\n- SLA\\_region = 0.99840065 (≈ 99.8401%)\n\nFailure probability for the region:\n\n- FailureRate\\_region = 1 − SLA\\_region\n- FailureRate\\_region = 1 − 0.99840065\n- FailureRate\\_region ≈ 0.00159935\n\nAnnual downtime (minutes per year):\n\n- MinutesPerYear = 365 × 24 × 60 = 525,600\n- DowntimeMinutes\\_region = FailureRate\\_region × MinutesPerYear\n- DowntimeMinutes\\_region ≈ 0.00159935 × 525,600\n- DowntimeMinutes\\_region ≈ 840.6 minutes ≈ 14 hours per year\n\nThat’s our baseline for a single region.\n\n4.2 Two regions (UK South + Sweden Central), active‑active\n\nNow deploy the same stack in UK South and Sweden Central, and consider the system “up” if either region can serve the request.\n\nAssumptions:\n\n- Both regions behave like the baseline above.\n- Regional failures are independent (simplifying assumption, standard for SLA math).\n\nFailure probability for one region (from above):\n\n- FailureRate\\_region ≈ 0.00159935\n\nProbability that both regions are down at the same time:\n\n- FailureRate\\_both = FailureRate\\_region × FailureRate\\_region\n- FailureRate\\_both ≈ 0.00159935 × 0.00159935\n- FailureRate\\_both ≈ 0.00000256 (2.56 × 10⁻⁶)\n\nMulti‑region SLA:\n\n- SLA\\_multi = 1 − FailureRate\\_both\n- SLA\\_multi ≈ 1 − 0.00000256\n- SLA\\_multi ≈ 0.99999744 (≈ 99.999744%)\n\nAnnual downtime:\n\n- DowntimeMinutes\\_multi = FailureRate\\_both × MinutesPerYear\n- DowntimeMinutes\\_multi ≈ 0.00000256 × 525,600\n- DowntimeMinutes\\_multi ≈ 1.34 minutes ≈ 80 seconds per year\n\nSo on paper, going from one region to two reduces expected downtime by about 600× – from ~14 hours to ~1.3 minutes per year.\n\n4.3 Single region with 3 Availability Zones\n\nNow consider a single region that uses three Availability Zones for each service. For this model, we assume zonal failures are independent.\n\nFor each service:\n\n1. Convert SLA to a per‑zone failure rate.\n2. Cube that value (failure in all three zones).\n3. Convert back to an “across‑zones” SLA.\n\nWeb App (99.95%)\n\n- SLA\\_web = 0.9995\n- FailureRate\\_web = 1 − SLA\\_web = 1 − 0.9995 = 0.0005\n\nFailure across all 3 zones:\n\n- FailureRate\\_web\\_all\\_zones = 0.0005 × 0.0005 × 0.0005\n- FailureRate\\_web\\_all\\_zones = 0.000000000125 (1.25 × 10⁻¹⁰)\n\nSLA across 3 zones:\n\n- SLA\\_web\\_AZ = 1 − FailureRate\\_web\\_all\\_zones\n- SLA\\_web\\_AZ = 1 − 0.000000000125\n- SLA\\_web\\_AZ ≈ 0.999999999875\n\nSQL Database (99.99%)\n\n- SLA\\_sql = 0.9999\n- FailureRate\\_sql = 1 − 0.9999 = 0.0001\n\nFailure across all 3 zones:\n\n- FailureRate\\_sql\\_all\\_zones = 0.0001 × 0.0001 × 0.0001\n- FailureRate\\_sql\\_all\\_zones = 0.000000000001 (1 × 10⁻¹²)\n\nSLA across 3 zones:\n\n- SLA\\_sql\\_AZ = 1 − FailureRate\\_sql\\_all\\_zones\n- SLA\\_sql\\_AZ ≈ 0.999999999999\n\nBlob Storage (99.9%)\n\n- SLA\\_blob = 0.9990\n- FailureRate\\_blob = 1 − 0.9990 = 0.001\n\nFailure across all 3 zones:\n\n- FailureRate\\_blob\\_all\\_zones = 0.001 × 0.001 × 0.001\n- FailureRate\\_blob\\_all\\_zones = 0.000000001 (1 × 10⁻⁹)\n\nSLA across 3 zones:\n\n- SLA\\_blob\\_AZ = 1 − FailureRate\\_blob\\_all\\_zones\n- SLA\\_blob\\_AZ ≈ 0.999999999\n\nComposite SLA for the zone‑redundant stack (one region)\n\n- SLA\\_region\\_AZ = SLA\\_web\\_AZ × SLA\\_sql\\_AZ × SLA\\_blob\\_AZ\n- SLA\\_region\\_AZ ≈ 0.999999999875 × 0.999999999999 × 0.999999999\n- SLA\\_region\\_AZ ≈ 0.999999998874\n\nSo:\n\n- Single region with 3 AZs ≈ 99.9999998874%\n- FailureRate\\_region\\_AZ = 1 − 0.999999998874\n- FailureRate\\_region\\_AZ ≈ 0.000000001126 (1.126 × 10⁻⁹)\n\nAnnual downtime:\n\n- DowntimeMinutes\\_region\\_AZ = FailureRate\\_region\\_AZ × MinutesPerYear\n- DowntimeMinutes\\_region\\_AZ ≈ 0.000000001126 × 525,600\n- DowntimeMinutes\\_region\\_AZ ≈ 0.000592 minutes ≈ 0.035 seconds per year\n\nOn paper, that’s a huge improvement\n\n4.4 Two regions + AZ: where the maths stops being the bottleneck\n\nFinally, if you deploy that zone‑redundant stack to both UK South and Sweden Central, and assume:\n\n- Independence between zones within a region, and\n- Independence between regions,\n\nthen:\n\nPer‑region failure rate with AZ:\n\n- FailureRate\\_region\\_AZ ≈ 0.000000001126\n\nBoth regions down at once:\n\n- FailureRate\\_both\\_AZ = FailureRate\\_region\\_AZ × FailureRate\\_region\\_AZ\n- FailureRate\\_both\\_AZ ≈ 0.000000001126 × 0.000000001126\n- FailureRate\\_both\\_AZ ≈ 0.00000000000000000127 (1.27 × 10⁻¹⁸)\n\nMulti‑region, multi‑AZ SLA:\n\n- SLA\\_multi\\_AZ = 1 − FailureRate\\_both\\_AZ\n- SLA\\_multi\\_AZ ≈ 1 − 0.00000000000000000127\n- SLA\\_multi\\_AZ ≈ 0.9999999999999999987 (effectively “18 nines”)\n\nAnnual downtime:\n\n- DowntimeMinutes\\_multi\\_AZ = FailureRate\\_both\\_AZ × MinutesPerYear\n- DowntimeMinutes\\_multi\\_AZ ≈ 1.27 × 10⁻¹⁸ × 525,600\n- DowntimeMinutes\\_multi\\_AZ is on the order of 10⁻¹³ minutes\n- That’s about 10⁻¹¹ seconds per year – effectively zero on human timescales\n\nAt this point, the maths is telling you:\n\nUnder these assumptions, this combination of service outages reduces the likelihood hood of an event happening to a statistically small interval\n\nAnd this is exactly where the assumptions break down:\n\n- Real‑world failures are not fully independent between zones and regions.\n- There are shared dependencies (control planes, DNS, identity, networking, global services).\n- And most importantly, there is us – our designs, our changes, our operational mistakes.\n\nSo while multi‑region + AZ absolutely are the way forward for building highly reliable systems, it is fair (and important) to say:\n\nThe system itself is only as reliable as its known failure modes and recovery behaviours, irrespective of how available the underlying platform looks on paper.\n\nSLAs and redundancy describe the potential for reliability.\n\nWhether you realise that potential depends on how well you manage entropy in your architecture, information, and operations.\n\n**5. The system is only as reliable as its failure and recovery behaviours**\n\nIt is tempting to look at the SLA math and conclude that:\n\n“If I have enough regions and zones, I’m basically okay.”\n\nIn practice, once you’re using zones and (where appropriate) multiple regions, the limiting factors for reliability are usually:\n\n- Unknown or unmanaged failure modes\n- Features that assume a dependency will “never fail”.\n- Data paths that haven’t been exercised under partial failure.\n- Poorly defined recovery behaviour\n- Services that don’t fail fast and cause cascading timeouts.\n- Workflows that get stuck in “unknown” or “zombie” states.\n- Gaps in observability\n- You can’t see which parts of the system are broken quickly enough.\n- You can’t distinguish between “region issue”, “dependency issue”, and “our bad deployment”.\n- Architectural and organisational entropy\n- Too many flags, modes, and special cases.\n- Too many teams involved in a single change or incident.\n- No clear owners for shared components.\n\nIn other words:\n\nThe platform might support “five nines” and beyond. But your system will only be as reliable as:\n\n- The failure states you’ve anticipated,\n- The recovery paths you’ve designed and automated, and\n- The signals you have to detect and act on them.\n\nThat’s where entropy, information, and chaos stop being metaphors and start being design inputs.\n\nHow do we fight this kind of entropy in practice?\n\n**6. Patterns for fighting entropy (and building real reliability)**\n\n6.1 Reduce state entropy: sharpen domains and data ownership\n\nAim: fewer ways for reality to drift from your mental model.\n\n- Establish strong domain boundaries:\n- One team owns each core business concept and its data.\n- Others consume via APIs or events, not direct writes to its store.\n- Limit data technology sprawl per domain:\n- Use the right tool (OLTP, analytics, search), but don’t allow every team to pick a different DB “just because”.\n- Each new engine adds states and failure modes to manage.\n- Version schemas and contracts explicitly:\n- Don’t rely on “schema by convention”.\n- Use forwards/backwards compatible changes, versioned contracts, and automated tests for compatibility.\n- Time‑box support for old versions and automate discovery of stragglers.\n\n6.2 Design for partial knowledge: assume you are never fully in sync\n\nAim: remain correct even when services have incomplete or delayed information.\n\nPatterns:\n\n- Idempotent APIs for all non‑trivial writes:\n- Every request has a stable identity.\n- Safe to retry without knowing if the last attempt “took”.\n- Exactly‑once effects on top of at‑least‑once delivery:\n- Use deduplication tables, sequence numbers, or commutative operations to ensure business effects happen once even if messages do not.\n- Explicit consistency boundaries:\n- Decide where you require strong consistency (often within a bounded context).\n- Everywhere else, accept eventual consistency and make the UX honest about it:\n- “Pending”, “processing”, last updated timestamps, etc.\n- Saga and workflow patterns for multi‑step business operations:\n- Model long‑lived operations as state machines with compensations.\n- Do not pretend a distributed transaction is a local one.\n\nThese patterns recognise that:\n\n- No node has perfect knowledge.\n- The network is asynchronous.\n- Timing and ordering are subject to chaotic behaviour.\n\nThey are your tools for living with that reality.\n\n6.3 Use observability to reduce Shannon entropy\n\nAim: shrink the set of plausible system states during an incident as fast as possible.\n\nStart from invariants:\n\n- Identify 3–5 non‑negotiable properties:\n- “No double‑billing.”\n- “No lost, confirmed orders.”\n- “Data accepted with a 200 must be durable.”\n\nDesign signals to track those:\n\n- SLIs/SLOs that reflect user‑visible health, not just CPU and memory.\n- Metrics that segment by region, tenant, version, the axes you debug along.\n- Correlation that lets you follow a business transaction across services, queues, and background jobs.\n\nThink information theory:\n\n- Every log, metric, and trace is a message.\n- Good observability maximises the information those messages carry about what you care about and minimises noise.\n\nYou know observability is working when:\n\n- In an incident, you can rule out entire classes of failure with one or two dashboards.\n- The set of hypotheses you are considering collapses quickly instead of expanding.\n\n6.4 Map your dynamics with load and chaos\n\nAim: understand how your system moves through its state space under stress.\n\nThink in terms of attractors:\n\n- Healthy steady state.\n- Graceful degradation states.\n- Unhealthy “meltdown” states (retry storms, cascading timeouts, data corruption).\n\nUse:\n\n- Load testing to explore capacity edges:\n- How do autoscaling, throttling, and back-pressure behave?\n- Do you see oscillations (scale up/down thrash)?\n- Chaos experiments to explore failure edges:\n- Inject realistic faults: latency, packet loss, node failures, partial region impairments.\n- Observe whether the system falls back to a safe state or spirals.\n\nOn Azure, this typically means combining services like Azure Load Testing and Azure Chaos Studio with your own playbooks and SLOs.\n\nThe goal is not to “break production for fun”. It is to:\n\n- Discover failure states on your terms,\n- Exercise recovery paths, and\n- Continuously refine both the architecture and the runbooks.\n\n6.5 Govern with an entropy budget\n\nAim: make complexity an explicit trade‑off, not a by‑product.\n\nIn reviews and planning, ask:\n\n- How much state entropy can this journey tolerate?\n- How much config entropy before we lose control?\n- How much interaction entropy before another dependency is a liability?\n- How much organisational entropy before ownership becomes a bottleneck?\n\nSet budgets:\n\n- For mission‑critical journeys (payments, identity, safety):\n- Extremely tight budgets on state and interaction entropy.\n- Aggressive simplification and standardisation.\n- For less critical or internal workloads:\n- Higher budgets but nonetheless bounded.\n- Emphasis on blast radius control and observability.\n\nThen hold teams (and yourself) to those budgets:\n\n“If we want to introduce this new data store / dependency / feature flag, what are we removing or standardising elsewhere to pay for it? And how are we making it observable and operable?”\n\nThat turns “this feels too complex” into a repeatable pattern.\n\n**7. Closing: living in the productive middle**\n\nEntropy, Shannon information, and chaos theory are not just metaphors. They are honest descriptions of the forces acting on modern cloud systems:\n\n- Structures decay.\n- The number of possible states grows faster than we like to admit.\n- Our knowledge of those states is limited and noisy.\n- Small uncertainties can grow into large incidents over time.\n\nThe good news is: interesting systems live in the middle.\n\n- Too little entropy and your architecture cannot scale or adapt.\n- Too much and it becomes inoperable.\n\nOur job as architects and leaders is to keep our systems in that productive middle zone:\n\n- Use zones and regions to raise the ceiling on what’s possible.\n- Use patterns, observability, and governance to make that potential real.\n- Continuously learn from incidents and experiments to push complexity back into structure.\n\nOr, in the language we’ve been using:\n\nCloud is a war against entropy. Infra redundancy wins you the right battles. Failure and recovery design decide whether you win the war.\n\nLavan Nallainathan Director - Customer Success UK Cloud & AI",
  "ProcessedDate": "2025-12-02 01:32:27",
  "Author": "lavann320",
  "PubDate": "2025-12-02T01:24:03+00:00",
  "EnhancedContent": "## In today's rapidly evolving technological landscape, the quest for reliability in cloud-native systems is more critical than ever. This post delves into the intricate relationship between physics, information theory, and practical reliability in the cloud. Building on concepts such as thermodynamic entropy, Shannon entropy, and chaos theory, we explore how these ideas manifest in Azure estates. From understanding SLAs and multi-region deployments to managing architectural and epistemic entropy, this post provides actionable insights and patterns for designing and operating highly reliable systems. Join us as we navigate the complexities of cloud architectures and uncover strategies to mitigate downtime and enhance system resilience.\n\n**From Physics and Information Theory to Practical Reliability in the Cloud**\n\nTwo years ago I wrote about [mitigating downtime and increasing reliability by managing complexity in cloud‑native systems](https://techcommunity.microsoft.com/blog/azurearchitectureblog/mitigating-downtime-and-increasing-reliability-strategies-for-managing-complexit/3810399). I used ideas from physics and chaos theory as a lens to explain why cloud architectures feel fragile as they grow: more moving parts, more states, more surprising failure modes.\n\nSince then, I’ve gone deeper, especially into three concepts:\n\n- Thermodynamic entropy: energy spreading out, order decaying.\n- Shannon entropy: uncertainty and information in signals.\n- Chaos theory: deterministic systems behaving unpredictably due to sensitivity to initial conditions.\n\nWhat is striking is how naturally these ideas line up with what we experience every day in Azure estates:\n\n- The system's that we design and deploy through their published SLA's that look good on paper but still experience weird outages.\n- Architectures that are theoretically resilient but operationally brittle.\n- Systems that only seem understandable right up until the moment they fail.\n\nThis post is the “Part 2” I did not write in 2023. I will start with entropy and information, map them onto cloud and distributed systems, and then land in concrete patterns for designing and operating highly reliable systems on Azure. Along the way, I’ll use a small SLA example (revisited, corrected, and extended to multi‑region) to connect the theory to real design decisions.\n\n**1. Entropy: from physics to information**\n\nYou do not need full statistical mechanics or information theory to make these concepts useful as an architect. You just need a few core ideas.\n\n1.1 Thermodynamic entropy: energy vs usefulness\n\nAt the physical level, two things are true:\n\n1. Energy is conserved. It does not disappear.\n2. Entropy tends to increase in a closed system.\n\nEntropy, in thermodynamics, measures how spread-out energy is and how many microscopic arrangements (microstates) correspond to the same macroscopic appearance (macrostate).\n\n- When energy is concentrated, for example, in a small number of particles with high energy, entropy is low and it’s easy to extract useful work.\n- When energy is dispersed, many particles with a little energy each, entropy is high and the energy is harder to harness.\n\nSame total energy, remarkably different usefulness.\n\nA classic example: the Earth and the Sun.\n\n- Earth receives low‑entropy energy: relatively few high‑energy photons.\n- Earth radiates back high‑entropy energy: many lower‑energy photons.\n\nEverything interesting that happens in between: weather, chemistry, life, is the process of converting concentrated energy into dispersed heat.\n\n1.2 Shannon entropy: uncertainty and information\n\nClaude Shannon came at entropy from a completely different direction: communication.\n\nHe wanted to quantify:\n\n- How unpredictable a source of messages is.\n- How much information each message carries.\n- How much we can compress data or correct errors over a noisy channel.\n\nThe key idea:\n\n- Information is linked to surprise.\n- A highly predictable event carries little information.\n- A rare, surprising event carries a lot.\n\nShannon entropy is the average surprise of a source. It is high when:\n\n- Many outcomes are possible, and\n- They are all likely.\n\nIt turns out the formula Shannon derived for entropy in information theory has the same mathematical form as entropy in statistical physics. The interpretations differ:\n\n- Thermodynamic entropy: missing information about the exact microscopic state of a physical system.\n- Shannon entropy: missing information (uncertainty) about the outcome of a random variable or source.\n\nBut they are connected by the same idea:\n\nEntropy is about uncertainty and the number of possible states.\n\nThat is going to matter a lot when we talk about observability and incident response.\n\n1.3 Chaos: when deterministic systems feel random\n\nChaos theory completes the picture.\n\nChaotic systems are:\n\n- Deterministic: future states are fully determined by the current state and rules.\n- But highly sensitive to initial conditions: tiny differences in the starting point grow into large differences in behaviour over time.\n\nWeather is the classic example:\n\n- The equations governing fluid flow are known.\n- But a tiny uncertainty in measurements today can grow into a completely different storm pattern in a week.\n\nFrom an information point of view:\n\n- Chaos amplifies small unknowns into large uncertainties about the future.\n- Shannon entropy of “where the system might be” grows quickly as you look further ahead.\n\nKeep those three in your head:\n\n- Thermodynamic entropy: the number of microstates and how energy spreads.\n- Shannon entropy: how uncertain we are about state or signals.\n- Chaos: how uncertainty evolves over time.\n\nIf we were look at a cloud system.\n\n**2. Cloud systems as entropy machines**\n\nIn the first post, I argued that cloud‑native architectures are highly ordered structures sitting on top of an unreliable substrate. That intuition still holds, but we can add more clarity to it.\n\n2.1 Resources vs structured capability\n\nOn Azure, we rarely run out of raw resources:\n\n- Compute, storage, and network are elastic.\n- We can usually scale up and out.\n\nWhat we run out of is structured capability:\n\n- Clear domain boundaries and ownership.\n- Clean, stable APIs and schemas.\n- Well‑understood patterns for retries, timeouts, and idempotency.\n- Bounded configuration and feature flag complexity.\n- Telemetry that tells us what’s happening, not just that “something is wrong”.\n- Human attention to support and evolve all the above.\n\nEvery time we:\n\n- Add a new microservice or function,\n- Introduce a new data store,\n- Add another “temporary” feature flag,\n- Create a special‑case code path for one tenant or one region,\n- Wire in another third‑party dependency,\n\nwe increase the number of possible states the system can be in.\n\nMany of those states are benign. Some are edge cases waiting to be discovered at 4 a.m.\n\nThis is architectural entropy: the space of states your design permits.\n\n2.2 Architectural entropy vs epistemic entropy\n\nIt helps to split entropy into two types:\n\n1. Architectural entropy : how many states the system can be in.\n- Number of services and instances.\n- Number of data stores, schemas, and caches.\n- Number of configuration combinations and feature modes.\n- Number of possible call paths and feedback loops.\n2. Epistemic (Shannon) entropy: how much uncertainty we have about which state it’s actually in right now.\n- What do we know from logs, metrics, and traces?\n- How many different hypotheses are still plausible when something goes wrong?\n\nYou can have:\n\n- Low architectural entropy but high epistemic entropy\n(simple system, almost no telemetry), or\n- High architectural entropy but low epistemic entropy\n(complex system, but excellent observability, automation, and discipline).\n\nMost real systems are somewhere in between.\n\nChaos matters because:\n\n- As systems grow, small unknowns (tiny config difference, slight timing change) can lead to very different behaviours.\n- The “distance” between your architecture diagram and real runtime behaviour grows.\n\nIn other words: Diagrams are macrostates; incidents happen in the microstates.\n\nOur job is to make sure the microstate space is constrained, observable, and survivable.\n\n**3. Four kinds of entropy in cloud architectures**\n\nTo make this actionable in design reviews, I want to talk about four dimensions of entropy.\n\n3.1 State entropy: the many ways reality can differ from the diagram\n\nQuestions:\n\n- How many data stores are involved (SQL, NoSQL, queues, caches, blobs, search, etc.)?\n- For each business concept (customer, order, balance):\n- Is there exactly one source of truth?\n- Or multiple systems that can write it?\n- How many schema versions or representations exist at once?\n- Do we have dual‑writes or “temporary” copies that became permanent?\n\nHigher state entropy means more ways the system can be ”mostly right but subtly wrong”.\n\n3.2 Configuration entropy: the number of behavioural knobs\n\nQuestions:\n\n- How many feature flags are active in production?\n- How many environment, region, or tenant‑specific settings are there?\n- Is there a lifecycle for config and flags (creation &gt; rollout &gt;retirement)?\n- Who can change config, and How (pipeline, portal, manual script)?\n\nHigher config entropy increases the number of “if these three flags are on, in that region, for that tenant, after that deploy…” states.\n\n3.3 Interaction entropy: tangles in the dependency graph\n\nQuestions:\n\n- For each critical user journey:\n- How many services and external dependencies are in the hot path?\n- What does the call graph look like?\n- Fan‑in, fan‑out, cycles.\n- How many asynchronous links:\n- Queues, topics, streams, background jobs?\n- Are there shared components multiple domains depend on?\n\nHigher interaction entropy means more ways failures can cascade and more opportunities for chaotic feedback (retries, timeouts, autoscaling) to bite.\n\n3.4 Organisational entropy: how many mental models are involved\n\nQuestions:\n\n- How many teams are involved in a single critical journey?\n- For each component, is there a clear accountable owner?\n- Are docs and runbooks up to date and used?\n- How many teams typically end up on incident calls?\n\nHigher organisational entropy means larger gaps between:\n\n- What people think the system does, and\n- What it actually does under pressure.\n\nThese four dimensions give you a way to talk about an entropy budget: Where are we comfortable adding more possible states and interactions, and where are we not?\n\nBefore we go into patterns, it’s worth doing a short SLA calculation with some Azure numbers, because it illustrates both the power of multi‑region and Availability Zones and where the maths stops being the true limiting factor.\n\n**4. Sidebar: SLAs, regions, zones: and where math stops helping**\n\nLet’s revisit a simple example using three common services:\n\n- Azure App Service: 99.95%\n- Azure SQL Database: 99.99%\n- Azure Storage (Blob): 99.9%\n\nAssume a request needs all three to succeed. If any one is unavailable, the scenario fails.\n\n4.1 Single region, no zones\n\nIn one region (say, UK South), convert the SLAs to decimals:\n\n- Web App SLA = 99.95% = 0.9995\n- SQL DB SLA = 99.99% = 0.9999\n- Blob SLA = 99.9% = 0.9990\n\nComposite regional SLA (all three must be up):\n\n- SLA\\_region = 0.9995 × 0.9999 × 0.9990\n- SLA\\_region = 0.99840065 (≈ 99.8401%)\n\nFailure probability for the region:\n\n- FailureRate\\_region = 1 − SLA\\_region\n- FailureRate\\_region = 1 − 0.99840065\n- FailureRate\\_region ≈ 0.00159935\n\nAnnual downtime (minutes per year):\n\n- MinutesPerYear = 365 × 24 × 60 = 525,600\n- DowntimeMinutes\\_region = FailureRate\\_region × MinutesPerYear\n- DowntimeMinutes\\_region ≈ 0.00159935 × 525,600\n- DowntimeMinutes\\_region ≈ 840.6 minutes ≈ 14 hours per year\n\nThat’s our baseline for a single region.\n\n4.2 Two regions (UK South + Sweden Central), active‑active\n\nNow deploy the same stack in UK South and Sweden Central, and consider the system “up” if either region can serve the request.\n\nAssumptions:\n\n- Both regions behave like the baseline above.\n- Regional failures are independent (simplifying assumption, standard for SLA math).\n\nFailure probability for one region (from above):\n\n- FailureRate\\_region ≈ 0.00159935\n\nProbability that both regions are down at the same time:\n\n- FailureRate\\_both = FailureRate\\_region × FailureRate\\_region\n- FailureRate\\_both ≈ 0.00159935 × 0.00159935\n- FailureRate\\_both ≈ 0.00000256 (2.56 × 10⁻⁶)\n\nMulti‑region SLA:\n\n- SLA\\_multi = 1 − FailureRate\\_both\n- SLA\\_multi ≈ 1 − 0.00000256\n- SLA\\_multi ≈ 0.99999744 (≈ 99.999744%)\n\nAnnual downtime:\n\n- DowntimeMinutes\\_multi = FailureRate\\_both × MinutesPerYear\n- DowntimeMinutes\\_multi ≈ 0.00000256 × 525,600\n- DowntimeMinutes\\_multi ≈ 1.34 minutes ≈ 80 seconds per year\n\nSo on paper, going from one region to two reduces expected downtime by about 600× – from ~14 hours to ~1.3 minutes per year.\n\n4.3 Single region with 3 Availability Zones\n\nNow consider a single region that uses three Availability Zones for each service. For this model, we assume zonal failures are independent.\n\nFor each service:\n\n1. Convert SLA to a per‑zone failure rate.\n2. Cube that value (failure in all three zones).\n3. Convert back to an “across‑zones” SLA.\n\nWeb App (99.95%)\n\n- SLA\\_web = 0.9995\n- FailureRate\\_web = 1 − SLA\\_web = 1 − 0.9995 = 0.0005\n\nFailure across all 3 zones:\n\n- FailureRate\\_web\\_all\\_zones = 0.0005 × 0.0005 × 0.0005\n- FailureRate\\_web\\_all\\_zones = 0.000000000125 (1.25 × 10⁻¹⁰)\n\nSLA across 3 zones:\n\n- SLA\\_web\\_AZ = 1 − FailureRate\\_web\\_all\\_zones\n- SLA\\_web\\_AZ = 1 − 0.000000000125\n- SLA\\_web\\_AZ ≈ 0.999999999875\n\nSQL Database (99.99%)\n\n- SLA\\_sql = 0.9999\n- FailureRate\\_sql = 1 − 0.9999 = 0.0001\n\nFailure across all 3 zones:\n\n- FailureRate\\_sql\\_all\\_zones = 0.0001 × 0.0001 × 0.0001\n- FailureRate\\_sql\\_all\\_zones = 0.000000000001 (1 × 10⁻¹²)\n\nSLA across 3 zones:\n\n- SLA\\_sql\\_AZ = 1 − FailureRate\\_sql\\_all\\_zones\n- SLA\\_sql\\_AZ ≈ 0.999999999999\n\nBlob Storage (99.9%)\n\n- SLA\\_blob = 0.9990\n- FailureRate\\_blob = 1 − 0.9990 = 0.001\n\nFailure across all 3 zones:\n\n- FailureRate\\_blob\\_all\\_zones = 0.001 × 0.001 × 0.001\n- FailureRate\\_blob\\_all\\_zones = 0.000000001 (1 × 10⁻⁹)\n\nSLA across 3 zones:\n\n- SLA\\_blob\\_AZ = 1 − FailureRate\\_blob\\_all\\_zones\n- SLA\\_blob\\_AZ ≈ 0.999999999\n\nComposite SLA for the zone‑redundant stack (one region)\n\n- SLA\\_region\\_AZ = SLA\\_web\\_AZ × SLA\\_sql\\_AZ × SLA\\_blob\\_AZ\n- SLA\\_region\\_AZ ≈ 0.999999999875 × 0.999999999999 × 0.999999999\n- SLA\\_region\\_AZ ≈ 0.999999998874\n\nSo:\n\n- Single region with 3 AZs ≈ 99.9999998874%\n- FailureRate\\_region\\_AZ = 1 − 0.999999998874\n- FailureRate\\_region\\_AZ ≈ 0.000000001126 (1.126 × 10⁻⁹)\n\nAnnual downtime:\n\n- DowntimeMinutes\\_region\\_AZ = FailureRate\\_region\\_AZ × MinutesPerYear\n- DowntimeMinutes\\_region\\_AZ ≈ 0.000000001126 × 525,600\n- DowntimeMinutes\\_region\\_AZ ≈ 0.000592 minutes ≈ 0.035 seconds per year\n\nOn paper, that’s a huge improvement\n\n4.4 Two regions + AZ: where the maths stops being the bottleneck\n\nFinally, if you deploy that zone‑redundant stack to both UK South and Sweden Central, and assume:\n\n- Independence between zones within a region, and\n- Independence between regions,\n\nthen:\n\nPer‑region failure rate with AZ:\n\n- FailureRate\\_region\\_AZ ≈ 0.000000001126\n\nBoth regions down at once:\n\n- FailureRate\\_both\\_AZ = FailureRate\\_region\\_AZ × FailureRate\\_region\\_AZ\n- FailureRate\\_both\\_AZ ≈ 0.000000001126 × 0.000000001126\n- FailureRate\\_both\\_AZ ≈ 0.00000000000000000127 (1.27 × 10⁻¹⁸)\n\nMulti‑region, multi‑AZ SLA:\n\n- SLA\\_multi\\_AZ = 1 − FailureRate\\_both\\_AZ\n- SLA\\_multi\\_AZ ≈ 1 − 0.00000000000000000127\n- SLA\\_multi\\_AZ ≈ 0.9999999999999999987 (effectively “18 nines”)\n\nAnnual downtime:\n\n- DowntimeMinutes\\_multi\\_AZ = FailureRate\\_both\\_AZ × MinutesPerYear\n- DowntimeMinutes\\_multi\\_AZ ≈ 1.27 × 10⁻¹⁸ × 525,600\n- DowntimeMinutes\\_multi\\_AZ is on the order of 10⁻¹³ minutes\n- That’s about 10⁻¹¹ seconds per year – effectively zero on human timescales\n\nAt this point, the maths is telling you:\n\nUnder these assumptions, this combination of service outages reduces the likelihood hood of an event happening to a statistically small interval\n\nAnd this is exactly where the assumptions break down:\n\n- Real‑world failures are not fully independent between zones and regions.\n- There are shared dependencies (control planes, DNS, identity, networking, global services).\n- And most importantly, there is us – our designs, our changes, our operational mistakes.\n\nSo while multi‑region + AZ absolutely are the way forward for building highly reliable systems, it is fair (and important) to say:\n\nThe system itself is only as reliable as its known failure modes and recovery behaviours, irrespective of how available the underlying platform looks on paper.\n\nSLAs and redundancy describe the potential for reliability.\n\nWhether you realise that potential depends on how well you manage entropy in your architecture, information, and operations.\n\n**5. The system is only as reliable as its failure and recovery behaviours**\n\nIt is tempting to look at the SLA math and conclude that:\n\n“If I have enough regions and zones, I’m basically okay.”\n\nIn practice, once you’re using zones and (where appropriate) multiple regions, the limiting factors for reliability are usually:\n\n- Unknown or unmanaged failure modes\n- Features that assume a dependency will “never fail”.\n- Data paths that haven’t been exercised under partial failure.\n- Poorly defined recovery behaviour\n- Services that don’t fail fast and cause cascading timeouts.\n- Workflows that get stuck in “unknown” or “zombie” states.\n- Gaps in observability\n- You can’t see which parts of the system are broken quickly enough.\n- You can’t distinguish between “region issue”, “dependency issue”, and “our bad deployment”.\n- Architectural and organisational entropy\n- Too many flags, modes, and special cases.\n- Too many teams involved in a single change or incident.\n- No clear owners for shared components.\n\nIn other words:\n\nThe platform might support “five nines” and beyond. But your system will only be as reliable as:\n\n- The failure states you’ve anticipated,\n- The recovery paths you’ve designed and automated, and\n- The signals you have to detect and act on them.\n\nThat’s where entropy, information, and chaos stop being metaphors and start being design inputs.\n\nHow do we fight this kind of entropy in practice?\n\n**6. Patterns for fighting entropy (and building real reliability)**\n\n6.1 Reduce state entropy: sharpen domains and data ownership\n\nAim: fewer ways for reality to drift from your mental model.\n\n- Establish strong domain boundaries:\n- One team owns each core business concept and its data.\n- Others consume via APIs or events, not direct writes to its store.\n- Limit data technology sprawl per domain:\n- Use the right tool (OLTP, analytics, search), but don’t allow every team to pick a different DB “just because”.\n- Each new engine adds states and failure modes to manage.\n- Version schemas and contracts explicitly:\n- Don’t rely on “schema by convention”.\n- Use forwards/backwards compatible changes, versioned contracts, and automated tests for compatibility.\n- Time‑box support for old versions and automate discovery of stragglers.\n\n6.2 Design for partial knowledge: assume you are never fully in sync\n\nAim: remain correct even when services have incomplete or delayed information.\n\nPatterns:\n\n- Idempotent APIs for all non‑trivial writes:\n- Every request has a stable identity.\n- Safe to retry without knowing if the last attempt “took”.\n- Exactly‑once effects on top of at‑least‑once delivery:\n- Use deduplication tables, sequence numbers, or commutative operations to ensure business effects happen once even if messages do not.\n- Explicit consistency boundaries:\n- Decide where you require strong consistency (often within a bounded context).\n- Everywhere else, accept eventual consistency and make the UX honest about it:\n- “Pending”, “processing”, last updated timestamps, etc.\n- Saga and workflow patterns for multi‑step business operations:\n- Model long‑lived operations as state machines with compensations.\n- Do not pretend a distributed transaction is a local one.\n\nThese patterns recognise that:\n\n- No node has perfect knowledge.\n- The network is asynchronous.\n- Timing and ordering are subject to chaotic behaviour.\n\nThey are your tools for living with that reality.\n\n6.3 Use observability to reduce Shannon entropy\n\nAim: shrink the set of plausible system states during an incident as fast as possible.\n\nStart from invariants:\n\n- Identify 3–5 non‑negotiable properties:\n- “No double‑billing.”\n- “No lost, confirmed orders.”\n- “Data accepted with a 200 must be durable.”\n\nDesign signals to track those:\n\n- SLIs/SLOs that reflect user‑visible health, not just CPU and memory.\n- Metrics that segment by region, tenant, version, the axes you debug along.\n- Correlation that lets you follow a business transaction across services, queues, and background jobs.\n\nThink information theory:\n\n- Every log, metric, and trace is a message.\n- Good observability maximises the information those messages carry about what you care about and minimises noise.\n\nYou know observability is working when:\n\n- In an incident, you can rule out entire classes of failure with one or two dashboards.\n- The set of hypotheses you are considering collapses quickly instead of expanding.\n\n6.4 Map your dynamics with load and chaos\n\nAim: understand how your system moves through its state space under stress.\n\nThink in terms of attractors:\n\n- Healthy steady state.\n- Graceful degradation states.\n- Unhealthy “meltdown” states (retry storms, cascading timeouts, data corruption).\n\nUse:\n\n- Load testing to explore capacity edges:\n- How do autoscaling, throttling, and back-pressure behave?\n- Do you see oscillations (scale up/down thrash)?\n- Chaos experiments to explore failure edges:\n- Inject realistic faults: latency, packet loss, node failures, partial region impairments.\n- Observe whether the system falls back to a safe state or spirals.\n\nOn Azure, this typically means combining services like Azure Load Testing and Azure Chaos Studio with your own playbooks and SLOs.\n\nThe goal is not to “break production for fun”. It is to:\n\n- Discover failure states on your terms,\n- Exercise recovery paths, and\n- Continuously refine both the architecture and the runbooks.\n\n6.5 Govern with an entropy budget\n\nAim: make complexity an explicit trade‑off, not a by‑product.\n\nIn reviews and planning, ask:\n\n- How much state entropy can this journey tolerate?\n- How much config entropy before we lose control?\n- How much interaction entropy before another dependency is a liability?\n- How much organisational entropy before ownership becomes a bottleneck?\n\nSet budgets:\n\n- For mission‑critical journeys (payments, identity, safety):\n- Extremely tight budgets on state and interaction entropy.\n- Aggressive simplification and standardisation.\n- For less critical or internal workloads:\n- Higher budgets but nonetheless bounded.\n- Emphasis on blast radius control and observability.\n\nThen hold teams (and yourself) to those budgets:\n\n“If we want to introduce this new data store / dependency / feature flag, what are we removing or standardising elsewhere to pay for it? And how are we making it observable and operable?”\n\nThat turns “this feels too complex” into a repeatable pattern.\n\n**7. Closing: living in the productive middle**\n\nEntropy, Shannon information, and chaos theory are not just metaphors. They are honest descriptions of the forces acting on modern cloud systems:\n\n- Structures decay.\n- The number of possible states grows faster than we like to admit.\n- Our knowledge of those states is limited and noisy.\n- Small uncertainties can grow into large incidents over time.\n\nThe good news is: interesting systems live in the middle.\n\n- Too little entropy and your architecture cannot scale or adapt.\n- Too much and it becomes inoperable.\n\nOur job as architects and leaders is to keep our systems in that productive middle zone:\n\n- Use zones and regions to raise the ceiling on what’s possible.\n- Use patterns, observability, and governance to make that potential real.\n- Continuously learn from incidents and experiments to push complexity back into structure.\n\nOr, in the language we’ve been using:\n\nCloud is a war against entropy. Infra redundancy wins you the right battles. Failure and recovery design decide whether you win the war.\n\nLavan Nallainathan Director - Customer Success UK Cloud & AI\n\nUpdated Dec 02, 2025\n\nVersion 1.0\n\n[!\\[lavann320&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-4.svg?image-dimensions=50x50)](/users/lavann320/528700) [lavann320](/users/lavann320/528700) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined January 22, 2020\n\n[View Profile](/users/lavann320/528700)\n\n/category/azure/blog/azurearchitectureblog [Azure Architecture Blog](/category/azure/blog/azurearchitectureblog) Follow this blog board to get notified when there's new activity",
  "Tags": [],
  "OutputDir": "_community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "FeedName": "Microsoft Tech Community",
  "Title": "Cloud as a War Against Entropy",
  "Link": "https://techcommunity.microsoft.com/t5/azure-architecture-blog/cloud-as-a-war-against-entropy/ba-p/4474111"
}
