{
  "Description": "As part of the Python advocacy team, I help maintain several open-source sample AI applications, like our[popular RAG chat demo](https://github.com/Azure-Samples/azure-search-openai-demo/). Through that work, I’ve learned a lot about what makes LLM-powered apps feel fast, reliable, and responsive.\n\nOne of the most important lessons: **use an asynchronous backend framework**. Concurrency is critical for LLM apps, which often juggle multiple API calls, database queries, and user requests at the same time. Without async, your app may spend most of its time waiting — blocking one user’s request while another sits idle.\n\n### The need for concurrency\n\nWhy? Let’s imagine we’re using a **synchronous framework** like [Flask](https://flask.palletsprojects.com/). We deploy that to a server with [gunicorn](https://gunicorn.org/) and several workers. One worker receives a POST request to the \"/chat\" endpoint, which in turn calls the Azure OpenAI Chat Completions API.\n\nThat API call can take several seconds to complete — and during that time, the worker is **completely tied up**, unable to handle any other requests. We could scale out by adding more CPU cores, workers, or threads, but that’s often wasteful and expensive.\n\nWithout concurrency, each request must be handled **serially:**\n\n![Diagram of worker handling requests one after the other](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXOK3LpaA-iv2FoukmcC5LbqbIsYNhJ0EyQBjZahJpJ4KvBAFDSl5bStAwYOVQei2H5vQk6GGa5XeGVGder6j_hogYsBQXzX9qc8siqMHPsZ3eRGvt9H-aN3iAaEz9ITOXPzfw4OqTcAeHCN-dz_VcL8t9BGU3g3PuLEMiEPLBvF_gIbIbpyfY_Ybb4w/s1600/sync@2x.png)\n\nWhen your app relies on long, blocking I/O operations — like model calls, database queries, or external API lookups — a better approach is to use an **asynchronous framework**. With async I/O, the Python runtime can pause a coroutine that’s waiting for a slow response and switch to handling another incoming request in the meantime.\n\nWith concurrency, your workers stay busy and can handle **new requests while others are waiting:**\n\n![Diagram of worker handling second request while first request waits for API response](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7sEpE3qFKdHyFUErfHBOWVAvkXXvFg_9wPAxT8cWIjc3gW6NoVJf2jeQsHRqO_AiqWo03ojDp83gNmJ3ygkFLTo2RCM57JGPga_veR-lM9Y30JC-Mi7HbzjTBSWcwYohmtVRGNsOs4GyMXMRJl4rHa79x7WMiGYxY2oILRu1b1JbdTZ-n44e975hfQw/s1600/async@2x.png)\n\n### Asynchronous Python backends\n\nIn the Python ecosystem, there are several asynchronous backend frameworks to choose from:\n\n- [Quart](https://quart.palletsprojects.com/): the asynchronous version of Flask\n- [FastAPI](https://github.com/pamelafox/chatgpt-backend-fastapi): an API-centric, async-only framework (built on Starlette)\n- [Litestar](https://docs.litestar.dev/): a batteries-included async framework (also built on Starlette)\n- [Django](https://www.djangoproject.com/): not async by default, but includes support for asynchronous views\n\nAll of these can be good options depending on your project’s needs. I’ve written more about the decision-making process in [another blog post](https://blog.pamelafox.org/2024/07/should-you-use-quart-or-fastapi-for-ai.html).\n\nAs an example, let's see what changes when we port a Flask app to a Quart app.\n\nFirst, our handlers now have `async` in front, signifying that they return a Python coroutine instead of a normal function:\n\n- async def chat\\_handler():\nrequest\\_message = (await request.get\\_json())[\"message\"]\n\nWhen deploying these apps, I often still use the **Gunicorn** production web server—but with the **Uvicorn worker**, which is designed for Python ASGI applications. Alternatively, you can run **Uvicorn** or **Hypercorn** directly as standalone servers.\n\n### Asynchronous API calls\n\nTo fully benefit from moving to an asynchronous framework, your app’s **API calls also need to be asynchronous**. That way, whenever a worker is waiting for an external response, it can pause that coroutine and start handling another incoming request.\n\nLet's see what that looks like when using the official OpenAI Python SDK. First, we initialize the async version of the OpenAI client:\n- openai\\_client = openai.AsyncOpenAI(\nbase\\_url=os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] + \"/openai/v1\", api\\_key=token\\_provider )\n\nThen, whenever we make API calls with methods on that client, we `await` their results:\n- chat\\_coroutine = await openai\\_client.chat.completions.create(\ndeployment\\_id=os.environ[\"AZURE\\_OPENAI\\_CHAT\\_DEPLOYMENT\"], messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": request\\_message}], stream=True, )\n\nFor the RAG sample, we also have calls to Azure services like Azure AI Search. To make those asynchronous, we first import the async variant of the credential and client classes in the `aio` module:\n- from azure.identity.aio import DefaultAzureCredential\nfrom azure.search.documents.aio import SearchClient\n\nThen, like with the OpenAI async clients, we must `await` results from any methods that make network calls:\n- r = await self.search\\_client.search(query\\_text)\n\nBy ensuring that every outbound network call is asynchronous, your app can make the most of Python’s event loop — handling multiple user sessions and API requests concurrently, without wasting worker time waiting on slow responses.\n\n## Sample applications\n\nWe’ve already linked to several of our samples that use async frameworks, but here’s a longer list so you can find the one that best fits your tech stack:\n\n| **Repository** | **App purpose** | **Backend** | **Frontend** | | --- | --- | --- | --- | | [azure-search-openai-demo](https://github.com/Azure-Samples/azure-search-openai-demo) | RAG with AI Search | Python + Quart | React | | [rag-postgres-openai-python](https://github.com/Azure-Samples/rag-postgres-openai-python/) | RAG with PostgreSQL | Python + FastAPI | React | | [openai-chat-app-quickstart](https://github.com/Azure-Samples/openai-chat-app-quickstart) | Simple chat with Azure OpenAI models | Python + Quart | plain JS | | [openai-chat-backend-fastapi](https://github.com/Azure-Samples/openai-chat-backend-fastapi) | Simple chat with Azure OpenAI models | Python + FastAPI | plain JS | | [deepseek-python](https://github.com/Azure-Samples/deepseek-python) | Simple chat with Azure AI Foundry models | Python + Quart | plain JS |",
  "EnhancedContent": "As part of the Python advocacy team, I help maintain several open-source sample AI applications, like our[popular RAG chat demo](https://github.com/Azure-Samples/azure-search-openai-demo/). Through that work, I’ve learned a lot about what makes LLM-powered apps feel fast, reliable, and responsive.\n\nOne of the most important lessons: **use an asynchronous backend framework**. Concurrency is critical for LLM apps, which often juggle multiple API calls, database queries, and user requests at the same time. Without async, your app may spend most of its time waiting — blocking one user’s request while another sits idle.\n\n### The need for concurrency\n\nWhy? Let’s imagine we’re using a **synchronous framework** like [Flask](https://flask.palletsprojects.com/). We deploy that to a server with [gunicorn](https://gunicorn.org/) and several workers. One worker receives a POST request to the \"/chat\" endpoint, which in turn calls the Azure OpenAI Chat Completions API.\n\nThat API call can take several seconds to complete — and during that time, the worker is **completely tied up**, unable to handle any other requests. We could scale out by adding more CPU cores, workers, or threads, but that’s often wasteful and expensive.\n\nWithout concurrency, each request must be handled **serially:**\n\n![Diagram of worker handling requests one after the other](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEjXOK3LpaA-iv2FoukmcC5LbqbIsYNhJ0EyQBjZahJpJ4KvBAFDSl5bStAwYOVQei2H5vQk6GGa5XeGVGder6j_hogYsBQXzX9qc8siqMHPsZ3eRGvt9H-aN3iAaEz9ITOXPzfw4OqTcAeHCN-dz_VcL8t9BGU3g3PuLEMiEPLBvF_gIbIbpyfY_Ybb4w/s1600/sync@2x.png)\n\nWhen your app relies on long, blocking I/O operations — like model calls, database queries, or external API lookups — a better approach is to use an **asynchronous framework**. With async I/O, the Python runtime can pause a coroutine that’s waiting for a slow response and switch to handling another incoming request in the meantime.\n\nWith concurrency, your workers stay busy and can handle **new requests while others are waiting:**\n\n![Diagram of worker handling second request while first request waits for API response](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEj7sEpE3qFKdHyFUErfHBOWVAvkXXvFg_9wPAxT8cWIjc3gW6NoVJf2jeQsHRqO_AiqWo03ojDp83gNmJ3ygkFLTo2RCM57JGPga_veR-lM9Y30JC-Mi7HbzjTBSWcwYohmtVRGNsOs4GyMXMRJl4rHa79x7WMiGYxY2oILRu1b1JbdTZ-n44e975hfQw/s1600/async@2x.png)\n\n### Asynchronous Python backends\n\nIn the Python ecosystem, there are several asynchronous backend frameworks to choose from:\n\n- [Quart](https://quart.palletsprojects.com/): the asynchronous version of Flask\n- [FastAPI](https://github.com/pamelafox/chatgpt-backend-fastapi): an API-centric, async-only framework (built on Starlette)\n- [Litestar](https://docs.litestar.dev/): a batteries-included async framework (also built on Starlette)\n- [Django](https://www.djangoproject.com/): not async by default, but includes support for asynchronous views\n\nAll of these can be good options depending on your project’s needs. I’ve written more about the decision-making process in [another blog post](https://blog.pamelafox.org/2024/07/should-you-use-quart-or-fastapi-for-ai.html).\n\nAs an example, let's see what changes when we port a Flask app to a Quart app.\n\nFirst, our handlers now have `async` in front, signifying that they return a Python coroutine instead of a normal function:\n\n``` async def chat_handler(): request_message = (await request.get_json())[\"message\"]\n\n```\n\nWhen deploying these apps, I often still use the **Gunicorn** production web server—but with the **Uvicorn worker**, which is designed for Python ASGI applications. Alternatively, you can run **Uvicorn** or **Hypercorn** directly as standalone servers.\n\n### Asynchronous API calls\n\nTo fully benefit from moving to an asynchronous framework, your app’s **API calls also need to be asynchronous**. That way, whenever a worker is waiting for an external response, it can pause that coroutine and start handling another incoming request.\n\nLet's see what that looks like when using the official OpenAI Python SDK. First, we initialize the async version of the OpenAI client:\n\n``` openai_client = openai.AsyncOpenAI( base_url=os.environ[\"AZURE_OPENAI_ENDPOINT\"] + \"/openai/v1\", api_key=token_provider ) ```\n\nThen, whenever we make API calls with methods on that client, we `await` their results:\n\n``` chat_coroutine = await openai_client.chat.completions.create( deployment_id=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT\"], messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": request_message}], stream=True, )\n\n```\n\nFor the RAG sample, we also have calls to Azure services like Azure AI Search. To make those asynchronous, we first import the async variant of the credential and client classes in the `aio` module:\n\n``` from azure.identity.aio import DefaultAzureCredential from azure.search.documents.aio import SearchClient\n\n```\n\nThen, like with the OpenAI async clients, we must `await` results from any methods that make network calls:\n\n``` r = await self.search_client.search(query_text)\n\n```\n\nBy ensuring that every outbound network call is asynchronous, your app can make the most of Python’s event loop — handling multiple user sessions and API requests concurrently, without wasting worker time waiting on slow responses.\n\n## Sample applications\n\nWe’ve already linked to several of our samples that use async frameworks, but here’s a longer list so you can find the one that best fits your tech stack:\n\n| **Repository** | **App purpose** | **Backend** | **Frontend** | | --- | --- | --- | --- | | [azure-search-openai-demo](https://github.com/Azure-Samples/azure-search-openai-demo) | RAG with AI Search | Python + Quart | React | | [rag-postgres-openai-python](https://github.com/Azure-Samples/rag-postgres-openai-python/) | RAG with PostgreSQL | Python + FastAPI | React | | [openai-chat-app-quickstart](https://github.com/Azure-Samples/openai-chat-app-quickstart) | Simple chat with Azure OpenAI models | Python + Quart | plain JS | | [openai-chat-backend-fastapi](https://github.com/Azure-Samples/openai-chat-backend-fastapi) | Simple chat with Azure OpenAI models | Python + FastAPI | plain JS | | [deepseek-python](https://github.com/Azure-Samples/deepseek-python) | Simple chat with Azure AI Foundry models | Python + Quart | plain JS |\n\nUpdated Oct 07, 2025\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[genai](/tag/genai?nodeId=board%3AAzureDevCommunityBlog)\n\n[pamela fox](/tag/pamela%20fox?nodeId=board%3AAzureDevCommunityBlog)\n\n[python](/tag/python?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[Pamela_Fox&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xNjA0MDc4LTQxODI4MWk5MjkyQjFBMEVGOUE5NkM5?image-dimensions=50x50)](/users/pamela_fox/1604078) [Pamela_Fox](/users/pamela_fox/1604078) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined November 08, 2022\n\n[View Profile](/users/pamela_fox/1604078)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "OutputDir": "_community",
  "FeedName": "Microsoft Tech Community",
  "Title": "Why your LLM-powered app needs concurrency",
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/why-your-llm-powered-app-needs-concurrency/ba-p/4459584",
  "PubDate": "2025-10-07T16:05:02+00:00",
  "Author": "Pamela_Fox",
  "ProcessedDate": "2025-10-07 17:04:58",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Tags": [],
  "FeedLevelAuthor": "rss.livelink.threads-in-node"
}
