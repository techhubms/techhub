{
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/on-device-ai-with-windows-ai-foundry/ba-p/4466236",
  "OutputDir": "_community",
  "ProcessedDate": "2025-11-04 08:04:56",
  "FeedName": "Microsoft Tech Community",
  "PubDate": "2025-11-04T08:00:00+00:00",
  "Tags": [],
  "Title": "On‑Device AI with Windows AI Foundry",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "EnhancedContent": "## Build AI that runs where the users are, on their devices.\nWhen every millisecond and every byte of data matter, on‑device AI helps you stay responsive and protect user data by keeping processing local.\n\n#### From “waiting” to “instant”- without sending data away\n\nAI is everywhere, but speed, privacy, and reliability are critical. Users expect instant answers without compromise. On-device AI makes that possible: fast, private and available, even when the network isn’t - empowering apps to deliver seamless experiences.\n\nImagine an intelligent assistant that works in seconds, without sending a text to the cloud. This approach brings *speed and data control* to the places that need it most; while still letting you tap into cloud power when it makes sense.\n\n#### Windows AI Foundry: A Local Home for Models\n\nWindows AI Foundry is a developer toolkit that makes it simple to run AI models directly on Windows devices. It uses *ONNX Runtime* under the hood and can leverage CPU, GPU (via DirectML), or NPU acceleration, without requiring you to manage those details.\n\nThe principle is straightforward:\n\n- Keep the model and the data on the same device.\n- Inference becomes faster, and data stays local by default unless you explicitly choose to use the cloud.\n\n#### Foundry Local\n\nFoundry Local is the engine that powers this experience. Think of it as *local AI runtime* - fast, private, and easy to integrate into an app.\n\n#### Why Adopt On‑Device AI?\n\n- **Faster, more responsive apps:** Local inference often reduces perceived latency and improves user experience.\n- **Privacy‑first by design:** Keep sensitive data on the device; avoid cloud round trips unless the user opts in.\n- **Offline capability:** An app can provide AI features even without a network connection.\n- **Cost control:** Reduce cloud compute and data costs for common, high‑volume tasks.\n\nThis approach is especially useful in regulated industries, field‑work tools, and any app where users expect quick, on‑device responses.\n\n#### Hybrid Pattern for Real Apps\n\nOn-device AI doesn’t replace the cloud, it complements it. Here’s how:\n\n- **Standalone On‑Device**: Quick, private actions like document summarization, local search, and offline assistants.\n- **Cloud‑Enhanced (Optional)**: Large-context models, up-to-date knowledge, or heavy multimodal workloads.\n\nDesign an app to keep data local by default and surface cloud options transparently with user consent and clear disclosures.\n\nWindows AI Foundry supports hybrid workflows:\n\n- Use *Foundry Local* for real-time inference.\n- Sync with *Azure AI services* for model updates, telemetry, and advanced analytics.\n- Implement fallback strategies for resource-intensive scenarios.\n\n#### Application Workflow\n\nApplication Workflow: On‑Device AI with Windows AI Foundry and Cloud Integration as Hybrid Path\n\n#### Code Example\n\n1. Only On-Device: Tries Foundry Local first, falls back to ONNX\n\n``` if foundry_runtime.check_foundry_available():\n# Use on-device Foundry Local models\ntry: answer = foundry_runtime.run_inference(question, context) return answer, source=\"Foundry Local (On-Device)\" except Exception as e: logger.warning(f\"Foundry failed: {e}, trying ONNX...\")\n\nif onnx_model.is_loaded():\n# Fallback to local BERT ONNX model\ntry: answer = bert_model.get_answer(question, context) return answer, source=\"BERT ONNX (On-Device)\" except Exception as e: logger.warning(f\"ONNX failed: {e}\")\n\nreturn \"Error: No local AI available\" ```\n\n2. Hybrid approach: On-device first, cloud as last resort\n\n``` def get_answer(question, context): \"\"\" Priority order:\n1. Foundry Local (best: advanced + private)\n2. ONNX Runtime (good: fast + private)\n3. Cloud API (fallback: requires internet, less private)\n# in case of Hybrid approach, based on real-time scenario\n\"\"\"\n\nif foundry_runtime.check_foundry_available():\n# Use on-device Foundry Local models\ntry: answer = foundry_runtime.run_inference(question, context) return answer, source=\"Foundry Local (On-Device)\" except Exception as e: logger.warning(f\"Foundry failed: {e}, trying ONNX...\")\n\nif onnx_model.is_loaded():\n# Fallback to local BERT ONNX model\ntry: answer = bert_model.get_answer(question, context) return answer, source=\"BERT ONNX (On-Device)\" except Exception as e: logger.warning(f\"ONNX failed: {e}, trying cloud...\")\n\n# Last resort: Cloud API (requires internet)\nif network_available(): try: import requests response = requests.post( '{BASE_URL_AI_CHAT_COMPLETION}', headers={'Authorization': f'Bearer {API_KEY}'}, json={ 'model': '{MODEL_NAME}', 'messages': [{ 'role': 'user', 'content': f'Context: {context}\\n\\nQuestion: {question}' }] }, timeout=10 ) answer = response.json()['choices'][0]['message']['content'] return answer, source=\"Cloud API (Online)\" except Exception as e: return \"Error: No AI runtime available\", source=\"Failed\" else: return \"Error: No internet and no local AI available\", source=\"Offline\" ```\n\n#### Demo Project Output: Foundry Local answering context-based questions offline\n\n**Answer found in the context**: The Foundry Local engine ran the Phi-4-mini model offline and retrieved context-based data.\n\n**No answer found in the context**: The Foundry Local engine ran the Phi-4-mini model offline and mentioned that there is no answer.\n\n#### Practical Use Cases\n\n- **Privacy-First Reading Assistant**: Summarize documents locally without sending text to the cloud.\n- **Healthcare Apps**: Analyze medical data on-device for compliance.\n- **Financial Tools**: Risk scoring without exposing sensitive financial data.\n- **IoT & Edge Devices**: Real-time anomaly detection without network dependency.\n\n#### Conclusion\n\nOn-device AI isn’t just a trend - it’s a shift toward smarter, faster, and more secure applications. With Windows AI Foundry and Foundry Local, developers can deliver experiences that respect user specific data, reduce latency, and work even when connectivity fails. By combining local inference with optional cloud enhancements, you get the best of both worlds: instant performance and scalable intelligence.\n\nWhether you’re creating document summarizers, offline assistants, or compliance-ready solutions, this approach ensures your apps stay responsive, reliable, and user-centric.\n\n#### References\n\n- [Get started with Foundry Local - Foundry Local | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started)\n- [What is Windows AI Foundry? | Microsoft Learn](https://learn.microsoft.com/en-us/windows/ai/overview)\n- [https://devblogs.microsoft.com/foundry/unlock-instant-on-device-ai-with-foundry-local/](https://devblogs.microsoft.com/foundry/unlock-instant-on-device-ai-with-foundry-local/)\n\nUpdated Nov 03, 2025\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai foundry](/tag/ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure ai foundry](/tag/azure%20ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[best practices](/tag/best%20practices?nodeId=board%3AAzureDevCommunityBlog)\n\n[developer](/tag/developer?nodeId=board%3AAzureDevCommunityBlog)\n\n[foundry local](/tag/foundry%20local?nodeId=board%3AAzureDevCommunityBlog)\n\n[get started](/tag/get%20started?nodeId=board%3AAzureDevCommunityBlog)\n\n[onnx](/tag/onnx?nodeId=board%3AAzureDevCommunityBlog)\n\n[python](/tag/python?nodeId=board%3AAzureDevCommunityBlog)\n\n[vs code](/tag/vs%20code?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[Nandhini_Elango&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yNjExMjUxLVUyakhwRA?image-coordinates=1691%2C0%2C2601%2C910&amp;image-dimensions=50x50)](/users/nandhini_elango/2611251) [Nandhini_Elango](/users/nandhini_elango/2611251) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined August 06, 2024\n\n[View Profile](/users/nandhini_elango/2611251)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "Author": "Nandhini_Elango",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Description": "#### From “waiting” to “instant”- without sending data away\n\nAI is everywhere, but speed, privacy, and reliability are critical. Users expect instant answers without compromise. On-device AI makes that possible: fast, private and available, even when the network isn’t - empowering apps to deliver seamless experiences.\n\nImagine an intelligent assistant that works in seconds, without sending a text to the cloud. This approach brings *speed and data control* to the places that need it most; while still letting you tap into cloud power when it makes sense.\n\n#### Windows AI Foundry: A Local Home for Models\n\nWindows AI Foundry is a developer toolkit that makes it simple to run AI models directly on Windows devices. It uses *ONNX Runtime* under the hood and can leverage CPU, GPU (via DirectML), or NPU acceleration, without requiring you to manage those details.\n\nThe principle is straightforward:\n\n- Keep the model and the data on the same device.\n- Inference becomes faster, and data stays local by default unless you explicitly choose to use the cloud.\n\n#### Foundry Local\n\nFoundry Local is the engine that powers this experience. Think of it as *local AI runtime* - fast, private, and easy to integrate into an app.\n\n#### Why Adopt On‑Device AI?\n\n- **Faster, more responsive apps:** Local inference often reduces perceived latency and improves user experience.\n- **Privacy‑first by design:** Keep sensitive data on the device; avoid cloud round trips unless the user opts in.\n- **Offline capability:** An app can provide AI features even without a network connection.\n- **Cost control:** Reduce cloud compute and data costs for common, high‑volume tasks.\n\nThis approach is especially useful in regulated industries, field‑work tools, and any app where users expect quick, on‑device responses.\n\n#### Hybrid Pattern for Real Apps\n\nOn-device AI doesn’t replace the cloud, it complements it. Here’s how:\n\n- **Standalone On‑Device**: Quick, private actions like document summarization, local search, and offline assistants.\n- **Cloud‑Enhanced (Optional)**: Large-context models, up-to-date knowledge, or heavy multimodal workloads.\n\nDesign an app to keep data local by default and surface cloud options transparently with user consent and clear disclosures.\n\nWindows AI Foundry supports hybrid workflows:\n\n- Use *Foundry Local* for real-time inference.\n- Sync with *Azure AI services* for model updates, telemetry, and advanced analytics.\n- Implement fallback strategies for resource-intensive scenarios.\n\n#### Application Workflow\n\n![]()Application Workflow: On‑Device AI with Windows AI Foundry and Cloud Integration as Hybrid Path\n\n#### Code Example\n\n1. Only On-Device: Tries Foundry Local first, falls back to ONNX\n\n- if foundry\\_runtime.check\\_foundry\\_available():\n# Use on-device Foundry Local models\ntry: answer = foundry\\_runtime.run\\_inference(question, context) return answer, source=\"Foundry Local (On-Device)\" except Exception as e: logger.warning(f\"Foundry failed: {e}, trying ONNX...\")\n\nif onnx\\_model.is\\_loaded():\n# Fallback to local BERT ONNX model\ntry: answer = bert\\_model.get\\_answer(question, context) return answer, source=\"BERT ONNX (On-Device)\" except Exception as e: logger.warning(f\"ONNX failed: {e}\")\n\nreturn \"Error: No local AI available\"\n\n2. Hybrid approach: On-device first, cloud as last resort\n- def get\\_answer(question, context):\n\"\"\" Priority order:\n1. Foundry Local (best: advanced + private)\n2. ONNX Runtime (good: fast + private)\n3. Cloud API (fallback: requires internet, less private)\n# in case of Hybrid approach, based on real-time scenario\n\"\"\"\n\nif foundry\\_runtime.check\\_foundry\\_available():\n# Use on-device Foundry Local models\ntry: answer = foundry\\_runtime.run\\_inference(question, context) return answer, source=\"Foundry Local (On-Device)\" except Exception as e: logger.warning(f\"Foundry failed: {e}, trying ONNX...\")\n\nif onnx\\_model.is\\_loaded():\n# Fallback to local BERT ONNX model\ntry: answer = bert\\_model.get\\_answer(question, context) return answer, source=\"BERT ONNX (On-Device)\" except Exception as e: logger.warning(f\"ONNX failed: {e}, trying cloud...\")\n\n# Last resort: Cloud API (requires internet)\nif network\\_available(): try: import requests response = requests.post( '{BASE\\_URL\\_AI\\_CHAT\\_COMPLETION}', headers={'Authorization': f'Bearer {API\\_KEY}'}, json={ 'model': '{MODEL\\_NAME}', 'messages': [{ 'role': 'user', 'content': f'Context: {context}\\n\\nQuestion: {question}' }] }, timeout=10 ) answer = response.json()['choices'][0]['message']['content'] return answer, source=\"Cloud API (Online)\" except Exception as e: return \"Error: No AI runtime available\", source=\"Failed\" else: return \"Error: No internet and no local AI available\", source=\"Offline\"\n\n#### Demo Project Output: Foundry Local answering context-based questions offline\n\n![]()**Answer found in the context**: The Foundry Local engine ran the Phi-4-mini model offline and retrieved context-based data.\n\n![]()**No answer found in the context**: The Foundry Local engine ran the Phi-4-mini model offline and mentioned that there is no answer.\n\n#### Practical Use Cases\n\n- **Privacy-First Reading Assistant**: Summarize documents locally without sending text to the cloud.\n- **Healthcare Apps**: Analyze medical data on-device for compliance.\n- **Financial Tools**: Risk scoring without exposing sensitive financial data.\n- **IoT & Edge Devices**: Real-time anomaly detection without network dependency.\n\n#### Conclusion\n\nOn-device AI isn’t just a trend - it’s a shift toward smarter, faster, and more secure applications. With Windows AI Foundry and Foundry Local, developers can deliver experiences that respect user specific data, reduce latency, and work even when connectivity fails. By combining local inference with optional cloud enhancements, you get the best of both worlds: instant performance and scalable intelligence.\n\nWhether you’re creating document summarizers, offline assistants, or compliance-ready solutions, this approach ensures your apps stay responsive, reliable, and user-centric.\n\n#### References\n\n- [Get started with Foundry Local - Foundry Local | Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started)\n- [What is Windows AI Foundry? | Microsoft Learn](https://learn.microsoft.com/en-us/windows/ai/overview)\n- [https://devblogs.microsoft.com/foundry/unlock-instant-on-device-ai-with-foundry-local/](https://devblogs.microsoft.com/foundry/unlock-instant-on-device-ai-with-foundry-local/)"
}
