{
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "OutputDir": "_community",
  "Tags": [],
  "Author": "CormacGarvey",
  "ProcessedDate": "2025-08-26 23:11:44",
  "EnhancedContent": "# Introduction\n\nFollowing our previous evaluation of [Llama 3.1 8B inference performance on Azure’s ND-H100-v5 infrastructure using vLLM,](https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/performance-of-llama-3-1-8b-ai-inference-using-vllm-on-nd-h100-v5/4448355) this report broadens the scope to compare inference performance across a range of GPU and CPU platforms. Using the Hugging Face inference benchmarker, we assess not only throughput and latency but also the **cost-efficiency** of each configuration—an increasingly critical factor for enterprise deployment.\n\nAs organizations seek scalable and budget-conscious solutions for deploying large language models (LLMs), understanding the trade-offs between compute-bound and memory-bound stages of inference becomes essential. Smaller models like Llama 3.1 8B offer a compelling balance between capability and resource demand, but the underlying hardware and software stack can dramatically influence both performance and operational cost.\n\nThis report presents a comparative analysis of inference performance across multiple hardware platforms, factoring in:\n\n- **Token throughput and latency** across chat, classification, and code generation workloads.\n- **Resource utilization**, including KV cache utilization and efficiency.\n- **Cost per token**, derived from cloud pricing models and hardware utilization metrics.\n\nBy combining performance metrics with cost analysis, we aim to identify the most effective deployment strategies for enterprise-grade LLMs, whether optimizing for speed, scalability, or budget.\n\n# Benchmark environment\n\n## Inference benchmark\n\nThe Hugging face Inference benchmarking code was used for the AI Inference benchmark. Three different popular AI inference profiles were examined.\n\n- **Chat**: Probably the most common use case, question and answer format on a wide range of topics.\n- **Classification**: Providing various documents and requesting a summary of its contents.\n- **Code generation**: Providing code and requesting code generation, e.g. create a new function.\n\n| **Profile** | **Data set** | **Input prompt** | **Output prompt** | | --- | --- | --- | --- | | **Chat** | hlarcher/inference-benchmarker/share\\_gpt\\_turns.json | N/A | min=50, max=800, variance=100 | | **Classification** | hlarcher/inference-benchmarker/classification.json | Min=8000, max=12000, variance=5000 | Min=30, max=80, variance=10 | | **Code generation** | hlarcher/inference-benchmarker/github\\_code.json | Min=3000, max=6000, variance=1000 | Min=30, max=80, variance=10 |\n\n| **Huggingface Lama 3.1 8B models used** | **Precision** | **Model Size (GiB)** | | --- | --- | --- | | **meta-llama/Llama-3.1-8B-Instruct** | FP16 | 14.9 |\n\n| **vLLM parameters** | **Default value** | | --- | --- | | **gpu\\_memory\\_utilization** | 0.9 | | **max\\_num\\_seqs** | 1024 | | **max\\_num\\_batched\\_tokens** | 2048 (A100), 8192 (H100,H200) | | **enable\\_chunked\\_prefill** | True | | **enable\\_prefix\\_caching** | True |\n\n## VM Configuration\n\n### GPU\n\nND-H100-v5, ND-H200-v5, HD-A100-v4 (8 H100 80GB  &40GB) running HPC Ubuntu 22.04 (Pytorch 2.7.0+cu128, GPU driver: 535.161.08 and NCCL 2.21.5-1). 1 GPU was used in benchmark tests.\n\n### CPU\n\nUbuntu 22.02 (HPC and Canonical/jammy)\n\n# Results\n\n| **GPU** | **Profile** | **Avg prompt throughput** | **Avg generation throughput** | **Max # Requests waiting** | **Max KV Cache usage %** | **Avg KV Cache hit rate %** | | --- | --- | --- | --- | --- | --- | --- | | **H100** | Chat | ~2667 | ~6067 | 0 | ~14% | ~75% | | Classification | ~254149 | ~1291 | 0 | ~46% | ~98% | | Code generation | ~22269 | ~266 | ~111 | ~93% | ~1% | | **H200** | Chat | ~3271 | ~7464 | 0 | ~2% | ~77% | | Classification | ~337301 | ~1635 | 0 | ~24% | ~99% | | Code generation | ~22726 | ~274 | ~57 | ~46% | ~1% | | **A100** | Chat | ~1177 | ~2622 | 0 | ~2% | ~75% | | Classification | ~64526 | ~333 | 0 | ~45% | ~97% | | Code generation | ~7926 | ~95 | ~106 | ~21% | ~1% | | **A100\\_40G** | Chat | ~1069 | ~2459 | 0 | ~27% | ~75% | | Classification | ~7846 | ~39 | ~116 | ~68% | ~5% | | Code generation | ~7836 | ~94 | ~123 | ~66% | ~1% |\n\n## Cost analysis\n\nCost analysis used pay-as-you-go pricing for the south-central region and measured throughput in tokens per second to calculate the metric $/(1K tokens).\n\n## CPU performance and takeaways\n\nThe Huggingface AI-MO/aimo-validation-aime data was by vllm bench to test the performance of Llama 3.1 8B on various VM types (left graph below). It is a struggle (insufficient FLOPs and memory bandwidth) to run Llama 3.1 8B on CPU VM’s, even the best performing CPU VM (HB176-96\\_v4) throughput and latency is significantly slower than the A100\\_40GB GPU.\n\n### Tips\n\n- Enable/use AVX512 (avx512f, avx512\\_bf16, avx512\\_vnni etc) (See what is supported/available via lscpu)\n- Put AI model on single socket (if it has sufficient memory). For larger models you can use tensor parallel to split the model across sockets.\n- Use pinning to specify which cores the threads will run on (in vLLM, VLLM\\_CPU\\_OMP\\_THREADS\\_BIND=0-22)\n- Specify large enough KVCache (on CPU memory). In vLLM, VLLM\\_CPU\\_KVCACHE\\_SPACE=100)\n\n# Analysis\n\n## Throughput & Latency\n\n- **H200 outperforms all other GPUs** across all workloads, with the highest prompt and generation throughput.\n- **H100** is a close second, showing strong performance especially in classification and code generation.\n- **A100 and A100\\_40G** lag significantly behind, particularly in classification tasks where throughput drops by an order of magnitude (on A100\\_40G, due to smaller GPU memory and lower KV Cache hit percentage).\n\n## KV Cache Utilization\n\n- **H200 and H100** show efficient cache usage with high hit rates (up to 99%) and low waiting requests. (The exception is code generation which has low hit rates (~1%))\n- **A100\\_40G** suffers from high KV cache usage and low hit rates, especially in classification and code generation, indicating memory bottlenecks. The strain on the inference server is observed by the higher number of waiting requests.\n\n## Cost Efficiency\n\n- **Chat profiles:** The A100 GPU (40G) offers the best value.\n- **Classification profiles:** The H200 is most cost-effective.\n- **Code-generation profiles:** The H100 provides the greatest cost efficiency.\n\n## CPU vs GPU\n\n- Llama 3.1 3B can run on CPU VM’s but the throughput and latency are so poor compared to GPU’s if does not make an practical or financial sense to do so.\n- Smaller AI models (&lt;= 1B parameters) may be OK on CPU’s for some light weight inference serves (like Chat).\n\n# Conclusion\n\nThe benchmarking results clearly demonstrate that hardware choice significantly impacts the inference performance and cost-efficiency of Llama 3.1 8B deployments. The H200 GPU consistently delivers the highest throughput and cache efficiency across workloads, making it the top performer overall. H100 follows closely, especially excelling in code generation tasks. While A100 and A100\\_40G offer budget-friendly options for chat workloads, their limitations in memory and cache performance make them less suitable for more demanding tasks. CPU virtual machines do not offer adequate performance—in terms of throughput and latency—for running AI models comparable in size to Llama 3.1 8B. These insights provide a practical foundation for selecting optimal infrastructure based on inference workload type and cost constraints.\n\n# References\n\n1. Hugging Face Inference Benchmarker\nhttps://github.com/huggingface/inference-benchmarker\n2. Datasets used for benchmarking:\n- Chat: hlarcher/inference-benchmarker/share\\_gpt\\_turns.json\n- Classification: hlarcher/inference-benchmarker/classification.json\n- Code Generation: hlarcher/inference-benchmarker/github\\_code.json\n3. Model:\n- meta-llama/Llama-3.1-8B-Instruct on Hugging Face\nhttps://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n4. vLLM Inference Engine\nhttps://github.com/vllm-project/vllm\n5. Azure ND-Series GPU Infrastructure\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/nd-series\n6. PyTorch 2.7.0 + CUDA 12.8\nhttps://pytorch.org\n7. NVIDIA GPU Drivers and NCCL\n- Driver: 535.161.08\n- NCCL: 2.21.5-1\nhttps://developer.nvidia.com/nccl\n8. Azure Pricing Calculator (South-Central US Region)\n[https://azure.microsoft.com/en-us/pricing/calculator](https://azure.microsoft.com/en-us/pricing/calculator)\n9. [CPU - vLLM](https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html)\n\n# Appendix\n\n## Install vLLM on CPU VM’s\n\ngit clone https://github.com/vllm-project/vllm.git vllm\\_source\n\ncd vllm\\_source\n\nedit Dockerfiles (vllm\\_source/docker/Dockerfile.cpu)\n\ncp Dockerfile.cpu Dockerfile\\_serve.cpu\n\nchange last line to “ENTRYPOINT [\"/opt/venv/bin/vllm\",\"serve\"]”\n\ncp Dockerfile.cpu Dockerfile\\_bench.cpu\n\nchange last line to “ENTRYPOINT [\"/opt/venv/bin/vllm\",\"bench\",\"serve\"]”\n\nBuild images (enable AVX512 supported features (see lscpu))\n\ndocker build -f docker/Dockerfile\\_serve.cpu --build-arg VLLM\\_CPU\\_AVX512BF16=true --build-arg VLLM\\_CPU\\_AVX512VNNI=true --build-arg VLLM\\_CPU\\_DISABLE\\_AVX512=false --tag vllm-serve-cpu-env --target vllm-openai .\n\ndocker build -f docker/Dockerfile\\_bench.cpu --build-arg VLLM\\_CPU\\_AVX512BF16=true --build-arg VLLM\\_CPU\\_AVX512VNNI=true --build-arg VLLM\\_CPU\\_DISABLE\\_AVX512=false --tag vllm-bench-cpu-env --target vllm-openai .\n\nStart vllm server\n\nRemember to set &lt;YOUR HF TOKEN&gt; and &lt;CPU CORE RANGE&gt;\n\ndocker run --rm --privileged=true --shm-size=8g -p 8000:8000 -e VLLM\\_CPU\\_KVCACHE\\_SPACE=&lt;SIZE in GiB&gt; -e VLLM\\_CPU\\_OMP\\_THREADS\\_BIND=&lt;CPU CORE RANGE&gt; -e HF\\_TOKEN=&lt;YOUR HF TOKEN&gt; -e LD\\_PRELOAD=\"/usr/lib/x86\\_64-linux-gnu/libtcmalloc\\_minimal.so.4:$LD\\_PRELOAD\" vllm-serve-cpu-env meta-llama/Llama-3.1-8B-Instruct --port 8000 --dtype=bfloat16\n\nRun vLLM benchmark\n\nRemember to set &lt;YOUR HF TOKEN&gt;\n\ndocker run --rm   --privileged=true     --shm-size=4g    -e HF\\_TOKEN=&lt;YOUR HF TOKEN&gt;   -e LD\\_PRELOAD=\"/usr/lib/x86\\_64-linux-gnu/libtcmalloc\\_minimal.so.4:$LD\\_PRELOAD\"      vllm-bench-cpu-env  --backend vllm    --model=meta-llama/Llama-3.1-8B-Instruct --endpoint /v1/completions --dataset-name hf --dataset-path AI-MO/aimo-validation-aime --ramp-up-strategy linear --ramp-up-start-rps 1 --ramp-up-end-rps 2 --num-prompts 200 --seed 42 --host 10.0.0.4\n\nUpdated Aug 26, 2025\n\nVersion 1.0\n\n[ai infrastructure](/tag/ai%20infrastructure?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[benchmarking](/tag/benchmarking?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[hpc](/tag/hpc?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[virtual machines](/tag/virtual%20machines?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[!\\[CormacGarvey&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-8.svg?image-dimensions=50x50)](/users/cormacgarvey/364170) [CormacGarvey](/users/cormacgarvey/364170) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined June 20, 2019\n\n[View Profile](/users/cormacgarvey/364170)\n\n/category/azure/blog/azurehighperformancecomputingblog [Azure High Performance Computing (HPC) Blog](/category/azure/blog/azurehighperformancecomputingblog) Follow this blog board to get notified when there's new activity",
  "Title": "Inference performance of Llama 3.1 8B using vLLM across various GPUs and CPUs",
  "FeedName": "Microsoft Tech Community",
  "PubDate": "2025-08-26T22:22:59+00:00",
  "Link": "https://techcommunity.microsoft.com/t5/azure-high-performance-computing/inference-performance-of-llama-3-1-8b-using-vllm-across-various/ba-p/4448420",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Description": "# Introduction\n\nFollowing our previous evaluation of [Llama 3.1 8B inference performance on Azure’s ND-H100-v5 infrastructure using vLLM,](https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/performance-of-llama-3-1-8b-ai-inference-using-vllm-on-nd-h100-v5/4448355) this report broadens the scope to compare inference performance across a range of GPU and CPU platforms. Using the Hugging Face inference benchmarker, we assess not only throughput and latency but also the **cost-efficiency** of each configuration—an increasingly critical factor for enterprise deployment.\n\nAs organizations seek scalable and budget-conscious solutions for deploying large language models (LLMs), understanding the trade-offs between compute-bound and memory-bound stages of inference becomes essential. Smaller models like Llama 3.1 8B offer a compelling balance between capability and resource demand, but the underlying hardware and software stack can dramatically influence both performance and operational cost.\n\nThis report presents a comparative analysis of inference performance across multiple hardware platforms, factoring in:\n\n- **Token throughput and latency** across chat, classification, and code generation workloads.\n- **Resource utilization**, including KV cache utilization and efficiency.\n- **Cost per token**, derived from cloud pricing models and hardware utilization metrics.\n\nBy combining performance metrics with cost analysis, we aim to identify the most effective deployment strategies for enterprise-grade LLMs, whether optimizing for speed, scalability, or budget.\n\n# Benchmark environment\n\n## Inference benchmark\n\nThe Hugging face Inference benchmarking code was used for the AI Inference benchmark. Three different popular AI inference profiles were examined.\n\n- **Chat**: Probably the most common use case, question and answer format on a wide range of topics.\n- **Classification**: Providing various documents and requesting a summary of its contents.\n- **Code generation**: Providing code and requesting code generation, e.g. create a new function.\n\n| **Profile** | **Data set** | **Input prompt** | **Output prompt** | | --- | --- | --- | --- | | **Chat** | hlarcher/inference-benchmarker/share\\_gpt\\_turns.json | N/A | min=50, max=800, variance=100 | | **Classification** | hlarcher/inference-benchmarker/classification.json | Min=8000, max=12000, variance=5000 | Min=30, max=80, variance=10 | | **Code generation** | hlarcher/inference-benchmarker/github\\_code.json | Min=3000, max=6000, variance=1000 | Min=30, max=80, variance=10 |\n\n| **Huggingface Lama 3.1 8B models used** | **Precision** | **Model Size (GiB)** | | --- | --- | --- | | **meta-llama/Llama-3.1-8B-Instruct** | FP16 | 14.9 |\n\n| **vLLM parameters** | **Default value** | | --- | --- | | **gpu\\_memory\\_utilization** | 0.9 | | **max\\_num\\_seqs** | 1024 | | **max\\_num\\_batched\\_tokens** | 2048 (A100), 8192 (H100,H200) | | **enable\\_chunked\\_prefill** | True | | **enable\\_prefix\\_caching** | True |\n\n## VM Configuration\n\n### GPU\n\nND-H100-v5, ND-H200-v5, HD-A100-v4 (8 H100 80GB &40GB) running HPC Ubuntu 22.04 (Pytorch 2.7.0+cu128, GPU driver: 535.161.08 and NCCL 2.21.5-1). 1 GPU was used in benchmark tests.\n\n### CPU\n\nUbuntu 22.02 (HPC and Canonical/jammy)\n\n# Results\n\n![]()\n\n| **GPU** | **Profile** | **Avg prompt throughput** | **Avg generation throughput** | **Max # Requests waiting** | **Max KV Cache usage %** | **Avg KV Cache hit rate %** | | --- | --- | --- | --- | --- | --- | --- | | **H100** | Chat | ~2667 | ~6067 | 0 | ~14% | ~75% | | Classification | ~254149 | ~1291 | 0 | ~46% | ~98% | | Code generation | ~22269 | ~266 | ~111 | ~93% | ~1% | | **H200** | Chat | ~3271 | ~7464 | 0 | ~2% | ~77% | | Classification | ~337301 | ~1635 | 0 | ~24% | ~99% | | Code generation | ~22726 | ~274 | ~57 | ~46% | ~1% | | **A100** | Chat | ~1177 | ~2622 | 0 | ~2% | ~75% | | Classification | ~64526 | ~333 | 0 | ~45% | ~97% | | Code generation | ~7926 | ~95 | ~106 | ~21% | ~1% | | **A100\\_40G** | Chat | ~1069 | ~2459 | 0 | ~27% | ~75% | | Classification | ~7846 | ~39 | ~116 | ~68% | ~5% | | Code generation | ~7836 | ~94 | ~123 | ~66% | ~1% |\n\n![]()\n\n## Cost analysis\n\nCost analysis used pay-as-you-go pricing for the south-central region and measured throughput in tokens per second to calculate the metric $/(1K tokens).\n\n![]()\n\n## CPU performance and takeaways\n\nThe Huggingface AI-MO/aimo-validation-aime data was by vllm bench to test the performance of Llama 3.1 8B on various VM types (left graph below). It is a struggle (insufficient FLOPs and memory bandwidth) to run Llama 3.1 8B on CPU VM’s, even the best performing CPU VM (HB176-96\\_v4) throughput and latency is significantly slower than the A100\\_40GB GPU.\n\n![]()\n\n### Tips\n\n- Enable/use AVX512 (avx512f, avx512\\_bf16, avx512\\_vnni etc) (See what is supported/available via lscpu)\n- Put AI model on single socket (if it has sufficient memory). For larger models you can use tensor parallel to split the model across sockets.\n- Use pinning to specify which cores the threads will run on (in vLLM, VLLM\\_CPU\\_OMP\\_THREADS\\_BIND=0-22)\n- Specify large enough KVCache (on CPU memory). In vLLM, VLLM\\_CPU\\_KVCACHE\\_SPACE=100)\n\n# Analysis\n\n## Throughput & Latency\n\n- **H200 outperforms all other GPUs** across all workloads, with the highest prompt and generation throughput.\n- **H100** is a close second, showing strong performance especially in classification and code generation.\n- **A100 and A100\\_40G** lag significantly behind, particularly in classification tasks where throughput drops by an order of magnitude (on A100\\_40G, due to smaller GPU memory and lower KV Cache hit percentage).\n\n## KV Cache Utilization\n\n- **H200 and H100** show efficient cache usage with high hit rates (up to 99%) and low waiting requests. (The exception is code generation which has low hit rates (~1%))\n- **A100\\_40G** suffers from high KV cache usage and low hit rates, especially in classification and code generation, indicating memory bottlenecks. The strain on the inference server is observed by the higher number of waiting requests.\n\n## Cost Efficiency\n\n- **Chat profiles:** The A100 GPU (40G) offers the best value.\n- **Classification profiles:** The H200 is most cost-effective.\n- **Code-generation profiles:** The H100 provides the greatest cost efficiency.\n\n## CPU vs GPU\n\n- Llama 3.1 3B can run on CPU VM’s but the throughput and latency are so poor compared to GPU’s if does not make an practical or financial sense to do so.\n- Smaller AI models (\n\n# Conclusion\n\nThe benchmarking results clearly demonstrate that hardware choice significantly impacts the inference performance and cost-efficiency of Llama 3.1 8B deployments. The H200 GPU consistently delivers the highest throughput and cache efficiency across workloads, making it the top performer overall. H100 follows closely, especially excelling in code generation tasks. While A100 and A100\\_40G offer budget-friendly options for chat workloads, their limitations in memory and cache performance make them less suitable for more demanding tasks. CPU virtual machines do not offer adequate performance—in terms of throughput and latency—for running AI models comparable in size to Llama 3.1 8B. These insights provide a practical foundation for selecting optimal infrastructure based on inference workload type and cost constraints.\n\n# References\n\n1. Hugging Face Inference Benchmarker\nhttps://github.com/huggingface/inference-benchmarker\n2. Datasets used for benchmarking:\n- Chat: hlarcher/inference-benchmarker/share\\_gpt\\_turns.json\n- Classification: hlarcher/inference-benchmarker/classification.json\n- Code Generation: hlarcher/inference-benchmarker/github\\_code.json\n3. Model:\n- meta-llama/Llama-3.1-8B-Instruct on Hugging Face\nhttps://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n4. vLLM Inference Engine\nhttps://github.com/vllm-project/vllm\n5. Azure ND-Series GPU Infrastructure\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/nd-series\n6. PyTorch 2.7.0 + CUDA 12.8\nhttps://pytorch.org\n7. NVIDIA GPU Drivers and NCCL\n- Driver: 535.161.08\n- NCCL: 2.21.5-1\nhttps://developer.nvidia.com/nccl\n8. Azure Pricing Calculator (South-Central US Region)\n[https://azure.microsoft.com/en-us/pricing/calculator](https://azure.microsoft.com/en-us/pricing/calculator)\n9. [CPU - vLLM](https://docs.vllm.ai/en/latest/getting_started/installation/cpu.html)\n\n# Appendix\n\n## Install vLLM on CPU VM’s\n\ngit clone https://github.com/vllm-project/vllm.git vllm\\_source\n\ncd vllm\\_source\n\nedit Dockerfiles (vllm\\_source/docker/Dockerfile.cpu)\n\ncp Dockerfile.cpu Dockerfile\\_serve.cpu\n\nchange last line to “ENTRYPOINT [\"/opt/venv/bin/vllm\",\"serve\"]”\n\ncp Dockerfile.cpu Dockerfile\\_bench.cpu\n\nchange last line to “ENTRYPOINT [\"/opt/venv/bin/vllm\",\"bench\",\"serve\"]”\n\nBuild images (enable AVX512 supported features (see lscpu))\n\ndocker build -f docker/Dockerfile\\_serve.cpu --build-arg VLLM\\_CPU\\_AVX512BF16=true --build-arg VLLM\\_CPU\\_AVX512VNNI=true --build-arg VLLM\\_CPU\\_DISABLE\\_AVX512=false --tag vllm-serve-cpu-env --target vllm-openai .\n\ndocker build -f docker/Dockerfile\\_bench.cpu --build-arg VLLM\\_CPU\\_AVX512BF16=true --build-arg VLLM\\_CPU\\_AVX512VNNI=true --build-arg VLLM\\_CPU\\_DISABLE\\_AVX512=false --tag vllm-bench-cpu-env --target vllm-openai .\n\nStart vllm server\n\nRemember to set and\n\ndocker run --rm --privileged=true --shm-size=8g -p 8000:8000 -e VLLM\\_CPU\\_KVCACHE\\_SPACE= -e VLLM\\_CPU\\_OMP\\_THREADS\\_BIND= -e HF\\_TOKEN= -e LD\\_PRELOAD=\"/usr/lib/x86\\_64-linux-gnu/libtcmalloc\\_minimal.so.4:$LD\\_PRELOAD\" vllm-serve-cpu-env meta-llama/Llama-3.1-8B-Instruct --port 8000 --dtype=bfloat16\n\nRun vLLM benchmark\n\nRemember to set\n\ndocker run --rm --privileged=true --shm-size=4g -e HF\\_TOKEN= -e LD\\_PRELOAD=\"/usr/lib/x86\\_64-linux-gnu/libtcmalloc\\_minimal.so.4:$LD\\_PRELOAD\" vllm-bench-cpu-env --backend vllm --model=meta-llama/Llama-3.1-8B-Instruct --endpoint /v1/completions --dataset-name hf --dataset-path AI-MO/aimo-validation-aime --ramp-up-strategy linear --ramp-up-start-rps 1 --ramp-up-end-rps 2 --num-prompts 200 --seed 42 --host 10.0.0.4"
}
