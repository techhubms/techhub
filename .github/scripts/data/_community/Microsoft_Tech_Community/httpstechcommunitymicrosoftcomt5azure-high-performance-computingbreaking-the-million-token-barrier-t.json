{
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Title": "Breaking the Million-Token Barrier: The Technical Achievement of Azure ND GB300 v6",
  "PubDate": "2025-11-03T17:00:00+00:00",
  "Link": "https://techcommunity.microsoft.com/t5/azure-high-performance-computing/breaking-the-million-token-barrier-the-technical-achievement-of/ba-p/4466080",
  "Tags": [],
  "ProcessedDate": "2025-11-03 17:04:23",
  "Author": "HugoAffaticati",
  "EnhancedContent": "## Azure ND GB300 v6 Virtual Machines with NVIDIA GB300 NVL72 rack-scale systems achieve unprecedented performance of 1,100,000 tokens/s on Llama2 70B Inference, beating the previous Azure ND GB200 v6 record of 865,000 tokens/s by 27%.\n\nBy Mark Gitau (Software Engineer) and Hugo Affaticati (Senior Cloud Infrastructure Engineer)\n\nThe new Azure ND GB300 v6 virtual machines, built on the cutting-edge [NVIDIA Blackwell](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)[architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/) introduced with the [ND GB200 v6](https://techcommunity.microsoft.com/blog/AzureHighPerformanceComputingBlog/unpacking-the-performance-of-microsoft-azure-nd-gb200-v6-virtual-machines/4390442), are more optimized for inference workloads with 50% more GPU memory and 16% higher TDP (Thermal Design Power).\n\nTo simulate the performance gains of the ND GB300 v6 virtual machines on customer workloads, we ran the Llama2 70B model from MLPerf Inference v5.1 on each of the 18 ND GB300 v6 virtual machines on one NVIDIA GB300 **** NVL72 domain. The Llama2 70B model is a widely adopted industry standard for large-scale AI deployments, making it a good representation of production inference workloads. **One NVL72 rack of Azure ND GB300 v6 achieved an aggregated 1,100,000 tokens/s** for an unverified MLPerf Inference v5.1 submission [1], detailed in Table 1 and [observed by Signal65](http://%20https://signal65.com/research/ai/azure-b300-performance/). This is a new record in AI inference, beating our own previous [record](https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/azure%E2%80%99s-nd-gb200-v6-delivers-record-performance-for-inference-workloads/4399253) of **865,000 tokens/s on one NVIDIA GB200 NVL72 rack with the ND GB200 v6 VMs**.\n\n| **Metric** | **Performance (Tokens/Second)** | | --- | --- | | Total Aggregated Throughput | 1,100,948.3 | | Maximum Single-Node Throughput | 62,803.9 | | Minimum Single-Node Throughput | 57,599.1 | | Average Single-Node Throughput | 61,163.8 | | Median Single-Node Throughput | 61,759.1 |\n\n*Table 1: Data compiled from 18 parallel test runs, observed by Signal65.*\n\nThis translates to 15,200 tokens/sec per [NVIDIA](https://www.nvidia.com/en-us/data-center/gb300-nvl72/)[Blackwell](https://www.nvidia.com/en-us/data-center/gb300-nvl72/)[Ultra](https://www.nvidia.com/en-us/data-center/gb300-nvl72/)[GPU](https://www.nvidia.com/en-us/data-center/gb300-nvl72/) (+/- 5%), a 27% performance speed up over the 12,022 tokens/s per NVIDIA Blackwell GPU. For comparison, the previous MLPerf Inference v4.1 results show that the [NVIDIA DGX H100 system](https://www.nvidia.com/en-gb/data-center/dgx-h100/) processed 24,525 tokens per second across 8 GPUs, or 3,066 tokens per second per [NVIDIA H100 GPU](https://www.nvidia.com/en-us/data-center/h100/) [2]. This means that Azure ND GB300 v6 VMs deliver 5× higher throughput per GPU than the previous-generation [ND H100 v5 virtual machines](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/ndh100v5-series?tabs=sizebasic).\n\nThis milestone was observed by the [third party Signal65](https://signal65.com/). They concluded that this record \"of over 1.1 million tokens per second on a single Azure rack is more than a benchmark record; it is a definitive proof point that the performance required for large-scale, transformative AI is now available as a reliable, efficient, and resilient utility\".\n\nAzure ND GB300 v6 virtual machine’s configurations (Table 2) benefit from performance gains across key hardware components (such as GEMM efficiency, high-bandwidth memory (HBM) throughput, NVLink connectivity, and NCCL communication). Our [benchmarks](https://github.com/Azure/AI-benchmarking-guide/tree/main/Azure_Results)****show that ND GB300 v6 achieves **2.5x more GEMM TFLOPS** per GPU than the ND H100 v5. Additionally, we measured **7.37 TB/s HBM bandwidth**(92% efficiency)****and**4x faster CPU-to-GPU**transfer speeds thanks to NVLink C2C.\n\n| **Component** | **Specification** | | --- | --- | | **Cloud Platform** | Microsoft Azure | | **VM Instance SKU** | ND\\_GB300\\_v6 | | **System Configuration** | 18 x NDv6 VM instances in a single NVL72 rack | | **GPU** | 4 x NVIDIA GB300 per VM (72 total) | | **GPU Memory** | 189,471 MiB per GPU | | **GPU Power Limit** | 1,400 Watts | | **Storage** | 14 TB Local NVMe RAID per VM | | **LLM Inference Engine** | NVIDIA TensorRT-LLM | | **Benchmark Harness** | MLCommons MLPerf Inference v5.1 | | **Benchmark Scenario** | Offline | | **Model** | Llama2-70B | | **Precision** | FP4 |\n\n*Table 2: ND GB300 v6 Configuration for the MLPerf Inference test*\n\nThe model was run using FP4 precision, a form of quantization that significantly accelerates inference speed while maintaining high accuracy. This was implemented via [NVIDIA TensorRT-LLM library](https://developer.nvidia.com/tensorrt), a highly optimized, production-ready software stack for LLM inference.\n\nAzure is once again raising the bar for enterprise-scale AI Inference with the ND GB300 v6 virtual machines.\n\n#### How to replicate the results on a single virtual machine in Azure\n\n**Clone the repository and enter working directory:**\n\ngit clone [https://github.com/Azure/AI-benchmarking-guide.git](https://github.com/Azure/AI-benchmarking-guide.git)  && cd AI-benchmarking-guide/Azure\\_Results\n\n**Download the models & datasets**\n\n- create models, data, and preprocessed\\_data directories in the working directory\n- download the [Llama 2 70B model](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-model)inside the models directory.\n- download the [datasets](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-dataset)inside data directory\n- [prepare the datasets](https://github.com/mlcommons/inference_results_v5.1/tree/main/closed/Azure/code/llama2-70b/tensorrt#download-and-prepare-data)\n\n**Setup container**\n\n- Inside the working directory:\n\n``` mkdir build && cd build ```\n\n``` git clone https://github.com/NVIDIA/TensorRT-LLM.git TRTLLM ```\n\n``` cd TRTLLM ```\n\n- Edit TRTLLM/docker/Makefile lines 135 and 136:\n\n``` SOURCE_DIR ?= AI-benchmarking-guide/Azure_Results (make sure it is an absolute path to the working directory) ```\n\n``` CODE_DIR ?= /work ```\n\n- Build & launch the container:\n\n``` make -C docker build ```\n\n``` make -C docker run ```\n\n- Once inside the container, install TensorRT-LLM:\n\n``` cd 1M_ND_GB300_v6_Inference/build/TRTLLM ```\n\n``` python3 ./scripts/build_wheel.py --trt_root /usr/local/tensorrt --benchmarks --cuda_architectures \"103-real\" --no-venv --clean ```\n\n``` pip install build/tensorrt_llm-1.1.0rc6-cp312-cp312-linux_aarch64.whl ```\n\n- Inside the 1M\\_ND\\_GB300\\_v6\\_Inference directory, install MLPerf dependencies:\n\n``` make clone_loadgen && make build_loadgen ```\n\n``` git clone https://github.com/NVIDIA/mitten.git ./build/mitten && pip install build/mitten ```\n\n``` pip install -r docker/common/requirements/requirements.llm.txt ```\n\n**Setup and run benchmark**\n\n- Export env variables and link model & data directories:\n\n``` export MLPERF_SCRATCH_PATH=/work ```\n\n``` export SYSTEM_NAME=ND_GB300_v6 ```\n\n``` make link_dirs ```\n\n- Run offline benchmark (wait a few minutes after the run server command for the server to start):\n\n``` make run_llm_server RUN_ARGS=\"--core_type=trtllm_endpoint --benchmarks=llama2-70b --scenarios=Offline\" ```\n\n``` make run_harness RUN_ARGS=\"--core_type=trtllm_endpoint --benchmarks=llama2-70b --scenarios=Offline\" ```\n\nFind all 18 log files of our run [here](https://github.com/Azure/AI-benchmarking-guide/tree/main/Azure_Results/1M_ND_GB300_v6_Inference/results)\n\n[1] Unverified MLPerf® v5.1 Inference Closed Llama 2 70B offline. Result not verified by MLCommons Association. Unverified results have not been through an MLPerf review and may use measurement methodologies and/or workload implementations that are inconsistent with the MLPerf specification for verified results. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information. Results obtained using NVIDIA MLPerf v5.1 code with NVIDIA TensorRT-LLM 1.1.0rc1\n\n[2] Verified result with ID 4.1-0043.\n\nUpdated Oct 31, 2025\n\nVersion 1.0\n\n[ai infrastructure](/tag/ai%20infrastructure?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[benchmarking](/tag/benchmarking?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[virtual machines](/tag/virtual%20machines?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[!\\[HugoAffaticati&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-1.svg?image-dimensions=50x50)](/users/hugoaffaticati/1467620) [HugoAffaticati](/users/hugoaffaticati/1467620) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined July 26, 2022\n\n[View Profile](/users/hugoaffaticati/1467620)\n\n/category/azure/blog/azurehighperformancecomputingblog [Azure High Performance Computing (HPC) Blog](/category/azure/blog/azurehighperformancecomputingblog) Follow this blog board to get notified when there's new activity",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "OutputDir": "_community",
  "FeedName": "Microsoft Tech Community",
  "Description": "By Mark Gitau (Software Engineer) and Hugo Affaticati (Senior Cloud Infrastructure Engineer)\n\nThe new Azure ND GB300 v6 virtual machines, built on the cutting-edge [NVIDIA Blackwell](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)[architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/) introduced with the [ND GB200 v6](https://techcommunity.microsoft.com/blog/AzureHighPerformanceComputingBlog/unpacking-the-performance-of-microsoft-azure-nd-gb200-v6-virtual-machines/4390442), are more optimized for inference workloads with 50% more GPU memory and 16% higher TDP (Thermal Design Power).\n\nTo simulate the performance gains of the ND GB300 v6 virtual machines on customer workloads, we ran the Llama2 70B model from MLPerf Inference v5.1 on each of the 18 ND GB300 v6 virtual machines on one NVIDIA GB300 **** NVL72 domain. The Llama2 70B model is a widely adopted industry standard for large-scale AI deployments, making it a good representation of production inference workloads. **One NVL72 rack of Azure ND GB300 v6 achieved an aggregated 1,100,000 tokens/s** for an unverified MLPerf Inference v5.1 submission [1], detailed in Table 1 and [observed by Signal65](http://%20https://signal65.com/research/ai/azure-b300-performance/). This is a new record in AI inference, beating our own previous [record](https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/azure%E2%80%99s-nd-gb200-v6-delivers-record-performance-for-inference-workloads/4399253) of **865,000 tokens/s on one NVIDIA GB200 NVL72 rack with the ND GB200 v6 VMs**.\n\n| **Metric** | **Performance (Tokens/Second)** | | --- | --- | | Total Aggregated Throughput | 1,100,948.3 | | Maximum Single-Node Throughput | 62,803.9 | | Minimum Single-Node Throughput | 57,599.1 | | Average Single-Node Throughput | 61,163.8 | | Median Single-Node Throughput | 61,759.1 |\n\n*Table 1: Data compiled from 18 parallel test runs, observed by Signal65.*\n\nThis translates to 15,200 tokens/sec per [NVIDIA](https://www.nvidia.com/en-us/data-center/gb300-nvl72/)[Blackwell](https://www.nvidia.com/en-us/data-center/gb300-nvl72/)[Ultra](https://www.nvidia.com/en-us/data-center/gb300-nvl72/)[GPU](https://www.nvidia.com/en-us/data-center/gb300-nvl72/) (+/- 5%), a 27% performance speed up over the 12,022 tokens/s per NVIDIA Blackwell GPU. For comparison, the previous MLPerf Inference v4.1 results show that the [NVIDIA DGX H100 system](https://www.nvidia.com/en-gb/data-center/dgx-h100/) processed 24,525 tokens per second across 8 GPUs, or 3,066 tokens per second per [NVIDIA H100 GPU](https://www.nvidia.com/en-us/data-center/h100/) [2]. This means that Azure ND GB300 v6 VMs deliver 5× higher throughput per GPU than the previous-generation [ND H100 v5 virtual machines](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/ndh100v5-series?tabs=sizebasic).\n\nThis milestone was observed by the [third party Signal65](https://signal65.com/). They concluded that this record \"of over 1.1 million tokens per second on a single Azure rack is more than a benchmark record; it is a definitive proof point that the performance required for large-scale, transformative AI is now available as a reliable, efficient, and resilient utility\".\n\nAzure ND GB300 v6 virtual machine’s configurations (Table 2) benefit from performance gains across key hardware components (such as GEMM efficiency, high-bandwidth memory (HBM) throughput, NVLink connectivity, and NCCL communication). Our [benchmarks](https://github.com/Azure/AI-benchmarking-guide/tree/main/Azure_Results) **** show that ND GB300 v6 achieves **2.5x more GEMM TFLOPS** per GPU than the ND H100 v5. Additionally, we measured **7.37 TB/s HBM bandwidth** (92% efficiency) **** and **4x faster CPU-to-GPU** transfer speeds thanks to NVLink C2C.\n\n| **Component** | **Specification** | | --- | --- | | **Cloud Platform** | Microsoft Azure | | **VM Instance SKU** | ND\\_GB300\\_v6 | | **System Configuration** | 18 x NDv6 VM instances in a single NVL72 rack | | **GPU** | 4 x NVIDIA GB300 per VM (72 total) | | **GPU Memory** | 189,471 MiB per GPU | | **GPU Power Limit** | 1,400 Watts | | **Storage** | 14 TB Local NVMe RAID per VM | | **LLM Inference Engine** | NVIDIA TensorRT-LLM | | **Benchmark Harness** | MLCommons MLPerf Inference v5.1 | | **Benchmark Scenario** | Offline | | **Model** | Llama2-70B | | **Precision** | FP4 |\n\n*Table 2: ND GB300 v6 Configuration for the MLPerf Inference test*\n\nThe model was run using FP4 precision, a form of quantization that significantly accelerates inference speed while maintaining high accuracy. This was implemented via [NVIDIA TensorRT-LLM library](https://developer.nvidia.com/tensorrt), a highly optimized, production-ready software stack for LLM inference.\n\nAzure is once again raising the bar for enterprise-scale AI Inference with the ND GB300 v6 virtual machines.\n\n#### How to replicate the results on a single virtual machine in Azure\n\n**Clone the repository and enter working directory:**\n\ngit clone [https://github.com/Azure/AI-benchmarking-guide.git](https://github.com/Azure/AI-benchmarking-guide.git) && cd AI-benchmarking-guide/Azure\\_Results\n\n**Download the models & datasets**\n\n- create models, data, and preprocessed\\_data directories in the working directory\n- download the [Llama 2 70B model](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-model)inside the models directory.\n- download the [datasets](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-dataset)inside data directory\n- [prepare the datasets](https://github.com/mlcommons/inference_results_v5.1/tree/main/closed/Azure/code/llama2-70b/tensorrt#download-and-prepare-data)\n\n**Setup container**\n\n- Inside the working directory:\n\n``` mkdir build && cd build ```\n\n``` git clone https://github.com/NVIDIA/TensorRT-LLM.git TRTLLM ```\n\n``` cd TRTLLM ```\n\n- Edit TRTLLM/docker/Makefile lines 135 and 136:\n\n``` SOURCE_DIR ?= AI-benchmarking-guide/Azure_Results (make sure it is an absolute path to the working directory) ```\n\n``` CODE_DIR ?= /work ```\n\n- Build & launch the container:\n\n``` make -C docker build ```\n\n``` make -C docker run ```\n\n- Once inside the container, install TensorRT-LLM:\n\n``` cd 1M_ND_GB300_v6_Inference/build/TRTLLM ```\n\n``` python3 ./scripts/build_wheel.py --trt_root /usr/local/tensorrt --benchmarks --cuda_architectures \"103-real\" --no-venv --clean ```\n\n``` pip install build/tensorrt_llm-1.1.0rc6-cp312-cp312-linux_aarch64.whl ```\n\n- Inside the 1M\\_ND\\_GB300\\_v6\\_Inference directory, install MLPerf dependencies:\n\n``` make clone_loadgen && make build_loadgen ```\n\n``` git clone https://github.com/NVIDIA/mitten.git ./build/mitten && pip install build/mitten ```\n\n``` pip install -r docker/common/requirements/requirements.llm.txt ```\n\n**Setup and run benchmark**\n\n- Export env variables and link model & data directories:\n\n``` export MLPERF_SCRATCH_PATH=/work ```\n\n``` export SYSTEM_NAME=ND_GB300_v6 ```\n\n``` make link_dirs ```\n\n- Run offline benchmark (wait a few minutes after the run server command for the server to start):\n\n``` make run_llm_server RUN_ARGS=\"--core_type=trtllm_endpoint --benchmarks=llama2-70b --scenarios=Offline\" ```\n\n``` make run_harness RUN_ARGS=\"--core_type=trtllm_endpoint --benchmarks=llama2-70b --scenarios=Offline\" ```\n\nFind all 18 log files of our run [here](https://github.com/Azure/AI-benchmarking-guide/tree/main/Azure_Results/1M_ND_GB300_v6_Inference/results)\n\n[1] Unverified MLPerf® v5.1 Inference Closed Llama 2 70B offline. Result not verified by MLCommons Association. Unverified results have not been through an MLPerf review and may use measurement methodologies and/or workload implementations that are inconsistent with the MLPerf specification for verified results. The MLPerf name and logo are registered and unregistered trademarks of MLCommons Association in the United States and other countries. All rights reserved. Unauthorized use strictly prohibited. See www.mlcommons.org for more information. Results obtained using NVIDIA MLPerf v5.1 code with NVIDIA TensorRT-LLM 1.1.0rc1\n\n[2] Verified result with ID 4.1-0043."
}
