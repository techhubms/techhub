{
  "Title": "Build an AI Image-Caption Generator on Azure App Service with Streamlit and GPT-4o-mini",
  "ProcessedDate": "2025-09-02 10:13:51",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Link": "https://techcommunity.microsoft.com/t5/apps-on-azure-blog/build-an-ai-image-caption-generator-on-azure-app-service-with/ba-p/4450313",
  "EnhancedContent": "This tiny app just does one thing: **upload an image → get a natural one-line caption**. Under the hood:\n\n- **Azure AI Vision** extracts high-confidence tags from the image.\n- **Azure OpenAI (GPT-4o-mini)** turns those tags into a fluent caption.\n- **Streamlit** provides a lightweight, Python-native UI so you can ship fast.\n\nAll code + infra templates: **image\\_caption\\_app** in the App Service AI Samples repo: [https://github.com/Azure-Samples/appservice-ai-samples/tree/main/image_caption_app](https://github.com/Azure-Samples/appservice-ai-samples/tree/main/image_caption_app)\n\n#### What are these components?\n\n- **What is Streamlit?** An open-source Python framework to build interactive data/AI apps with just a few lines of code—perfect for quick, clean UIs.\n- **What is Azure AI Vision (Vision API)?** A cloud service that analyzes images and returns rich signals like **tags** with confidence scores, which we use as grounded inputs for captioning.\n\n#### How it works (at a glance)\n\n1. User uploads a photo in Streamlit.\n2. The app calls **Azure AI Vision** → gets a list of tags (keeps only high-confidence ones).\n3. The app sends those tags to **GPT-4o-mini** → generates a one-line caption.\n4. Caption is shown instantly in the browser.\n\n#### Prerequisites\n\n- **Azure subscription** — [https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account](https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account?utm_source=chatgpt.com)\n- **Azure CLI** — [https://learn.microsoft.com/azure/cli/azure/install-azure-cli-linux](https://learn.microsoft.com/azure/cli/azure/install-azure-cli-linux)\n- **Azure Developer CLI (azd)** — [https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd)\n- **Python 3.10+** — [https://www.python.org/downloads/](https://www.python.org/downloads/)\n- **Visual Studio Code** (optional) — [https://code.visualstudio.com/download](https://code.visualstudio.com/download)\n- **Streamlit** (optional for local runs) — [https://docs.streamlit.io/get-started/installation](https://docs.streamlit.io/get-started/installation)\n- **Managed Identity on App Service (recommended)** — [https://learn.microsoft.com/azure/app-service/overview-managed-identity](https://learn.microsoft.com/azure/app-service/overview-managed-identity)\n\n#### Resources you’ll deploy\n\nYou can create everything **manually** or **with the provided azd template**.\n\n**What you need**\n\n- **Azure App Service (Linux)** to host the Streamlit app.\n- **Azure AI Foundry/OpenAI** with a **gpt-4o-mini** deployment for caption generation.\n- **Azure AI Vision** (Computer Vision) for image tagging.\n- **Managed Identity** enabled on the Web App, with RBAC grants so the app can call Vision and OpenAI without secrets.\n\n**One-command deploy with azd (recommended)** The sample includes infra under image\\_caption\\_app/infra so azd up can provision + deploy in one go.\n\n```\n# 1) Clone and move into the sample\ngit clone https://github.com/Azure-Samples/appservice-ai-samples cd appservice-ai-samples/image_caption_app\n\n# 2) Log in and provision + deploy\nazd auth login azd up ```\n\n**Manual path (if you prefer doing it yourself)**\n\n1. Create **Azure AI Vision**, note the endpoint (custom subdomain).\n2. Create **Azure AI Foundry/OpenAI** and deploy **gpt-4o-mini**.\n3. Create **App Service (Linux, Python)** and enable **System-Assigned Managed Identity**.\n4. Assign roles to the Web App’s Managed Identity:\n- **Cognitive Services OpenAI User** on your OpenAI resource.\n- **Cognitive Services User** on your Vision resource.\n5. Add app settings for endpoints and deployment names (see repo), deploy the code, and run.\n\n**Startup command (manual setting):** If you’re configuring the Web App yourself (instead of using the Bicep), set the **Startup Command** to:\n\n``` streamlit run app.py --server.port 8000 --server.address 0.0.0.0 ```\n\nPortal path: **App Service → Configuration → General settings → Startup Command**. CLI example:\n\n``` az webapp config set \\ --name <your-webapp-name> \\ --resource-group <your-rg> \\ --startup-file \"streamlit run app.py --server.port 8000 --server.address 0.0.0.0\" ```\n\n(The provided Bicep template already sets this for you.)\n\n#### Code tour (the important bits)\n\n**Top-level flow (app.py)** First we get tags from Vision, then ask GPT-4o-mini for a one-liner:\n\n``` tags = extract_tags(image_bytes) caption = generate_caption(tags)\n\n```\n\n**Vision call (utils/vision.py)** Call the Vision REST API, parse JSON, and keep high-confidence tags (&gt; 0.6):\n\n``` response = requests.post( VISION_API_URL, headers=headers, params=PARAMS, data=image_bytes, timeout=30, ) response.raise_for_status() analysis = response.json()\n\ntags = [ t.get('name') for t in analysis.get('tags', []) if t.get('name') and t.get('confidence', 0) > 0.6 ]\n\n```\n\n**Caption generation (utils/openai\\_caption.py)** Join tags and ask GPT-4o-mini for a natural caption:\n\n``` tag_text = \", \".join(tags) prompt = f\"\"\" You are an assistant that generates vivid, natural-sounding captions for images. Create a one-line caption for an image that contains the following: {tag_text}. \"\"\"\n\nresponse = client.chat.completions.create( model=DEPLOYMENT_NAME, messages=[ {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, {\"role\": \"user\", \"content\": prompt.strip()} ], max_tokens=60, temperature=0.7 ) return response.choices[0].message.content.strip()\n\n```\n\n#### Security & auth: Managed Identity by default (recommended)\n\nThis sample ships to **use Managed Identity** on App Service—no keys in config.\n\n- The Web App’s **Managed Identity** authenticates to **Vision** and **Azure OpenAI** via Microsoft Entra ID.\n- Prefer Managed Identity in production; if you need to test locally, you can switch to **key-based auth** by supplying the service keys in your environment.\n\n#### Run it locally (optional)\n\n```\n# From the sample folder\npython -m venv .venv && source .venv/bin/activate # Windows: .venv\\Scripts\\activate pip install -r requirements.txt\n\n# Set env vars for endpoints + deployment (and keys if not using MI locally)\nstreamlit run app.py\n\n```\n\n#### Repo map\n\n- App + Streamlit UI + helpers: *image\\_caption\\_app/*\n- Bicep infrastructure (used by azd up): i*mage\\_caption\\_app/infra/*\n\n#### What’s next — ways to extend this sample\n\n- **Richer vision signals:** Add object detection, OCR, or brand detection; blend those into the prompt for sharper captions.\n- **Persistence & gallery:** Save images to Blob Storage and captions/metadata to Cosmos DB or SQLite; add a Streamlit gallery.\n- **Performance & cost:** Cache tags by image hash; cap image size; track tokens/latency.\n- **Observability:** Wire up Application Insights with custom events (e.g., *caption\\_generated*).\n\nLooking for more Python samples? Check out the repo: [https://github.com/Azure-Samples/appservice-ai-samples/tree/main](https://github.com/Azure-Samples/appservice-ai-samples/tree/main)\n\nFor more Azure App Service AI samples and best practices, check out the [Azure App Service AI integration documentation](https://learn.microsoft.com/en-us/azure/app-service/overview-ai-integration)\n\nPublished Sep 02, 2025\n\nVersion 1.0\n\n[azure app service](/tag/azure%20app%20service?nodeId=board%3AAppsonAzureBlog)\n\n[python](/tag/python?nodeId=board%3AAppsonAzureBlog)\n\n[!\\[TulikaC&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-1.svg?image-dimensions=50x50)](/users/tulikac/1661700) [TulikaC](/users/tulikac/1661700) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined December 21, 2022\n\n[View Profile](/users/tulikac/1661700)\n\n/category/azure/blog/appsonazureblog [Apps on Azure Blog](/category/azure/blog/appsonazureblog) Follow this blog board to get notified when there's new activity",
  "Tags": [],
  "PubDate": "2025-09-02T10:08:03+00:00",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "OutputDir": "_community",
  "Description": "This tiny app just does one thing: **upload an image → get a natural one-line caption**. Under the hood:\n\n- **Azure AI Vision** extracts high-confidence tags from the image.\n- **Azure OpenAI (GPT-4o-mini)** turns those tags into a fluent caption.\n- **Streamlit** provides a lightweight, Python-native UI so you can ship fast.\n\nAll code + infra templates: **image\\_caption\\_app** in the App Service AI Samples repo: [https://github.com/Azure-Samples/appservice-ai-samples/tree/main/image_caption_app](https://github.com/Azure-Samples/appservice-ai-samples/tree/main/image_caption_app)\n\n#### What are these components?\n\n- **What is Streamlit?** An open-source Python framework to build interactive data/AI apps with just a few lines of code—perfect for quick, clean UIs.\n- **What is Azure AI Vision (Vision API)?** A cloud service that analyzes images and returns rich signals like **tags** with confidence scores, which we use as grounded inputs for captioning.\n\n#### How it works (at a glance)\n\n1. User uploads a photo in Streamlit.\n2. The app calls **Azure AI Vision** → gets a list of tags (keeps only high-confidence ones).\n3. The app sends those tags to **GPT-4o-mini** → generates a one-line caption.\n4. Caption is shown instantly in the browser.\n\n#### Prerequisites\n\n- **Azure subscription** — [https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account](https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account?utm_source=chatgpt.com)\n- **Azure CLI** — [https://learn.microsoft.com/azure/cli/azure/install-azure-cli-linux](https://learn.microsoft.com/azure/cli/azure/install-azure-cli-linux)\n- **Azure Developer CLI (azd)** — [https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd](https://learn.microsoft.com/azure/developer/azure-developer-cli/install-azd)\n- **Python 3.10+** — [https://www.python.org/downloads/](https://www.python.org/downloads/)\n- **Visual Studio Code** (optional) — [https://code.visualstudio.com/download](https://code.visualstudio.com/download)\n- **Streamlit** (optional for local runs) — [https://docs.streamlit.io/get-started/installation](https://docs.streamlit.io/get-started/installation)\n- **Managed Identity on App Service (recommended)** — [https://learn.microsoft.com/azure/app-service/overview-managed-identity](https://learn.microsoft.com/azure/app-service/overview-managed-identity)\n\n#### Resources you’ll deploy\n\nYou can create everything **manually** or **with the provided azd template**.\n\n**What you need**\n\n- **Azure App Service (Linux)** to host the Streamlit app.\n- **Azure AI Foundry/OpenAI** with a **gpt-4o-mini** deployment for caption generation.\n- **Azure AI Vision** (Computer Vision) for image tagging.\n- **Managed Identity** enabled on the Web App, with RBAC grants so the app can call Vision and OpenAI without secrets.\n\n**One-command deploy with azd (recommended)** The sample includes infra under image\\_caption\\_app/infra so azd up can provision + deploy in one go.\n\n- # 1) Clone and move into the sample\ngit clone https://github.com/Azure-Samples/appservice-ai-samples cd appservice-ai-samples/image\\_caption\\_app\n\n# 2) Log in and provision + deploy\nazd auth login azd up\n\n**Manual path (if you prefer doing it yourself)**\n\n1. Create **Azure AI Vision**, note the endpoint (custom subdomain).\n2. Create **Azure AI Foundry/OpenAI** and deploy **gpt-4o-mini**.\n3. Create **App Service (Linux, Python)** and enable **System-Assigned Managed Identity**.\n4. Assign roles to the Web App’s Managed Identity:\n- **Cognitive Services OpenAI User** on your OpenAI resource.\n- **Cognitive Services User** on your Vision resource.\n5. Add app settings for endpoints and deployment names (see repo), deploy the code, and run.\n\n**Startup command (manual setting):** If you’re configuring the Web App yourself (instead of using the Bicep), set the **Startup Command** to:\n- streamlit run app.py --server.port 8000 --server.address 0.0.0.0\n\nPortal path: **App Service → Configuration → General settings → Startup Command**. CLI example:\n- az webapp config set \\\n--name \\ --resource-group \\ --startup-file \"streamlit run app.py --server.port 8000 --server.address 0.0.0.0\"\n\n(The provided Bicep template already sets this for you.)\n\n#### Code tour (the important bits)\n\n**Top-level flow (app.py)** First we get tags from Vision, then ask GPT-4o-mini for a one-liner:\n- tags = extract\\_tags(image\\_bytes)\ncaption = generate\\_caption(tags)\n\n**Vision call (utils/vision.py)** Call the Vision REST API, parse JSON, and keep high-confidence tags (> 0.6):\n- response = requests.post(\nVISION\\_API\\_URL, headers=headers, params=PARAMS, data=image\\_bytes, timeout=30, ) response.raise\\_for\\_status() analysis = response.json()\n\ntags = [ t.get('name') for t in analysis.get('tags', []) if t.get('name') and t.get('confidence', 0) > 0.6 ]\n\n**Caption generation (utils/openai\\_caption.py)** Join tags and ask GPT-4o-mini for a natural caption:\n- tag\\_text = \", \".join(tags)\nprompt = f\"\"\" You are an assistant that generates vivid, natural-sounding captions for images. Create a one-line caption for an image that contains the following: {tag\\_text}. \"\"\"\n\nresponse = client.chat.completions.create( model=DEPLOYMENT\\_NAME, messages=[ {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, {\"role\": \"user\", \"content\": prompt.strip()} ], max\\_tokens=60, temperature=0.7 ) return response.choices[0].message.content.strip()\n\n#### Security & auth: Managed Identity by default (recommended)\n\nThis sample ships to **use Managed Identity** on App Service—no keys in config.\n\n- The Web App’s **Managed Identity** authenticates to **Vision** and **Azure OpenAI** via Microsoft Entra ID.\n- Prefer Managed Identity in production; if you need to test locally, you can switch to **key-based auth** by supplying the service keys in your environment.\n\n#### Run it locally (optional)\n- # From the sample folder\npython -m venv .venv && source .venv/bin/activate # Windows: .venv\\Scripts\\activate pip install -r requirements.txt\n\n# Set env vars for endpoints + deployment (and keys if not using MI locally)\nstreamlit run app.py\n\n#### Repo map\n\n- App + Streamlit UI + helpers: *image\\_caption\\_app/*\n- Bicep infrastructure (used by azd up): i*mage\\_caption\\_app/infra/*\n\n#### What’s next — ways to extend this sample\n\n- **Richer vision signals:** Add object detection, OCR, or brand detection; blend those into the prompt for sharper captions.\n- **Persistence & gallery:** Save images to Blob Storage and captions/metadata to Cosmos DB or SQLite; add a Streamlit gallery.\n- **Performance & cost:** Cache tags by image hash; cap image size; track tokens/latency.\n- **Observability:** Wire up Application Insights with custom events (e.g., *caption\\_generated*).\n\nLooking for more Python samples? Check out the repo: [https://github.com/Azure-Samples/appservice-ai-samples/tree/main](https://github.com/Azure-Samples/appservice-ai-samples/tree/main)\n\nFor more Azure App Service AI samples and best practices, check out the [Azure App Service AI integration documentation](https://learn.microsoft.com/en-us/azure/app-service/overview-ai-integration)",
  "FeedName": "Microsoft Tech Community",
  "Author": "TulikaC"
}
