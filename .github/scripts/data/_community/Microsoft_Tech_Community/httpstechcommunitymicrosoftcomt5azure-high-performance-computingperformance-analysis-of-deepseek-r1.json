{
  "Tags": [],
  "Author": "CormacGarvey",
  "Description": "# Introduction\n\nThe DeepSeek R1 model represents a new frontier in large-scale reasoning for AI applications. Designed to tackle complex inference tasks, R1 pushes the boundaries of what’s possible—but not without significant infrastructure demands. To deploy DeepSeek R1 effectively in an inference service like vLLM, high-performance hardware is essential. Specifically, the model requires two Azure ND\\_H100\\_v5 nodes, each equipped with 8 NVIDIA H100 GPUs, totaling 16 H100s. These nodes are interconnected via InfiniBand and NVLink, ensuring the bandwidth and latency characteristics necessary to support the model’s massive memory footprint and parallel processing needs.\n\nIn this post, we’ll present inference benchmark results for DeepSeek R1, measuring performance across GPU utilization, memory throughput, and interconnect efficiency. While R1 excels in reasoning tasks, it’s important to recognize that such models are not universally optimal. For many general-purpose AI applications, smaller models like Llama 3.1 8B offer a compelling alternative, delivering sufficient accuracy and performance at a fraction of the cost.\n\nWe explore the performance characteristics of DeepSeek R1 and help you decide when a large reasoning model is worth the investment—and when a leaner solution might be the better choice.\n\n# Benchmark environment\n\n2 ND\\_H100\\_v5 nodes (16 H100) were required to load the DeepSeek R1 model into GPU memory. vLLM was used to set-up the online inference server using a single API server endpoint. vLLM bench (specifically hugging face AI-MO/aimo-validation-aime) was used to run the inference benchmark. Detailed specifications are available in the tables below.\n\n![]()![]()![]()\n\n# Results\n\n## Reasoning model’s additional chain of thought tokens\n\nWe show that DeepSeek R1 generates many additional tokens (for chain of thought), which helps reasoning models answer complex logical questions. In this example the DeepSeek R1 generates 1162 completion tokens, but Llama 3.1 8B only generates 37. This example also highlights that this simple question is not suitable for DeepSeek R1, because it over analyses the question (Which is expensive).\n\n> >\n> #!/bin/bash\n> > >\n> curl http://localhost:8000/v1/chat/completions \\\n> > >\n> -H \"Content-Type: application/json\" \\\n> > >\n> -d '{\n> > >\n> \"model\": \"deepseek-ai/DeepSeek-R1\",\n> > >\n> \"messages\": [\n> > >\n> {\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater? Explain your reasoning\"}\n> > >\n> ]\n> > >\n> }'\n> >\n\n**Note:** The REST API json output is formatted to make the output more readable. (./run\\_restapi\\_query.sh 2> /dev/null | python3 ./convert\\_json\\_2\\_txt.py >& run\\_query.out\\_$$)\n\n> >\n> Reasoning:\n> > >\n> Okay, so I need to figure out whether 9.11 is greater than 9.8 or if 9.8 is greater. Hmm, let me start by looking at both numbers. They both have a 9 in the units place, so that part is equal. Now, I need to compare the decimal parts: .11 and .8.\n> > > > > > >\n> Wait, but decimals can be tricky because the places matter. The first number is 9.11, which has a 1 in the tenths place and a 1 in the hundredths place. The second number is 9.8, which is the same as 9.80 when you add a zero to the hundredths place for comparison. So, 9.80 versus 9.11. Now, looking at the tenths place: 8 is greater than 1. Therefore, 9.80 should be greater than 9.11.\n> > > > > > >\n> But let me double-check because sometimes when numbers have different decimal lengths, it's easy to make a mistake. If I convert both to fractions, maybe that will help.\n> > > > > > >\n> 9.11 is the same as 9 + 0.11, and 0.11 is 11/100. On the other hand, 9.8 is 9 + 0.8, which is 9 + 8/10. To compare 8/10 and 11/100, I can convert them to have the same denominator. 8/10 is equal to 80/100. Now, 80/100 is much larger than 11/100. So yes, 9.8 is indeed greater than 9.11.\n> > > > > > >\n> Another way to see it is by subtracting the two numbers. If I subtract 9.11 from 9.8, what do I get? Let's see: 9.8 - 9.11. Since 9.8 is the same as 9.80, subtracting 9.11 gives 0.69. Since the result is positive, that means 9.8 is larger.\n> > > > > > >\n> Wait, maybe I can also think about money to make it clearer. If I have $9.11 and you have $9.80, which is more? $9.80 is $9 and 80 cents, whereas $9.11 is $9 and 11 cents. Definitely, 80 cents is more than 11 cents, so $9.80 is more. That helps visualize it.\n> > > > > > >\n> Alternatively, using a number line. If I imagine a number line with 9.0, 9.1, 9.2,... up to 10.0. 9.11 is just a little past 9.1, while 9.8 is almost at 10.0. So clearly, 9.8 is much further to the right, meaning it's a larger number.\n> > > > > > >\n> I think that covers different methods: decimal place comparison, converting to fractions, subtraction, real-life money examples, and number line visualization. All of these methods point to 9.8 being greater than 9.11. I don't see any contradictions here. Maybe I can also consider writing both numbers with the same number of decimal places to make it even clearer.\n> > > > > > >\n> 9.11 is already two decimal places, and 9.8 can be written as 9.80. Now, comparing 9.80 and 9.11 digit by digit:\n> > > > > > >\n> - The units place: 9 vs. 9 (equal)\n> > >\n> - Tenths place: 8 vs. 1 (8 is greater)\n> > >\n> - Since the tenths place of 9.80 is greater, the entire number is larger, regardless of the hundredths place.\n> > > > > > >\n> So yes, that confirms it again. The key idea here is that when comparing decimals, you start from the leftmost digit after the decimal point and move right until you find a difference. In this case, the tenths place already shows that 9.8 is larger.\n> > > > > > >\n> I guess another common mistake could be thinking that because 11 is more than 8, 0.11 is more than 0.8, but that's not correct because the places are different. Tenths versus hundredths. 0.8 is equivalent to 0.80, which is way bigger than 0.11.\n> > > > > > >\n> To prevent that mistake, always align the decimal points and add trailing zeros if necessary. Then compare each digit one by one from left to right. That method avoids confusion.\n> > > > > > >\n> So, all in all, through multiple approaches, 9.8 is definitely greater than 9.11.\n> > >\n> Answer:\n> > >\n> To determine which number is greater between \\*\\*9.11\\*\\* and \\*\\*9.8\\*\\*, follow these steps:\n> > > > > > >\n> 1. \\*\\*Align the decimal places\\*\\* by rewriting 9.8 as \\*\\*9.80\\*\\*.\n> 2. \\*\\*Compare digit by digit\\*\\* from left to right:\n> > >\n> - \\*\\*Units place\\*\\*: Both have \\*\\*9\\*\\* (equal).\n> > >\n> - \\*\\*Tenths place\\*\\*: \\*\\*8\\*\\* (in 9.80) vs. \\*\\*1\\*\\* (in 9.11). Since \\(8 > 1\\), \\*\\*9.80 is greater\\*\\* at this point.\n> > >\n> - No need to compare hundredths place once a difference is found.\n> > > > > > >\n> \\*\\*Conclusion\\*\\*:\n> > >\n> \\(9.8\\) (or \\(9.80\\)) is greater than \\(9.11\\) because \\*\\*8 tenths\\*\\* (\\(0.8\\)) is larger than \\*\\*1 tenth + 1 hundredth\\*\\* (\\(0.11\\)).\n> > > > > > >\n> \\(\\boxed{9.8 \\text{ is greater}}\\)\n> > > > > > >\n> Completion Info:\n> > >\n> Finish reason: stop\n> > >\n> Prompt tokens: 21\n> > >\n> Completion tokens: 1162\n> > >\n> Total tokens: 1183\n> >\n\nWhen running using Llama 3.1 8B (Not reasoning model).\n\n> >\n> Answer:\n> > >\n> To determine which is greater, we need to compare the two numbers.\n> > > > > > >\n> 9.11 is greater than 9.8 because 9.11 is a larger decimal value.\n> > > > > > >\n> Completion Info:\n> > >\n> Finish reason: stop\n> > >\n> Prompt tokens: 51\n> > >\n> Completion tokens: 37\n> > >\n> Total tokens: 88\n> >\n\n## Throughput and latency results\n\n![]()![]()![]()![]()![]()![]()![]()\n\n## Cost comparison\n\nIn this cost analysis we use the ND-H100-v5 and ND-H200-v4 pay as you go pricing in south central US region and the measured total throughput tokens/sec to compute the $/(1K tokens).\n\nNote: ND-H200-v5 pricing was estimated at 20% more than ND-H100-v5 pricing.\n\n![]()\n\n# Analysis\n\nThe DeepSeek R1 is a large, complex reasoning model that is costlier and slower than smaller models. It needs 16 H100 GPUs for FP8 precision and generates many more intermediate tokens in its chain of thought process—about 31 times more than Llama 3.1 8B—but at a much slower rate (~54 times slower). Its latency is also higher, with TTFT and ITL being roughly 6 and 3 times slower, respectively.\n\nThe DeepSeek R1 model has small intranode and internode network requirements (~14% of available InfiniBand network bandwidth was used, and\n\nThe cost analysis shows that the cost to generate DeepSeek R1 tokens is ~54 times more expensive than Llama 3.1 8B on the same 16 H100 GPU’s and ~34 times more expensive on 8 H200 GPU’s.\n\nDeepSeek R1 model is very capability, but due to its higher TCO it should be only used in specific AI applications that require its strong reasoning abilities.\n\n# Conclusion\n\nThe DeepSeek R1 model demonstrates exceptional reasoning capabilities, but its deployment demands substantial infrastructure and incurs high latency and cost. While it excels in generating detailed chains of thought, its throughput and efficiency lag significantly behind smaller models like Llama 3.1 8B. For applications requiring deep logical analysis, DeepSeek R1 is a powerful tool. However, for general-purpose inference tasks, more lightweight models offer better performance and cost-effectiveness. Strategic use of DeepSeek R1 should be reserved for scenarios where its advanced reasoning justifies the resource investment.\n\n# References\n\n1. **Deepseek R1 model on Hugging Face**\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1\n2. **vLLM GitHub repository**\nhttps://github.com/vllm-project/vllm\n3. **Azure ND H100 v5 documentation**\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/nd-h100-v5-series\n4. **FlashInfer GitHub repository**\nhttps://github.com/flashinfer-ai/flashinfer\n5. **DeepGEMM GitHub repository**\nhttps://github.com/deepseek-ai/DeepGEMM\n6. **AI-MO validation dataset on Hugging Face**\nhttps://huggingface.co/datasets/AI-MO/aimo-validation-aime\n\n# Appendix\n\n## Install vLLM\n\n> >\n> curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh\n> > >\n> uv venv myvllm --python 3.11 --seed\n> > >\n> source myvllm/bin/activate\n> > >\n> uv pip install vllm --torch-backend=auto\n> > >\n> git clone [https://github.com/flashinfer-ai/flashinfer.git](https://github.com/flashinfer-ai/flashinfer.git) --recursive\n> > >\n> uv pip install ninja\n> > > > > > >\n> cd flashinfer\n> > >\n> uv pip install --no-build-isolation --verbose .\n> >\n\n## Install DeepSeek DeepEP\n\n> >\n> git clone [https://github.com/vllm-project/vllm.git](https://github.com/vllm-project/vllm.git)\n> > >\n> cd ~/vllm/tools/ep\\_kernels\n> > >\n> export CUDA\\_HOME=/usr/local/cuda-12.8\n> > >\n> TORCH\\_CUDA\\_ARCH\\_LIST=\"9.0\" (For Hopper)\n> > >\n> bash install\\_python\\_libraries.sh 2.&1 | tee install\\_python\\_libraries.log\\_$$\n> > >\n> sudo bash configure\\_system\\_drivers.sh 2>&1 | tee configure\\_system\\_drivers.log\\_$$\n> > >\n> sudo reboot\n> >\n\n## Install DeepSeek DeepGEMM\n\n> >\n> git clone --recursive [https://github.com/deepseek-ai/DeepGEMM.git](https://github.com/deepseek-ai/DeepGEMM.git)\n> > >\n> cd deepGEMM\n> > >\n> ./install.sh 2>&1 | tee install.log\\_$$\n> >\n\n## Configure DeepSeek R1 with vLLM on 2 ND\\_H100\\_v5\n\n### Second node configuration\n\nExecute this script on second node before the script on the primary node.\n\n> >\n> #!/bin/bash\n> > >\n> MODEL=\"deepseek-ai/DeepSeek-R1\"\n> > >\n> PORT=8000\n> > >\n> export VLLM\\_LOGGING\\_LEVEL=INFO\n> > >\n> export HF\\_HUB\\_CACHE=/home/azureuser/cgshared/hf\\_cache\n> > >\n> #export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_high\\_throughput\n> > >\n> export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_low\\_latency\n> > >\n> export VLLM\\_USE\\_DEEP\\_GEMM=1\n> > >\n> export GLOO\\_SOCKET\\_IFNAME=eth0\n> > >\n> vllm serve $MODEL --port $PORT --tensor-parallel-size 1 --enable-expert-parallel --data-parallel-size 16 --data-parallel-size-local 8 --data-parallel-start-rank 8 --data-parallel-address 10.0.0.6 --data-parallel-rpc-port 23345 --headless --max-model-len 32768 --reasoning-parser deepseek\\_r1\n> >\n\n### Primary node configuration\n\n> >\n> #!/bin/bash\n> > >\n> MODEL=\"deepseek-ai/DeepSeek-R1\"\n> > >\n> PORT=8000\n> > >\n> export VLLM\\_LOGGING\\_LEVEL=INFO\n> > >\n> export HF\\_HUB\\_CACHE=/home/azureuser/cgshared/hf\\_cache\n> > >\n> #export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_high\\_throughput\n> > >\n> export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_low\\_latency\n> > >\n> export VLLM\\_USE\\_DEEP\\_GEMM=1\n> > >\n> export GLOO\\_SOCKET\\_IFNAME=eth0\n> > >\n> vllm serve $MODEL --port $PORT --tensor-parallel-size 1 --enable-expert-parallel --data-parallel-size 16 --data-parallel-size-local 8 --data-parallel-address 10.0.0.6 --data-parallel-rpc-port 23345 --api-server-count 1 --max-model-len 32768 --reasoning-parser deepseek\\_r1\n> > > > > >\n\n### Install vLLM benchmark environment\n\n> >\n> cd vllm\n> > >\n> uv pip install vllm[bench]\n> >\n\n## Run vLLM benchmark\n\n> >\n> #!/bin/bash\n> > >\n> vllm bench serve \\\n> > >\n> --backend vllm \\\n> > >\n> --model deepseek-ai/DeepSeek-R1 \\\n> > >\n> --endpoint /v1/completions \\\n> > >\n> --dataset-name hf \\\n> > >\n> --dataset-path AI-MO/aimo-validation-aime \\\n> > >\n> --ramp-up-strategy linear \\\n> > >\n> --ramp-up-start-rps 1 \\\n> > >\n> --ramp-up-end-rps 10 \\\n> > >\n> --num-prompts 400 \\\n> > >\n> --seed 42\n> >",
  "EnhancedContent": "# Introduction\n\nThe DeepSeek R1 model represents a new frontier in large-scale reasoning for AI applications. Designed to tackle complex inference tasks, R1 pushes the boundaries of what’s possible—but not without significant infrastructure demands. To deploy DeepSeek R1 effectively in an inference service like vLLM, high-performance hardware is essential. Specifically, the model requires two Azure ND\\_H100\\_v5 nodes, each equipped with 8 NVIDIA H100 GPUs, totaling 16 H100s. These nodes are interconnected via InfiniBand and NVLink, ensuring the bandwidth and latency characteristics necessary to support the model’s massive memory footprint and parallel processing needs.\n\nIn this post, we’ll present inference benchmark results for DeepSeek R1, measuring performance across GPU utilization, memory throughput, and interconnect efficiency. While R1 excels in reasoning tasks, it’s important to recognize that such models are not universally optimal. For many general-purpose AI applications, smaller models like Llama 3.1 8B offer a compelling alternative, delivering sufficient accuracy and performance at a fraction of the cost.\n\nWe explore the performance characteristics of DeepSeek R1 and help you decide when a large reasoning model is worth the investment—and when a leaner solution might be the better choice.\n\n# Benchmark environment\n\n2 ND\\_H100\\_v5 nodes (16 H100) were required to load the DeepSeek R1 model into GPU memory. vLLM was used to set-up the online inference server using a single API server endpoint. vLLM bench (specifically hugging face AI-MO/aimo-validation-aime) was used to run the inference benchmark. Detailed specifications are available in the tables below.\n\n# Results\n\n## Reasoning model’s additional chain of thought tokens\n\nWe show that DeepSeek R1 generates many additional tokens (for chain of thought), which helps reasoning models answer complex logical questions. In this example the DeepSeek R1 generates 1162 completion tokens, but Llama 3.1 8B only generates 37. This example also highlights that this simple question is not suitable for DeepSeek R1, because it over analyses the question (Which is expensive).\n\n> >\n> #!/bin/bash\n> > >\n> curl http://localhost:8000/v1/chat/completions \\\n> > >\n>     -H \"Content-Type: application/json\" \\\n> > >\n>     -d '{\n> > >\n>         \"model\": \"deepseek-ai/DeepSeek-R1\",\n> > >\n>         \"messages\": [\n> > >\n>             {\"role\": \"user\", \"content\": \"9.11 and 9.8, which is greater? Explain your reasoning\"}\n> > >\n>         ]\n> > >\n>     }'\n> >\n\n**Note:** The REST API json output is formatted to make the output more readable. (./run\\_restapi\\_query.sh 2&gt; /dev/null | python3 ./convert\\_json\\_2\\_txt.py &gt;& run\\_query.out\\_$$)\n\n> >\n> Reasoning:\n> > >\n> Okay, so I need to figure out whether 9.11 is greater than 9.8 or if 9.8 is greater. Hmm, let me start by looking at both numbers. They both have a 9 in the units place, so that part is equal. Now, I need to compare the decimal parts: .11 and .8.\n> > > > > > >\n> Wait, but decimals can be tricky because the places matter. The first number is 9.11, which has a 1 in the tenths place and a 1 in the hundredths place. The second number is 9.8, which is the same as 9.80 when you add a zero to the hundredths place for comparison. So, 9.80 versus 9.11. Now, looking at the tenths place: 8 is greater than 1. Therefore, 9.80 should be greater than 9.11.\n> > > > > > >\n> But let me double-check because sometimes when numbers have different decimal lengths, it's easy to make a mistake. If I convert both to fractions, maybe that will help.\n> > > > > > >\n> 9.11 is the same as 9 + 0.11, and 0.11 is 11/100. On the other hand, 9.8 is 9 + 0.8, which is 9 + 8/10. To compare 8/10 and 11/100, I can convert them to have the same denominator. 8/10 is equal to 80/100. Now, 80/100 is much larger than 11/100. So yes, 9.8 is indeed greater than 9.11.\n> > > > > > >\n> Another way to see it is by subtracting the two numbers. If I subtract 9.11 from 9.8, what do I get? Let's see: 9.8 - 9.11. Since 9.8 is the same as 9.80, subtracting 9.11 gives 0.69. Since the result is positive, that means 9.8 is larger.\n> > > > > > >\n> Wait, maybe I can also think about money to make it clearer. If I have $9.11 and you have $9.80, which is more? $9.80 is $9 and 80 cents, whereas $9.11 is $9 and 11 cents. Definitely, 80 cents is more than 11 cents, so $9.80 is more. That helps visualize it.\n> > > > > > >\n> Alternatively, using a number line. If I imagine a number line with 9.0, 9.1, 9.2,... up to 10.0. 9.11 is just a little past 9.1, while 9.8 is almost at 10.0. So clearly, 9.8 is much further to the right, meaning it's a larger number.\n> > > > > > >\n> I think that covers different methods: decimal place comparison, converting to fractions, subtraction, real-life money examples, and number line visualization. All of these methods point to 9.8 being greater than 9.11. I don't see any contradictions here. Maybe I can also consider writing both numbers with the same number of decimal places to make it even clearer.\n> > > > > > >\n> 9.11 is already two decimal places, and 9.8 can be written as 9.80. Now, comparing 9.80 and 9.11 digit by digit:\n> > > > > > >\n> - The units place: 9 vs. 9 (equal)\n> > >\n> - Tenths place: 8 vs. 1 (8 is greater)\n> > >\n> - Since the tenths place of 9.80 is greater, the entire number is larger, regardless of the hundredths place.\n> > > > > > >\n> So yes, that confirms it again. The key idea here is that when comparing decimals, you start from the leftmost digit after the decimal point and move right until you find a difference. In this case, the tenths place already shows that 9.8 is larger.\n> > > > > > >\n> I guess another common mistake could be thinking that because 11 is more than 8, 0.11 is more than 0.8, but that's not correct because the places are different. Tenths versus hundredths. 0.8 is equivalent to 0.80, which is way bigger than 0.11.\n> > > > > > >\n> To prevent that mistake, always align the decimal points and add trailing zeros if necessary. Then compare each digit one by one from left to right. That method avoids confusion.\n> > > > > > >\n> So, all in all, through multiple approaches, 9.8 is definitely greater than 9.11.\n> > >\n> Answer:\n> > >\n> To determine which number is greater between \\*\\*9.11\\*\\* and \\*\\*9.8\\*\\*, follow these steps:\n> > > > > > >\n> 1. \\*\\*Align the decimal places\\*\\* by rewriting 9.8 as \\*\\*9.80\\*\\*.\n> 2. \\*\\*Compare digit by digit\\*\\* from left to right:\n> > >\n>    - \\*\\*Units place\\*\\*: Both have \\*\\*9\\*\\* (equal).\n> > >\n>    - \\*\\*Tenths place\\*\\*: \\*\\*8\\*\\* (in 9.80) vs. \\*\\*1\\*\\* (in 9.11). Since \\(8 &gt; 1\\), \\*\\*9.80 is greater\\*\\* at this point.\n> > >\n>    - No need to compare hundredths place once a difference is found.\n> > > > > > >\n> \\*\\*Conclusion\\*\\*:\n> > >\n> \\(9.8\\) (or \\(9.80\\)) is greater than \\(9.11\\) because \\*\\*8 tenths\\*\\* (\\(0.8\\)) is larger than \\*\\*1 tenth + 1 hundredth\\*\\* (\\(0.11\\)).\n> > > > > > >\n> \\(\\boxed{9.8 \\text{ is greater}}\\)\n> > > > > > >\n> Completion Info:\n> > >\n>   Finish reason: stop\n> > >\n>   Prompt tokens: 21\n> > >\n>   Completion tokens: 1162\n> > >\n>   Total tokens: 1183\n> >\n\nWhen running using Llama 3.1 8B (Not reasoning model).\n\n> >\n> Answer:\n> > >\n> To determine which is greater, we need to compare the two numbers.\n> > > > > > >\n> 9.11 is greater than 9.8 because 9.11 is a larger decimal value.\n> > > > > > >\n> Completion Info:\n> > >\n>   Finish reason: stop\n> > >\n>   Prompt tokens: 51\n> > >\n>   Completion tokens: 37\n> > >\n>   Total tokens: 88\n> >\n\n## Throughput and latency results\n\n## Cost comparison\n\nIn this cost analysis we use the ND-H100-v5 and ND-H200-v4 pay as you go pricing in south central US region and the measured total throughput tokens/sec to compute the $/(1K tokens).\n\nNote: ND-H200-v5 pricing was estimated at 20% more than ND-H100-v5 pricing.\n\n# Analysis\n\nThe DeepSeek R1 is a large, complex reasoning model that is costlier and slower than smaller models. It needs 16 H100 GPUs for FP8 precision and generates many more intermediate tokens in its chain of thought process—about 31 times more than Llama 3.1 8B—but at a much slower rate (~54 times slower). Its latency is also higher, with TTFT and ITL being roughly 6 and 3 times slower, respectively.\n\nThe DeepSeek R1 model has small intranode and internode network requirements (~14% of available InfiniBand network bandwidth was used, and &lt; 1% of available NVLink bandwidth is used. GPUs with higher memory bandwidth and higher FLOPS would help improve its performance.\n\nThe cost analysis shows that the cost to generate DeepSeek R1 tokens is ~54 times more expensive than Llama 3.1 8B on the same 16 H100 GPU’s and ~34 times more expensive on 8 H200 GPU’s.\n\nDeepSeek R1 model is very capability, but due to its higher TCO it should be only used in specific AI applications that require its strong reasoning abilities.\n\n# Conclusion\n\nThe DeepSeek R1 model demonstrates exceptional reasoning capabilities, but its deployment demands substantial infrastructure and incurs high latency and cost. While it excels in generating detailed chains of thought, its throughput and efficiency lag significantly behind smaller models like Llama 3.1 8B. For applications requiring deep logical analysis, DeepSeek R1 is a powerful tool. However, for general-purpose inference tasks, more lightweight models offer better performance and cost-effectiveness. Strategic use of DeepSeek R1 should be reserved for scenarios where its advanced reasoning justifies the resource investment.\n\n# References\n\n1. **Deepseek R1 model on Hugging Face**\nhttps://huggingface.co/deepseek-ai/DeepSeek-R1\n2. **vLLM GitHub repository**\nhttps://github.com/vllm-project/vllm\n3. **Azure ND H100 v5 documentation**\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/nd-h100-v5-series\n4. **FlashInfer GitHub repository**\nhttps://github.com/flashinfer-ai/flashinfer\n5. **DeepGEMM GitHub repository**\nhttps://github.com/deepseek-ai/DeepGEMM\n6. **AI-MO validation dataset on Hugging Face**\nhttps://huggingface.co/datasets/AI-MO/aimo-validation-aime\n\n# Appendix\n\n## Install vLLM\n\n> >\n> curl -LsSf [https://astral.sh/uv/install.sh](https://astral.sh/uv/install.sh) | sh\n> > >\n> uv venv myvllm --python 3.11 --seed\n> > >\n> source myvllm/bin/activate\n> > >\n> uv pip install vllm --torch-backend=auto\n> > >\n> git clone [https://github.com/flashinfer-ai/flashinfer.git](https://github.com/flashinfer-ai/flashinfer.git) --recursive\n> > >\n> uv pip install ninja\n> > > > > > >\n> cd flashinfer\n> > >\n> uv pip install --no-build-isolation --verbose .\n> >\n\n## Install DeepSeek DeepEP\n\n> >\n> git clone [https://github.com/vllm-project/vllm.git](https://github.com/vllm-project/vllm.git)\n> > >\n> cd ~/vllm/tools/ep\\_kernels\n> > >\n> export CUDA\\_HOME=/usr/local/cuda-12.8\n> > >\n> TORCH\\_CUDA\\_ARCH\\_LIST=\"9.0\"                 (For Hopper)\n> > >\n> bash install\\_python\\_libraries.sh 2.&1 | tee install\\_python\\_libraries.log\\_$$\n> > >\n> sudo bash configure\\_system\\_drivers.sh 2&gt;&1 | tee configure\\_system\\_drivers.log\\_$$\n> > >\n> sudo reboot\n> >\n\n## Install DeepSeek DeepGEMM\n\n> >\n> git clone --recursive  [https://github.com/deepseek-ai/DeepGEMM.git](https://github.com/deepseek-ai/DeepGEMM.git)\n> > >\n> cd deepGEMM\n> > >\n> ./install.sh 2&gt;&1 | tee install.log\\_$$\n> >\n\n## Configure DeepSeek R1 with vLLM on 2 ND\\_H100\\_v5\n\n### Second node configuration\n\nExecute this script on second node before the script on the primary node.\n\n> >\n> #!/bin/bash\n> > >\n> MODEL=\"deepseek-ai/DeepSeek-R1\"\n> > >\n> PORT=8000\n> > >\n> export VLLM\\_LOGGING\\_LEVEL=INFO\n> > >\n> export HF\\_HUB\\_CACHE=/home/azureuser/cgshared/hf\\_cache\n> > >\n> #export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_high\\_throughput\n> > >\n> export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_low\\_latency\n> > >\n> export VLLM\\_USE\\_DEEP\\_GEMM=1\n> > >\n> export GLOO\\_SOCKET\\_IFNAME=eth0\n> > >\n> vllm serve  $MODEL --port $PORT --tensor-parallel-size 1 --enable-expert-parallel --data-parallel-size 16 --data-parallel-size-local 8 --data-parallel-start-rank 8 --data-parallel-address 10.0.0.6 --data-parallel-rpc-port 23345 --headless --max-model-len 32768 --reasoning-parser deepseek\\_r1\n> >\n\n### Primary node configuration\n\n> >\n> #!/bin/bash\n> > >\n> MODEL=\"deepseek-ai/DeepSeek-R1\"\n> > >\n> PORT=8000\n> > >\n> export VLLM\\_LOGGING\\_LEVEL=INFO\n> > >\n> export HF\\_HUB\\_CACHE=/home/azureuser/cgshared/hf\\_cache\n> > >\n> #export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_high\\_throughput\n> > >\n> export VLLM\\_ALL2ALL\\_BACKEND=deepep\\_low\\_latency\n> > >\n> export VLLM\\_USE\\_DEEP\\_GEMM=1\n> > >\n> export GLOO\\_SOCKET\\_IFNAME=eth0\n> > >\n> vllm serve $MODEL --port $PORT --tensor-parallel-size 1 --enable-expert-parallel --data-parallel-size 16 --data-parallel-size-local 8 --data-parallel-address 10.0.0.6 --data-parallel-rpc-port 23345 --api-server-count 1 --max-model-len 32768 --reasoning-parser deepseek\\_r1\n> > > > > >\n\n### Install vLLM benchmark environment\n\n> >\n> cd vllm\n> > >\n> uv pip install vllm[bench]\n> >\n\n## Run vLLM benchmark\n\n> >\n> #!/bin/bash\n> > >\n> vllm bench serve \\\n> > >\n>         --backend vllm \\\n> > >\n>         --model deepseek-ai/DeepSeek-R1 \\\n> > >\n>         --endpoint /v1/completions \\\n> > >\n>         --dataset-name hf \\\n> > >\n>         --dataset-path AI-MO/aimo-validation-aime \\\n> > >\n>         --ramp-up-strategy linear \\\n> > >\n>         --ramp-up-start-rps 1 \\\n> > >\n>         --ramp-up-end-rps 10 \\\n> > >\n>         --num-prompts 400 \\\n> > >\n>         --seed 42\n> >\n\nUpdated Aug 28, 2025\n\nVersion 1.0\n\n[ai infrastructure](/tag/ai%20infrastructure?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[benchmarking](/tag/benchmarking?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[hpc](/tag/hpc?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[virtual machines](/tag/virtual%20machines?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[!\\[CormacGarvey&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-8.svg?image-dimensions=50x50)](/users/cormacgarvey/364170) [CormacGarvey](/users/cormacgarvey/364170) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined June 20, 2019\n\n[View Profile](/users/cormacgarvey/364170)\n\n/category/azure/blog/azurehighperformancecomputingblog [Azure High Performance Computing (HPC) Blog](/category/azure/blog/azurehighperformancecomputingblog) Follow this blog board to get notified when there's new activity",
  "PubDate": "2025-08-28T22:13:32+00:00",
  "FeedName": "Microsoft Tech Community",
  "Link": "https://techcommunity.microsoft.com/t5/azure-high-performance-computing/performance-analysis-of-deepseek-r1-ai-inference-using-vllm-on/ba-p/4449351",
  "Title": "Performance analysis of DeepSeek R1 AI Inference using vLLM on ND-H100-v5",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "OutputDir": "_community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "ProcessedDate": "2025-08-28 23:12:13"
}
