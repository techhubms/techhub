{
  "PubDate": "2025-12-07T09:46:09+00:00",
  "Tags": [],
  "Title": "End-to-End Observability for Azure Databricks: From Infrastructure to Internal Application Logging",
  "Author": "Rafia_Aqil",
  "ProcessedDate": "2025-12-07 10:05:28",
  "FeedName": "Microsoft Tech Community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "EnhancedContent": "> >\n> ***Author's:*** Amudha Palani, Peter Lo [PeterLo​](javascript:void%280%29) , Rafia Aqil [Rafia_Aqil​](javascript:void%280%29)\n> >\n\nObservability in Azure Databricks is the ability to continuously monitor and troubleshoot the health, performance, and usage of data workloads by capturing metrics, logs, and traces. In a structured observability approach, we consider two broad categories of logging: Internal Databricks Logging (within the Databricks environment) and External Databricks Logging (leveraging Azure services). Each plays a distinct role in providing insights.\n\nBy combining internal and external observability mechanisms, organizations can achieve a comprehensive view: internal logs enable detailed analysis of Spark jobs and data quality, while external logs ensure global visibility, auditing, and integration with broader monitoring dashboards and alerting systems. The article is organized into two main sections:\n\n- Infrastructure Logging for Azure Databricks (external observability)\n- Internal Databricks Logging (in-platform observability)\n\n### **Considerations**\n\nAddressing key questions upfront ensures your observability strategy is tailored to your organization’s unique workloads, risk profile, and operational needs. By proactively evaluating what to monitor, where to store logs, and who needs access, you can avoid blind spots, streamline incident response, and align monitoring investments with business priorities.\n\n1. **What types of workloads are running in Databricks?**\n\n- - **Why it matters****:** Different workloads (e.g., batch ETL, streaming pipelines, ML training, interactive notebooks) have distinct performance profiles and failure modes.\n\n- - **Business impact****:** Understanding workload types helps prioritize monitoring for mission-critical processes like real-time fraud detection or daily financial reporting.\n\n1. **What failure scenarios need to be monitored?**\n\n- - **Examples****:** Job failures, cluster provisioning errors, quota limits, authentication issues.\n\n- - **Business impact****:** Early detection of failures reduces downtime, improves SLA adherence, and prevents data pipeline disruptions that could affect reporting or customer-facing analytics.\n\n1. **Where should logs be stored and analyzed?**\n\n- - **Options****:** Centralized Log Analytics workspace, Azure Storage for archival, Event Hub for streaming analysis.\n\n- - **Business impact****:** Centralized logging enables unified dashboards, cross-team visibility, and faster incident response across data engineering, operations, and compliance teams.\n\n1. **Who needs access to logs and alerts?**\n\n- - **Stakeholders****:** Data engineers, platform administrators, security analysts, compliance officers.\n\n- - **Business impact****:** Role-based access ensures that the right teams can act on insights while maintaining data governance and privacy controls.\n\n### **Infrastructure Logging for Azure Databricks**\n\n#### **Approach 1: Diagnostic Settings for Azure Databricks**\n\n[Diagnostic settings](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/diagnostic-settings?WT.mc_id=Portal-Microsoft_Azure_Monitoring&amp;tabs=portal#create-a-diagnostic-setting)in Azure Monitor allow you to capture detailed logs and metrics from your Azure Databricks workspace, supporting operational monitoring, troubleshooting, and compliance. By configuring diagnostic settings at the workspace level, administrators can route Databricks logs—including cluster events, job statuses, and audit logs—to destinations such as Log Analytics, Azure Storage, or Event Hub. This enables unified analysis, alerting, and long-term retention of critical operational data.\n\n**Configuration Overview**\n\n- - [Enable Diagnostic Settings](https://docs.azure.cn/en-us/databricks/admin/account-settings/audit-log-delivery)on the Databricks workspace to route logs to [Log Analytics Workspace](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-tutorial).\n- Logs can also be combined with other logs mentioned below for full Azure Databricks observability.\n- Here is a Guide to Azure Databricks Diagnostic Settings Log Reference: [Configure diagnostic log delivery](https://docs.azure.cn/en-us/databricks/admin/account-settings/audit-log-delivery)\n- [Implement tagging strategy](https://docs.azure.cn/en-us/databricks/admin/account-settings/usage-detail-tags)-organizations can gain granular visibility into resource consumption and align spending with business priorities.\n\n- **Default tags:** Automatically applied by Databricks to cloud-deployed resources.\n- **Custom tags:** User-defined tags that you can add to compute resources and serverless workloads.\n\n**Use Cases**\n\n- - Operational Monitoring: Detect job or resource bottlenecks.\n- Security & Compliance: Audit user actions and enforce governance policies.\n- Incident Response: Correlate Databricks logs with infrastructure events for faster troubleshooting.\n\n**Best Practices**\n\n- - Enable only relevant log categories to optimize cost and performance.\n- Use role-based access control (RBAC) to secure access to logs.\n\n#### **Approach 2: Azure Databricks Compute Log Delivery**\n\nCompute log delivery in Azure Databricks enables you to automatically collect and archive logs from Spark driver nodes, worker nodes, and cluster events for both all-purpose and job compute resources. When you create a cluster, you can specify a log delivery location—such as DBFS, Azure Storage, or a Unity Catalog volume—where logs are delivered every five minutes and archived hourly. All logs generated up until the compute resource is terminated are guaranteed to be delivered, supporting troubleshooting, auditing, and compliance.\n\n**Configure**: To configure the log delivery location:\n\n1. 1. On the compute page, click the Advanced toggle.\n2. Click the Logging tab.\n3. Select a destination type: DBFS or Volumes (*Preview*).\n4. Enter the Log path.\n5. To store the logs, Databricks creates a subfolder in your chosen log path named after the compute's **cluster\\_id**.\n\n#### **Approach 3: Azure Activity Logs**\n\nWhenever you create, update, or delete Databricks resources (such as provisioning a new workspace, scaling a cluster, or modifying network settings), these actions are captured in the [Activity Log](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/activity-log?tabs=log-analytics). This enables teams to track who made changes, when, and what impact those changes had on the environment. Each event in the Activity Log has a particular category that is described in the following document: [Azure Activity Log event schema](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/activity-log-schema#categories). For Databricks, this is especially valuable for:\n\n- - Auditing resource deployments and configuration changes\n- Investigating failed provisioning or quota errors\n- Monitoring compliance with organizational policies\n- Responding to incidents or unauthorized actions\n\n**Use Cases**\n\n- - Auditing infrastructure-level changes outside the Databricks workspace.\n- Monitoring provisioning delays or resource availability.\n\n**Best Practices**\n\n- - Use Activity Logs in conjunction with other logs for full-stack visibility.\n- Set up alerts for critical infrastructure events.\n- Review logs regularly to ensure compliance and operational health.\n\n#### **Approach 4: Azure Monitor VM Insights**\n\nAzure Databricks cluster nodes run on Azure virtual machines (VMs), and their infrastructure-level performance can be monitored using [Azure Monitor VM Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-overview) (formerly OMS). This approach provides visibility into resource utilization across individual cluster VMs, helping identify bottlenecks that may affect Spark job performance or overall workload efficiency.\n\n**Configuration Overview:**To enable VM performance monitoring:\n\n- - [Enable VM Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-enable?tabs=portal) on the Databricks cluster for VMs.\n\n**Monitored Metrics:** Once enabled, VM Insights collects: [CPU usage, Memory consumption, Disk I/O, Network throughput, Process-level statistics.](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-performance) These metrics help assess whether Spark workloads are constrained by infrastructure limits, such as insufficient memory or high disk latency.\n\n**Considerations**\n\n- - This is a standard Azure VM monitoring technique and is not specific to Databricks.\n- Use role-based access control (RBAC) to secure access to performance data.\n\n#### **Approach 5: Virtual Network Flow Logs**\n\nFor Azure Databricks workspaces deployed in a custom Azure Virtual Network (VNet-injected mode), enabling [Virtual Network Flow Logs](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-overview?tabs=Americas) provides deep visibility into IP traffic flowing through the virtual network. These logs help monitor and optimize resources or support large enterprises that are trying to detect intrusion, flow logs can help. Review common use cases here: [Vnet Flow Logs Common Usecases](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-overview?tabs=Americas#common-use-cases) and how logging works here: [Key properties of virtual network flow logs.](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-overview?tabs=Americas#how-logging-works) Follow these steps to setup Vnet Flow Logs: [Create a flow log](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-manage?tabs=portal#create-a-flow-log)\n\n**Configuration Overview**\n\n- - Virtual Network Flow Logs are a feature of Azure Network Watcher.\n- Optionally, logs can be analyzed using Traffic Analytics for deeper insights.\n\n**These logs help identify:**\n\n- - Unexpected or unauthorized traffic\n- Bandwidth usage patterns\n- Effectiveness of NSG rules and network segmentation\n\n**Considerations**\n\n- - NSG flow logging is only available for VNet-injected deployment modes.\n- Ensure Network Watcher is enabled in the region where the Databricks workspace is deployed.\n- Use Traffic Analytics to visualize trends and detect anomalies in network flows.\n\n#### **Approach 6: Spark Monitoring Logging & Metrics**\n\nThe **spark-monitoring** library is a Python toolkit designed to interact with the Spark History Server REST API. Its main purpose is to help users programmatically access, analyze, and visualize Spark application metrics and job details after execution. Here’s what it offers:\n\n-\n\n- - **Application Listing:** Retrieve a list of all Spark applications available on the History Server, including metadata such as application ID, name, start/end time, and status.\n- **Job and Stage Details:** Access detailed information about jobs and stages within each application, including execution times, status, and resource usage.\n- **Task** **Metrics:** Extract metrics for individual tasks, such as duration, input/output size, and shuffle statistics, supporting performance analysis and bottleneck identification.\n\n**Considerations**\n\n- - The Spark Monitoring Library must be installed, see [Git Repository here.](https://github.com/bliseng/spark-monitoring)\n- Metrics can be exported to external observability platforms for long-term retention and alerting.\n\n**Use cases**\n\n- - Automated reporting of Spark job performance and resource usage\n- Batch analysis of completed Spark applications\n- Integration of Spark metrics into external dashboards or monitoring systems\n- Post-execution troubleshooting and optimization\n\n### **Internal Databricks Logging**\n\n#### **Approach 7: Databricks System Tables (Unity Catalog)**\n\n[Databricks System Tables](https://docs.databricks.com/aws/en/admin/system-tables/) are a recent addition to Azure Databricks observability, offering structured, SQL-accessible insights into workspace usage, performance, and cost. These tables reside in the Unity Catalog and are organized into schemas such as system.billing, system.lakeflow, and system.compute. You can enable System Tables through these steps: [_enable_system_tables - Databricks](https://notebooks.databricks.com/demos/uc-04-system-tables/_enable_system_tables.html)\n\n**Overview and Capabilities**\n\nWhen enabled by an administrator, system tables allow users to query operational metadata directly using SQL. Examples include:\n\n- - [system.billing.usage](https://docs.databricks.com/aws/en/admin/system-tables/billing): Tracks compute usage (CPU core-hours, memory) per job.\n- [system.compute.clusters](https://docs.databricks.com/aws/en/admin/system-tables/compute): Captures cluster lifecycle events.\n- [system.lakeflow.job\\_run:](https://docs.databricks.com/aws/en/admin/system-tables/jobs) Provides job execution details.\n\n**Use Cases**\n\n- - [Cost Monitoring:](https://docs.databricks.com/aws/en/admin/usage/system-tables) Aggregate usage records to identify high-cost jobs or users.\n- Import pre-built usage dashboards to your workspaces to monitor account- and workspace-level usage: [Usage dashboards](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/usage) and [Create and monitor budgets](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/budgets)\n- Operational Efficiency: Track job durations, cluster concurrency, and resource utilization.\n- In-Platform BI: Build dashboards in Databricks SQL to visualize usage trends without relying on external billing tools.\n\n**Best Practices**\n\n- - Schedule regular queries to track cost trends, job performance, and resource usage.\n- Apply role-based access control to restrict sensitive usage data.\n- Integrate system table insights into Databricks SQL dashboards for real-time visibility.\n\n#### **Approach 8: Data Quality Monitoring**\n\n[Data Quality](https://www.databricks.com/product/machine-learning/lakehouse-monitoring)Monitoring is a native Azure Databricks feature designed to track data quality and machine learning model performance over time. It enables automated monitoring of Delta tables and ML inference outputs, helping teams detect anomalies, data drift, and reliability issues directly within the Databricks environment. Follow these steps to [enable Data Quality Monitoring](https://learn.microsoft.com/en-us/azure/databricks/data-quality-monitoring/data-profiling/create-monitor-ui). Data Quality Monitoring supports three profile types:\n\n- - **Time Series****:** Monitors time-partitioned data, computing metrics per time window.\n- **Inference**: Tracks prediction drift and anomalies in model request/response logs.\n- **Snapshot****:** Performs full-table scans to compute metrics across the entire dataset.\n- From the enabling Lakehouse Monitoring, on step 5 you can also enable data profiling to view [Data Profiling Dashboards](https://learn.microsoft.com/en-us/azure/databricks/data-quality-monitoring/data-profiling/monitor-dashboard).\n\n**Use Cases**\n\n- - Data Quality Monitoring: Track null values, column distributions, and schema changes.\n- Model Performance Monitoring: Detect concept drift, prediction anomalies, and accuracy degradation.\n- Operational Reliability: Ensure consistent data pipelines and ML inference behavior.\n\n#### **Approach 9: Databricks SQL Dashboards and Alerts**\n\nDatabricks SQL [Dashboards](https://docs.databricks.com/aws/en/dashboards/)and [Alerts](https://learn.microsoft.com/en-us/azure/databricks/sql/user/alerts/)provide in-platform observability for operational monitoring, enabling teams to visualize metrics and receive notifications based on SQL query results. This approach complements infrastructure-level monitoring by focusing on application-level conditions, data correctness, and workflow health. Users can build dashboards using Databricks SQL or SQL Warehouses by querying: System tables (e.g., job runs, billing usage), Data Quality Monitoring metric tables, Custom operational datasets. You can create alerts through these steps: [Databricks SQL alerts.](https://learn.microsoft.com/en-us/azure/databricks/sql/user/alerts/#create-alert)\n\n**Alerting Features****:** Databricks SQL supports alerting on query results, allowing users to define conditions that trigger notifications via: Email, Slack (via webhook integration). Alerts can be configured for scenarios such as:\n\n- - Job failure counts exceeding thresholds\n- Row count drops in critical tables\n- Cost/Workload spikes or resource usage anomalies\n\n**Considerations**\n\n- - Alerts are query-driven and run on a schedule; ensure queries are optimized for performance.\n- Dashboards and alerts are workspace-specific and require appropriate permissions.\n\n**Best Practices**\n\n- - Use system tables and Data Quality Monitoring metrics as data sources for dashboards.\n- Schedule alerts to run at appropriate intervals (e.g., hourly for job failures).\n- Combine internal alerts with external monitoring for full-stack coverage.\n\n#### **Approach 10: Custom Tags for Workspace-Level Assets**\n\n[Custom tags](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/usage-detail-tags#custom-tags) allow organizations to classify and organize Databricks resources (clusters, jobs, pools, notebooks) for better governance, cost tracking, and observability. Tags are key-value pairs applied at the resource level and can be propagated to Azure for billing and monitoring.\n\n**Why Use Custom Tags?**\n\n- - Cost Attribution: Assign tags like Environment=Prod, Project=HealthcareAnalytics to track costs in Azure Cost Management.\n- Governance: Enforce policies based on tags (e.g., restrict high-cost clusters to Environment=Dev).\n- Observability: Filter logs and metrics by tags for dashboards and alerts.\n\n**Taggable Assets**\n\n- - Clusters: Apply tags during cluster creation via the Databricks UI or REST API.\n- Jobs: Include tags in job configurations for workload-level tracking.\n- Instance Pools: Tag pools to manage shared compute resources.\n- Notebooks & Workflows: Use tags in metadata for classification and reporting.\n\n**Best Practices**\n\n- - Define a standard tag taxonomy (e.g., Environment, Owner, CostCenter, Compliance).\n- Validate tags regularly to ensure consistency across workspaces.\n- Use tags in Log Analytics queries for cost and performance dashboards.\n\nUpdated Dec 07, 2025\n\nVersion 1.0\n\n[analytics](/tag/analytics?nodeId=board%3AAnalyticsonAzure)\n\n[azure](/tag/azure?nodeId=board%3AAnalyticsonAzure)\n\n[azure databricks](/tag/azure%20databricks?nodeId=board%3AAnalyticsonAzure)\n\n[!\\[Rafia_Aqil&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0zMDcyNDQwLUZEQmYzMQ?image-coordinates=60%2C75%2C544%2C559&amp;image-dimensions=50x50)](/users/rafia_aqil/3072440) [Rafia_Aqil](/users/rafia_aqil/3072440) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined June 13, 2025\n\n[View Profile](/users/rafia_aqil/3072440)\n\n/category/azure/blog/analyticsonazure [Analytics on Azure Blog](/category/azure/blog/analyticsonazure) Follow this blog board to get notified when there's new activity",
  "Description": "> >\n> ***Author's:*** Amudha Palani, Peter Lo [PeterLo​](javascript:void%280%29) , Rafia Aqil [Rafia_Aqil​](javascript:void%280%29)\n> >\n\nObservability in Azure Databricks is the ability to continuously monitor and troubleshoot the health, performance, and usage of data workloads by capturing metrics, logs, and traces. In a structured observability approach, we consider two broad categories of logging: Internal Databricks Logging (within the Databricks environment) and External Databricks Logging (leveraging Azure services). Each plays a distinct role in providing insights.\n\nBy combining internal and external observability mechanisms, organizations can achieve a comprehensive view: internal logs enable detailed analysis of Spark jobs and data quality, while external logs ensure global visibility, auditing, and integration with broader monitoring dashboards and alerting systems. The article is organized into two main sections:\n\n- Infrastructure Logging for Azure Databricks (external observability)\n- Internal Databricks Logging (in-platform observability)\n\n### **Considerations**\n\nAddressing key questions upfront ensures your observability strategy is tailored to your organization’s unique workloads, risk profile, and operational needs. By proactively evaluating what to monitor, where to store logs, and who needs access, you can avoid blind spots, streamline incident response, and align monitoring investments with business priorities.\n\n1. **What types of workloads are running in Databricks?**\n\n- - **Why it matters****:** Different workloads (e.g., batch ETL, streaming pipelines, ML training, interactive notebooks) have distinct performance profiles and failure modes.\n\n- - **Business impact****:** Understanding workload types helps prioritize monitoring for mission-critical processes like real-time fraud detection or daily financial reporting.\n\n1. **What failure scenarios need to be monitored?**\n\n- - **Examples****:** Job failures, cluster provisioning errors, quota limits, authentication issues.\n\n- - **Business impact****:** Early detection of failures reduces downtime, improves SLA adherence, and prevents data pipeline disruptions that could affect reporting or customer-facing analytics.\n\n1. **Where should logs be stored and analyzed?**\n\n- - **Options****:** Centralized Log Analytics workspace, Azure Storage for archival, Event Hub for streaming analysis.\n\n- - **Business impact****:** Centralized logging enables unified dashboards, cross-team visibility, and faster incident response across data engineering, operations, and compliance teams.\n\n1. **Who needs access to logs and alerts?**\n\n- - **Stakeholders****:** Data engineers, platform administrators, security analysts, compliance officers.\n\n- - **Business impact****:** Role-based access ensures that the right teams can act on insights while maintaining data governance and privacy controls.\n\n### **Infrastructure Logging for Azure Databricks**\n\n#### **Approach 1: Diagnostic Settings for Azure Databricks**\n\n[Diagnostic settings](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/diagnostic-settings?WT.mc_id=Portal-Microsoft_Azure_Monitoring&tabs=portal#create-a-diagnostic-setting)in Azure Monitor allow you to capture detailed logs and metrics from your Azure Databricks workspace, supporting operational monitoring, troubleshooting, and compliance. By configuring diagnostic settings at the workspace level, administrators can route Databricks logs—including cluster events, job statuses, and audit logs—to destinations such as Log Analytics, Azure Storage, or Event Hub. This enables unified analysis, alerting, and long-term retention of critical operational data.\n\n**Configuration Overview**\n\n- - [Enable Diagnostic Settings](https://docs.azure.cn/en-us/databricks/admin/account-settings/audit-log-delivery)on the Databricks workspace to route logs to [Log Analytics Workspace](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-tutorial).\n- Logs can also be combined with other logs mentioned below for full Azure Databricks observability.\n- Here is a Guide to Azure Databricks Diagnostic Settings Log Reference: [Configure diagnostic log delivery](https://docs.azure.cn/en-us/databricks/admin/account-settings/audit-log-delivery)\n- [Implement tagging strategy](https://docs.azure.cn/en-us/databricks/admin/account-settings/usage-detail-tags)-organizations can gain granular visibility into resource consumption and align spending with business priorities.\n\n- **Default tags:** Automatically applied by Databricks to cloud-deployed resources.\n- **Custom tags:** User-defined tags that you can add to compute resources and serverless workloads.\n\n**Use Cases**\n\n- - Operational Monitoring: Detect job or resource bottlenecks.\n- Security & Compliance: Audit user actions and enforce governance policies.\n- Incident Response: Correlate Databricks logs with infrastructure events for faster troubleshooting.\n\n**Best Practices**\n\n- - Enable only relevant log categories to optimize cost and performance.\n- Use role-based access control (RBAC) to secure access to logs.\n\n![]()\n\n#### **Approach 2: Azure Databricks Compute Log Delivery**\n\nCompute log delivery in Azure Databricks enables you to automatically collect and archive logs from Spark driver nodes, worker nodes, and cluster events for both all-purpose and job compute resources. When you create a cluster, you can specify a log delivery location—such as DBFS, Azure Storage, or a Unity Catalog volume—where logs are delivered every five minutes and archived hourly. All logs generated up until the compute resource is terminated are guaranteed to be delivered, supporting troubleshooting, auditing, and compliance.\n\n**Configure**: To configure the log delivery location:\n\n1. 1. On the compute page, click the Advanced toggle.\n2. Click the Logging tab.\n3. Select a destination type: DBFS or Volumes (*Preview*).\n4. Enter the Log path.\n5. To store the logs, Databricks creates a subfolder in your chosen log path named after the compute's **cluster\\_id**.\n\n![]()\n\n#### **Approach 3: Azure Activity Logs**\n\nWhenever you create, update, or delete Databricks resources (such as provisioning a new workspace, scaling a cluster, or modifying network settings), these actions are captured in the [Activity Log](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/activity-log?tabs=log-analytics). This enables teams to track who made changes, when, and what impact those changes had on the environment. Each event in the Activity Log has a particular category that is described in the following document: [Azure Activity Log event schema](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/activity-log-schema#categories). For Databricks, this is especially valuable for:\n\n- - Auditing resource deployments and configuration changes\n- Investigating failed provisioning or quota errors\n- Monitoring compliance with organizational policies\n- Responding to incidents or unauthorized actions\n\n**Use Cases**\n\n- - Auditing infrastructure-level changes outside the Databricks workspace.\n- Monitoring provisioning delays or resource availability.\n\n**Best Practices**\n\n- - Use Activity Logs in conjunction with other logs for full-stack visibility.\n- Set up alerts for critical infrastructure events.\n- Review logs regularly to ensure compliance and operational health.\n\n![]()\n\n#### **Approach 4: Azure Monitor VM Insights**\n\nAzure Databricks cluster nodes run on Azure virtual machines (VMs), and their infrastructure-level performance can be monitored using [Azure Monitor VM Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-overview) (formerly OMS). This approach provides visibility into resource utilization across individual cluster VMs, helping identify bottlenecks that may affect Spark job performance or overall workload efficiency.\n\n**Configuration Overview:** To enable VM performance monitoring:\n\n- - [Enable VM Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-enable?tabs=portal) on the Databricks cluster for VMs.\n\n**Monitored Metrics:** Once enabled, VM Insights collects: [CPU usage, Memory consumption, Disk I/O, Network throughput, Process-level statistics.](https://learn.microsoft.com/en-us/azure/azure-monitor/vm/vminsights-performance) These metrics help assess whether Spark workloads are constrained by infrastructure limits, such as insufficient memory or high disk latency.\n\n![]()\n\n**Considerations**\n\n- - This is a standard Azure VM monitoring technique and is not specific to Databricks.\n- Use role-based access control (RBAC) to secure access to performance data.\n\n#### **Approach 5: Virtual Network Flow Logs**\n\nFor Azure Databricks workspaces deployed in a custom Azure Virtual Network (VNet-injected mode), enabling [Virtual Network Flow Logs](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-overview?tabs=Americas) provides deep visibility into IP traffic flowing through the virtual network. These logs help monitor and optimize resources or support large enterprises that are trying to detect intrusion, flow logs can help. Review common use cases here: [Vnet Flow Logs Common Usecases](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-overview?tabs=Americas#common-use-cases) and how logging works here: [Key properties of virtual network flow logs.](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-overview?tabs=Americas#how-logging-works) Follow these steps to setup Vnet Flow Logs: [Create a flow log](https://learn.microsoft.com/en-us/azure/network-watcher/vnet-flow-logs-manage?tabs=portal#create-a-flow-log)\n\n**Configuration Overview**\n\n- - Virtual Network Flow Logs are a feature of Azure Network Watcher.\n- Optionally, logs can be analyzed using Traffic Analytics for deeper insights.\n\n**These logs help identify:**\n\n- - Unexpected or unauthorized traffic\n- Bandwidth usage patterns\n- Effectiveness of NSG rules and network segmentation\n\n**Considerations**\n\n- - NSG flow logging is only available for VNet-injected deployment modes.\n- Ensure Network Watcher is enabled in the region where the Databricks workspace is deployed.\n- Use Traffic Analytics to visualize trends and detect anomalies in network flows.\n\n![]()\n\n#### **Approach 6: Spark Monitoring Logging & Metrics**\n\nThe **spark-monitoring** library is a Python toolkit designed to interact with the Spark History Server REST API. Its main purpose is to help users programmatically access, analyze, and visualize Spark application metrics and job details after execution. Here’s what it offers:\n\n-\n\n- - **Application Listing:** Retrieve a list of all Spark applications available on the History Server, including metadata such as application ID, name, start/end time, and status.\n- **Job and Stage Details:** Access detailed information about jobs and stages within each application, including execution times, status, and resource usage.\n- **Task** **Metrics:** Extract metrics for individual tasks, such as duration, input/output size, and shuffle statistics, supporting performance analysis and bottleneck identification.\n\n**Considerations**\n\n- - The Spark Monitoring Library must be installed, see [Git Repository here.](https://github.com/bliseng/spark-monitoring)\n- Metrics can be exported to external observability platforms for long-term retention and alerting.\n\n**Use cases**\n\n- - Automated reporting of Spark job performance and resource usage\n- Batch analysis of completed Spark applications\n- Integration of Spark metrics into external dashboards or monitoring systems\n- Post-execution troubleshooting and optimization\n\n### **Internal Databricks Logging**\n\n#### **Approach 7: Databricks System Tables (Unity Catalog)**\n\n[Databricks System Tables](https://docs.databricks.com/aws/en/admin/system-tables/) are a recent addition to Azure Databricks observability, offering structured, SQL-accessible insights into workspace usage, performance, and cost. These tables reside in the Unity Catalog and are organized into schemas such as system.billing, system.lakeflow, and system.compute. You can enable System Tables through these steps: [_enable_system_tables - Databricks](https://notebooks.databricks.com/demos/uc-04-system-tables/_enable_system_tables.html)\n\n**Overview and Capabilities**\n\nWhen enabled by an administrator, system tables allow users to query operational metadata directly using SQL. Examples include:\n\n- - [system.billing.usage](https://docs.databricks.com/aws/en/admin/system-tables/billing): Tracks compute usage (CPU core-hours, memory) per job.\n- [system.compute.clusters](https://docs.databricks.com/aws/en/admin/system-tables/compute): Captures cluster lifecycle events.\n- [system.lakeflow.job\\_run:](https://docs.databricks.com/aws/en/admin/system-tables/jobs) Provides job execution details.\n\n**Use Cases**\n\n- - [Cost Monitoring:](https://docs.databricks.com/aws/en/admin/usage/system-tables) Aggregate usage records to identify high-cost jobs or users.\n- Import pre-built usage dashboards to your workspaces to monitor account- and workspace-level usage: [Usage dashboards](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/usage) and [Create and monitor budgets](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/budgets)\n- Operational Efficiency: Track job durations, cluster concurrency, and resource utilization.\n- In-Platform BI: Build dashboards in Databricks SQL to visualize usage trends without relying on external billing tools.\n\n**Best Practices**\n\n- - Schedule regular queries to track cost trends, job performance, and resource usage.\n- Apply role-based access control to restrict sensitive usage data.\n- Integrate system table insights into Databricks SQL dashboards for real-time visibility.\n\n![]()\n\n#### **Approach 8: Data Quality Monitoring**\n\n[Data Quality](https://www.databricks.com/product/machine-learning/lakehouse-monitoring)Monitoring is a native Azure Databricks feature designed to track data quality and machine learning model performance over time. It enables automated monitoring of Delta tables and ML inference outputs, helping teams detect anomalies, data drift, and reliability issues directly within the Databricks environment. Follow these steps to [enable Data Quality Monitoring](https://learn.microsoft.com/en-us/azure/databricks/data-quality-monitoring/data-profiling/create-monitor-ui). Data Quality Monitoring supports three profile types:\n\n- - **Time Series****:** Monitors time-partitioned data, computing metrics per time window.\n- **Inference**: Tracks prediction drift and anomalies in model request/response logs.\n- **Snapshot****:** Performs full-table scans to compute metrics across the entire dataset.\n- From the enabling Lakehouse Monitoring, on step 5 you can also enable data profiling to view [Data Profiling Dashboards](https://learn.microsoft.com/en-us/azure/databricks/data-quality-monitoring/data-profiling/monitor-dashboard).\n\n**Use Cases**\n\n- - Data Quality Monitoring: Track null values, column distributions, and schema changes.\n- Model Performance Monitoring: Detect concept drift, prediction anomalies, and accuracy degradation.\n- Operational Reliability: Ensure consistent data pipelines and ML inference behavior.\n\n![]()\n\n#### **Approach 9: Databricks SQL Dashboards and Alerts**\n\nDatabricks SQL [Dashboards](https://docs.databricks.com/aws/en/dashboards/)and [Alerts](https://learn.microsoft.com/en-us/azure/databricks/sql/user/alerts/)provide in-platform observability for operational monitoring, enabling teams to visualize metrics and receive notifications based on SQL query results. This approach complements infrastructure-level monitoring by focusing on application-level conditions, data correctness, and workflow health. Users can build dashboards using Databricks SQL or SQL Warehouses by querying: System tables (e.g., job runs, billing usage), Data Quality Monitoring metric tables, Custom operational datasets. You can create alerts through these steps: [Databricks SQL alerts.](https://learn.microsoft.com/en-us/azure/databricks/sql/user/alerts/#create-alert)\n\n**Alerting Features****:** Databricks SQL supports alerting on query results, allowing users to define conditions that trigger notifications via: Email, Slack (via webhook integration). Alerts can be configured for scenarios such as:\n\n- - Job failure counts exceeding thresholds\n- Row count drops in critical tables\n- Cost/Workload spikes or resource usage anomalies\n\n**Considerations**\n\n- - Alerts are query-driven and run on a schedule; ensure queries are optimized for performance.\n- Dashboards and alerts are workspace-specific and require appropriate permissions.\n\n**Best Practices**\n\n- - Use system tables and Data Quality Monitoring metrics as data sources for dashboards.\n- Schedule alerts to run at appropriate intervals (e.g., hourly for job failures).\n- Combine internal alerts with external monitoring for full-stack coverage.\n\n![]()\n\n#### **Approach 10: Custom Tags for Workspace-Level Assets**\n\n[Custom tags](https://learn.microsoft.com/en-us/azure/databricks/admin/account-settings/usage-detail-tags#custom-tags) allow organizations to classify and organize Databricks resources (clusters, jobs, pools, notebooks) for better governance, cost tracking, and observability. Tags are key-value pairs applied at the resource level and can be propagated to Azure for billing and monitoring.\n\n**Why Use Custom Tags?**\n\n- - Cost Attribution: Assign tags like Environment=Prod, Project=HealthcareAnalytics to track costs in Azure Cost Management.\n- Governance: Enforce policies based on tags (e.g., restrict high-cost clusters to Environment=Dev).\n- Observability: Filter logs and metrics by tags for dashboards and alerts.\n\n**Taggable Assets**\n\n- - Clusters: Apply tags during cluster creation via the Databricks UI or REST API.\n- Jobs: Include tags in job configurations for workload-level tracking.\n- Instance Pools: Tag pools to manage shared compute resources.\n- Notebooks & Workflows: Use tags in metadata for classification and reporting.\n\n**Best Practices**\n\n- - Define a standard tag taxonomy (e.g., Environment, Owner, CostCenter, Compliance).\n- Validate tags regularly to ensure consistency across workspaces.\n- Use tags in Log Analytics queries for cost and performance dashboards.\n\n![]()",
  "OutputDir": "_community",
  "Link": "https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/end-to-end-observability-for-azure-databricks-from/ba-p/4475692",
  "FeedLevelAuthor": "rss.livelink.threads-in-node"
}
