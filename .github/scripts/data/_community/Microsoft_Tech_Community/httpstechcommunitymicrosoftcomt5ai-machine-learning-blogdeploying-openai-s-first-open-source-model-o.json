{
  "OutputDir": "_community",
  "Link": "https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/deploying-openai-s-first-open-source-model-on-azure-aks-with/ba-p/4444234",
  "Title": "Deploying OpenAI’s First Open-Source Model on Azure AKS with KAITO",
  "ProcessedDate": "2025-08-15 17:13:29",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Community",
  "PubDate": "2025-08-15T17:08:39+00:00",
  "Description": "**Special Thanks** Thanks to **Andrew Thomas**, **Kurt Niebuhr**, and **Sachi Desai** for their invaluable support, insightful discussions, and for providing the compute resources that made testing this deployment possible. Your contributions were essential in bringing this project to life.**Introduction**\n\nOpenAI recently released **GPT-OSS**, its first open-source large language model. With the rise of high-performance GPUs in the cloud, running advanced AI inference workloads has become easier than ever.\n\nMicrosoft Azure’s **AKS (Azure Kubernetes Service)** paired with **KAITO** (Kubernetes AI Toolchain Operator) provides a powerful, scalable environment for deploying such models. KAITO simplifies provisioning GPU nodes, managing inference workloads, and integrating AI-optimized runtimes like **vLLM**.\n\nIn this tutorial, we’ll walk through deploying the **openai/gpt-oss-20B** model on Azure’s **Standard\\_NV36ads\\_A10\\_v5** GPU instances using **vLLM** for fast inference — all running on AKS via KAITO. By the end, you’ll have a public endpoint where you can send API requests in OpenAI’s format.\n\n### **Step-by-Step Deployment**\n\nBefore you begin, ensure you have an active **Azure subscription** with permissions to create resource groups, virtual networks, and AKS clusters. You’ll need to request and be approved for **NVIDIA GPU quotas** in your target region — specifically for **Standard\\_NVads\\_A10\\_v5** or similar GPU SKUs. Basic familiarity with Kubernetes (kubectl) and Azure CLI will help you follow the steps smoothly.\n\n### **1. Set Up Environment Variables**\n\nWe’ll define reusable variables for resource naming and region.\n\n- export RANDOM\\_ID=\"33000\"\nexport REGION=\"swedencentral\" export AZURE\\_RESOURCE\\_GROUP=\"myKaitoResourceGroup$RANDOM\\_ID\" export CLUSTER\\_NAME=\"myClusterName$RANDOM\\_ID\"\n\n### **2. Create Resource Group**\n- az group create \\\n--name $AZURE\\_RESOURCE\\_GROUP \\ --location $REGION\n\n### **3. Install & Enable AKS Preview Features**\n\nKAITO is currently in preview and requires the aks-preview extension and AI Toolchain feature.\n- az extension add --name aks-preview\naz extension update --name aks-preview\n\naz feature register \\ --namespace \"Microsoft.ContainerService\" \\ --name \"AIToolchainOperatorPreview\"\n\n(It may take several minutes for the feature to register — you can check status with az feature list.)\n\n### **4. Create AKS Cluster with AI Toolchain Operator**\n- az aks create \\\n--location $REGION \\ --resource-group $AZURE\\_RESOURCE\\_GROUP \\ --name $CLUSTER\\_NAME \\ --node-count 1 \\ --enable-ai-toolchain-operator \\ --enable-oidc-issuer \\ --generate-ssh-keys\n\n### **5. Connect kubectl to the Cluster**\n- az aks get-credentials \\\n--resource-group ${AZURE\\_RESOURCE\\_GROUP} \\ --name ${CLUSTER\\_NAME}\n\n### **6. Create KAITO Workspace for GPT-OSS with vLLM**\n\nSave the following to workspace-gptoss.yaml:\n- apiVersion: kaito.sh/v1alpha1\nkind: Workspace metadata: name: workspace-gpt-oss-vllm-nv-a10 resource: instanceType: \"Standard\\_NV36ads\\_A10\\_v5\" count: 1 labelSelector: matchLabels: app: gpt-oss-20b-vllm inference: template: spec: containers:\n- name: vllm-openai\nimage: vllm/vllm-openai:gptoss imagePullPolicy: IfNotPresent args:\n- --model\n- openai/gpt-oss-20b\n- --swap-space\n- \"4\"\n- --gpu-memory-utilization\n- \"0.85\"\n- --port\n- \"5000\"\nports:\n- name: http\ncontainerPort: 5000 resources: limits: nvidia.com/gpu: 1 cpu: \"36\" memory: \"440Gi\" requests: nvidia.com/gpu: 1 cpu: \"18\" memory: \"220Gi\" readinessProbe: httpGet: path: /health port: 5000 initialDelaySeconds: 30 periodSeconds: 10 livenessProbe: httpGet: path: /health port: 5000 initialDelaySeconds: 600 periodSeconds: 20 env: #special configs for A10 gpu\n- name: VLLM\\_ATTENTION\\_BACKEND\nvalue: \"TRITON\\_ATTN\\_VLLM\\_V1\"\n- name: VLLM\\_DISABLE\\_SINKS\nvalue: \"1\"\n\nApply it:\n- kubectl apply -f workspace-gptoss.yaml\n\n### **7. Expose the Service Publicly**\n\n#### Expose via LoadBalancer\n- kubectl expose deployment workspace-gpt-oss-vllm-nv-a10 \\\n--type=LoadBalancer \\ --name=workspace-gpt-oss-vllm-nv-a10-pub\n\nCheck the IP:\n- kubectl get svc workspace-gpt-oss-vllm-nv-a10-pub\n\n### **8. Test the Endpoint**\n\nOnce the LoadBalancer IP is ready:\n- export CLUSTERIP=\nkubectl run -it --rm --restart=Never curl \\ --image=curlimages/curl -- \\ curl -X POST http://$CLUSTERIP/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"openai/gpt-oss-20b\", \"messages\": [{\"role\": \"user\", \"content\": \"What is Kubernetes?\"}], \"max\\_tokens\": 50, \"temperature\": 0 }'\n\nUsing OpenAI python SDK:\n- from openai import OpenAI\n\nclient = OpenAI( base\\_url=\"http://:5000/v1/\", api\\_key=\"EMPTY\" )\n\nresult = client.chat.completions.create( model=\"openai/gpt-oss-20b\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"} ] )\n\nprint(result.choices[0].message)\n\n### **8.** **Load Testing the GPT-OSS Endpoint**\n\nTo evaluate real-world performance, we used the [llm-load-test-azure](https://github.com/maljazaery/llm-load-test-azure) tool, which is designed for stress-testing Azure-hosted LLM endpoints.\n\n- **Input Tokens per Request:** 250\n- **Output Tokens per Request:** ~1,500 (target)\n- **Test Duration:** ~10 minutes\n- **Concurrency:** 10\n- **Model:** 20B\n- **GPU:** A10\n\n| Metric | Value | Description | | --- | --- | --- | | **TT\\_ACK** | 0.60 s | Time to acknowledge the request | | **TTFT** | 0.77 s | Time to first token | | **ITL** | 0.038 s | Idle time per request | | **TPOT** | 0.039 s | Time per output token | | **Avg. Response Time** | 57.77 s | Total latency for a single request | | **Output Tokens Throughput** | 257.77 tokens/sec | Avg output tokens per seconds |\n\n###### **Performance Comparison - Concurrency 10 vs 5:**\n\nThere is always a trade between latency and throughput- By reducing the target concurrency from 10 to 5, we can achieve lower response time. The choice between the two depends on whether lower latency per request or higher overall throughput is the priority for your workload.\n\n| Metric | Concurrency 10 | Concurrency 5 | Observation | | --- | --- | --- | --- | | **Average Response Time** | 57.77 s | 45.80 s | Lower concurrency reduces per-request latency by ~21% | | **Output Tokens per Second** | 257.77 | 162.75 | Higher concurrency yields higher total throughput | | **Completed Requests/sec** | 0.173 | 0.109 | Concurrency 10 processes ~58% more requests per second |\n\n```\n\n```\n\n### **Conclusion**\n\nWith these steps, we’ve successfully:\n\n- Provisioned an Azure AKS cluster with NVIDIA A10 GPUs\n- Installed KAITO to manage AI workloads\n- Deployed the **openai/gpt-oss-20B** model with **vLLM** for fast inference\n- Exposed it to the internet using a LoadBalancer\n- Sent an OpenAI-style API request to our own hosted model\n\nThis setup can be scaled by increasing GPU count, tuning vLLM parameters, or integrating with your existing application pipelines.",
  "FeedName": "Microsoft Tech Community",
  "Author": "maljazaery",
  "Tags": [],
  "EnhancedContent": "## In this tutorial, we’ll walk through deploying the openai/gpt-oss-20B model on Azure’s Standard\\_NV36ads\\_A10\\_v5 GPU instances using vLLM for fast inference — all running on AKS via KAITO. By the end, you’ll have a public endpoint where you can send API requests in OpenAI’s format.\n\n**Special Thanks** Thanks to **Andrew Thomas**, **Kurt Niebuhr**, and **Sachi Desai** for their invaluable support, insightful discussions, and for providing the compute resources that made testing this deployment possible. Your contributions were essential in bringing this project to life.**Introduction**\n\nOpenAI recently released **GPT-OSS**, its first open-source large language model. With the rise of high-performance GPUs in the cloud, running advanced AI inference workloads has become easier than ever.\n\nMicrosoft Azure’s **AKS (Azure Kubernetes Service)** paired with **KAITO** (Kubernetes AI Toolchain Operator) provides a powerful, scalable environment for deploying such models. KAITO simplifies provisioning GPU nodes, managing inference workloads, and integrating AI-optimized runtimes like **vLLM**.\n\nIn this tutorial, we’ll walk through deploying the **openai/gpt-oss-20B** model on Azure’s **Standard\\_NV36ads\\_A10\\_v5** GPU instances using **vLLM** for fast inference — all running on AKS via KAITO. By the end, you’ll have a public endpoint where you can send API requests in OpenAI’s format.\n\n### **Step-by-Step Deployment**\n\nBefore you begin, ensure you have an active **Azure subscription** with permissions to create resource groups, virtual networks, and AKS clusters. You’ll need to request and be approved for **NVIDIA GPU quotas** in your target region — specifically for **Standard\\_NVads\\_A10\\_v5** or similar GPU SKUs. Basic familiarity with Kubernetes (kubectl) and Azure CLI will help you follow the steps smoothly.\n\n### **1. Set Up Environment Variables**\n\nWe’ll define reusable variables for resource naming and region.\n\n``` export RANDOM_ID=\"33000\" export REGION=\"swedencentral\" export AZURE_RESOURCE_GROUP=\"myKaitoResourceGroup$RANDOM_ID\" export CLUSTER_NAME=\"myClusterName$RANDOM_ID\"\n\n```\n\n### **2. Create Resource Group**\n\n``` az group create \\ --name $AZURE_RESOURCE_GROUP \\ --location $REGION\n\n```\n\n### **3. Install & Enable AKS Preview Features**\n\nKAITO is currently in preview and requires the aks-preview extension and AI Toolchain feature.\n\n``` az extension add --name aks-preview az extension update --name aks-preview\n\naz feature register \\ --namespace \"Microsoft.ContainerService\" \\ --name \"AIToolchainOperatorPreview\"\n\n```\n\n(It may take several minutes for the feature to register — you can check status with az feature list.)\n\n### **4. Create AKS Cluster with AI Toolchain Operator**\n\n``` az aks create \\ --location $REGION \\ --resource-group $AZURE_RESOURCE_GROUP \\ --name $CLUSTER_NAME \\ --node-count 1 \\ --enable-ai-toolchain-operator \\ --enable-oidc-issuer \\ --generate-ssh-keys\n\n```\n\n### **5. Connect kubectl to the Cluster**\n\n``` az aks get-credentials \\ --resource-group ${AZURE_RESOURCE_GROUP} \\ --name ${CLUSTER_NAME}\n\n```\n\n### **6. Create KAITO Workspace for GPT-OSS with vLLM**\n\nSave the following to workspace-gptoss.yaml:\n\n``` apiVersion: kaito.sh/v1alpha1 kind: Workspace metadata: name: workspace-gpt-oss-vllm-nv-a10 resource: instanceType: \"Standard_NV36ads_A10_v5\" count: 1 labelSelector: matchLabels: app: gpt-oss-20b-vllm inference: template: spec: containers:\n- name: vllm-openai\nimage: vllm/vllm-openai:gptoss imagePullPolicy: IfNotPresent args:\n- --model\n- openai/gpt-oss-20b\n- --swap-space\n- \"4\"\n- --gpu-memory-utilization\n- \"0.85\"\n- --port\n- \"5000\"\nports:\n- name: http\ncontainerPort: 5000 resources: limits: nvidia.com/gpu: 1 cpu: \"36\" memory: \"440Gi\" requests: nvidia.com/gpu: 1 cpu: \"18\" memory: \"220Gi\" readinessProbe: httpGet: path: /health port: 5000 initialDelaySeconds: 30 periodSeconds: 10 livenessProbe: httpGet: path: /health port: 5000 initialDelaySeconds: 600 periodSeconds: 20 env: #special configs for A10 gpu\n- name: VLLM_ATTENTION_BACKEND\nvalue: \"TRITON_ATTN_VLLM_V1\"\n- name: VLLM_DISABLE_SINKS\nvalue: \"1\"\n\n```\n\nApply it:\n\n``` kubectl apply -f workspace-gptoss.yaml\n\n```\n\n### **7. Expose the Service Publicly**\n\n#### Expose via LoadBalancer\n\n``` kubectl expose deployment workspace-gpt-oss-vllm-nv-a10 \\ --type=LoadBalancer \\ --name=workspace-gpt-oss-vllm-nv-a10-pub\n\n```\n\nCheck the IP:\n\n``` kubectl get svc workspace-gpt-oss-vllm-nv-a10-pub\n\n```\n\n### **8. Test the Endpoint**\n\nOnce the LoadBalancer IP is ready:\n\n``` export CLUSTERIP=<IP> kubectl run -it --rm --restart=Never curl \\ --image=curlimages/curl -- \\ curl -X POST http://$CLUSTERIP/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"openai/gpt-oss-20b\", \"messages\": [{\"role\": \"user\", \"content\": \"What is Kubernetes?\"}], \"max_tokens\": 50, \"temperature\": 0 }'\n\n```\n\nUsing OpenAI python SDK:\n\n``` from openai import OpenAI\n\nclient = OpenAI( base_url=\"http://<ip>:5000/v1/\", api_key=\"EMPTY\" )\n\nresult = client.chat.completions.create( model=\"openai/gpt-oss-20b\", messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Explain what MXFP4 quantization is.\"} ] )\n\nprint(result.choices[0].message)\n\n```\n\n### **8.** **Load Testing the GPT-OSS Endpoint**\n\nTo evaluate real-world performance, we used the [llm-load-test-azure](https://github.com/maljazaery/llm-load-test-azure) tool, which is designed for stress-testing Azure-hosted LLM endpoints.\n\n- **Input Tokens per Request:** 250\n- **Output Tokens per Request:** ~1,500 (target)\n- **Test Duration:** ~10 minutes\n- **Concurrency:** 10\n- **Model:** 20B\n- **GPU:** A10\n\n| Metric | Value | Description | | --- | --- | --- | | **TT\\_ACK** | 0.60 s | Time to acknowledge the request | | **TTFT** | 0.77 s | Time to first token | | **ITL** | 0.038 s | Idle time per request | | **TPOT** | 0.039 s | Time per output token | | **Avg. Response Time** | 57.77 s | Total latency for a single request | | **Output Tokens Throughput** | 257.77 tokens/sec | Avg output tokens per seconds |\n\n###### **Performance Comparison - Concurrency 10 vs 5:**\n\nThere is always a trade between latency and throughput- By reducing the target concurrency from 10 to 5, we can achieve lower response time. The choice between the two depends on whether lower latency per request or higher overall throughput is the priority for your workload.\n\n| Metric | Concurrency 10 | Concurrency 5 | Observation | | --- | --- | --- | --- | | **Average Response Time** | 57.77 s | 45.80 s | Lower concurrency reduces per-request latency by ~21% | | **Output Tokens per Second** | 257.77 | 162.75 | Higher concurrency yields higher total throughput | | **Completed Requests/sec** | 0.173 | 0.109 | Concurrency 10 processes ~58% more requests per second |\n\n```\n\n```\n\n### **Conclusion**\n\nWith these steps, we’ve successfully:\n\n- Provisioned an Azure AKS cluster with NVIDIA A10 GPUs\n- Installed KAITO to manage AI workloads\n- Deployed the **openai/gpt-oss-20B** model with **vLLM** for fast inference\n- Exposed it to the internet using a LoadBalancer\n- Sent an OpenAI-style API request to our own hosted model\n\nThis setup can be scaled by increasing GPU count, tuning vLLM parameters, or integrating with your existing application pipelines.\n\nUpdated Aug 15, 2025\n\nVersion 3.0\n\n[!\\[maljazaery&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yNDYwMjEzLTU4MDkyNGk3RTJCQjYyRUZDODYyNzUy?image-dimensions=50x50)](/users/maljazaery/2460213) [maljazaery](/users/maljazaery/2460213) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined May 07, 2024\n\n[View Profile](/users/maljazaery/2460213)\n\n/category/ai/blog/machinelearningblog [AI - Machine Learning Blog](/category/ai/blog/machinelearningblog) Follow this blog board to get notified when there's new activity"
}
