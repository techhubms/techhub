{
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "OutputDir": "_community",
  "Tags": [],
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Description": "### **TL;DR**\n\nWe’ll build a real-time AI app where **Azure OpenAI streams responses** and **SignalR broadcasts them live** to an Angular client. Users see answers appear incrementally just like ChatGPT while Azure SignalR Service handles scale. You’ll learn the architecture, streaming code, Angular integration, and optional enhancements like typing indicators and multi-agent scenarios.\n\n## Why This Matters\n\nModern users expect instant feedback. Waiting for a full AI response feels slow and breaks engagement. Streaming responses:\n\n- **Reduces perceived latency**: Users see content as it’s generated.\n- **Improves UX**: Mimics ChatGPT’s typing effect.\n- **Keeps users engaged**: Especially for long-form answers.\n- **Scales for enterprise**: Azure SignalR Service handles thousands of concurrent connections.\n\n## What you’ll build\n\n- A **SignalR Hub** that calls Azure OpenAI with streaming enabled and **forwards partial output** to clients as it arrives.\n- An **Angular** client that connects over **WebSockets/SSE** to the hub and **renders partial content** with a typing indicator.\n- An optional **Azure SignalR Service** layer for **scalable connection management** (thousands to millions of long‑lived connections).\n**References:** SignalR hosting & scale; Azure SignalR Service concepts.\n\n## Architecture\n\n![]()\n\n- The **hub** calls Azure OpenAI with **streaming** enabled (await foreach over updates) and **broadcasts** partials to clients.\n- **Azure SignalR Service** (optional) offloads connection scale and removes sticky‑session complexity in multi‑node deployments.\n**References:** Streaming code pattern; scale/ARR affinity; Azure SignalR integration.\n\n## Prerequisites\n\n- **Azure OpenAI** resource with a deployed model (e.g., gpt-4o or gpt-4o-mini)\n- **.NET 8** API + **ASP.NET Core SignalR** backend\n- **Angular 16+** frontend (using [microsoft​](javascript:void%280%29)/signalr)\n\n## Step‑by‑Step Implementation\n\n### 1) Backend: ASP.NET Core + SignalR\n\n**Install packages**\n\n- dotnet add package Microsoft.AspNetCore.SignalR dotnet add package Azure.AI.OpenAI --prerelease dotnet add package Azure.Identity dotnet add package Microsoft.Extensions.AI dotnet add package Microsoft.Extensions.AI.OpenAI --prerelease dotnet add package Microsoft.Azure.SignalR\n\nUsing DefaultAzureCredential (Entra ID) avoids storing raw keys in code and is the recommended auth model for Azure services.\n\n**Program.cs**\n- var builder = WebApplication.CreateBuilder(args); builder.Services.AddSignalR(); // To offload connection management to Azure SignalR Service, uncomment: // builder.Services.AddSignalR().AddAzureSignalR(); builder.Services.AddSingleton(); var app = builder.Build(); app.MapHub(\"/chat\"); app.Run();\n\n**AiStreamingService.cs** - streams content from Azure OpenAI\n- using Microsoft.Extensions.AI; using Azure.AI.OpenAI; using Azure.Identity; public class AiStreamingService { private readonly IChatClient \\_chatClient; public AiStreamingService(IConfiguration config) { var endpoint = new Uri(config[\"AZURE\\_OPENAI\\_ENDPOINT\"]!); var deployment = config[\"AZURE\\_OPENAI\\_DEPLOYMENT\"]!; // e.g., \"gpt-4o-mini\" var azureClient = new AzureOpenAIClient(endpoint, new DefaultAzureCredential()); \\_chatClient = azureClient.GetChatClient(deployment).AsIChatClient(); } public async IAsyncEnumerable StreamReplyAsync(string userMessage) { var messages = new List { ChatMessage.CreateSystemMessage(\"You are a helpful assistant.\"), ChatMessage.CreateUserMessage(userMessage) }; await foreach (var update in \\_chatClient.CompleteChatStreamingAsync(messages)) { // Only text parts; ignore tool calls/annotations var chunk = string.Join(\"\", update.Content .Where(p => p.Kind == ChatMessageContentPartKind.Text) .Select(p => ((TextContent)p).Text)); if (!string.IsNullOrEmpty(chunk)) yield return chunk; } } }\n\nModern .NET AI extensions (Microsoft.Extensions.AI) expose a unified streaming pattern via CompleteChatStreamingAsync.\n\n**ChatHub.cs** - pushes partials to the caller\n- using Microsoft.AspNetCore.SignalR; public class ChatHub : Hub { private readonly AiStreamingService \\_ai; public ChatHub(AiStreamingService ai) => \\_ai = ai; // Client calls: connection.invoke(\"AskAi\", prompt) public async Task AskAi(string prompt) { var messageId = Guid.NewGuid().ToString(\"N\"); await Clients.Caller.SendAsync(\"typing\", messageId, true); await foreach (var partial in \\_ai.StreamReplyAsync(prompt)) { await Clients.Caller.SendAsync(\"partial\", messageId, partial); } await Clients.Caller.SendAsync(\"typing\", messageId, false); await Clients.Caller.SendAsync(\"completed\", messageId); } }\n\n### 2) Frontend: Angular client with [microsoft​](javascript:void%280%29)/signalr\n\n**Install the SignalR client**\n- npm i microsoft/signalr\n\n**Create a SignalR service (Angular)**\n- // src/app/services/ai-stream.service.ts import { Injectable } from '@angular/core'; import \\* as signalR from '@microsoft/signalr'; import { BehaviorSubject, Observable } from 'rxjs'; @Injectable({ providedIn: 'root' }) export class AiStreamService { private connection?: signalR.HubConnection; private typing$ = new BehaviorSubject(false); private partial$ = new BehaviorSubject(''); private completed$ = new BehaviorSubject(false); get typing(): Observable { return this.typing$.asObservable(); } get partial(): Observable { return this.partial$.asObservable(); } get completed(): Observable { return this.completed$.asObservable(); } async start(): Promise { this.connection = new signalR.HubConnectionBuilder() .withUrl('/chat') // same origin; use absolute URL if CORS .withAutomaticReconnect() .configureLogging(signalR.LogLevel.Information) .build(); this.connection.on('typing', (\\_id: string, on: boolean) => this.typing$.next(on)); this.connection.on('partial', (\\_id: string, text: string) => { // Append incremental content this.partial$.next((this.partial$.value || '') + text); }); this.connection.on('completed', (\\_id: string) => this.completed$.next(true)); await this.connection.start(); } async ask(prompt: string): Promise { // Reset state per request this.partial$.next(''); this.completed$.next(false); await this.connection?.invoke('AskAi', prompt); } } ``\n\n**Angular component**\n- // src/app/components/ai-chat/ai-chat.component.ts import { Component, OnInit } from '@angular/core'; import { AiStreamService } from '../../services/ai-stream.service'; @Component({ selector: 'app-ai-chat', templateUrl: './ai-chat.component.html', styleUrls: ['./ai-chat.component.css'] }) export class AiChatComponent implements OnInit { prompt = ''; output = ''; typing = false; done = false; constructor(private ai: AiStreamService) {} async ngOnInit() { await this.ai.start(); this.ai.typing.subscribe(on => this.typing = on); this.ai.partial.subscribe(text => this.output = text); this.ai.completed.subscribe(done => this.done = done); } async send() { this.output = ''; this.done = false; await this.ai.ask(this.prompt); } }\n\n**HTML Template**\n- ```\n{{ output }} ```\n\nAssistant is typing…\n\n✓ Completed\n\n## Streaming modes, content filters, and UX\n\nAzure OpenAI streaming interacts with content filtering in two ways:\n\n- **Default streaming**: The service **buffers** output into **content chunks** and runs content filters before each chunk is emitted; you still stream, but not necessarily token‑by‑token.\n- **Asynchronous Filter (optional)**: The service returns **token‑level** updates immediately and runs filters asynchronously. You get ultra‑smooth streaming but must handle delayed moderation signals (e.g., redaction or halting the stream).\n\n## Best practices\n\n- Append partials **in small batches** client‑side to avoid DOM thrash; finalize formatting on \"completed\".\n- Log **full messages** server‑side only after completion to keep histories consistent (mirrors agent frameworks).\n\n## Security & compliance\n\n- **Auth:** Prefer **Microsoft Entra ID** (DefaultAzureCredential) to avoid key sprawl; use RBAC and Managed Identities where possible.\n- **Secrets:** Store Azure SignalR connection strings in **Key Vault** and **rotate** periodically; never hardcode.\n- **CORS & cross‑domain:** When hosting frontend and hub on different origins, configure CORS and use **absolute URLs** in withUrl(...).\n\n## Connection management & scaling tips\n\n- **Persistent connection load:** SignalR consumes TCP resources; separate heavy real‑time workloads or use Azure SignalR to protect other apps.\n- **Sticky sessions (self‑hosted):** Required in most multi‑server scenarios unless **WebSockets‑only + SkipNegotiation** applies; Azure SignalR removes this requirement.\n\n## Learn more\n\n- [AI‑Powered Group Chat sample (ASP.NET Core):](https://learn.microsoft.com/en-us/aspnet/core/tutorials/ai-powered-group-chat/ai-powered-group-chat?view=aspnetcore-9.0)\n- [Azure OpenAI .NET client (auth & streaming):](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.openai-readme?view=azure-dotnet)\n- **[SignalR JavaScript Client](https://learn.microsoft.com/en-us/aspnet/core/signalr/javascript-client?view=aspnetcore-9.0)**",
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/real-time-ai-streaming-with-azure-openai-and-signalr/ba-p/4468833",
  "Author": "pranav_pratik",
  "Title": "Real‑Time AI Streaming with Azure OpenAI and SignalR",
  "ProcessedDate": "2025-11-12 10:05:25",
  "FeedName": "Microsoft Tech Community",
  "PubDate": "2025-11-12T09:38:46+00:00",
  "EnhancedContent": "## Deliver ChatGPT‑style “typing” experiences in your own apps - securely and at cloud scale.\n\n### **TL;DR**\n\nWe’ll build a real-time AI app where **Azure OpenAI streams responses** and **SignalR broadcasts them live** to an Angular client. Users see answers appear incrementally just like ChatGPT while Azure SignalR Service handles scale. You’ll learn the architecture, streaming code, Angular integration, and optional enhancements like typing indicators and multi-agent scenarios.\n\n## Why This Matters\n\nModern users expect instant feedback. Waiting for a full AI response feels slow and breaks engagement. Streaming responses:\n\n- **Reduces perceived latency**: Users see content as it’s generated.\n- **Improves UX**: Mimics ChatGPT’s typing effect.\n- **Keeps users engaged**: Especially for long-form answers.\n- **Scales for enterprise**: Azure SignalR Service handles thousands of concurrent connections.\n\n## What you’ll build\n\n- A **SignalR Hub** that calls Azure OpenAI with streaming enabled and **forwards partial output** to clients as it arrives.\n- An **Angular** client that connects over **WebSockets/SSE** to the hub and **renders partial content** with a typing indicator.\n- An optional **Azure SignalR Service** layer for **scalable connection management** (thousands to millions of long‑lived connections).\n**References:** SignalR hosting & scale; Azure SignalR Service concepts.\n\n## Architecture\n\n- The **hub** calls Azure OpenAI with **streaming** enabled (await foreach over updates) and **broadcasts** partials to clients.\n- **Azure SignalR Service** (optional) offloads connection scale and removes sticky‑session complexity in multi‑node deployments.\n**References:** Streaming code pattern; scale/ARR affinity; Azure SignalR integration.\n\n## Prerequisites\n\n- **Azure OpenAI** resource with a deployed model (e.g., gpt-4o or gpt-4o-mini)\n- **.NET 8** API + **ASP.NET Core SignalR** backend\n- **Angular 16+** frontend (using [microsoft​](javascript:void%280%29)/signalr)\n\n## Step‑by‑Step Implementation\n\n### 1) Backend: ASP.NET Core + SignalR\n\n**Install packages**\n\n``` dotnet add package Microsoft.AspNetCore.SignalR dotnet add package Azure.AI.OpenAI --prerelease dotnet add package Azure.Identity dotnet add package Microsoft.Extensions.AI dotnet add package Microsoft.Extensions.AI.OpenAI --prerelease dotnet add package Microsoft.Azure.SignalR ```\n\nUsing DefaultAzureCredential (Entra ID) avoids storing raw keys in code and is the recommended auth model for Azure services.\n\n**Program.cs**\n\n``` var builder = WebApplication.CreateBuilder(args); builder.Services.AddSignalR(); // To offload connection management to Azure SignalR Service, uncomment: // builder.Services.AddSignalR().AddAzureSignalR(); builder.Services.AddSingleton<AiStreamingService>(); var app = builder.Build(); app.MapHub<ChatHub>(\"/chat\"); app.Run(); ```\n\n**AiStreamingService.cs**  - streams content from Azure OpenAI\n\n``` using Microsoft.Extensions.AI; using Azure.AI.OpenAI; using Azure.Identity; public class AiStreamingService { private readonly IChatClient _chatClient; public AiStreamingService(IConfiguration config) { var endpoint = new Uri(config[\"AZURE_OPENAI_ENDPOINT\"]!); var deployment = config[\"AZURE_OPENAI_DEPLOYMENT\"]!; // e.g., \"gpt-4o-mini\" var azureClient = new AzureOpenAIClient(endpoint, new DefaultAzureCredential()); _chatClient = azureClient.GetChatClient(deployment).AsIChatClient(); } public async IAsyncEnumerable<string> StreamReplyAsync(string userMessage) { var messages = new List<ChatMessage> { ChatMessage.CreateSystemMessage(\"You are a helpful assistant.\"), ChatMessage.CreateUserMessage(userMessage) }; await foreach (var update in _chatClient.CompleteChatStreamingAsync(messages)) { // Only text parts; ignore tool calls/annotations var chunk = string.Join(\"\", update.Content .Where(p => p.Kind == ChatMessageContentPartKind.Text) .Select(p => ((TextContent)p).Text)); if (!string.IsNullOrEmpty(chunk)) yield return chunk; } } } ```\n\nModern .NET AI extensions (Microsoft.Extensions.AI) expose a unified streaming pattern via CompleteChatStreamingAsync.\n\n**ChatHub.cs** - pushes partials to the caller\n\n``` using Microsoft.AspNetCore.SignalR; public class ChatHub : Hub { private readonly AiStreamingService _ai; public ChatHub(AiStreamingService ai) => _ai = ai; // Client calls: connection.invoke(\"AskAi\", prompt) public async Task AskAi(string prompt) { var messageId = Guid.NewGuid().ToString(\"N\"); await Clients.Caller.SendAsync(\"typing\", messageId, true); await foreach (var partial in _ai.StreamReplyAsync(prompt)) { await Clients.Caller.SendAsync(\"partial\", messageId, partial); } await Clients.Caller.SendAsync(\"typing\", messageId, false); await Clients.Caller.SendAsync(\"completed\", messageId); } } ```\n\n### 2) Frontend: Angular client with [microsoft​](javascript:void%280%29)/signalr\n\n**Install the SignalR client**\n\n``` npm i microsoft/signalr ```\n\n**Create a SignalR service (Angular)**\n\n``` // src/app/services/ai-stream.service.ts import { Injectable } from '@angular/core'; import * as signalR from '@microsoft/signalr'; import { BehaviorSubject, Observable } from 'rxjs'; @Injectable({ providedIn: 'root' }) export class AiStreamService { private connection?: signalR.HubConnection; private typing$ = new BehaviorSubject<boolean>(false); private partial$ = new BehaviorSubject<string>(''); private completed$ = new BehaviorSubject<boolean>(false); get typing(): Observable<boolean> { return this.typing$.asObservable(); } get partial(): Observable<string> { return this.partial$.asObservable(); } get completed(): Observable<boolean> { return this.completed$.asObservable(); } async start(): Promise<void> { this.connection = new signalR.HubConnectionBuilder() .withUrl('/chat') // same origin; use absolute URL if CORS .withAutomaticReconnect() .configureLogging(signalR.LogLevel.Information) .build(); this.connection.on('typing', (_id: string, on: boolean) => this.typing$.next(on)); this.connection.on('partial', (_id: string, text: string) => { // Append incremental content this.partial$.next((this.partial$.value || '') + text); }); this.connection.on('completed', (_id: string) => this.completed$.next(true)); await this.connection.start(); } async ask(prompt: string): Promise<void> { // Reset state per request this.partial$.next(''); this.completed$.next(false); await this.connection?.invoke('AskAi', prompt); } } `` ```\n\n**Angular component**\n\n``` // src/app/components/ai-chat/ai-chat.component.ts import { Component, OnInit } from '@angular/core'; import { AiStreamService } from '../../services/ai-stream.service'; @Component({ selector: 'app-ai-chat', templateUrl: './ai-chat.component.html', styleUrls: ['./ai-chat.component.css'] }) export class AiChatComponent implements OnInit { prompt = ''; output = ''; typing = false; done = false; constructor(private ai: AiStreamService) {} async ngOnInit() { await this.ai.start(); this.ai.typing.subscribe(on => this.typing = on); this.ai.partial.subscribe(text => this.output = text); this.ai.completed.subscribe(done => this.done = done); } async send() { this.output = ''; this.done = false; await this.ai.ask(this.prompt); } } ```\n\n**HTML Template**\n\n``` <!-- src/app/components/ai-chat/ai-chat.component.html --> <div class=\"chat\"> <div class=\"prompt\"> <input [(ngModel)]=\"prompt\" placeholder=\"Ask me anything…\" /> <button (click)=\"send()\">Send</button> </div> <div class=\"response\"> <pre>{{ output }}</pre> <div class=\"typing\" *ngIf=\"typing\">Assistant is typing…</div> <div class=\"done\" *ngIf=\"done\">✓ Completed</div> </div> </div> ```\n\n## Streaming modes, content filters, and UX\n\nAzure OpenAI streaming interacts with content filtering in two ways:\n\n- **Default streaming**: The service **buffers** output into **content chunks** and runs content filters before each chunk is emitted; you still stream, but not necessarily token‑by‑token.\n- **Asynchronous Filter (optional)**: The service returns **token‑level** updates immediately and runs filters asynchronously. You get ultra‑smooth streaming but must handle delayed moderation signals (e.g., redaction or halting the stream).\n\n## Best practices\n\n- Append partials **in small batches** client‑side to avoid DOM thrash; finalize formatting on \"completed\".\n- Log **full messages** server‑side only after completion to keep histories consistent (mirrors agent frameworks).\n\n## Security & compliance\n\n- **Auth:** Prefer **Microsoft Entra ID** (DefaultAzureCredential) to avoid key sprawl; use RBAC and Managed Identities where possible.\n- **Secrets:** Store Azure SignalR connection strings in **Key Vault** and **rotate** periodically; never hardcode.\n- **CORS & cross‑domain:** When hosting frontend and hub on different origins, configure CORS and use **absolute URLs** in withUrl(...).\n\n## Connection management & scaling tips\n\n- **Persistent connection load:** SignalR consumes TCP resources; separate heavy real‑time workloads or use Azure SignalR to protect other apps.\n- **Sticky sessions (self‑hosted):** Required in most multi‑server scenarios unless **WebSockets‑only + SkipNegotiation** applies; Azure SignalR removes this requirement.\n\n## Learn more\n\n- [AI‑Powered Group Chat sample (ASP.NET Core):](https://learn.microsoft.com/en-us/aspnet/core/tutorials/ai-powered-group-chat/ai-powered-group-chat?view=aspnetcore-9.0)\n- [Azure OpenAI .NET client (auth & streaming):](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.openai-readme?view=azure-dotnet)\n- **[SignalR JavaScript Client](https://learn.microsoft.com/en-us/aspnet/core/signalr/javascript-client?view=aspnetcore-9.0)**\n\nUpdated Nov 12, 2025\n\nVersion 1.0\n\n[.net](/tag/.net?nodeId=board%3AAzureDevCommunityBlog)\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[javascript](/tag/javascript?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[pranav_pratik&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0zMjUzODgyLVR0UHQyTA?image-coordinates=0%2C0%2C800%2C800&amp;image-dimensions=50x50)](/users/pranav_pratik/3253882) [pranav_pratik](/users/pranav_pratik/3253882) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined November 04, 2025\n\n[View Profile](/users/pranav_pratik/3253882)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity"
}
