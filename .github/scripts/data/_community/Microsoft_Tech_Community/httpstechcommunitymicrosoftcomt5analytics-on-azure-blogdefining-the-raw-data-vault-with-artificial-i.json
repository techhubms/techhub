{
  "ProcessedDate": "2025-10-10 13:10:22",
  "Author": "Naveed-Hussain",
  "OutputDir": "_community",
  "Tags": [],
  "Description": "- This Article is **Authored By**https://www.linkedin.com/in/jonasdekeuster/[Michael Olschimke](https://www.linkedin.com/in/olschimke/), co-founder and CEO at [Scalefree](https://www.scalefree.com/) International GmbH.\n- The **Technical Review** is done by [Ian Clarke](https://www.linkedin.com/in/ian-clarke-4a992a6/), [Naveed Hussain](https://www.linkedin.com/in/navhus/) – GBBs (Cloud Scale Analytics) for EMEA at Microsoft\n\nThe Data Vault concept is used across the industry to build robust and agile data solutions. Traditionally, the definition (and subsequent modelling) of the Raw Data Vault, which captures the unmodified raw data, is done manually. This work demands significant human intervention and expertise. However, with the advent of artificial intelligence (AI), we are witnessing a paradigm shift in how we approach this foundational task. This article explores the transformative potential of leveraging AI to define the Raw Data Vault, demonstrating how intelligent automation can enhance efficiency, accuracy, and scalability, ultimately unlocking new levels of insight and agility for organizations.\n\nNote that this article describes a solution to AI-generated Raw Data Vault models. However, the solution is not limited to Data Vault, but allows the definition of any data-driven, schema-on-read model to integrate independent data sets in an enterprise environment. We discuss this towards the end of this article.\n\n# Metadata-Driven Data Warehouse Automation\n\nIn the early days of Data Vault, all engineering was done manually: an engineer would analyse the data sources and their datasets, come up with a Raw Data Vault model in an E/R tool or Microsoft Visio, and then develop both the DDL code (CREATE TABLE) and the ELT / ETL code (INSERT INTO statements).\n\nHowever, Data Vault follows many patterns. Hubs look very similar (the difference lies in the business keys) and are loaded similarly. We discussed these patterns in previous articles of this series, for example, when covering the Data Vault model and implementation. In most projects where Data Vault entities are created and loaded manually, a data engineer eventually develops the idea of creating a metadata-driven Data Vault generator due to these existing patterns.\n\nThe effort to build a generator is too considerable, and most projects are better off using an off-the-shelf solution such as Vaultspeed. These tools come with a metadata repository and a user interface for setting up the metadata and code templates required to generate the Raw Data Vault (and often subsequent layers). We have discussed Vaultspeed in previous articles of this series.\n\nBy applying the code templates to the metadata defined by the user, the actual code for the physical model is generated for a data platform, such as Microsoft Fabric. The code templates define ***the appearance of*** hubs, links, and satellites, as well as how they are loaded. The metadata defines ***which*** hubs, links, and satellites should exist to capture the incoming data set consistently.\n\nManual development often introduces mistakes and errors that result in deviations in code quality. By generating the data platform code, deviations from the defined templates are not possible (without manual intervention), thus raising the overall quality. But the major driver for most project teams is to increase productivity. Instead of manually developing code, they generate the code.\n\nMetadata-driven generation of the Raw Data Vault is standard practice in today's projects.\n\nToday’s project tasks have therefore changed: while engineers still need to analyse the source data sets and develop a Raw Data Vault model, they no longer create the code (DDL/ELT). Instead, they set up the metadata that represents the Raw Data Vault model in the tool of their choice. Each data warehouse automation tool comes with its specific features, limitations, and metadata formats. The data engineer/modeler must understand how to transfer the Raw Data Vault model into the data warehouse automation tool by correctly setting up the metadata. This is also true for Vaultspeed; the data modeler can set up the metadata either through the user interface or via the SDK.\n\nThis is the most labour-intensive task concerning the Raw Data Vault layer.\n\nIt also requires experts who not only know Data Vault modelling but also know (or can analyse) the source systems' data and understand the selected data warehouse automation solution. Additionally, Data Vault is not equal to Data Vault in many cases, as it allows for a very flexible interpretation of how to model a Data Vault, which also leads to quality issues.\n\nBut what if the organization has no access to such experts? What if budgets are limited, time is of the essence, or there are no available experts in sufficient numbers in the field? As Data Vault experts, we can debate the value of Data Vault as much as we want, but if there are no experts capable of modeling it, the debate will remain inconclusive.\n\nAnd what if this problem is only getting worse? In the past, a few dozen source tables might have been sufficient to be processed by the data platform. Today, several hundred source tables could be considered a medium-sized data platform. Tomorrow, there will be thousands of source tables. The reason? There is not only an exponential growth in the volume of data to be produced and processed, but it also comes with an exponential growth in the complexity of data shape. The source of this exponential growth in data shape comes from more complex source databases, APIs that produce and deliver semi-structured JSON data, and, ultimately, more complex business processes and an increasing amount of generated and available data that needs to be analysed for meaningful business results.\n\n# Generating the Data Vault using Artificial Intelligence\n\nIncreasingly, this data is generated using artificial intelligence (AI) and still requires integration, transformation, and analysis. The issue is that the number of data engineers, data modelers, and data scientists is not growing exponentially. Universities around the world only produce a limited number of these roles, and some of us would like to retire one day. Based on our experience, the increase in these roles is linear at best. Even if you argue for exponential growth in these roles, it is evident that there is no debate about a growing gap between the increasing data volume and the people who should analyse it.\n\n![]()\n\nThis gap cannot be closed by humans in the future. Even in a world where all kids want to become and eventually work in a data role. Sorry for all the pilots, police officers, nurses, doctors, etc., there is no way for you to retire without the whole economy imploding.\n\nTherefore, the only way to close the gap is through the use of artificial intelligence. It is not about reducing the data roles. It's about making them efficient so that they can deal with the growing data shape (and not just the volume).\n\nFor a long time, it was common sense in the industry that, if an artificial intelligence could generate or define the Raw Data Vault, it would be an assisting technology. The AI would make recommendations, for example, such as which hubs or links to model and which business keys to use. The human data modeler would make the final decision, with input from the AI.\n\nBut what if the AI made the final decision? What would it look like? What if one could attach data sources to the AI platform and the AI would analyze the source datasets, come up with a Raw Data Vault model, and load that model into Vaultspeed or another data warehouse automation tool, know the source system’s data, know Data Vault modelling, and understand the selected data warehouse automation?\n\nThese questions were posed by Michael Olschimke, a Data Vault and AI expert, when initially considering the challenge. He researched the [distribution of neural networks on massively parallel processing (MPP) clusters to classify unstructured data](https://go.flow.bi/71rnu3) at Santa Clara University in Silicon Valley. This prior AI research, combined with the knowledge he accumulated in the Data Vault, enabled him to build a solution that later became known as [Flow.BI](https://go.flow.bi/yc6kaf).\n\n# Flow.BI as a Generative AI to Define the Raw Data Vault\n\nThe solution is simple, at least from the outside: attach a few data sources, let the AI do the rest.\n\n[Flow.BI](https://go.flow.bi/yc6kaf) supports several data sources already, including Microsoft SQL Server and derivatives, such as Synapse and Fabric, as long as a JDBC driver is available, [Flow.BI](https://go.flow.bi/yc6kaf) should eventually be able to analyze the data source.\n\nAnd the AI doesn’t care if the data originates from a CRM system, such as Microsoft Dynamics, or an e-commerce platform; it's just data. There are no provisions in the code to deal with specific datasets, at least for now.\n\nThe goal of [Flow.BI](https://go.flow.bi/yc6kaf) is to produce a valid, that is, consistent and integrated, enterprise data model. Typically, this follows a Data Vault design, but it's not limited to that (we’ll discuss this later in the article).\n\nThis is achieved by following a strict data-driven approach that imitates the human data modeler. [Flow.BI](https://go.flow.bi/yc6kaf) needs data to make decisions, just like its human counterpart. Source entities with no data will be ignored. It only requires some metadata, such as the available entities and their columns. Datatypes are nice-to-have; primary keys and foreign keys would improve the target model, just like entity and column descriptions. But they are not required to define a valid Raw Data Vault model.\n\nHumans write this text, and as such, we like to influence the result of the modelling exercise. [Flow.BI](https://go.flow.bi/yc6kaf) is appreciating this by offering many options for the human data modeler to influence the engine. Some of them will be discussed in this article, but there are many more already available and more to come.\n\n[Flow.BI](https://go.flow.bi/yc6kaf)’s user interface is kept as lean and straightforward as possible: the solution is designed so that the AI should take the lead and model the whole Raw Data Vault. The UI’s purpose is to interact with human data modelers, allowing them to influence the results. That’s what many screens are related to - and the configuration of the security system.\n\nA client can have multiple instances, which result in independent Data Vault models. This is particularly useful when dealing with independent data platforms, such as those used by HR, the compliance department, or specific business use cases, or when creating the raw data foundation for data products within a data mesh. In this case, a [Flow.BI](https://go.flow.bi/yc6kaf) instance equals a data product.\n\nBut don’t underestimate the complexity of [Flow.BI](https://go.flow.bi/yc6kaf): The frontend is used to manage a large number of compute clusters that implement scalable agents to work on defining the Raw Data Vault. The platform is implementing full separation of data and processing, not only by client but also by instance.\n\n# Mapping Raw Data to Organizational Ontology\n\nThe very first step in the process is to identify the concepts in the attached datasets. For this purpose, there is a concept classifier that analyses the data and recognizes datasets and their classified concepts that it has seen in the past.\n\n![]()Semi-Automated Concept Classification in Flow.BI\n\nA common requirement of clients is that they would like to leverage their organizational requirements in this process. While [Flow.BI](https://go.flow.bi/yc6kaf) doesn’t know a client’s ontology; it is possible to override (and in some cases, complete) the concept classifications and refer to concepts from the organizational ontology. By doing so, [Flow.BI](https://go.flow.bi/yc6kaf) will integrate the source system’s raw data into the organization's ontology. It will not create a logical Data Vault, which is where the Data Vault model reflects the ***desired*** business, but instead ***model the raw data as the business uses it,*** and therefore follow the data-driven Data Vault modeling principles that Michael Olschimke has taught to thousands of students over the years at [Scalefree](https://go.flow.bi/jtan7v).\n\n[Flow.BI](https://go.flow.bi/yc6kaf) also allows the definition of a multi-tenant Data Vault model, where source systems either provide multi-tenant data or are assigned to a specific tenant. In both cases, the integrated enterprise data model will be extended to allow queries across multiple tenants or within a single tenant, depending on the information consumer’s needs.\n\n# Ensuring Security and Privacy\n\n[Flow.BI](https://go.flow.bi/yc6kaf) was designed with security and privacy in mind. From a design perspective, this has two aspects:\n\n1. Security and privacy in the service itself, to protect client solutions and related assets\n2. Security and privacy are integral to the defined model, allowing for the effective utilization of Data Vault’s capabilities in addressing security and privacy requirements, such as satellite splits.\n\nWhile [Flow.BI](https://go.flow.bi/yc6kaf) is using a shared architecture; all data and metadata storage and processing are separated by client and instance. However, this is often not sufficient for clients as they hesitate to share their highly sensitive data with a third party. For this reason, [Flow.BI](https://go.flow.bi/yc6kaf) allows two critical features:\n\n1. Local data storage: instead of storing client data on [Flow.BI](https://go.flow.bi/yc6kaf) infrastructure, the client provides an Azure Data Lake Storage to be used for storing the data.\n2. Local data processing: A Docker container can be deployed into the client’s infrastructure to access the client's data sources, extract the data, and process it.\n\nWhen using both options, only metadata, such as entity and column names, constraints, and descriptions, are shared with [Flow.BI](https://go.flow.bi/yc6kaf). No data is transferred from the client’s infrastructure to [Flow.BI](https://go.flow.bi/yc6kaf).\n\nThe metadata is secured on [Flow.BI](https://go.flow.bi/yc6kaf)’s premises as if it were actual data: row-level security separates the metadata by instance, and roles and permissions are defined per client who can access the metadata and what they can do with it.\n\n![]()Privacy Classification of Attributes in Flow.BI\n\nBut security and privacy are not limited to the service itself. The defined model also utilizes the security and privacy features of Data Vault. For example, it enables the classification of source columns based on security and privacy. The user can set up security and privacy classes and apply them to the influence screen for both. By doing so, the column classifications are used when defining the Raw Data Vault and can later be used to implement a satellite split in the physical model (if necessary). An upcoming release will include an AI model for classifying columns based on privacy, utilizing data and metadata to automate this task.\n\n# Tackling Multilingual Challenges\n\nA common challenge for clients is navigating multilingual data environments. Many data sources use English entity and column names, but there are systems using metadata in a different language. Also, the assumption that the data platform should use English metadata is not always correct. Especially in government clients, the use of the official language is mandatory.\n\nBoth options, translating the source metadata to English (the default within [Flow.BI](https://go.flow.bi/yc6kaf)) and translating the defined target model into any target language, are supported by [Flow.BI](https://go.flow.bi/yc6kaf)’s translations tab on the influence screen:\n\n![]()Metadata Translation in Flow.BI\n\nThe tab utilizes an AI translator to fully automatically translate the incoming table names, column names, and concept names. However, the user can step in and override the translation to improve it to their needs. All strings of the source metadata and the defined model are passed through the translation module. It is also possible to reuse existing translations for a growing list of popular data sources.\n\nThis feature enables readable names for satellites and their attributes (as well as hubs and links), resulting in a significantly improved user experience for the defined Raw Data Vault.\n\n# Generating the Physical Model\n\nYou should have noticed by now that we consistently discuss the ***defined*** Raw Data Vault model. [Flow.BI](https://go.flow.bi/yc6kaf) is not generating the physical model, that is, the CREATE TABLE and INSERT INTO statements for the Raw Data Vault. Instead, it “just” defines the hubs, links, and satellites required for capturing all incoming data from the attached data sources, including business key selection, satellite splits, and special entity types, such as non-historized links and their satellites, multi-active satellites, hierarchical links, effectivity satellites, and reference tables.\n\n[Video on Generating Physical Models](https://www.dropbox.com/scl/fi/tst0woibgefkz4fl4x7qa/2025-07-15_14-32-19.mp4?rlkey=byyfm65iqdvzzo91c0z55yqlv&st=jtlt1vdw&dl=0)\n\nThis logical model (not to be confused with “logical Data Vault modelling”) is then provided to our growing number of ISV partner solutions that will consume our defined model, set up the required metadata in their tool, and generate the physical model.\n\nAs a result, [Flow.BI](https://go.flow.bi/yc6kaf) acts as a team member that analyses your organizational data sources and their data, knows how to model the Raw Data Vault, and how to set up metadata in the tool of your choice.\n\nThe metadata is provided by [Flow.BI](https://go.flow.bi/yc6kaf) can be used to model the landing zone/staging area (either on a data lake or a relational database such as Microsoft Fabric) and the Raw Data Vault in a data-driven Data Vault architecture, which is the recommended practice.\n\nWith this in mind, [Flow.BI](https://go.flow.bi/yc6kaf) is not a competition to Vaultspeed or your other existing data warehouse automation solution, but a valid extension that integrates with your existing tool stack. This makes it much easier to justify the introduction of [Flow.BI](https://go.flow.bi/yc6kaf) to the project.\n\n# Going Beyond Data Vault\n\n[Flow.BI](https://go.flow.bi/yc6kaf) is not limited to the definition of Data Vault models. While it has been designed with the Data Vault concepts in mind, a customizable expert system is used to define the Data Vault model. Although the expert system is not yet publicly available, it has already been implemented and is in use for every model generation.\n\nThis expert system enables the implementation of alternative data models, provided they adhere to data-driven, schema-on-read principles. Data Vault is such an example, but many others are possible, as well:\n\n- Customized Data Vault models\n- Inmon-style enterprise models in third-normal form (3NF, if no business logic is required\n- Kimball-style analytical models with facts and dimensions, again without business logic\n- Semi-structured JSON and XML document collections\n- Key-value stores\n- “One Big Table (OBT)” models\n- “Many Big Related Table (MBRT)” models\n\nOkay, we’ve just invented the MBRT model as we're writing the article, but you get the idea: many large, fully denormalized tables with foreign–key relationships between each other. If you've developed your data-driven model, please get in touch with us.\n\n# About the Authors\n\nMichael Olschimke is co-founder and CEO of Flow.BI, a generative AI that defines integrated enterprise data models, such as (but not limited to) Data Vault. Michael has trained thousands of industry data warehousing professionals, taught academic classes, and published regularly on topics around data platforms, data engineering, and Data Vault. He has over two decades of experience in information technology, with a specialization in business intelligence topics, artificial intelligence and data platforms.\n\nhttps://techcommunity.microsoft.com/t5/analytics-on-azure-blog/data-vault-2-0-warehouse-automation-on-azure/ba-p/3876206",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "EnhancedContent": "- This Article is **Authored By**https://www.linkedin.com/in/jonasdekeuster/[Michael Olschimke](https://www.linkedin.com/in/olschimke/), co-founder and CEO at [Scalefree](https://www.scalefree.com/) International GmbH.\n- The **Technical Review** is done by [Ian Clarke](https://www.linkedin.com/in/ian-clarke-4a992a6/), [Naveed Hussain](https://www.linkedin.com/in/navhus/) – GBBs (Cloud Scale Analytics) for EMEA at Microsoft\n\nThe Data Vault concept is used across the industry to build robust and agile data solutions. Traditionally, the definition (and subsequent modelling) of the Raw Data Vault, which captures the unmodified raw data, is done manually. This work demands significant human intervention and expertise. However, with the advent of artificial intelligence (AI), we are witnessing a paradigm shift in how we approach this foundational task. This article explores the transformative potential of leveraging AI to define the Raw Data Vault, demonstrating how intelligent automation can enhance efficiency, accuracy, and scalability, ultimately unlocking new levels of insight and agility for organizations.\n\nNote that this article describes a solution to AI-generated Raw Data Vault models. However, the solution is not limited to Data Vault, but allows the definition of any data-driven, schema-on-read model to integrate independent data sets in an enterprise environment. We discuss this towards the end of this article.\n\n# Metadata-Driven Data Warehouse Automation\n\nIn the early days of Data Vault, all engineering was done manually: an engineer would analyse the data sources and their datasets, come up with a Raw Data Vault model in an E/R tool or Microsoft Visio, and then develop both the DDL code (CREATE TABLE) and the ELT / ETL code (INSERT INTO statements).\n\nHowever, Data Vault follows many patterns. Hubs look very similar (the difference lies in the business keys) and are loaded similarly. We discussed these patterns in previous articles of this series, for example, when covering the Data Vault model and implementation. In most projects where Data Vault entities are created and loaded manually, a data engineer eventually develops the idea of creating a metadata-driven Data Vault generator due to these existing patterns.\n\nThe effort to build a generator is too considerable, and most projects are better off using an off-the-shelf solution such as Vaultspeed. These tools come with a metadata repository and a user interface for setting up the metadata and code templates required to generate the Raw Data Vault (and often subsequent layers). We have discussed Vaultspeed in previous articles of this series.\n\nBy applying the code templates to the metadata defined by the user, the actual code for the physical model is generated for a data platform, such as Microsoft Fabric. The code templates define ***the appearance of*** hubs, links, and satellites, as well as how they are loaded. The metadata defines ***which*** hubs, links, and satellites should exist to capture the incoming data set consistently.\n\nManual development often introduces mistakes and errors that result in deviations in code quality. By generating the data platform code, deviations from the defined templates are not possible (without manual intervention), thus raising the overall quality. But the major driver for most project teams is to increase productivity. Instead of manually developing code, they generate the code.\n\nMetadata-driven generation of the Raw Data Vault is standard practice in today's projects.\n\nToday’s project tasks have therefore changed: while engineers still need to analyse the source data sets and develop a Raw Data Vault model, they no longer create the code (DDL/ELT). Instead, they set up the metadata that represents the Raw Data Vault model in the tool of their choice. Each data warehouse automation tool comes with its specific features, limitations, and metadata formats. The data engineer/modeler must understand how to transfer the Raw Data Vault model into the data warehouse automation tool by correctly setting up the metadata. This is also true for Vaultspeed; the data modeler can set up the metadata either through the user interface or via the SDK.\n\nThis is the most labour-intensive task concerning the Raw Data Vault layer.\n\nIt also requires experts who not only know Data Vault modelling but also know (or can analyse) the source systems' data and understand the selected data warehouse automation solution. Additionally, Data Vault is not equal to Data Vault in many cases, as it allows for a very flexible interpretation of how to model a Data Vault, which also leads to quality issues.\n\nBut what if the organization has no access to such experts? What if budgets are limited, time is of the essence, or there are no available experts in sufficient numbers in the field? As Data Vault experts, we can debate the value of Data Vault as much as we want, but if there are no experts capable of modeling it, the debate will remain inconclusive.\n\nAnd what if this problem is only getting worse? In the past, a few dozen source tables might have been sufficient to be processed by the data platform. Today, several hundred source tables could be considered a medium-sized data platform. Tomorrow, there will be thousands of source tables. The reason? There is not only an exponential growth in the volume of data to be produced and processed, but it also comes with an exponential growth in the complexity of data shape. The source of this exponential growth in data shape comes from more complex source databases, APIs that produce and deliver semi-structured JSON data, and, ultimately, more complex business processes and an increasing amount of generated and available data that needs to be analysed for meaningful business results.\n\n# Generating the Data Vault using Artificial Intelligence\n\nIncreasingly, this data is generated using artificial intelligence (AI) and still requires integration, transformation, and analysis. The issue is that the number of data engineers, data modelers, and data scientists is not growing exponentially. Universities around the world only produce a limited number of these roles, and some of us would like to retire one day. Based on our experience, the increase in these roles is linear at best. Even if you argue for exponential growth in these roles, it is evident that there is no debate about a growing gap between the increasing data volume and the people who should analyse it.\n\nThis gap cannot be closed by humans in the future. Even in a world where all kids want to become and eventually work in a data role. Sorry for all the pilots, police officers, nurses, doctors, etc., there is no way for you to retire without the whole economy imploding.\n\nTherefore, the only way to close the gap is through the use of artificial intelligence. It is not about reducing the data roles. It's about making them efficient so that they can deal with the growing data shape (and not just the volume).\n\nFor a long time, it was common sense in the industry that, if an artificial intelligence could generate or define the Raw Data Vault, it would be an assisting technology. The AI would make recommendations, for example, such as which hubs or links to model and which business keys to use. The human data modeler would make the final decision, with input from the AI.\n\nBut what if the AI made the final decision? What would it look like? What if one could attach data sources to the AI platform and the AI would analyze the source datasets, come up with a Raw Data Vault model, and load that model into Vaultspeed or another data warehouse automation tool, know the source system’s data, know Data Vault modelling, and understand the selected data warehouse automation?\n\nThese questions were posed by Michael Olschimke, a Data Vault and AI expert, when initially considering the challenge. He researched the [distribution of neural networks on massively parallel processing (MPP) clusters to classify unstructured data](https://go.flow.bi/71rnu3) at Santa Clara University in Silicon Valley. This prior AI research, combined with the knowledge he accumulated in the Data Vault, enabled him to build a solution that later became known as [Flow.BI](https://go.flow.bi/yc6kaf).\n\n# Flow.BI as a Generative AI to Define the Raw Data Vault\n\nThe solution is simple, at least from the outside: attach a few data sources, let the AI do the rest.\n\n[Flow.BI](https://go.flow.bi/yc6kaf) supports several data sources already, including Microsoft SQL Server and derivatives, such as Synapse and Fabric, as long as a JDBC driver is available, [Flow.BI](https://go.flow.bi/yc6kaf) should eventually be able to analyze the data source.\n\nAnd the AI doesn’t care if the data originates from a CRM system, such as Microsoft Dynamics, or an e-commerce platform; it's just data. There are no provisions in the code to deal with specific datasets, at least for now.\n\nThe goal of [Flow.BI](https://go.flow.bi/yc6kaf) is to produce a valid, that is, consistent and integrated, enterprise data model. Typically, this follows a Data Vault design, but it's not limited to that (we’ll discuss this later in the article).\n\nThis is achieved by following a strict data-driven approach that imitates the human data modeler. [Flow.BI](https://go.flow.bi/yc6kaf) needs data to make decisions, just like its human counterpart. Source entities with no data will be ignored. It only requires some metadata, such as the available entities and their columns. Datatypes are nice-to-have; primary keys and foreign keys would improve the target model, just like entity and column descriptions. But they are not required to define a valid Raw Data Vault model.\n\nHumans write this text, and as such, we like to influence the result of the modelling exercise. [Flow.BI](https://go.flow.bi/yc6kaf) is appreciating this by offering many options for the human data modeler to influence the engine. Some of them will be discussed in this article, but there are many more already available and more to come.\n\n[Flow.BI](https://go.flow.bi/yc6kaf)’s user interface is kept as lean and straightforward as possible: the solution is designed so that the AI should take the lead and model the whole Raw Data Vault. The UI’s purpose is to interact with human data modelers, allowing them to influence the results. That’s what many screens are related to - and the configuration of the security system.\n\nA client can have multiple instances, which result in independent Data Vault models. This is particularly useful when dealing with independent data platforms, such as those used by HR, the compliance department, or specific business use cases, or when creating the raw data foundation for data products within a data mesh. In this case, a [Flow.BI](https://go.flow.bi/yc6kaf) instance equals a data product.\n\nBut don’t underestimate the complexity of [Flow.BI](https://go.flow.bi/yc6kaf): The frontend is used to manage a large number of compute clusters that implement scalable agents to work on defining the Raw Data Vault. The platform is implementing full separation of data and processing, not only by client but also by instance.\n\n# Mapping Raw Data to Organizational Ontology\n\nThe very first step in the process is to identify the concepts in the attached datasets. For this purpose, there is a concept classifier that analyses the data and recognizes datasets and their classified concepts that it has seen in the past.\n\nSemi-Automated Concept Classification in Flow.BI\n\nA common requirement of clients is that they would like to leverage their organizational requirements in this process. While [Flow.BI](https://go.flow.bi/yc6kaf) doesn’t know a client’s ontology; it is possible to override (and in some cases, complete) the concept classifications and refer to concepts from the organizational ontology. By doing so, [Flow.BI](https://go.flow.bi/yc6kaf) will integrate the source system’s raw data into the organization's ontology. It will not create a logical Data Vault, which is where the Data Vault model reflects the ***desired*** business, but instead ***model the raw data as the business uses it,*** and therefore follow the data-driven Data Vault modeling principles that Michael Olschimke has taught to thousands of students over the years at [Scalefree](https://go.flow.bi/jtan7v).\n\n[Flow.BI](https://go.flow.bi/yc6kaf) also allows the definition of a multi-tenant Data Vault model, where source systems either provide multi-tenant data or are assigned to a specific tenant. In both cases, the integrated enterprise data model will be extended to allow queries across multiple tenants or within a single tenant, depending on the information consumer’s needs.\n\n# Ensuring Security and Privacy\n\n[Flow.BI](https://go.flow.bi/yc6kaf) was designed with security and privacy in mind. From a design perspective, this has two aspects:\n\n1. Security and privacy in the service itself, to protect client solutions and related assets\n2. Security and privacy are integral to the defined model, allowing for the effective utilization of Data Vault’s capabilities in addressing security and privacy requirements, such as satellite splits.\n\nWhile [Flow.BI](https://go.flow.bi/yc6kaf) is using a shared architecture; all data and metadata storage and processing are separated by client and instance. However, this is often not sufficient for clients as they hesitate to share their highly sensitive data with a third party. For this reason, [Flow.BI](https://go.flow.bi/yc6kaf) allows two critical features:\n\n1. Local data storage: instead of storing client data on [Flow.BI](https://go.flow.bi/yc6kaf) infrastructure, the client provides an Azure Data Lake Storage to be used for storing the data.\n2. Local data processing: A Docker container can be deployed into the client’s infrastructure to access the client's data sources, extract the data, and process it.\n\nWhen using both options, only metadata, such as entity and column names, constraints, and descriptions, are shared with [Flow.BI](https://go.flow.bi/yc6kaf). No data is transferred from the client’s infrastructure to [Flow.BI](https://go.flow.bi/yc6kaf).\n\nThe metadata is secured on [Flow.BI](https://go.flow.bi/yc6kaf)’s premises as if it were actual data: row-level security separates the metadata by instance, and roles and permissions are defined per client who can access the metadata and what they can do with it.\n\nPrivacy Classification of Attributes in Flow.BI\n\nBut security and privacy are not limited to the service itself. The defined model also utilizes the security and privacy features of Data Vault. For example, it enables the classification of source columns based on security and privacy. The user can set up security and privacy classes and apply them to the influence screen for both. By doing so, the column classifications are used when defining the Raw Data Vault and can later be used to implement a satellite split in the physical model (if necessary). An upcoming release will include an AI model for classifying columns based on privacy, utilizing data and metadata to automate this task.\n\n# Tackling Multilingual Challenges\n\nA common challenge for clients is navigating multilingual data environments. Many data sources use English entity and column names, but there are systems using metadata in a different language. Also, the assumption that the data platform should use English metadata is not always correct. Especially in government clients, the use of the official language is mandatory.\n\nBoth options, translating the source metadata to English (the default within [Flow.BI](https://go.flow.bi/yc6kaf)) and translating the defined target model into any target language, are supported by [Flow.BI](https://go.flow.bi/yc6kaf)’s translations tab on the influence screen:\n\nMetadata Translation in Flow.BI\n\nThe tab utilizes an AI translator to fully automatically translate the incoming table names, column names, and concept names. However, the user can step in and override the translation to improve it to their needs. All strings of the source metadata and the defined model are passed through the translation module. It is also possible to reuse existing translations for a growing list of popular data sources.\n\nThis feature enables readable names for satellites and their attributes (as well as hubs and links), resulting in a significantly improved user experience for the defined Raw Data Vault.\n\n# Generating the Physical Model\n\nYou should have noticed by now that we consistently discuss the ***defined*** Raw Data Vault model. [Flow.BI](https://go.flow.bi/yc6kaf) is not generating the physical model, that is, the CREATE TABLE and INSERT INTO statements for the Raw Data Vault. Instead, it “just” defines the hubs, links, and satellites required for capturing all incoming data from the attached data sources, including business key selection, satellite splits, and special entity types, such as non-historized links and their satellites, multi-active satellites, hierarchical links, effectivity satellites, and reference tables.\n\n[Video on Generating Physical Models](https://www.dropbox.com/scl/fi/tst0woibgefkz4fl4x7qa/2025-07-15_14-32-19.mp4?rlkey=byyfm65iqdvzzo91c0z55yqlv&amp;st=jtlt1vdw&amp;dl=0)\n\nThis logical model (not to be confused with “logical Data Vault modelling”) is then provided to our growing number of ISV partner solutions that will consume our defined model, set up the required metadata in their tool, and generate the physical model.\n\nAs a result, [Flow.BI](https://go.flow.bi/yc6kaf) acts as a team member that analyses your organizational data sources and their data, knows how to model the Raw Data Vault, and how to set up metadata in the tool of your choice.\n\nThe metadata is provided by [Flow.BI](https://go.flow.bi/yc6kaf) can be used to model the landing zone/staging area (either on a data lake or a relational database such as Microsoft Fabric) and the Raw Data Vault in a data-driven Data Vault architecture, which is the recommended practice.\n\nWith this in mind, [Flow.BI](https://go.flow.bi/yc6kaf) is not a competition to Vaultspeed or your other existing data warehouse automation solution, but a valid extension that integrates with your existing tool stack. This makes it much easier to justify the introduction of [Flow.BI](https://go.flow.bi/yc6kaf) to the project.\n\n# Going Beyond Data Vault\n\n[Flow.BI](https://go.flow.bi/yc6kaf) is not limited to the definition of Data Vault models. While it has been designed with the Data Vault concepts in mind, a customizable expert system is used to define the Data Vault model. Although the expert system is not yet publicly available, it has already been implemented and is in use for every model generation.\n\nThis expert system enables the implementation of alternative data models, provided they adhere to data-driven, schema-on-read principles. Data Vault is such an example, but many others are possible, as well:\n\n- Customized Data Vault models\n- Inmon-style enterprise models in third-normal form (3NF, if no business logic is required\n- Kimball-style analytical models with facts and dimensions, again without business logic\n- Semi-structured JSON and XML document collections\n- Key-value stores\n- “One Big Table (OBT)” models\n- “Many Big Related Table (MBRT)” models\n\nOkay, we’ve just invented the MBRT model as we're writing the article, but you get the idea: many large, fully denormalized tables with foreign–key relationships between each other. If you've developed your data-driven model, please get in touch with us.\n\n# About the Authors\n\nMichael Olschimke is co-founder and CEO of Flow.BI, a generative AI that defines integrated enterprise data models, such as (but not limited to) Data Vault. Michael has trained thousands of industry data warehousing professionals, taught academic classes, and published regularly on topics around data platforms, data engineering, and Data Vault. He has over two decades of experience in information technology, with a specialization in business intelligence topics, artificial intelligence and data platforms.\n\n[&lt;&lt;&lt; Back to Blog Series Title Page](https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/data-vault-2-0-warehouse-automation-on-azure/ba-p/3876206)\n\nUpdated Oct 10, 2025\n\nVersion 1.0\n\n[analytics](/tag/analytics?nodeId=board%3AAnalyticsonAzure)\n\n[delta lake](/tag/delta%20lake?nodeId=board%3AAnalyticsonAzure)\n\n[machine learning](/tag/machine%20learning?nodeId=board%3AAnalyticsonAzure)\n\n[microsoft fabric](/tag/microsoft%20fabric?nodeId=board%3AAnalyticsonAzure)\n\n[!\\[Naveed-Hussain&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-2.svg?image-dimensions=50x50)](/users/naveed-hussain/1741331) [Naveed-Hussain](/users/naveed-hussain/1741331) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined February 20, 2023\n\n[View Profile](/users/naveed-hussain/1741331)\n\n/category/azure/blog/analyticsonazure [Analytics on Azure Blog](/category/azure/blog/analyticsonazure) Follow this blog board to get notified when there's new activity",
  "Title": "Defining the Raw Data Vault with Artificial Intelligence",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Link": "https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/defining-the-raw-data-vault-with-artificial-intelligence/ba-p/4453557",
  "FeedName": "Microsoft Tech Community",
  "PubDate": "2025-10-10T12:33:30+00:00"
}
