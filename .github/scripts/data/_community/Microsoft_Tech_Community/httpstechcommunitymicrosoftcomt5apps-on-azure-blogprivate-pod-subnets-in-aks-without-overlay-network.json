{
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Community",
  "Link": "https://techcommunity.microsoft.com/t5/apps-on-azure-blog/private-pod-subnets-in-aks-without-overlay-networking/ba-p/4442510",
  "OutputDir": "_community",
  "EnhancedContent": "When deploying AKS clusters, a common concern is the amount of IP address space required. If you are deploying your AKS cluster into your corporate network, the size of the IP address space you can obtain may be quite small, which can cause problems with the number of pods you are able to deploy.\n\nThe simplest and most common solution to this is to use an overlay network, which is [fully supported in AKS](https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay?tabs=kubectl). In an overlay network, pods are deployed to a private, non-routed address space that can be as large as you want. Translation between the routable and non-routed network is handled by AKS. For most people, this is the best option for dealing with IP addressing in AKS, and there is no need to complicate things further.\n\nHowever, there are some limitations with overlay networking, primarily that you cannot address the pods directly from the rest of the network— all inbound communication must go via services. There are also some advanced features that are not supported, such as Virtual Nodes.\n\nIf you are in a scenario where you need some of these features, and overlay networking will not work for you, it is possible to use the more traditional vNet-based deployment method, with some tweaks.\n\n## Azure CNI Pod Subnet\n\nThe alternative to using the Azure CNI Overlay is to use the Azure CNI Pod Subnet. In this setup, you deploy a vNet with two subnets - one for your nodes and one for pods. You are in control of the IP address configuration for these subnets.\n\nTo conserve IP addresses, you can create your pod subnet using an IP range that is not routable to the rest of your corporate network, allowing you to make it as large as you like. The node subnet remains routable from your corporate network. In this setup, if you want to talk to the pods directly, you would need to do so from within the AKS vNet or peer another network to your pod subnet. You would not be able to address these pods from the rest of your corporate network, even without using overlay networking.\n\n### The Routing Problem\n\nWhen you deploy a setup using Azure CNI Pod Subnet, all the subnets in the vNet are configured with routes and can talk to each other. You may wish to connect this vNet to other Azure vNets via peering, or to your corporate network using ExpressRoute or VPN.\n\nHowever, where you will encounter an issue is if your pods try to connect to resources outside of your AKS vNet but inside your corporate network, or any peered Azure vNets (which are not peered to this isolated subnet). In this scenario, the pods will route their traffic directly out of the vNet using their private IP address. This private IP is not a valid, routable IP, so the resources on the other network will not be able to reply, and the request will fail.\n\n## IP Masquerading\n\nTo resolve this issue, we need a way to have traffic going to other networks present a private IP that is routable within the network. This can be achieved through several methods.\n\nOne method would be to introduce a separate solution for routing this traffic, such as Azure Firewall or another Network Virtual Appliance (NVA). Traffic is routable between the pod and node subnet, so the pod can send its requests to the firewall, and then the requests to the remote network come from the IP of the firewall, which is routable.\n\nThis solution will work but does require another resource to be deployed, with additional costs. If you are already using an Azure Firewall for outbound traffic, then this may be something you could use, but we are looking for a simpler and more cost-effective solution.\n\nRather than implementing another device to present a routable IP, we can use the nodes of our AKS clusters. The AKS nodes are in the routable node subnet, so ideally we want our outbound traffic from the pods to use the node IP when it needs to leave the vNet to go to the rest of the private network.\n\nThere are several different ways you could achieve this goal. You could look at using Egress Gateway services through tools like Istio, or you could look at making changes to the iptables configuration on the nodes using a DaemonSet. In this article, we will focus on using a tool called [ip-masq-agent-v2](https://github.com/Azure/ip-masq-agent-v2). This tool provides a means for traffic to \"masquerade\" as coming from the IP address of the node it is running on and have the node perform Network Address Translation (NAT).\n\nIf you deploy a cluster with an overlay network, this tool is already deployed and configured on your cluster. This is the tool that Microsoft uses to configure NAT for traffic leaving the overlay network. When using pod subnet clusters, this tool is not deployed, but you can deploy it yourself to provide the same functionality. Under the hood, this tool is making changes to iptables using a DaemonSet that runs on each node, so you could replicate this behaviour yourself—but this provides a simpler process that has been tested with AKS through overlay networking.\n\nThe Microsoft v2 version of this is based on the original Kubernetes contribution, aiming to solve more specific networking cases, allow for more configuration options, and improve observability.\n\n### Deploy ip-masq-agent-v2\n\nThere are two parts to deploying the agent. First, we deploy the agent, which runs as a DaemonSet, spawning a pod on each node in the cluster. This is important, as each node needs to have the iptables altered by the tool, and it needs to run anytime a new node is created.\n\nTo deploy the agent, we need to create the DaemonSet in our cluster. The [ip-masq-agent-v2 repo](https://github.com/Azure/ip-masq-agent-v2) includes several examples, including [an example of deploying the DaemonSet](https://github.com/Azure/ip-masq-agent-v2/blob/master/examples/ip-masq-agent.yaml). The example is slightly out of date on the version of ip-masq-agent-v2 to use, so make sure you update this to the latest version.\n\nIf you would prefer to build and manage your own containers for this, the repository also includes a Dockerfile to allow you to do this.\n\nBelow is an example deployment using the Microsoft-hosted images. It references the ConfigMap we will create in the next step, and it is important that the same name is used as is referenced here.\n\n``` apiVersion: apps/v1 kind: DaemonSet metadata: name: ip-masq-agent namespace: kube-system labels: component: ip-masq-agent kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: ip-masq-agent template: metadata: labels: k8s-app: ip-masq-agent spec: hostNetwork: true containers:\n- name: ip-masq-agent\nimage: mcr.microsoft.com/aks/ip-masq-agent-v2:v0.1.15 imagePullPolicy: Always securityContext: privileged: false capabilities: add: [\"NET_ADMIN\", \"NET_RAW\"] volumeMounts:\n- name: ip-masq-agent-volume\nmountPath: /etc/config readOnly: true volumes:\n- name: ip-masq-agent-volume\nprojected: sources:\n- configMap:\nname: ip-masq-agent-config optional: true items:\n- key: ip-masq-agent\npath: ip-masq-agent mode: 0444 ```\n\nOnce you deploy this DaemonSet, you should see instances of the agent running on each node in your cluster.\n\n### Create Configuration\n\nNext, we need to create a ConfigMap that contains any configuration data we need to vary from the default deployed with the agent.\n\nThe main thing we need to configure is the IP ranges that will be masqueraded as an agent IP. The default deployment of ip-masq-agent-v2 disables masquerading for all three private IP ranges specified by [RFC 1918](https://tools.ietf.org/html/rfc1918) (10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16). In our example above, this will therefore not masquerade traffic to the 10.1.64.0/18 subnet in the app network, and our routing problem will still exist. We need to amend the configuration so that these private IPs are masqueraded. However, we do want to avoid masquerading within our AKS network, as this traffic needs to come from the pod IPs. Therefore, we need to ensure we do not masquerade for traffic going from the pods to:\n\n- The pod subnet\n- The node subnet\n- The AKS service CIDR range, for internal networking in AKS\n\nTo do this, we need to add these IP ranges to the nonMasqueradeCIDRs array in the configuration. This is the list of IP addresses which, when traffic is **sent to** them, will continue to come from the pod IP and not the node IP.\n\nIn addition, the configuration also allows us to define if we masquerade the link-local IPs, which we do not want to do. Below is an example ConfigMap that works for the setup detailed above.\n\n``` apiVersion: v1 kind: ConfigMap metadata: name: ip-masq-agent-config namespace: kube-system labels: component: ip-masq-agent kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: EnsureExists data: ip-masq-agent: |- nonMasqueradeCIDRs:\n- 10.0.0.0/16 # Entire VNet and service CIDR\n- 192.168.0.0/16\nmasqLinkLocal: false masqLinkLocalIPv6: false\n\n```\n\nThere are a couple of things to be aware of here:\n\n1. The node subnet and AKS Service CIDR are two contiguous address spaces in my setup, so both are covered by 10.0.0.0/16. I could have called them out separately.\n2. 192.168.0.0/16 covers the whole of my pod subnet.\n3. I do not enable masquerading on link-local.\n4. The ConfigMap needs to be created in the same namespace as the DaemonSet.\n5. The ConfigMap name needs to match what is used in the mount in the DaemonSet manifest.\n\nOnce you apply this configuration, the agent will pick up the new configuration changes within around 60 seconds. Once these are applied, you should find that traffic going to private addresses outside of the list of nonMasqueradeCIDRs will now present from the node IP.\n\n## Summary\n\nIf you’re deploying AKS into an IP-constrained environment, overlay networking is generally the best and simplest option. It allows you to use non-routed pod IP ranges, conserve address space, and avoid complex routing considerations without additional configuration. If you can use it, then this should be your default approach.\n\nHowever, there are cases where overlay networking will not meet your needs. You might require features only available with pod subnet mode, such as the ability to send traffic directly to pods and nodes without tunnelling, or support for features like Virtual Nodes. In these situations, you can still keep your pod subnet private and non-routed by carefully controlling IP masquerading. With ip-masq-agent-v2, you can configure which destinations should (and should not) be NAT’d, ensuring isolated subnets while maintaining the functionality you need.\n\nUpdated Aug 12, 2025\n\nVersion 1.0\n\n[azure kubernetes service](/tag/azure%20kubernetes%20service?nodeId=board%3AAppsonAzureBlog)\n\n[containers](/tag/containers?nodeId=board%3AAppsonAzureBlog)\n\n[!\\[samcogan&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yOTgxNTI4LVdSRVN4Rg?image-coordinates=64%2C0%2C1938%2C1873&amp;image-dimensions=50x50)](/users/samcogan/2981528) [samcogan](/users/samcogan/2981528) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined April 03, 2025\n\n[View Profile](/users/samcogan/2981528)\n\n/category/azure/blog/appsonazureblog [Apps on Azure Blog](/category/azure/blog/appsonazureblog) Follow this blog board to get notified when there's new activity",
  "Title": "Private Pod Subnets in AKS Without Overlay Networking",
  "FeedName": "Microsoft Tech Community",
  "Tags": [],
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Description": "When deploying AKS clusters, a common concern is the amount of IP address space required. If you are deploying your AKS cluster into your corporate network, the size of the IP address space you can obtain may be quite small, which can cause problems with the number of pods you are able to deploy.\n\nThe simplest and most common solution to this is to use an overlay network, which is [fully supported in AKS](https://learn.microsoft.com/en-us/azure/aks/azure-cni-overlay?tabs=kubectl). In an overlay network, pods are deployed to a private, non-routed address space that can be as large as you want. Translation between the routable and non-routed network is handled by AKS. For most people, this is the best option for dealing with IP addressing in AKS, and there is no need to complicate things further.\n\nHowever, there are some limitations with overlay networking, primarily that you cannot address the pods directly from the rest of the network— all inbound communication must go via services. There are also some advanced features that are not supported, such as Virtual Nodes.\n\nIf you are in a scenario where you need some of these features, and overlay networking will not work for you, it is possible to use the more traditional vNet-based deployment method, with some tweaks.\n\n## Azure CNI Pod Subnet\n\nThe alternative to using the Azure CNI Overlay is to use the Azure CNI Pod Subnet. In this setup, you deploy a vNet with two subnets - one for your nodes and one for pods. You are in control of the IP address configuration for these subnets.\n\nTo conserve IP addresses, you can create your pod subnet using an IP range that is not routable to the rest of your corporate network, allowing you to make it as large as you like. The node subnet remains routable from your corporate network. In this setup, if you want to talk to the pods directly, you would need to do so from within the AKS vNet or peer another network to your pod subnet. You would not be able to address these pods from the rest of your corporate network, even without using overlay networking.\n\n### The Routing Problem\n\nWhen you deploy a setup using Azure CNI Pod Subnet, all the subnets in the vNet are configured with routes and can talk to each other. You may wish to connect this vNet to other Azure vNets via peering, or to your corporate network using ExpressRoute or VPN.\n\nHowever, where you will encounter an issue is if your pods try to connect to resources outside of your AKS vNet but inside your corporate network, or any peered Azure vNets (which are not peered to this isolated subnet). In this scenario, the pods will route their traffic directly out of the vNet using their private IP address. This private IP is not a valid, routable IP, so the resources on the other network will not be able to reply, and the request will fail.\n\n![]()\n\n## IP Masquerading\n\nTo resolve this issue, we need a way to have traffic going to other networks present a private IP that is routable within the network. This can be achieved through several methods.\n\nOne method would be to introduce a separate solution for routing this traffic, such as Azure Firewall or another Network Virtual Appliance (NVA). Traffic is routable between the pod and node subnet, so the pod can send its requests to the firewall, and then the requests to the remote network come from the IP of the firewall, which is routable.\n\n![]()\n\nThis solution will work but does require another resource to be deployed, with additional costs. If you are already using an Azure Firewall for outbound traffic, then this may be something you could use, but we are looking for a simpler and more cost-effective solution.\n\nRather than implementing another device to present a routable IP, we can use the nodes of our AKS clusters. The AKS nodes are in the routable node subnet, so ideally we want our outbound traffic from the pods to use the node IP when it needs to leave the vNet to go to the rest of the private network.\n\n![]()\n\nThere are several different ways you could achieve this goal. You could look at using Egress Gateway services through tools like Istio, or you could look at making changes to the iptables configuration on the nodes using a DaemonSet. In this article, we will focus on using a tool called [ip-masq-agent-v2](https://github.com/Azure/ip-masq-agent-v2). This tool provides a means for traffic to \"masquerade\" as coming from the IP address of the node it is running on and have the node perform Network Address Translation (NAT).\n\nIf you deploy a cluster with an overlay network, this tool is already deployed and configured on your cluster. This is the tool that Microsoft uses to configure NAT for traffic leaving the overlay network. When using pod subnet clusters, this tool is not deployed, but you can deploy it yourself to provide the same functionality. Under the hood, this tool is making changes to iptables using a DaemonSet that runs on each node, so you could replicate this behaviour yourself—but this provides a simpler process that has been tested with AKS through overlay networking.\n\nThe Microsoft v2 version of this is based on the original Kubernetes contribution, aiming to solve more specific networking cases, allow for more configuration options, and improve observability.\n\n### Deploy ip-masq-agent-v2\n\nThere are two parts to deploying the agent. First, we deploy the agent, which runs as a DaemonSet, spawning a pod on each node in the cluster. This is important, as each node needs to have the iptables altered by the tool, and it needs to run anytime a new node is created.\n\nTo deploy the agent, we need to create the DaemonSet in our cluster. The [ip-masq-agent-v2 repo](https://github.com/Azure/ip-masq-agent-v2) includes several examples, including [an example of deploying the DaemonSet](https://github.com/Azure/ip-masq-agent-v2/blob/master/examples/ip-masq-agent.yaml). The example is slightly out of date on the version of ip-masq-agent-v2 to use, so make sure you update this to the latest version.\n\nIf you would prefer to build and manage your own containers for this, the repository also includes a Dockerfile to allow you to do this.\n\nBelow is an example deployment using the Microsoft-hosted images. It references the ConfigMap we will create in the next step, and it is important that the same name is used as is referenced here.\n\n- apiVersion: apps/v1\nkind: DaemonSet metadata: name: ip-masq-agent namespace: kube-system labels: component: ip-masq-agent kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: ip-masq-agent template: metadata: labels: k8s-app: ip-masq-agent spec: hostNetwork: true containers:\n- name: ip-masq-agent\nimage: mcr.microsoft.com/aks/ip-masq-agent-v2:v0.1.15 imagePullPolicy: Always securityContext: privileged: false capabilities: add: [\"NET\\_ADMIN\", \"NET\\_RAW\"] volumeMounts:\n- name: ip-masq-agent-volume\nmountPath: /etc/config readOnly: true volumes:\n- name: ip-masq-agent-volume\nprojected: sources:\n- configMap:\nname: ip-masq-agent-config optional: true items:\n- key: ip-masq-agent\npath: ip-masq-agent mode: 0444\n\nOnce you deploy this DaemonSet, you should see instances of the agent running on each node in your cluster.\n\n![]()\n\n### Create Configuration\n\nNext, we need to create a ConfigMap that contains any configuration data we need to vary from the default deployed with the agent.\n\nThe main thing we need to configure is the IP ranges that will be masqueraded as an agent IP. The default deployment of ip-masq-agent-v2 disables masquerading for all three private IP ranges specified by [RFC 1918](https://tools.ietf.org/html/rfc1918) (10.0.0.0/8, 172.16.0.0/12, and 192.168.0.0/16). In our example above, this will therefore not masquerade traffic to the 10.1.64.0/18 subnet in the app network, and our routing problem will still exist. We need to amend the configuration so that these private IPs are masqueraded. However, we do want to avoid masquerading within our AKS network, as this traffic needs to come from the pod IPs. Therefore, we need to ensure we do not masquerade for traffic going from the pods to:\n\n- The pod subnet\n- The node subnet\n- The AKS service CIDR range, for internal networking in AKS\n\nTo do this, we need to add these IP ranges to the nonMasqueradeCIDRs array in the configuration. This is the list of IP addresses which, when traffic is **sent to** them, will continue to come from the pod IP and not the node IP.\n\nIn addition, the configuration also allows us to define if we masquerade the link-local IPs, which we do not want to do. Below is an example ConfigMap that works for the setup detailed above.\n- apiVersion: v1\nkind: ConfigMap metadata: name: ip-masq-agent-config namespace: kube-system labels: component: ip-masq-agent kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: EnsureExists data: ip-masq-agent: |- nonMasqueradeCIDRs:\n- 10.0.0.0/16 # Entire VNet and service CIDR\n- 192.168.0.0/16\nmasqLinkLocal: false masqLinkLocalIPv6: false\n\nThere are a couple of things to be aware of here:\n\n1. The node subnet and AKS Service CIDR are two contiguous address spaces in my setup, so both are covered by 10.0.0.0/16. I could have called them out separately.\n2. 192.168.0.0/16 covers the whole of my pod subnet.\n3. I do not enable masquerading on link-local.\n4. The ConfigMap needs to be created in the same namespace as the DaemonSet.\n5. The ConfigMap name needs to match what is used in the mount in the DaemonSet manifest.\n\nOnce you apply this configuration, the agent will pick up the new configuration changes within around 60 seconds. Once these are applied, you should find that traffic going to private addresses outside of the list of nonMasqueradeCIDRs will now present from the node IP.\n\n## Summary\n\nIf you’re deploying AKS into an IP-constrained environment, overlay networking is generally the best and simplest option. It allows you to use non-routed pod IP ranges, conserve address space, and avoid complex routing considerations without additional configuration. If you can use it, then this should be your default approach.\n\nHowever, there are cases where overlay networking will not meet your needs. You might require features only available with pod subnet mode, such as the ability to send traffic directly to pods and nodes without tunnelling, or support for features like Virtual Nodes. In these situations, you can still keep your pod subnet private and non-routed by carefully controlling IP masquerading. With ip-masq-agent-v2, you can configure which destinations should (and should not) be NAT’d, ensuring isolated subnets while maintaining the functionality you need.",
  "PubDate": "2025-08-12T13:36:46+00:00",
  "ProcessedDate": "2025-08-12 14:08:30",
  "Author": "samcogan"
}
