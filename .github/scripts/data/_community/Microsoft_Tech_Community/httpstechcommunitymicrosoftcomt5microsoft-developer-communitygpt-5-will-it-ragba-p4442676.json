{
  "Link": "https://techcommunity.microsoft.com/t5/microsoft-developer-community/gpt-5-will-it-rag/ba-p/4442676",
  "OutputDir": "_community",
  "Title": "GPT-5: Will it RAG?",
  "Author": "Pamela_Fox",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Tags": [],
  "Description": "OpenAI released the GPT-5 model family last week, with an emphasis on accurate tool calling and reduced hallucinations. For those of us working on RAG (Retrieval-Augmented Generation), it's particularly exciting to see a model specifically trained to reduce hallucination.\n\nThere are five variants in the family:\n\n- **gpt-5**\n- **gpt-5-mini**\n- **gpt-5-nano**\n- **gpt-5-chat**: Not a reasoning model, optimized for chat applications\n- **gpt-5-pro**: Only available in ChatGPT, not via the API\n\nAs soon as GPT-5 models were [available in Azure AI Foundry](https://azure.microsoft.com/en-us/blog/gpt-5-in-azure-ai-foundry-the-future-of-ai-apps-and-agents-starts-here/), I deployed them and evaluated them inside our [most popular open source RAG template](https://github.com/Azure-Samples/azure-search-openai-demo/). I was immediately impressed - not by the model's ability to answer a question, but by it's ability to admit it *could not* answer a question!\n\nYou see, we have one test question for our sample data (HR documents for a fictional company's) that sounds like it should be an easy question: \"What does a Product Manager do?\" But, if you actually look at the company documents, there's no job description for \"Product Manager\", only related jobs like \"Senior Manager of Product Management\". Every other model, including the reasoning models, has still pretended that it could answer that question. For example, here's a response from o4-mini:\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQ0L7EVyhtpyQnwFXKqH2WKoa-e5jT32iG_1zoNk66mlu8ZCZgNZapfqCJV2ZchzA9Wg6z5bZHBfhn8QxDXG0bK8hoa_8wPPjmaHm_tTPthMpyJFVcgZOig7Hfg-7JnyIOkRv6Az0e2BvUoYD9RIWzHk4jVCOv8qzYCxth-nhrlpoTp4IXEHtkTESJg/s1600/o4mini.png)\n\nHowever, the gpt-5 model realizes that it doesn't have the information necessary, and responds that it cannot answer the question:\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCnp58zoe-CvbIuf6orq5jCYhA2mpRLKMOMYr9EJwU3wPEh4FN0M0RAvwo3xSVo3TPMRWYvrUk1I1lnEiyAPTpcuLTLG2zaFktLaIqr4nel2y-5b4J5WwhsgvuzzjjkRIGsTP-72n-cc8q8c-Itrr96FkGBfk8MemT3DG27VRqj1l8vrgksbpCm9BUbQ/s1600/gpt5mini.png)\n\n[As I always say](https://techcommunity.microsoft.com/blog/educatordeveloperblog/evaluating-a-rag-chat-app-can-your-app-say-i-dont-know/4079118): I would much rather have an LLM admit that it doesn't have enough information instead of making up an answer.\n\n## Bulk evaluation\n\nBut that's just a single question! What we really need to know is whether the GPT-5 models will generally do a better job across the board, on a wide range of questions. So I ran bulk evaluations using the [azure-ai-evaluations SDK](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/evaluation/azure-ai-evaluation), checking my favorite metrics: **`groundedness`** (LLM-judged), **`relevance`** (LLM-judged), and **`citations_matched`** (regex based off ground truth citations). I didn't bother evaluating gpt-5-nano, as I did some quick manual tests and wasn't impressed enough - plus, we've never used a nano sized model for our RAG scenarios. Here are the results for 50 Q/A pairs:\n\n| metric | stat | gpt-4.1-mini | gpt-4o-mini | gpt-5-chat | gpt-5 | gpt-5-mini | o3-mini | | --- | --- | --- | --- | --- | --- | --- | --- | | **`groundedness`** | pass % | 94% | 86% | 96% | 100% üèÜ | 94% | 96% | | ‚Üë | mean score | 4.76 | 4.50 | 4.86 | 5.00 üèÜ | 4.82 | 4.80 | | **`relevance`** | pass % | 94% üèÜ | 84% | 90% | 90% | 74% | 90% | | ‚Üë | mean score | 4.42 üèÜ | 4.22 | 4.06 | 4.20 | 4.02 | 4.00 | | **`answer_length`** | mean | 829 | 919 | 549 | 844 | 940 | 499 | | **`latency`** | mean | 2.9 | 4.5 | 2.9 | 9.6 | 7.5 | 19.4 | | **`citations_matched`** | % | 52% | 49% | 52% | 47% | 49% | 51% |\n\nFor the LLM-judged metrics of **`groundedness`** and **`relevance`**, the LLM awards a score of 1-5, and both 4 and 5 are considered passing scores. That's why you see both a \"pass %\" (percentage with 4 or 5 score) and an average score in the table above.\n\nFor the **`groundedness`** metric, which measures whether an answer is grounded in the retrieved search results, the gpt-5 model does the best (100%), while the other gpt-5 models do quite well as well, on par with our current default model of gpt-4.1-mini. For the **`relevance`** metric, which measures whether an answer fully answers a question, the gpt-5 models don't score as highly as gpt-4.1-mini. I looked into the discrepancies there, and I think that's actually due to gpt-5 being less willing to give an answer when it's not fully confident in it - it would rather give a partial answer instead. That's a good thing for RAG apps, so I am comfortable with that metric being less than 100%.\n\nThe **`latency`** metric is generally higher for the gpt-5 reasoning models, as would be expected, but is also variable based on on deployment region, region capacity, etc, assuming you're not using a \"provisioned thoroughput\" deployment. Also note that the latency here records the total time taken, from first token to last token, whereas the most important metric for a user-facing streaming chat is the time to *first* token.\n\nFor the gpt-5 reasoning models in this evaluation, I set the `reasoning_effort` to \"minimal\", which means the model chooses whether to use reasoning tokens. I have never seen it actually use any reasoning tokens when I set the effort to minimal, so maybe that means that a higher reasoning effort is really only needed for longer or more complex tasks, and RAG answering is a non-reasoning task. A higher reasoning effort would definitely affect the latency and likely also affect the answer quality. I did not test that out, since the \"minimal\" effort setting already results in high quality answers.\n\n## Answer differences\n\nNow that we've seen the overall metrics, let's dig into some actual answers, and see some of the key ways that GPT-5 model answers differ.\n\n### Quicker to say \"I don't know\" ü§∑üèª\n\nLet's start with the first thing I noticed: gpt-5 was happy to admit when it couldn't fully answer a question. In the 50 questions evaluated, the gpt-5 model started off their answer with \"I don't know\" for 3 of the answers, while gpt-5-mini started that way for 10 of the answers. In table form:\n\n| gpt-4.1-mini | gpt-5 | gpt-5-mini | gpt-5-chat | | --- | --- | --- | --- | | 1 (1.6%) | 3 (6%) | 10 (20%) | 1 (1.6%) |\n\nHere's an example of a gpt-4.1-mini answer versus a gpt-5 answer:\n\n**Q: Explain the eligibility criteria for the Northwind Standard plan offered to Contoso employees?**\n\n| gpt-4.1-mini | gpt-5 | | --- | --- | | The eligibility criteria for the Northwind Standard plan for Contoso employees are not explicitly detailed in the provided sources. However, it is mentioned that prior-authorization may be required for certain services or treatments, which involves approval from Northwind Health based on medical necessity and coverage under the plan. Employees must have their physician submit a request for prior-authorization, and in some cases, meet specific criteria such as medical tests, treatments, or lifestyle changes to be approved. Preventive care, emergency services, mental health, and substance abuse coverage do not require prior-authorization. Out-of-network services do require prior-authorization and may incur additional costs [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=63][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=102]. If you need information on basic eligibility (such as employment status or tenure), that is not provided in these sources. | I don‚Äôt know. The provided sources describe compliance (ERISA, COBRA, HIPAA, ACA) and note that Northwind Standard is a group plan covering ‚Äúall eligible employees of Contoso and their spouses or dependents,‚Äù but they do not define the specific eligibility criteria (e.g., hours worked, waiting period, employment class). [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=88][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=85][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=86] |\n\nBoth of the answers admit some degree of not-knowing, but gpt-5 comes right out with it at the start of the answer. Notably, our [system prompt](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_answer_question.prompty#L23) includes this instruction:\n\n> Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n\nSo what we may be seeing is a combination of two factors: 1) the gpt-5 reasoning models are generally better at following instructions, 2) the gpt-5 reasoning models went through a specific alignment process to reduce hallucination. Notably, however, the gpt-5-chat model rarely responded with \"I don't know\", so it is not as affected by whatever factors are increasing the likelihood of the reasoning models to answer that way.\n\nYou may be wondering why I have questions in the evaluation set that aren't fully answerable - it's hard to generate a good set of ground truth for evaluation, and my ground truth data is not the optimal set of questions. Ideally, I would do a separate evaluation for all the fully knowable questions versus all the unknowable questions - and I even have a specific custom metric for \"don't-know-ness\" that I run on those - but a few partially knowable questions snuck into this ground truth data set.\n\n### Requests for clarification üôãüèª‚Äç‚ôÄÔ∏è\n\nOur [system prompt](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_answer_question.prompty#L23) also encourages the model to ask clarifying questions when necessary:\n\n> If asking a clarifying question to the user would help, ask the question.\n\nBut I had never actually seen a clarifying question from a model until gpt-5-chat, which replied with a question to 2 of the 50 questions. Example:\n\n**Q: What should I do if I need healthcare services outside Washington with Northwind Health?**\n\n| gpt-4.1-mini | gpt-5-chat | | --- | --- | | If you have Northwind Standard and need healthcare services outside Washington, you must meet these criteria for coverage: be outside Washington for no more than 12 consecutive months, be more than 100 miles away from your primary care provider, and have had your plan in effect for at least 6 months before receiving out-of-area care. You must contact Northwind Health in advance and obtain prior authorization for the care; otherwise, you may be responsible for the full cost. Emergency services outside Washington are covered, but non-emergency services are not unless these conditions are met [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=59][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=60]. If you have Northwind Health Plus, first ensure the provider is in the nationwide network by checking the online directory or calling customer service. Keep receipts and documentation for any out-of-state services, as you may need to submit claims for reimbursement. Contact Northwind Health Plus before receiving services to confirm coverage, as not all services outside Washington are covered [Northwind\\_Health\\_Plus\\_Benefits\\_Details.pdf#page=62]. | Which plan do you have ‚Äî Northwind Standard or Northwind Health Plus? |\n\nLike in the example above, other models typically avoid asking a clarifying question by instead addressing every possible scenario (covering all their bases!), resulting in a fairly long answer. Arguably, it's a better user experience to ask the clarifying question, so that the user can get an answer tailored to their specific situation and not get confused by other possibilities. The primary drawback is the additional back-and-forth for the final answer.\n\nI did not personally see clarifying questions from the GPT-5 reasoning models, but my colleagues noticed an increase in such questions when using gpt-5 with GitHub Copilot Agent mode, so it's possible that every model in the family has an increased likelihood to ask for clarification when faced with ambiguity.\n\n### Reasoning models ‚ù§Ô∏è lists\n\nI saw a definite tendency of the GPT-5 reasoning models to output the answer as a list, even though there's nothing in the RAG system prompt that tells it to use lists. Out of the 50 questions evaluated, here are the number with answers formatted as a top-level bulleted list:\n\n| gpt-4.1-mini | gpt-5 | gpt-5-mini | gpt-5-chat | | --- | --- | --- | --- | | 0 | 36 (72%) | 26 (52%) | 0 |\n\nHere's an example of a gpt-4.1-mini answer (paragraph) versus a gpt-5 answer (list):\n\n**Q: What are the coverage limitations of the Northwind Standard plan regarding clinical trials?**\n\n| gpt-4.1-mini | gpt-5 | | --- | --- | | The Northwind Standard plan covers certain clinical trial services such as diagnostic testing, treatment of the condition being studied, medications, lab services, and imaging services. However, it does not cover travel expenses associated with attending clinical trials. Additionally, any experimental treatments or services not part of the clinical trial are not covered. Coverage for other clinical trial services not explicitly listed may be considered on a case-by-case basis. Members should contact Northwind Health customer service for more details [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=23][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=24]. | <ul><br><li>Travel expenses for attending clinical trials are not covered [Northwind_Standard_Benefits_Details.pdf#page=23][Northwind_Standard_Benefits_Details.pdf#page=24].</li><br><br><li>Experimental treatments or services that are not part of the clinical trial are not covered [Northwind_Standard_Benefits_Details.pdf#page=24].</li><br><br><li>Services or treatments deemed experimental or investigational are not covered by the plan [Northwind_Standard_Benefits_Details.pdf#page=53].</li><br><br></ul> |\n\nNow, is it a bad thing that the gpt-5 reasoning models use lists? Not necessarily! But if that's not the style you're looking for, then you either want to consider the gpt-5-chat model or add specific messaging in the system prompt to veer the model away from top level lists.\n\n### Longer answers\n\nAs we saw in overall metrics above, there was an impact of the answer length (measured in the number of characters, not tokens). Let's isolate those stats:\n\n| gpt-4.1-mini | gpt-5 | gpt-5-mini | gpt-5-chat | | --- | --- | --- | --- | | 829 | 844 | 990 | 549 |\n\nThe gpt-5 reasoning models are generating answers of similar length to the current baseline of gpt-4.1-mini, though the gpt-5-mini model seems to be a bit more verbose. The API now has a new parameter to control verbosity for those models, which defaults to \"medium\". I did not try an evaluation with that set to \"low\" or \"high\", which would be an interesting evaluation to run.\n\nThe gpt-5-chat model outputs relatively short answers, which are actually closer in length to the answer length that I used to see from [gpt-3.5-turbo](https://blog.pamelafox.org/2025/03/gpt-4o-mini-vs-gpt-35-turbo-for-rag.html).\n\nWhat answer length is best? A longer answer will take longer to finish rendering to the user (even when streaming), and will cost the developer more tokens. However, sometimes answers are longer due to better formatting that is easier to skim, so longer does not always mean less readable. For the user-facing RAG chat scenario, I generally think that shorter answers are better. If I was putting these gpt-5 reasoning models in production, I'd probably try out the \"low\" verbosity value, or put instructions in the system prompt, so that users get their answers more quickly. They can always ask follow-up questions as needed.\n\n### Fancy punctuation\n\nThis is a weird difference that I discovered while researching the other differences: the GPT-5 models are more likely to use ‚Äúsmart‚Äù quotes instead of standard ASCII quotes. Specifically:\n\n- Left single: ‚Äò (U+2018)\n- Right single / apostrophe: ‚Äô (U+2019)\n- Left double: ‚Äú (U+201C)\n- Right double: ‚Äù (U+201D)\n\nFor example, the gpt-5 model actually responded with \"`I don‚Äôt know` \", not with \"`I don't know` \". It's a subtle difference, but if you are doing any sort of post-processing or analysis, it's good to know. I've also seen the models using the smart quotes incorrectly in coding contexts (like GitHub Copilot Agent mode), so that's another potential issue to look out for. I assumed that the models were trained on data that tended to use smart quotes more often, perhaps synthetic data or book text. I know that as a normal human, I rarely use them, given the extra effort required to type them.\n\n### Query rewriting to the extreme\n\nOur RAG flow makes two LLM calls: the second answers the question, as you‚Äôd expect, but the first rewrites the user‚Äôs query into a strong search query. This step can fix spelling mistakes, but it‚Äôs even more important for filling in missing context in multi-turn conversations‚Äîlike when the user simply asks, ‚Äúwhat else?‚Äù A well-crafted rewritten query leads to better search results, and ultimately, a more complete and accurate answer.\n\nDuring my manual tests, I noticed that the rewritten queries from the GPT-5 models are *much* longer, filled to the brim with synonyms. For example:\n\n**Q: What does a Product Manager do?**\n\n| gpt-4.1-mini | gpt-5-mini | | --- | --- | | product manager responsibilities | Product Manager role responsibilities duties skills day-to-day tasks product management overview |\n\nAre these new rewritten queries better or worse than the previous short ones? It's hard to tell, since they're just one factor in the overall answer output, and I haven't set up a retrieval-specific metric. The closest metric is **`citations_matched`**, since the new answer from the app can only match the citations in the ground truth if the app managed to retrieve all the same citations. That metric was generally high for these models, and when I looked into the cases where the citations didn't match, I typically thought the gpt-5 family of responses were still good answers. I suspect that the rewritten query does not have a huge effect either way, since our retrieval step uses hybrid search from Azure AI Search, and the combined power of both hybrid and vector search generally compensates for differences in search query wording.\n\nIt's worth evaluating this further however, and considering using a different model for the query rewriting step. Developers often choose to use a smaller, faster model for that stage, since query rewriting is an easier task than answering a question.\n\n## So, are the answers accurate?\n\nEven with a 100% groundedness score from an LLM judge, it's possible that a RAG app can be producing inaccurate answers, like if the LLM judge is biased or the retrieved context is incomplete. The only way to really know if RAG answers are accurate is to send them to a human expert. For the sample data in this blog, there is no human expert available, since they're based off synthetically generated documents. Despite two years of staring at those documents and running dozens of evaluations, I still am not an expert in the HR benefits of the fictional Contoso company.\n\nThat's why I also ran the same evaluations on the same RAG codebase, but with data that I know intimately: my own personal blog. I looked through 200 answers from gpt-5, and did not notice any inaccuracies in the answers. Yes, there are times when it says \"I don't know\" or asks a clarifying question, but I consider those to be accurate answers, since they do not spread misinformation. I imagine that I could find some way to trick up the gpt-5 model, but on the whole, it looks like a model with a high likelihood of generating accurate answers when given relevant context.\n\n## Evaluate for yourself!\n\nI share my evaluations on our sample RAG app as a way to share general learnings on model differences, but I encourage every developer to evaluate these models for your specific domain, alongside domain experts that can reason about the correctness of the answers. How can you evaluate?\n\n- If you are using the same [open source RAG project for Azure](https://github.com/Azure-Samples/azure-search-openai-demo/), deploy the GPT-5 models and follow the steps in the [evaluation guide](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/docs/evaluation.md).\n- If you have your own solution, you can use an open-source SDK for evaluating, like [azure-ai-evaluation](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/evaluation/azure-ai-evaluation) (the one that I use), [DeepEval](https://deepeval.com/), [promptfoo](https://github.com/promptfoo/promptfoo), etc. If you are using an observability platform like Langfuse, Arize, or Langsmith, they have evaluation strategies baked in. Or if you're using an agents framework like Pydantic AI, those also often have built-in eval mechanisms.\n\nIf you can share what you learn from evaluations, please do! We are all learning about the strange new world of LLMs together.",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Community",
  "EnhancedContent": "## An evaluation of four new models for Retrieval-Augmented Generation\n\nOpenAI released the GPT-5 model family last week, with an emphasis on accurate tool calling and reduced hallucinations. For those of us working on RAG (Retrieval-Augmented Generation), it's particularly exciting to see a model specifically trained to reduce hallucination.\n\nThere are five variants in the family:\n\n- **gpt-5**\n- **gpt-5-mini**\n- **gpt-5-nano**\n- **gpt-5-chat**: Not a reasoning model, optimized for chat applications\n- **gpt-5-pro**: Only available in ChatGPT, not via the API\n\nAs soon as GPT-5 models were [available in Azure AI Foundry](https://azure.microsoft.com/en-us/blog/gpt-5-in-azure-ai-foundry-the-future-of-ai-apps-and-agents-starts-here/), I deployed them and evaluated them inside our [most popular open source RAG template](https://github.com/Azure-Samples/azure-search-openai-demo/). I was immediately impressed - not by the model's ability to answer a question, but by it's ability to admit it *could not* answer a question!\n\nYou see, we have one test question for our sample data (HR documents for a fictional company's) that sounds like it should be an easy question: \"What does a Product Manager do?\" But, if you actually look at the company documents, there's no job description for \"Product Manager\", only related jobs like \"Senior Manager of Product Management\". Every other model, including the reasoning models, has still pretended that it could answer that question. For example, here's a response from o4-mini:\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgbQ0L7EVyhtpyQnwFXKqH2WKoa-e5jT32iG_1zoNk66mlu8ZCZgNZapfqCJV2ZchzA9Wg6z5bZHBfhn8QxDXG0bK8hoa_8wPPjmaHm_tTPthMpyJFVcgZOig7Hfg-7JnyIOkRv6Az0e2BvUoYD9RIWzHk4jVCOv8qzYCxth-nhrlpoTp4IXEHtkTESJg/s1600/o4mini.png)\n\nHowever, the gpt-5 model realizes that it doesn't have the information necessary, and responds that it cannot answer the question:\n\n![](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhCnp58zoe-CvbIuf6orq5jCYhA2mpRLKMOMYr9EJwU3wPEh4FN0M0RAvwo3xSVo3TPMRWYvrUk1I1lnEiyAPTpcuLTLG2zaFktLaIqr4nel2y-5b4J5WwhsgvuzzjjkRIGsTP-72n-cc8q8c-Itrr96FkGBfk8MemT3DG27VRqj1l8vrgksbpCm9BUbQ/s1600/gpt5mini.png)\n\n[As I always say](https://techcommunity.microsoft.com/blog/educatordeveloperblog/evaluating-a-rag-chat-app-can-your-app-say-i-dont-know/4079118): I would much rather have an LLM admit that it doesn't have enough information instead of making up an answer.\n\n## Bulk evaluation\n\nBut that's just a single question! What we really need to know is whether the GPT-5 models will generally do a better job across the board, on a wide range of questions. So I ran bulk evaluations using the [azure-ai-evaluations SDK](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/evaluation/azure-ai-evaluation), checking my favorite metrics: **`groundedness`** (LLM-judged), **`relevance`** (LLM-judged), and **`citations_matched`** (regex based off ground truth citations). I didn't bother evaluating gpt-5-nano, as I did some quick manual tests and wasn't impressed enough - plus, we've never used a nano sized model for our RAG scenarios. Here are the results for 50 Q/A pairs:\n\n| metric | stat | gpt-4.1-mini | gpt-4o-mini | gpt-5-chat | gpt-5 | gpt-5-mini | o3-mini | | --- | --- | --- | --- | --- | --- | --- | --- | | **`groundedness`** | pass % | 94% | 86% | 96% | 100% üèÜ | 94% | 96% | | ‚Üë | mean score | 4.76 | 4.50 | 4.86 | 5.00 üèÜ | 4.82 | 4.80 | | **`relevance`** | pass % | 94% üèÜ | 84% | 90% | 90% | 74% | 90% | | ‚Üë | mean score | 4.42 üèÜ | 4.22 | 4.06 | 4.20 | 4.02 | 4.00 | | **`answer_length`** | mean | 829 | 919 | 549 | 844 | 940 | 499 | | **`latency`** | mean | 2.9 | 4.5 | 2.9 | 9.6 | 7.5 | 19.4 | | **`citations_matched`** | % | 52% | 49% | 52% | 47% | 49% | 51% |\n\nFor the LLM-judged metrics of **`groundedness`** and **`relevance`**, the LLM awards a score of 1-5, and both 4 and 5 are considered passing scores. That's why you see both a \"pass %\" (percentage with 4 or 5 score) and an average score in the table above.\n\nFor the **`groundedness`** metric, which measures whether an answer is grounded in the retrieved search results, the gpt-5 model does the best (100%), while the other gpt-5 models do quite well as well, on par with our current default model of gpt-4.1-mini. For the **`relevance`** metric, which measures whether an answer fully answers a question, the gpt-5 models don't score as highly as gpt-4.1-mini. I looked into the discrepancies there, and I think that's actually due to gpt-5 being less willing to give an answer when it's not fully confident in it - it would rather give a partial answer instead. That's a good thing for RAG apps, so I am comfortable with that metric being less than 100%.\n\nThe **`latency`** metric is generally higher for the gpt-5 reasoning models, as would be expected, but is also variable based on on deployment region, region capacity, etc, assuming you're not using a \"provisioned thoroughput\" deployment. Also note that the latency here records the total time taken, from first token to last token, whereas the most important metric for a user-facing streaming chat is the time to *first* token.\n\nFor the gpt-5 reasoning models in this evaluation, I set the `reasoning_effort` to \"minimal\", which means the model chooses whether to use reasoning tokens. I have never seen it actually use any reasoning tokens when I set the effort to minimal, so maybe that means that a higher reasoning effort is really only needed for longer or more complex tasks, and RAG answering is a non-reasoning task. A higher reasoning effort would definitely affect the latency and likely also affect the answer quality. I did not test that out, since the \"minimal\" effort setting already results in high quality answers.\n\n## Answer differences\n\nNow that we've seen the overall metrics, let's dig into some actual answers, and see some of the key ways that GPT-5 model answers differ.\n\n### Quicker to say \"I don't know\" ü§∑üèª\n\nLet's start with the first thing I noticed: gpt-5 was happy to admit when it couldn't fully answer a question. In the 50 questions evaluated, the gpt-5 model started off their answer with \"I don't know\" for 3 of the answers, while gpt-5-mini started that way for 10 of the answers. In table form:\n\n| gpt-4.1-mini | gpt-5 | gpt-5-mini | gpt-5-chat | | --- | --- | --- | --- | | 1 (1.6%) | 3 (6%) | 10 (20%) | 1 (1.6%) |\n\nHere's an example of a gpt-4.1-mini answer versus a gpt-5 answer:\n\n**Q: Explain the eligibility criteria for the Northwind Standard plan offered to Contoso employees?**\n\n| gpt-4.1-mini | gpt-5 | | --- | --- | | The eligibility criteria for the Northwind Standard plan for Contoso employees are not explicitly detailed in the provided sources. However, it is mentioned that prior-authorization may be required for certain services or treatments, which involves approval from Northwind Health based on medical necessity and coverage under the plan. Employees must have their physician submit a request for prior-authorization, and in some cases, meet specific criteria such as medical tests, treatments, or lifestyle changes to be approved. Preventive care, emergency services, mental health, and substance abuse coverage do not require prior-authorization. Out-of-network services do require prior-authorization and may incur additional costs [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=63][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=102]. If you need information on basic eligibility (such as employment status or tenure), that is not provided in these sources. | I don‚Äôt know. The provided sources describe compliance (ERISA, COBRA, HIPAA, ACA) and note that Northwind Standard is a group plan covering ‚Äúall eligible employees of Contoso and their spouses or dependents,‚Äù but they do not define the specific eligibility criteria (e.g., hours worked, waiting period, employment class). [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=88][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=85][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=86] |\n\nBoth of the answers admit some degree of not-knowing, but gpt-5 comes right out with it at the start of the answer. Notably, our [system prompt](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_answer_question.prompty#L23) includes this instruction:\n\n> Answer ONLY with the facts listed in the list of sources below. If there isn't enough information below, say you don't know.\n\nSo what we may be seeing is a combination of two factors: 1) the gpt-5 reasoning models are generally better at following instructions, 2) the gpt-5 reasoning models went through a specific alignment process to reduce hallucination. Notably, however, the gpt-5-chat model rarely responded with \"I don't know\", so it is not as affected by whatever factors are increasing the likelihood of the reasoning models to answer that way.\n\nYou may be wondering why I have questions in the evaluation set that aren't fully answerable - it's hard to generate a good set of ground truth for evaluation, and my ground truth data is not the optimal set of questions. Ideally, I would do a separate evaluation for all the fully knowable questions versus all the unknowable questions - and I even have a specific custom metric for \"don't-know-ness\" that I run on those - but a few partially knowable questions snuck into this ground truth data set.\n\n### Requests for clarification üôãüèª‚Äç‚ôÄÔ∏è\n\nOur [system prompt](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/app/backend/approaches/prompts/chat_answer_question.prompty#L23) also encourages the model to ask clarifying questions when necessary:\n\n> If asking a clarifying question to the user would help, ask the question.\n\nBut I had never actually seen a clarifying question from a model until gpt-5-chat, which replied with a question to 2 of the 50 questions.¬† Example:\n\n**Q: What should I do if I need healthcare services outside Washington with Northwind Health?**\n\n| gpt-4.1-mini | gpt-5-chat | | --- | --- | | If you have Northwind Standard and need healthcare services outside Washington, you must meet these criteria for coverage: be outside Washington for no more than 12 consecutive months, be more than 100 miles away from your primary care provider, and have had your plan in effect for at least 6 months before receiving out-of-area care. You must contact Northwind Health in advance and obtain prior authorization for the care; otherwise, you may be responsible for the full cost. Emergency services outside Washington are covered, but non-emergency services are not unless these conditions are met [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=59][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=60]. If you have Northwind Health Plus, first ensure the provider is in the nationwide network by checking the online directory or calling customer service. Keep receipts and documentation for any out-of-state services, as you may need to submit claims for reimbursement. Contact Northwind Health Plus before receiving services to confirm coverage, as not all services outside Washington are covered [Northwind\\_Health\\_Plus\\_Benefits\\_Details.pdf#page=62]. | Which plan do you have ‚Äî Northwind Standard or Northwind Health Plus? |\n\nLike in the example above, other models typically avoid asking a clarifying question by instead addressing every possible scenario (covering all their bases!), resulting in a fairly long answer. Arguably, it's a better user experience to ask the clarifying question, so that the user can get an answer tailored to their specific situation and not get confused by other possibilities. The primary drawback is the additional back-and-forth for the final answer.\n\nI did not personally see clarifying questions from the GPT-5 reasoning models, but my colleagues noticed an increase in such questions when using gpt-5 with GitHub Copilot Agent mode, so it's possible that every model in the family has an increased likelihood to ask for clarification when faced with ambiguity.\n\n### Reasoning models ‚ù§Ô∏è lists\n\nI saw a definite tendency of the GPT-5 reasoning models to output the answer as a list, even though there's nothing in the RAG system prompt that tells it to use lists. Out of the 50 questions evaluated, here are the number with answers formatted as a top-level bulleted list:\n\n| gpt-4.1-mini | gpt-5 | gpt-5-mini | gpt-5-chat | | --- | --- | --- | --- | | 0 | 36 (72%) | 26 (52%) | 0 |\n\nHere's an example of a gpt-4.1-mini answer (paragraph) versus a gpt-5 answer (list):\n\n**Q: What are the coverage limitations of the Northwind Standard plan regarding clinical trials?**\n\n| gpt-4.1-mini | gpt-5 | | --- | --- | | The Northwind Standard plan covers certain clinical trial services such as diagnostic testing, treatment of the condition being studied, medications, lab services, and imaging services. However, it does not cover travel expenses associated with attending clinical trials. Additionally, any experimental treatments or services not part of the clinical trial are not covered. Coverage for other clinical trial services not explicitly listed may be considered on a case-by-case basis. Members should contact Northwind Health customer service for more details [Northwind\\_Standard\\_Benefits\\_Details.pdf#page=23][Northwind\\_Standard\\_Benefits\\_Details.pdf#page=24]. | <ul><br><li>Travel expenses for attending clinical trials are not covered [Northwind_Standard_Benefits_Details.pdf#page=23][Northwind_Standard_Benefits_Details.pdf#page=24].</li><br><br><li>Experimental treatments or services that are not part of the clinical trial are not covered [Northwind_Standard_Benefits_Details.pdf#page=24].</li><br><br><li>Services or treatments deemed experimental or investigational are not covered by the plan [Northwind_Standard_Benefits_Details.pdf#page=53].</li><br><br></ul> |\n\nNow, is it a bad thing that the gpt-5 reasoning models use lists? Not necessarily! But if that's not the style you're looking for, then you either want to consider the gpt-5-chat model or add specific messaging in the system prompt to veer the model away from top level lists.\n\n### Longer answers\n\nAs we saw in overall metrics above, there was an impact of the answer length (measured in the number of characters, not tokens). Let's isolate those stats:\n\n| gpt-4.1-mini | gpt-5 | gpt-5-mini | gpt-5-chat | | --- | --- | --- | --- | | 829 | 844 | 990 | 549 |\n\nThe gpt-5 reasoning models are generating answers of similar length to the current baseline of gpt-4.1-mini, though the gpt-5-mini model seems to be a bit more verbose. The API now has a new parameter to control verbosity for those models, which defaults to \"medium\". I did not try an evaluation with that set to \"low\" or \"high\", which would be an interesting evaluation to run.\n\nThe gpt-5-chat model outputs relatively short answers, which are actually closer in length to the answer length that I used to see from [gpt-3.5-turbo](https://blog.pamelafox.org/2025/03/gpt-4o-mini-vs-gpt-35-turbo-for-rag.html).\n\nWhat answer length is best? A longer answer will take longer to finish rendering to the user (even when streaming), and will cost the developer more tokens. However, sometimes answers are longer due to better formatting that is easier to skim, so longer does not always mean less readable. For the user-facing RAG chat scenario, I generally think that shorter answers are better. If I was putting these gpt-5 reasoning models in production, I'd probably try out the \"low\" verbosity value, or put instructions in the system prompt, so that users get their answers more quickly. They can always ask follow-up questions as needed.\n\n### Fancy punctuation\n\nThis is a weird difference that I discovered while researching the other differences: the GPT-5 models are more likely to use ‚Äúsmart‚Äù quotes instead of standard ASCII quotes. Specifically:\n\n- Left single: ‚Äò (U+2018)\n- Right single / apostrophe: ‚Äô (U+2019)\n- Left double: ‚Äú (U+201C)\n- Right double: ‚Äù (U+201D)\n\nFor example, the gpt-5 model actually responded with \"`I don‚Äôt know` \", not with \"`I don't know` \". It's a subtle difference, but if you are doing any sort of post-processing or analysis, it's good to know. I've also seen the models using the smart quotes incorrectly in coding contexts (like GitHub Copilot Agent mode), so that's another potential issue to look out for. I assumed that the models were trained on data that tended to use smart quotes more often, perhaps synthetic data or book text. I know that as a normal human, I rarely use them, given the extra effort required to type them.\n\n### Query rewriting to the extreme\n\nOur RAG flow makes two LLM calls: the second answers the question, as you‚Äôd expect, but the first rewrites the user‚Äôs query into a strong search query. This step can fix spelling mistakes, but it‚Äôs even more important for filling in missing context in multi-turn conversations‚Äîlike when the user simply asks, ‚Äúwhat else?‚Äù A well-crafted rewritten query leads to better search results, and ultimately, a more complete and accurate answer.\n\nDuring my manual tests, I noticed that the rewritten queries from the GPT-5 models are *much* longer, filled to the brim with synonyms. For example:\n\n**Q: What does a Product Manager do?**\n\n| gpt-4.1-mini | gpt-5-mini | | --- | --- | | product manager responsibilities | Product Manager role responsibilities duties skills day-to-day tasks product management overview |\n\nAre these new rewritten queries better or worse than the previous short ones? It's hard to tell, since they're just one factor in the overall answer output, and I haven't set up a retrieval-specific metric. The closest metric is **`citations_matched`**, since the new answer from the app can only match the citations in the ground truth if the app managed to retrieve all the same citations. That metric was generally high for these models, and when I looked into the cases where the citations didn't match, I typically thought the gpt-5 family of responses were still good answers. I suspect that the rewritten query does not have a huge effect either way, since our retrieval step uses hybrid search from Azure AI Search, and the combined power of both hybrid and vector search generally compensates for differences in search query wording.\n\nIt's worth evaluating this further however, and considering using a different model for the query rewriting step. Developers often choose to use a smaller, faster model for that stage, since query rewriting is an easier task than answering a question.\n\n## So, are the answers accurate?\n\nEven with a 100% groundedness score from an LLM judge, it's possible that a RAG app can be producing inaccurate answers, like if the LLM judge is biased or the retrieved context is incomplete. The only way to really know if RAG answers are accurate is to send them to a human expert. For the sample data in this blog, there is no human expert available, since they're based off synthetically generated documents. Despite two years of staring at those documents and running dozens of evaluations, I still am not an expert in the HR benefits of the fictional Contoso company.\n\nThat's why I also ran the same evaluations on the same RAG codebase, but with data that I know intimately: my own personal blog. I looked through 200 answers from gpt-5, and did not notice any inaccuracies in the answers. Yes, there are times when it says \"I don't know\" or asks a clarifying question, but I consider those to be accurate answers, since they do not spread misinformation. I imagine that I could find some way to trick up the gpt-5 model, but on the whole, it looks like a model with a high likelihood of generating accurate answers when given relevant context.\n\n## Evaluate for yourself!\n\nI share my evaluations on our sample RAG app as a way to share general learnings on model differences, but I encourage every developer to evaluate these models for your specific domain, alongside domain experts that can reason about the correctness of the answers. How can you evaluate?\n\n- If you are using the same [open source RAG project for Azure](https://github.com/Azure-Samples/azure-search-openai-demo/), deploy the GPT-5 models and follow the steps in the [evaluation guide](https://github.com/Azure-Samples/azure-search-openai-demo/blob/main/docs/evaluation.md).\n- If you have your own solution, you can use an open-source SDK for evaluating, like [azure-ai-evaluation](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/evaluation/azure-ai-evaluation) (the one that I use), [DeepEval](https://deepeval.com/), [promptfoo](https://github.com/promptfoo/promptfoo), etc. If you are using an observability platform like Langfuse, Arize, or Langsmith, they have evaluation strategies baked in. Or if you're using an agents framework like Pydantic AI, those also often have built-in eval mechanisms.\n\nIf you can share what you learn from evaluations, please do! We are all learning about the strange new world of LLMs together.\n\nPublished Aug 13, 2025\n\nVersion 1.0\n\n[ai](/tag/ai?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure](/tag/azure?nodeId=board%3AAzureDevCommunityBlog)\n\n[azure ai foundry](/tag/azure%20ai%20foundry?nodeId=board%3AAzureDevCommunityBlog)\n\n[pamela fox](/tag/pamela%20fox?nodeId=board%3AAzureDevCommunityBlog)\n\n[!\\[Pamela_Fox&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xNjA0MDc4LTQxODI4MWk5MjkyQjFBMEVGOUE5NkM5?image-dimensions=50x50)](/users/pamela_fox/1604078) [Pamela_Fox](/users/pamela_fox/1604078) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined November 08, 2022\n\n[View Profile](/users/pamela_fox/1604078)\n\n/category/azure/blog/azuredevcommunityblog [Microsoft Developer Community Blog](/category/azure/blog/azuredevcommunityblog) Follow this blog board to get notified when there's new activity",
  "FeedName": "Microsoft Tech Community",
  "ProcessedDate": "2025-08-13 08:06:37",
  "PubDate": "2025-08-13T07:54:51+00:00"
}
