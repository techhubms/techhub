{
  "OutputDir": "_community",
  "Link": "https://techcommunity.microsoft.com/t5/educator-developer-blog/building-ai-agents-with-ease-function-calling-in-vs-code-ai/ba-p/4442637",
  "Title": "Building AI Agents with Ease: Function Calling in VS Code AI Toolkit",
  "ProcessedDate": "2025-08-15 17:13:29",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Community",
  "PubDate": "2025-08-15T17:00:24+00:00",
  "Description": "Function calling is a powerful technique that allows Large Language Models (LLMs) to go beyond their static training data and interact with real-world, dynamic information sources like databases and APIs. This capability turns a simple chat interface into a powerful tool for fetching real-time data, executing code, and much more.\n\nThe process of Tool/Function calling typically involves two main components: a client application and the LLM. A user's request, such as \"*Do I need to carry an umbrella in Bangalore today?*\", is sent from the client application to the LLM. Along with this message, a tool definition is provided. This definition gives the LLM the context it needs to understand what tools are available along usage methods.\n\nThe LLM analyses the user's request and the list of available tools. Based on this analysis, it identifies the most appropriate tool to use (e.g., a weather API) and determines the correct way to call it, including any necessary input parameters.\n\nOnce the LLM recommends a tool call, the client application is responsible for executing it. The output of this tool—for example, the weather API's response of \"*rainy*\"—is then sent back to the LLM. The LLM processes this new information and generates a final, conversational human-friendly response for the user, such as \"Yes, it's rainy in Bangalore, so you should definitely carry an umbrella!”\n\nTool definitions are what make this process work. A tool definition must include:\n\n- *Name*: A unique name for the tool.\n- *Description*: A clear explanation of what the tool does and when it should be used.\n- *Input Parameters*: A list of all required input values for the tool.\n\nBy carefully crafting these definitions, developers can enable LLMs to perform a wide variety of tasks with external data, significantly expanding their capabilities.\n\n**Agent Development with Function Calling:**\n\nFunction calling is the technical mechanism that allows an agent to take action. Think of the agent as the intelligent system or \"brain\" that reasons and plans, and the function calls as the agent's hands—the specific actions it can perform in the real world.\n\nThe agent's workflow is powered by the following process:\n\n1. The agent (the LLM) receives a request from the user.\n2. It reasons about the request and decides what actions are needed to fulfil it.\n3. It then uses **function calling** to generate a structured call to one of its predefined tools (e.g., an API, a database query, or a piece of code).\n4. Your code executes this function call.\n5. The result of that action is fed back to the agent so it can decide on the next step or provide a final answer.\n\nSo, using function calling to develop an agent is a precise and correct way to describe how we are giving agent the ability to interact with external systems.\n\n![]()Agent Development with Function Calling\n\n**Developing Agent with AI Toolkit:**\n\nAn \"agent\" is a model that decides what action to perform. As a first step, we augment the LLM call with the ability to take action via **Function calling** or **Tool Calling**. This can be further extended with the [**Model Context Protocol (MCP)**](https://techcommunity.microsoft.com/blog/educatordeveloperblog/unlocking-ai-potential-exploring-the-model-context-protocol-with-ai-toolkit/4411198). We will demonstrate this using the [AI Toolkit](https://techcommunity.microsoft.com/blog/educatordeveloperblog/visual-studio-code-ai-toolkit-run-llms-locally/4163192).\n\nTo begin, make sure the [AI Toolkit](https://aka.ms/AIToolkit) is installed on the machine. For detailed stepwise installation and usage refer details refer to the [Blog](https://techcommunity.microsoft.com/blog/educatordeveloperblog/visual-studio-code-ai-toolkit-run-llms-locally/4163192)or [AI Sparks series](https://www.youtube.com/watch?v=crFcWa_9hK0&list=PLmsFUfdnGr3yysvu8fPA9ka5gW2fkJci1) on YouTube.\n\nOnce we have the environment ready, navigate to the ***Agent Builder*** (earlier known as Prompt builder).\n\n![]()AI toolkit: Agent Builder\n\nNow click on the **“+”** icon and select “Custom Tool” from the dropdown.\n\n![]()AI Toolkit: Custom tool\n\nPost this there will be a popup window on which all the tool configuration must be completed. There is an option to use the example tool or to upload the existing schema, in this beginner tutorial, “*get\\_weather*” example will be used. Upon selection, it populates values in all the fields automatically. Whereas in a custom tool upload, the schema must be defined by the user as per the tool call.\n\n![]()AI Toolkit: get\\_weather\n\n“*get\\_weather*” is an imitation to the real API call. We will use a default value of “rainy”, but it can be modified to imitate a real API call to OpenWeather API or any other weather API provider.\n\n- {\n\"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state e.g. San Francisco, CA\" }, \"unit\": { \"type\": \"string\", \"enum\": [ \"c\", \"f\" ] } }, \"additionalProperties\": false, \"required\": [ \"location\" ] } ![]()AI Toolkit: Schema\n\nFinally, our Agent is just a click away! All we need to do is to click the “Add button” and the tool gets added! Congratulations on the first successful agent development!\n\nIts now time to test the first agent application with the GPT model (hosted via GitHub). Notice that the tool is now added. Add the user prompt *“* ***Do I need to carry an umbrella today in Bangalore?*** *”*\n\n![]()AI Toolkit: Custom tool configuration\n\nLet’s first run it as it is and check if the model is able to recognize if it needs external data. To do this simply click on ***RUN***.\n\n![]()AI Toolkit: Run\n\nAs it is clearly evident the model is now identifying that it needs external data. It returns the function name and the parameters required. Next step is to simulate the function by providing mock weather data. In order to do this in the custom tools section add “***rainy***” in the *“enter tool response”* placeholder.\n\n![]()AI Toolkit: Tool\n\nPost this let’s check if this agent can identify, connect to API and provide the response. Lets click on “***RUN***” again,\n\n![]()AI Toolkit: Model response\n\nIn the model response section, now we can see the agent responding with the response as *“Yes, you should carry an umbrella today in Bangalore, as it is rainy. Stay dry!”.* It can also be added to the conversational history by simply clicking on the “*Add to Prompts*”.\n\nThat’s it! Our first Agent is ready to tell us the weather updates!\n\nFor further flexibility or deployment, AI Toolkit also provides the code in various languages,\n\nUpon clicking on *“* ***View Code*** *”,* the toolkit prompts to select a language. We'll use Python and select the Azure inference SDK. Following is the code we have received from AI Toolkit.\n- \"\"\"Run this model in Python\n\n> pip install azure-ai-inference\n\"\"\" import os from azure.ai.inference import ChatCompletionsClient from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage, ToolMessage from azure.ai.inference.models import ImageContentItem, ImageUrl, TextContentItem from azure.core.credentials import AzureKeyCredential\n\n# To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n# Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\nclient = ChatCompletionsClient( endpoint = \"https://models.github.ai/inference\", credential = AzureKeyCredential(os.environ[\"GITHUB\\_TOKEN\"]), api\\_version = \"2024-08-01-preview\", )\n\ndef get\\_weather(): return \"rainy\"\n\nmessages = [ SystemMessage(content = \"You are a helpful AI Assistant.\"), UserMessage(content = [ TextContentItem(text = \"Do I need to carry an umbrella today in Bangalore?\"), ]), ]\n\ntools = [ { \"type\": \"function\", \"function\": { \"name\": \"get\\_weather\", \"description\": \"Determine weather in my location\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state e.g. San Francisco, CA\" }, \"unit\": { \"type\": \"string\", \"enum\": [ \"c\", \"f\" ] } }, \"additionalProperties\": False, \"required\": [ \"location\" ] } } } ]\n\nresponse\\_format = \"text\"\n\nwhile True: response = client.complete( messages = messages, model = \"openai/gpt-4o\", tools = tools, response\\_format = response\\_format, temperature = 1, top\\_p = 1, )\n\nif response.choices[0].message.tool\\_calls: print(response.choices[0].message.tool\\_calls) messages.append(response.choices[0].message) for tool\\_call in response.choices[0].message.tool\\_calls: messages.append(ToolMessage( content=locals()[tool\\_call.function.name](), tool\\_call\\_id=tool\\_call.id, )) else: print(f\"[Model Response] {response.choices[0].message.content}\") break\n\nIn the upcoming blogs we will explore and implement more complicated agentic systems with the help of the AI Toolkit! Stay tuned.",
  "FeedName": "Microsoft Tech Community",
  "Author": "shreyanfern",
  "Tags": [],
  "EnhancedContent": "## Function calling is the technical mechanism that allows an agent to take action. Think of the agent as the intelligent system or \"brain\" that reasons and plans, and the function calls as the agent's hands—the specific actions it can perform in the real world.\n\nFunction calling is a powerful technique that allows Large Language Models (LLMs) to go beyond their static training data and interact with real-world, dynamic information sources like databases and APIs. This capability turns a simple chat interface into a powerful tool for fetching real-time data, executing code, and much more.\n\nThe process of Tool/Function calling typically involves two main components: a client application and the LLM. A user's request, such as \"*Do I need to carry an umbrella in Bangalore today?*\", is sent from the client application to the LLM. Along with this message, a tool definition is provided. This definition gives the LLM the context it needs to understand what tools are available along usage methods.\n\nThe LLM analyses the user's request and the list of available tools. Based on this analysis, it identifies the most appropriate tool to use (e.g., a weather API) and determines the correct way to call it, including any necessary input parameters.\n\nOnce the LLM recommends a tool call, the client application is responsible for executing it. The output of this tool—for example, the weather API's response of \"*rainy*\"—is then sent back to the LLM. The LLM processes this new information and generates a final, conversational human-friendly response for the user, such as \"Yes, it's rainy in Bangalore, so you should definitely carry an umbrella!”\n\nTool definitions are what make this process work. A tool definition must include:\n\n- *Name*: A unique name for the tool.\n- *Description*: A clear explanation of what the tool does and when it should be used.\n- *Input Parameters*: A list of all required input values for the tool.\n\nBy carefully crafting these definitions, developers can enable LLMs to perform a wide variety of tasks with external data, significantly expanding their capabilities.\n\n**Agent Development with Function Calling:**\n\nFunction calling is the technical mechanism that allows an agent to take action. Think of the agent as the intelligent system or \"brain\" that reasons and plans, and the function calls as the agent's hands—the specific actions it can perform in the real world.\n\nThe agent's workflow is powered by the following process:\n\n1. The agent (the LLM) receives a request from the user.\n2. It reasons about the request and decides what actions are needed to fulfil it.\n3. It then uses **function calling** to generate a structured call to one of its predefined tools (e.g., an API, a database query, or a piece of code).\n4. Your code executes this function call.\n5. The result of that action is fed back to the agent so it can decide on the next step or provide a final answer.\n\nSo, using function calling to develop an agent is a precise and correct way to describe how we are giving agent the ability to interact with external systems.\n\nAgent Development with Function Calling\n\n**Developing Agent with AI Toolkit:**\n\nAn \"agent\" is a model that decides what action to perform. As a first step, we augment the LLM call with the ability to take action via **Function calling** or **Tool Calling**. This can be further extended with the [**Model Context Protocol (MCP)**](https://techcommunity.microsoft.com/blog/educatordeveloperblog/unlocking-ai-potential-exploring-the-model-context-protocol-with-ai-toolkit/4411198). We will demonstrate this using the [AI Toolkit](https://techcommunity.microsoft.com/blog/educatordeveloperblog/visual-studio-code-ai-toolkit-run-llms-locally/4163192).\n\nTo begin, make sure the [AI Toolkit](https://aka.ms/AIToolkit) is installed on the machine. For detailed stepwise installation and usage refer details refer to the [Blog](https://techcommunity.microsoft.com/blog/educatordeveloperblog/visual-studio-code-ai-toolkit-run-llms-locally/4163192)or [AI Sparks series](https://www.youtube.com/watch?v=crFcWa_9hK0&amp;list=PLmsFUfdnGr3yysvu8fPA9ka5gW2fkJci1) on YouTube.\n\nOnce we have the environment ready, navigate to the ***Agent Builder*** (earlier known as Prompt builder).\n\nAI toolkit: Agent Builder\n\nNow click on the **“+”** icon and select “Custom Tool” from the dropdown.\n\nAI Toolkit: Custom tool\n\nPost this there will be a popup window on which all the tool configuration must be completed. There is an option to use the example tool or to upload the existing schema, in this beginner tutorial, “*get\\_weather*” example will be used. Upon selection, it populates values in all the fields automatically. Whereas in a custom tool upload, the schema must be defined by the user as per the tool call.\n\nAI Toolkit: get\\_weather\n\n“*get\\_weather*” is an imitation to the real API call. We will use a default value of “rainy”, but it can be modified to imitate a real API call to OpenWeather API or any other weather API provider.\n\n``` { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state e.g. San Francisco, CA\" }, \"unit\": { \"type\": \"string\", \"enum\": [ \"c\", \"f\" ] } }, \"additionalProperties\": false, \"required\": [ \"location\" ] }\n\n```\n\nAI Toolkit: Schema\n\nFinally, our Agent is just a click away! All we need to do is to click the “Add button” and the tool gets added! Congratulations on the first successful agent development!\n\nIts now time to test the first agent application with the GPT model (hosted via GitHub).  Notice that the tool is now added. Add the user prompt *“* ***Do I need to carry an umbrella today in Bangalore?*** *”*\n\nAI Toolkit: Custom tool configuration\n\nLet’s first run it as it is and check if the model is able to recognize if it needs external data. To do this simply click on ***RUN***.\n\nAI Toolkit: Run\n\nAs it is clearly evident the model is now identifying that it needs external data. It returns the function name and the parameters required. Next step is to simulate the function by providing mock weather data. In order to do this in the custom tools section add “***rainy***” in the *“enter tool response”* placeholder.\n\nAI Toolkit: Tool\n\nPost this let’s check if this agent can identify, connect to API and provide the response. Lets click on “***RUN***” again,\n\nAI Toolkit: Model response\n\nIn the model response section, now we can see the agent responding with the response as *“Yes, you should carry an umbrella today in Bangalore, as it is rainy. Stay dry!”.* It can also be added to the conversational history by simply clicking on the “*Add to Prompts*”.\n\nThat’s it! Our first Agent is ready to tell us the weather updates!\n\nFor further flexibility or deployment, AI Toolkit also provides the code in various languages,\n\nUpon clicking on *“* ***View Code*** *”,* the toolkit prompts to select a language. We'll use Python and select the Azure inference SDK. Following is the code we have received from AI Toolkit.\n\n``` \"\"\"Run this model in Python\n\n> pip install azure-ai-inference\n\"\"\" import os from azure.ai.inference import ChatCompletionsClient from azure.ai.inference.models import AssistantMessage, SystemMessage, UserMessage, ToolMessage from azure.ai.inference.models import ImageContentItem, ImageUrl, TextContentItem from azure.core.credentials import AzureKeyCredential\n\n# To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.\n# Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\nclient = ChatCompletionsClient( endpoint = \"https://models.github.ai/inference\", credential = AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"]), api_version = \"2024-08-01-preview\", )\n\ndef get_weather(): return \"rainy\"\n\nmessages = [ SystemMessage(content = \"You are a helpful AI Assistant.\"), UserMessage(content = [ TextContentItem(text = \"Do I need to carry an umbrella today in Bangalore?\"), ]), ]\n\ntools = [ { \"type\": \"function\", \"function\": { \"name\": \"get_weather\", \"description\": \"Determine weather in my location\", \"parameters\": { \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state e.g. San Francisco, CA\" }, \"unit\": { \"type\": \"string\", \"enum\": [ \"c\", \"f\" ] } }, \"additionalProperties\": False, \"required\": [ \"location\" ] } } } ]\n\nresponse_format = \"text\"\n\nwhile True: response = client.complete( messages = messages, model = \"openai/gpt-4o\", tools = tools, response_format = response_format, temperature = 1, top_p = 1, )\n\nif response.choices[0].message.tool_calls: print(response.choices[0].message.tool_calls) messages.append(response.choices[0].message) for tool_call in response.choices[0].message.tool_calls: messages.append(ToolMessage( content=locals()[tool_call.function.name](), tool_call_id=tool_call.id, )) else: print(f\"[Model Response] {response.choices[0].message.content}\") break\n\n```\n\nIn the upcoming blogs we will explore and implement more complicated agentic systems with the help of the AI Toolkit! Stay tuned.\n\nUpdated Aug 15, 2025\n\nVersion 1.0\n\n[!\\[shreyanfern&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0yMjgyODQ4LTU0NzI2OWk4QjFDQTg2Mjk2Njk5QUYy?image-dimensions=50x50)](/users/shreyanfern/2282848) [shreyanfern](/users/shreyanfern/2282848) Brass Contributor\n\nJoined January 30, 2024\n\n[View Profile](/users/shreyanfern/2282848)\n\n/category/educationsector/blog/educatordeveloperblog [Educator Developer Blog](/category/educationsector/blog/educatordeveloperblog) Follow this blog board to get notified when there's new activity"
}
