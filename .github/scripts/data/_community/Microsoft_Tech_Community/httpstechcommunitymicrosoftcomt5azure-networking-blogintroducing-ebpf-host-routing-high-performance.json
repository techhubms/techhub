{
  "Description": "AI-driven applications demand low-latency workloads for optimal user experience. To meet this need, services are moving to containerized environments, with Kubernetes as the standard. Kubernetes networking relies on the Container Network Interface (CNI) for pod connectivity and routing. Traditional CNI implementations use iptables for packet processing, adding latency and reducing throughput.\n\n**Azure CNI powered by Cilium** natively integrates Azure Kubernetes service (AKS) data plane with Azure CNI networking modes for superior performance, hardware offload support, and enterprise-grade reliability. Azure CNI powered by Cilium delivers up to 30% higher throughput in both benchmark and real-world customer tests compared to a bring-your-own Cilium setup on AKS.\n\n**The next leap forward:** Now, AKS data plane performance can be optimized even further with eBPF host routing, which is an open-source Cilium CNI capability that accelerates packet forwarding by executing routing logic directly in eBPF. As shown in the figure, this architecture eliminates reliance on iptables and connection tracking (conntrack) within the host network namespace. As a result, significantly improving packet processing efficiency, reducing CPU overhead and optimized performance for modern workloads.\n\n![]()\n\n``` Comparison of host routing using the Linux kernel stack vs eBPF ```\n\nAzure CNI powered by Cilium is battle-tested for mission-critical workloads, backed by Microsoft support, and enriched with [Advanced Container Networking Services](https://aka.ms/acns) features for security, observability, and accelerated performance. eBPF host routing is now included as part of Advanced Container Networking Services suite, delivering network performance acceleration.\n\nIn this blog, we highlight the performance benefits of eBPF host routing, explain how to enable it in an AKS cluster, and provide a deep dive into its implementation on Azure. We start by examining AKS cluster performance before and after enabling eBPF host routing.\n\n## Performance comparison\n\nOur comparative benchmarks measure the difference in Azure CNI Powered by Cilium, by enabling eBPF host routing. To perform these measurements, we use AKS clusters on K8s version 1.33, with host nodes of 16 cores, running Ubuntu 24.04. We are interested in throughput and latency numbers for pod-to-pod traffic in these clusters. For throughput measurements, we deploy netperf client and server pods, and measure TCP\\_STREAM throughput at varying message sizes in tests running 20 seconds each. The wide range of message sizes are meant to capture the variety of workloads running on AKS clusters, ranging from AI training and inference to messaging systems and media streaming. For latency, we run TCP\\_RR tests, measuring latency at various percentiles, as well as transaction rates.\n\nThe following figure compares pods on the same node; eBPF-based routing results in a dramatic improvement in throughput (~30%). This is because, on the same node, the throughput is not constrained by factors such as the VM NIC limits and is almost entirely determined by host routing performance.\n\n![]()\n\nFor pod-to-pod throughput across different nodes in the cluster. eBPF host routing results in better pod-to-pod throughput across nodes, and the difference is more pronounced with smaller message sizes (3x more). This is because, with smaller messages, the per-message overhead incurred in the host network stack has a bigger impact on performance.\n\n![]()\n\nNext, we compare latency for pod-to-pod traffic. We limit this benchmark to intra-node traffic, because cross-node traffic latency is determined by factors other than the routing latency incurred in the hosts. eBPF host routing results in reduced latency compared to the non-accelerated configuration at all measured percentiles.\n\n![]()\n\nWe have also measured the transaction rate between client and server pods, with and without eBPF host routing. This benchmark is an alternative measurement of latency because a transaction is essentially a small TCP request/response pair. We observe that eBPF host routing improves transactions per second by around 27% as compared to legacy host routing.\n\n**Transactions/second (same node)**\n\n| **Azure CNI configuration** | **Transactions/second** | | --- | --- | | **eBPF host routing** | **20396.9** | | **Traditional host routing** | **16003.7** |\n\n## Enabling eBPF routing through Advanced Container Networking Services\n\neBPF host routing is disabled by default in Advanced Container Networking Services because bypassing iptables in the host network namespace can ignore custom user rules and host-level security policies. This may lead to visible failures such as dropped traffic or broken network policies, as well as silent issues like unintended access or missed audit logs. To mitigate these risks, eBPF host routing is offered as an opt-in feature, enabled through Advanced Container Networking Services on Azure CNI powered by Cilium.\n\n**The Advanced Container Networking Services advantage: Built-in safeguards:** Enabling eBPF Host Routing in ACNS enhances the open-source offering with strong built-in safeguards. Before activation, ACNS validates existing iptables rules in the host network namespace and blocks enablement if user-defined rules are detected. Once enabled, kernel-level protections prevent new iptables rules and generate Kubernetes events for visibility. These measures allow customers to benefit from eBPF’s performance gains while maintaining security and reliability.\n\nThanks to the additional safeguards, eBPF host routing in Advanced Container Networking Services is a safer and more robust option for customers who wish to obtain the best possible networking performance on their Kubernetes infrastructure.\n\n### How to enable eBPF Host Routing with ACNS\n\nVisit the [documentation on how to enable eBPF Host Routing](https://learn.microsoft.com/en-us/azure/aks/how-to-enable-ebpf-host-routing) for new and existing Azure CNI Powered by Cilium clusters.\n\nVerify the network profile with the new performance `accelerationMode`field set to `BpfVeth`.\n\n| ```<br> \"networkProfile\": { \"advancedNetworking\": { \"enabled\": true, \"performance\": { \"accelerationMode\": \"BpfVeth\" },…<br>``` | | --- |\n\nFor more information on Advanced Container Networking Services and ACNS Performance, please visit [https://aka.ms/acnsperformance](https://aka.ms/acnsperformance).\n\n## Resources\n\n- For more info about Advanced Container Networking Services please visit ([Container Network Security with Advanced Container Networking Services (ACNS) - Azure Kubernetes Service | Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-concepts)).\n- For more info about Azure CNI Powered by Cilium please visit ([Configure Azure CNI Powered by Cilium in Azure Kubernetes Service (AKS) - Azure Kubernetes Service | Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/azure-cni-powered-by-cilium)).",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "EnhancedContent": "AI-driven applications demand low-latency workloads for optimal user experience. To meet this need, services are moving to containerized environments, with Kubernetes as the standard. Kubernetes networking relies on the Container Network Interface (CNI) for pod connectivity and routing. Traditional CNI implementations use iptables for packet processing, adding latency and reducing throughput.\n\n**Azure CNI powered by Cilium** natively integrates Azure Kubernetes service (AKS) data plane with Azure CNI networking modes for superior performance, hardware offload support, and enterprise-grade reliability. Azure CNI powered by Cilium delivers up to 30% higher throughput in both benchmark and real-world customer tests compared to a bring-your-own Cilium setup on AKS.\n\n**The next leap forward:** Now, AKS data plane performance can be optimized even further with eBPF host routing, which is an open-source Cilium CNI capability that accelerates packet forwarding by executing routing logic directly in eBPF. As shown in the figure, this architecture eliminates reliance on iptables and connection tracking (conntrack) within the host network namespace. As a result, significantly improving packet processing efficiency, reducing CPU overhead and optimized performance for modern workloads.\n\n``` Comparison of host routing using the Linux kernel stack vs eBPF ```\n\nAzure CNI powered by Cilium is battle-tested for mission-critical workloads, backed by Microsoft support, and enriched with [Advanced Container Networking Services](https://aka.ms/acns) features for security, observability, and accelerated performance. eBPF host routing is now included as part of Advanced Container Networking Services suite, delivering network performance acceleration.\n\nIn this blog, we highlight the performance benefits of eBPF host routing, explain how to enable it in an AKS cluster, and provide a deep dive into its implementation on Azure. We start by examining AKS cluster performance before and after enabling eBPF host routing.\n\n## Performance comparison\n\nOur comparative benchmarks measure the difference in Azure CNI Powered by Cilium, by enabling eBPF host routing. To perform these measurements, we use AKS clusters on K8s version 1.33, with host nodes of 16 cores, running Ubuntu 24.04. We are interested in throughput and latency numbers for pod-to-pod traffic in these clusters. For throughput measurements, we deploy netperf client and server pods, and measure TCP\\_STREAM throughput at varying message sizes in tests running 20 seconds each. The wide range of message sizes are meant to capture the variety of workloads running on AKS clusters, ranging from AI training and inference to messaging systems and media streaming. For latency, we run TCP\\_RR tests, measuring latency at various percentiles, as well as transaction rates.\n\nThe following figure compares pods on the same node; eBPF-based routing results in a dramatic improvement in throughput (~30%). This is because, on the same node, the throughput is not constrained by factors such as the VM NIC limits and is almost entirely determined by host routing performance.\n\nFor pod-to-pod throughput across different nodes in the cluster. eBPF host routing results in better pod-to-pod throughput across nodes, and the difference is more pronounced with smaller message sizes (3x more). This is because, with smaller messages, the per-message overhead incurred in the host network stack has a bigger impact on performance.\n\nNext, we compare latency for pod-to-pod traffic. We limit this benchmark to intra-node traffic, because cross-node traffic latency is determined by factors other than the routing latency incurred in the hosts. eBPF host routing results in reduced latency compared to the non-accelerated configuration at all measured percentiles.\n\nWe have also measured the transaction rate between client and server pods, with and without eBPF host routing. This benchmark is an alternative measurement of latency because a transaction is essentially a small TCP request/response pair. We observe that eBPF host routing improves transactions per second by around 27% as compared to legacy host routing.\n\n**Transactions/second (same node)**\n\n| **Azure CNI configuration** | **Transactions/second** | | --- | --- | | **eBPF host routing** | **20396.9** | | **Traditional host routing** | **16003.7** |\n\n## Enabling eBPF routing through Advanced Container Networking Services\n\neBPF host routing is disabled by default in Advanced Container Networking Services because bypassing iptables in the host network namespace can ignore custom user rules and host-level security policies. This may lead to visible failures such as dropped traffic or broken network policies, as well as silent issues like unintended access or missed audit logs. To mitigate these risks,  eBPF host routing is offered as an opt-in feature, enabled through Advanced Container Networking Services on Azure CNI powered by Cilium.\n\n**The Advanced Container Networking Services advantage: Built-in safeguards:** Enabling eBPF Host Routing in ACNS enhances the open-source offering with strong built-in safeguards. Before activation, ACNS validates existing iptables rules in the host network namespace and blocks enablement if user-defined rules are detected. Once enabled, kernel-level protections prevent new iptables rules and generate Kubernetes events for visibility. These measures allow customers to benefit from eBPF’s performance gains while maintaining security and reliability.\n\nThanks to the additional safeguards, eBPF host routing in Advanced Container Networking Services is a safer and more robust option for customers who wish to obtain the best possible networking performance on their Kubernetes infrastructure.\n\n### How to enable eBPF Host Routing with ACNS\n\nVisit the [documentation on how to enable eBPF Host Routing](https://learn.microsoft.com/en-us/azure/aks/how-to-enable-ebpf-host-routing) for new and existing Azure CNI Powered by Cilium clusters.\n\nVerify the network profile with the new performance `accelerationMode`field set to `BpfVeth`.\n\n| ```<br> \"networkProfile\": {    \"advancedNetworking\": {      \"enabled\": true,      \"performance\": {        \"accelerationMode\": \"BpfVeth\"      },…<br>``` | | --- |\n\nFor more information on Advanced Container Networking Services and ACNS Performance, please visit [https://aka.ms/acnsperformance](https://aka.ms/acnsperformance).\n\n## Resources\n\n- For more info about Advanced Container Networking Services please visit ([Container Network Security with Advanced Container Networking Services (ACNS) - Azure Kubernetes Service | Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/container-network-security-concepts)).\n- For more info about Azure CNI Powered by Cilium please visit ([Configure Azure CNI Powered by Cilium in Azure Kubernetes Service (AKS) - Azure Kubernetes Service | Microsoft Learn](https://learn.microsoft.com/en-us/azure/aks/azure-cni-powered-by-cilium)).\n\nUpdated Nov 08, 2025\n\nVersion 2.0\n\n[azure networking](/tag/azure%20networking?nodeId=board%3AAzureNetworkingBlog)\n\n[cilium](/tag/cilium?nodeId=board%3AAzureNetworkingBlog)\n\n[container networking](/tag/container%20networking?nodeId=board%3AAzureNetworkingBlog)\n\n[ebpf](/tag/ebpf?nodeId=board%3AAzureNetworkingBlog)\n\n[open-source](/tag/open-source?nodeId=board%3AAzureNetworkingBlog)\n\n[!\\[Sam_Foo&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-8.svg?image-dimensions=50x50)](/users/sam_foo/3006132) [Sam_Foo](/users/sam_foo/3006132) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined April 22, 2025\n\n[View Profile](/users/sam_foo/3006132)\n\n/category/azure/blog/azurenetworkingblog [Azure Networking Blog](/category/azure/blog/azurenetworkingblog) Follow this blog board to get notified when there's new activity",
  "Author": "Sam_Foo",
  "Link": "https://techcommunity.microsoft.com/t5/azure-networking-blog/introducing-ebpf-host-routing-high-performance-ai-networking/ba-p/4468216",
  "FeedName": "Microsoft Tech Community",
  "PubDate": "2025-11-08T00:45:49+00:00",
  "OutputDir": "_community",
  "Title": "Introducing eBPF Host Routing: High performance AI networking with Azure CNI powered by Cilium",
  "ProcessedDate": "2025-11-08 01:31:53",
  "Tags": [],
  "FeedLevelAuthor": "rss.livelink.threads-in-node"
}
