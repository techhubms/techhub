{
  "Author": "Sally_Dabbah",
  "Description": "## **Introduction**\n\nData fuels analytics, machine learning, and AI but only if itâ€™s trustworthy. Most organizations struggle with inconsistent schemas, nulls, data drift, or unexpected upstream changes that silently break dashboards, models, and business logic.\n\n**Microsoft Fabric** provides a unified analytics platform with OneLake, pipelines, notebooks, and governance capabilities. When combined with **Great Expectations**, an open-source data quality framework, Fabric becomes a powerful environment for enforcing data quality at scale.\n\nIn this article, we explore how to implement **enterprise-ready, parameterized data validation** inside Fabric notebooks using Great Expectations including row-count drift detection, schema checks, primary-key uniqueness, and time-series batch validation.\n\nA quick reminder: **ETL (Extract, Transform, Load)** is the process of pulling raw data from source systems, applying business logic and quality validations, and delivering clean, curated datasets for analytics and AI. While ETL spans the full Medallion architecture, this guide focuses specifically on **data quality checks in the Bronze layer** using the NYC Taxi sample dataset.\n\nðŸ”— **Full implementation is available in my GitHub repository:** [sallydabbahmsft/Data-Quality-Checks-in-Microsoft-Fabric: Data Quality Checks in Microsoft Fabric](https://github.com/sallydabbahmsft/Data-Quality-Checks-in-Microsoft-Fabric)\n\n## **Why Data Quality Matters More Than Ever?**\n\nAI and analytics initiatives fail not because of model quality but because the underlying data is inaccurate, incomplete, or inconsistent. Organizations adopting Microsoft Fabric often ask:\n\n- How can we validate data as it lands in Bronze?\n- How do we detect schema changes before they break downstream pipelines?\n- How do we prevent silent failures, anomalies, and drift?\n- How do we standardize data quality checks across multiple tables and pipelines?\n\nGreat Expectations provides a unified, testable, automation-friendly way to answer these questions.\n\n## **Great Expectations in Fabric**\n\nGreat Expectations (GX) is an open-source library for:\n\nâœ” Declarative data quality rules (\"expectations\") âœ” Automated validation during ETL âœ” Rich documentation and reporting âœ” Batch-based validation for time-series or large datasets âœ” Integration with Python, Spark, SQL, and cloud data platforms\n\nFabric notebooks now support Great Expectations natively (via PySpark), enabling engineering teams to:\n\n- Build reusable DQ suites\n- Parameterize expectations by pipeline\n- Validate full datasets or daily partitions\n- Integrate validation into Fabric pipelines and alerting\n\n# **Data Quality Across the Medallion Architecture**\n\nThis solution follows the **Medallion Architecture**, with validation at every layer.\n\nThis pipeline follows a Medallion Architecture, moving data through the Bronze, Silver, and Gold layers while enforcing data quality checks at every stage.\n\nðŸ“˜ *P.S. Fabric also supports this via built-in Medallion task flows:* [Task flows overview - Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/en-us/fabric/fundamentals/task-flow-overview)\n\n![]()\n\n### **ðŸ¥‰Bronze Layer: Ingestion & Validation**\n\n- Ingest raw source data into Bronze without transformations.\n- Run foundational DQ checks to ensure structural integrity.\n\n**Bronze DQ answers:** âž¡ *Did the data arrive correctly?*\n\n### **ðŸ¥ˆSilver Layer: Transformation & Validation**\n\n- Clean, standardize, and enrich Bronze data.\n- Validate business rules, schema consistency, reference values, and more.\n\n**Silver DQ answers:** âž¡ *Is the data accurate and logically correct?*\n\n### ðŸ¥‡ **Gold Layer: Enrichment & Consumption**\n\n- Produce curated, analytics-ready datasets.\n- Validate metrics, aggregates, and business KPIs.\n\n**Gold DQ answers:** âž¡ *Can executives trust the numbers?*\n\n### **Recommended Data Quality Validations:**\n\n#### **Bronze Layer (Raw Ingestion)**\n\n1. **Ingestion Volume & Row Drift** â€“ Validate total row count and detect unexpected volume drops or spikes.\n2. **Schema & Data Type Compliance** â€“ Ensure the table structure and column data types match the expected schema.\n3. **Null / Empty Column Checks** â€“ Identify missing or empty values in required fields.\n4. **Primary Key Uniqueness** â€“ Detect duplicate records based on the defined composite or natural key.\n\n#### **Silver Layer (Cleaned & Standardized Data)**\n\n- **Reference & Domain Value Validation** â€“ Confirm that values match valid categories, lookups, or reference datasets.\n- **Business Rule Enforcement** â€“ Validate logic constraints (e.g., StartDate\n- **Anomaly / Outlier Detection** â€“ Identify unusual patterns or values that deviate from historical behavior.\n- **Post-Standardization Deduplication** â€“ Ensure standardized and enriched records no longer contain duplicates.\n\n#### **Gold Layer (Curated, Business-Ready Data)**\n\n- **Metric & Aggregation Consistency** â€“ Validate totals, ratios, rollups, and other aggregated metrics.\n- **KPI Threshold Monitoring** â€“ Trigger alerts when KPIs exceed defined thresholds.\n- **Data / Feature Drift Detection (for ML)** â€“ Monitor changes in distributions across time.\n- **Cross-System Consistency Checks** â€“ Compare business metrics across internal systems to ensure alignment.\n\n###\n\n**Implementing Data Quality with Great Expectations in Fabric**\n\n**Step 1 - Read data from Lakehouse (parametrized):**\n\n- lakehouse\\_name = \"Bronze\"\ntable\\_name = \"NYC Taxi - Green\"\n\nquery = f\"SELECT \\* FROM {lakehouse\\_name}.`{table_name}`\" df = spark.sql(query)\n\n**Step 2 - Create and Register a Suite:**\n- context = gx.get\\_context()\nsuite = context.suites.add( gx.ExpectationSuite(name=\"nyc\\_bronze\\_suite\") )\n\n##### **Step 3 - Add Bronze Layer Expectations (Reusable Function):**\n- import great\\_expectations as gx\n\ndef add\\_bronze\\_expectations( suite: gx.ExpectationSuite, primary\\_key\\_columns: list[str], required\\_columns: list[str], expected\\_schema: list[str], expected\\_row\\_count: int | None = None, max\\_row\\_drift\\_pct: float = 0.2, ) -> gx.ExpectationSuite:\n# 1. Ingestion Count & Row Drift\nif expected\\_row\\_count is not None: min\\_rows = int(expected\\_row\\_count \\* (1 - max\\_row\\_drift\\_pct)) max\\_rows = int(expected\\_row\\_count \\* (1 + max\\_row\\_drift\\_pct))\n\nrow\\_count\\_expectation = gx.expectations.ExpectTableRowCountToBeBetween( min\\_value=min\\_rows, max\\_value=max\\_rows, ) suite.add\\_expectation(expectation=row\\_count\\_expectation)\n\n# 2. Schema Compliance\nschema\\_expectation = gx.expectations.ExpectTableColumnsToMatchSet( column\\_set=expected\\_schema, exact\\_match=True, ) suite.add\\_expectation(expectation=schema\\_expectation)\n\n# 3. Required columns: NOT NULL\nfor col in required\\_columns: not\\_null\\_expectation = gx.expectations.ExpectColumnValuesToNotBeNull( column=col ) suite.add\\_expectation(expectation=not\\_null\\_expectation)\n\n# 4. Primary key uniqueness (if provided)\nif primary\\_key\\_columns: unique\\_pk\\_expectation = gx.expectations.ExpectCompoundColumnsToBeUnique( column\\_list=primary\\_key\\_columns ) suite.add\\_expectation(expectation=unique\\_pk\\_expectation)\n\nreturn suite\n\n##### **Step 4 - Attach Data Asset & Batch Definition:**\n- data\\_source = context.data\\_sources.add\\_spark(name=\"bronze\\_datasource\")\ndata\\_asset = data\\_source.add\\_dataframe\\_asset(name=\"nyc\\_bronze\\_data\") batch\\_definition = data\\_asset.add\\_batch\\_definition\\_whole\\_dataframe(\"full\\_bronze\\_batch\")\n\n##### **Step 5 - Run Validation:**\n- validation\\_definition = gx.ValidationDefinition(\ndata=batch\\_definition, suite=suite, name=\"Bronze\\_DQ\\_Validation\" )\n\nresults = validation\\_definition.run( batch\\_parameters={\"dataframe\": df} ) print(results)\n\n##### **7. Optional: Time-Series Batch Validation (Daily Slices)**\n\n**Fabric does not yet support add\\_batch\\_definition\\_timeseries, so your notebook implements custom logic to validate each day independently:**\n- dates\\_df = df.select(F.to\\_date(\"lpepPickupDatetime\").alias(\"dt\")).distinct()\n\nfor d in dates: df\\_day = df.filter(F.to\\_date(\"lpepPickupDatetime\") == d) results = validation\\_definition.run(batch\\_parameters={\"dataframe\": df\\_day})\n\nThis enables:\n\n1. Daily anomaly detection\n2. Partition-level completeness checks\n3. Early schema drift detection\n\n## **Automating DQ with Fabric Pipelines**\n\nFabric pipelines can orchestrate your data quality workflow:\n\n- Trigger notebook after ingestion\n- Pass parameters (table, layer, suite name)\n- Persist DQ results to Lakehouse or Log Analytics\n- Configure alerts in Fabric Monitor\n\n### **Production workflow**\n\n1. Run the notebook\n2. Check validation results\n3. If failures exist:\n- Raise an incident\n- Fail the pipeline\n- Notify the on-call engineer\n\nThis creates a **closed loop of ingestion â†’ validation â†’ monitoring â†’ alerting**.\n\nAn example of DQ pipeline:\n\n![]()\n\n**Results:**\n\n![]()\n\n## **How Enterprises Benefit**\n\nBy standardizing data quality rules across all domains, organizations ensure consistent expectations and uniform validation practices , improved observability makes data quality issues visible and actionable, enabling teams to detect and resolve failures early.\n\nThis, in turn, enhances overall reliability, ensuring downstream transformations and Power BI reports operate on clean, trustworthy data.\n\nUltimately, stronger data quality directly contributes to AI readiness high-quality, well-validated data produces significantly better analytics and machine learning outcomes.\n\n## **Conclusion**\n\nGreat Expectations + Microsoft Fabric creates a scalable, modular, enterprise-ready approach for ensuring data quality across the entire medallion architecture. Whether you're validating raw ingested data, transformed datasets, or business-ready tables, the approach demonstrated here enables consistency, observability, and automation across all pipelines.\n\nWith Fabricâ€™s unified compute, orchestration, and monitoring, teams can now integrate DQ as a first-class citizen not an afterthought.\n\n**Links:**\n\n1. [Implement medallion lakehouse architecture in Fabric - Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/en-us/fabric/onelake/onelake-medallion-lakehouse-architecture)\n2. [GX Expectations Gallery â€¢ Great Expectations](https://greatexpectations.io/expectations/)",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "FeedName": "Microsoft Tech Community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "OutputDir": "_community",
  "Title": "From Bronze to Gold: Data Quality Strategies for ETL in Microsoft Fabric",
  "Tags": [],
  "ProcessedDate": "2025-12-09 09:06:24",
  "Link": "https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/from-bronze-to-gold-data-quality-strategies-for-etl-in-microsoft/ba-p/4476303",
  "PubDate": "2025-12-09T08:12:15+00:00",
  "EnhancedContent": "## How to build reusable, automated validation across the Medallion Architecture\n\n## **Introduction**\n\nData fuels analytics, machine learning, and AIÂ  but only if itâ€™s trustworthy. Most organizations struggle with inconsistent schemas, nulls, data drift, or unexpected upstream changes that silently break dashboards, models, and business logic.\n\n**Microsoft Fabric** provides a unified analytics platform with OneLake, pipelines, notebooks, and governance capabilities. When combined with **Great Expectations**, an open-source data quality framework, Fabric becomes a powerful environment for enforcing data quality at scale.\n\nIn this article, we explore how to implement **enterprise-ready, parameterized data validation** inside Fabric notebooks using Great ExpectationsÂ  including row-count drift detection, schema checks, primary-key uniqueness, and time-series batch validation.\n\nA quick reminder: **ETL (Extract, Transform, Load)** is the process of pulling raw data from source systems, applying business logic and quality validations, and delivering clean, curated datasets for analytics and AI. While ETL spans the full Medallion architecture, this guide focuses specifically on **data quality checks in the Bronze layer** using the NYC Taxi sample dataset.\n\nðŸ”— **Full implementation is available in my GitHub repository:** [sallydabbahmsft/Data-Quality-Checks-in-Microsoft-Fabric: Data Quality Checks in Microsoft Fabric](https://github.com/sallydabbahmsft/Data-Quality-Checks-in-Microsoft-Fabric)\n\n## **Why Data Quality Matters More Than Ever?**\n\nAI and analytics initiatives fail not because of model quality but because the underlying data is inaccurate, incomplete, or inconsistent. Organizations adopting Microsoft Fabric often ask:\n\n- How can we validate data as it lands in Bronze?\n- How do we detect schema changes before they break downstream pipelines?\n- How do we prevent silent failures, anomalies, and drift?\n- How do we standardize data quality checks across multiple tables and pipelines?\n\nGreat Expectations provides a unified, testable, automation-friendly way to answer these questions.\n\n## **Great Expectations in Fabric**\n\nGreat Expectations (GX) is an open-source library for:\n\nâœ” Declarative data quality rules (\"expectations\") âœ” Automated validation during ETL âœ” Rich documentation and reporting âœ” Batch-based validation for time-series or large datasets âœ” Integration with Python, Spark, SQL, and cloud data platforms\n\nFabric notebooks now support Great Expectations natively (via PySpark), enabling engineering teams to:\n\n- Build reusable DQ suites\n- Parameterize expectations by pipeline\n- Validate full datasets or daily partitions\n- Integrate validation into Fabric pipelines and alerting\n\n# **Data Quality Across the Medallion Architecture**\n\nThis solution follows the **Medallion Architecture**, with validation at every layer.\n\nThis pipeline follows a Medallion Architecture, moving data through the Bronze, Silver, and Gold layers while enforcing data quality checks at every stage.\n\nðŸ“˜ *P.S. Fabric also supports this via built-in Medallion task flows:* [Task flows overview - Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/en-us/fabric/fundamentals/task-flow-overview)\n\n### **ðŸ¥‰Bronze Layer: Ingestion & Validation**\n\n- Ingest raw source data into Bronze without transformations.\n- Run foundational DQ checks to ensure structural integrity.\n\n**Bronze DQ answers:** âž¡ *Did the data arrive correctly?*\n\n### **ðŸ¥ˆSilver Layer: Transformation & Validation**\n\n- Clean, standardize, and enrich Bronze data.\n- Validate business rules, schema consistency, reference values, and more.\n\n**Silver DQ answers:** âž¡ *Is the data accurate and logically correct?*\n\n### ðŸ¥‡ **Gold Layer: Enrichment & Consumption**\n\n- Produce curated, analytics-ready datasets.\n- Validate metrics, aggregates, and business KPIs.\n\n**Gold DQ answers:** âž¡ *Can executives trust the numbers?*\n\n### **Recommended Data Quality Validations:**\n\n#### **Bronze Layer (Raw Ingestion)**\n\n1. **Ingestion Volume & Row Drift** â€“ Validate total row count and detect unexpected volume drops or spikes.\n2. **Schema & Data Type Compliance** â€“ Ensure the table structure and column data types match the expected schema.\n3. **Null / Empty Column Checks** â€“ Identify missing or empty values in required fields.\n4. **Primary Key Uniqueness** â€“ Detect duplicate records based on the defined composite or natural key.\n\n#### **Silver Layer (Cleaned & Standardized Data)**\n\n- **Reference & Domain Value Validation** â€“ Confirm that values match valid categories, lookups, or reference datasets.\n- **Business Rule Enforcement** â€“ Validate logic constraints (e.g., StartDate &lt;= EndDate, percentages within range).\n- **Anomaly / Outlier Detection** â€“ Identify unusual patterns or values that deviate from historical behavior.\n- **Post-Standardization Deduplication** â€“ Ensure standardized and enriched records no longer contain duplicates.\n\n#### **Gold Layer (Curated, Business-Ready Data)**\n\n- **Metric & Aggregation Consistency** â€“ Validate totals, ratios, rollups, and other aggregated metrics.\n- **KPI Threshold Monitoring** â€“ Trigger alerts when KPIs exceed defined thresholds.\n- **Data / Feature Drift Detection (for ML)** â€“ Monitor changes in distributions across time.\n- **Cross-System Consistency Checks** â€“ Compare business metrics across internal systems to ensure alignment.\n\n###\n\n**Implementing Data Quality with Great Expectations in Fabric**\n\n**Step 1 - Read data from Lakehouse (parametrized):**\n\n``` lakehouse_name = \"Bronze\" table_name = \"NYC Taxi - Green\"\n\nquery = f\"SELECT * FROM {lakehouse_name}.`{table_name}`\" df = spark.sql(query)\n\n```\n\n**Step 2 - Create and Register a Suite:**\n\n``` context = gx.get_context() suite = context.suites.add( gx.ExpectationSuite(name=\"nyc_bronze_suite\") )\n\n```\n\n##### **Step 3 - Add Bronze Layer Expectations (Reusable Function):**\n\n``` import great_expectations as gx\n\ndef add_bronze_expectations( suite: gx.ExpectationSuite, primary_key_columns: list[str], required_columns: list[str], expected_schema: list[str], expected_row_count: int | None = None, max_row_drift_pct: float = 0.2, ) -> gx.ExpectationSuite:\n# 1. Ingestion Count & Row Drift\nif expected_row_count is not None: min_rows = int(expected_row_count * (1 - max_row_drift_pct)) max_rows = int(expected_row_count * (1 + max_row_drift_pct))\n\nrow_count_expectation = gx.expectations.ExpectTableRowCountToBeBetween( min_value=min_rows, max_value=max_rows, ) suite.add_expectation(expectation=row_count_expectation)\n\n# 2. Schema Compliance\nschema_expectation = gx.expectations.ExpectTableColumnsToMatchSet( column_set=expected_schema, exact_match=True, ) suite.add_expectation(expectation=schema_expectation)\n\n# 3. Required columns: NOT NULL\nfor col in required_columns: not_null_expectation = gx.expectations.ExpectColumnValuesToNotBeNull( column=col ) suite.add_expectation(expectation=not_null_expectation)\n\n# 4. Primary key uniqueness (if provided)\nif primary_key_columns: unique_pk_expectation = gx.expectations.ExpectCompoundColumnsToBeUnique( column_list=primary_key_columns ) suite.add_expectation(expectation=unique_pk_expectation)\n\nreturn suite ```\n\n##### **Step 4 - Attach Data Asset & Batch Definition:**\n\n``` data_source = context.data_sources.add_spark(name=\"bronze_datasource\") data_asset = data_source.add_dataframe_asset(name=\"nyc_bronze_data\") batch_definition = data_asset.add_batch_definition_whole_dataframe(\"full_bronze_batch\")\n\n```\n\n##### **Step 5 - Run Validation:**\n\n``` validation_definition = gx.ValidationDefinition( data=batch_definition, suite=suite, name=\"Bronze_DQ_Validation\" )\n\nresults = validation_definition.run( batch_parameters={\"dataframe\": df} ) print(results)\n\n```\n\n##### **7. Optional: Time-Series Batch Validation (Daily Slices)**\n\n**Fabric does not yet support add\\_batch\\_definition\\_timeseries, so your notebook implements custom logic to validate each day independently:**\n\n``` dates_df = df.select(F.to_date(\"lpepPickupDatetime\").alias(\"dt\")).distinct()\n\nfor d in dates: df_day = df.filter(F.to_date(\"lpepPickupDatetime\") == d) results = validation_definition.run(batch_parameters={\"dataframe\": df_day})\n\n```\n\nThis enables:\n\n1. Daily anomaly detection\n2. Partition-level completeness checks\n3. Early schema drift detection\n\n## **Automating DQ with Fabric Pipelines**\n\nFabric pipelines can orchestrate your data quality workflow:\n\n- Trigger notebook after ingestion\n- Pass parameters (table, layer, suite name)\n- Persist DQ results to Lakehouse or Log Analytics\n- Configure alerts in Fabric Monitor\n\n### **Production workflow**\n\n1. Run the notebook\n2. Check validation results\n3. If failures exist:\n- Raise an incident\n- Fail the pipeline\n- Notify the on-call engineer\n\nThis creates a **closed loop of ingestion â†’ validation â†’ monitoring â†’ alerting**.\n\nAn example of DQ pipeline:\n\n**Results:**\n\n## **How Enterprises Benefit**\n\nBy standardizing data quality rules across all domains, organizations ensure consistent expectations and uniform validation practices , improved observability makes data quality issues visible and actionable, enabling teams to detect and resolve failures early.\n\nThis, in turn, enhances overall reliability, ensuring downstream transformations and Power BI reports operate on clean, trustworthy data.\n\nUltimately, stronger data quality directly contributes to AI readiness high-quality, well-validated data produces significantly better analytics and machine learning outcomes.\n\n## **Conclusion**\n\nGreat Expectations + Microsoft Fabric creates a scalable, modular, enterprise-ready approach for ensuring data quality across the entire medallion architecture. Whether you're validating raw ingested data, transformed datasets, or business-ready tables, the approach demonstrated here enables consistency, observability, and automation across all pipelines.\n\nWith Fabricâ€™s unified compute, orchestration, and monitoring, teams can now integrate DQ as a first-class citizen not an afterthought.\n\n**Links:**\n\n1. [Implement medallion lakehouse architecture in Fabric - Microsoft Fabric | Microsoft Learn](https://learn.microsoft.com/en-us/fabric/onelake/onelake-medallion-lakehouse-architecture)\n2. [GX Expectations Gallery â€¢ Great Expectations](https://greatexpectations.io/expectations/)\n\nUpdated Dec 09, 2025\n\nVersion 1.0\n\n[analytics](/tag/analytics?nodeId=board%3AAnalyticsonAzure)\n\n[azure](/tag/azure?nodeId=board%3AAnalyticsonAzure)\n\n[delta lake](/tag/delta%20lake?nodeId=board%3AAnalyticsonAzure)\n\n[microsoft fabric](/tag/microsoft%20fabric?nodeId=board%3AAnalyticsonAzure)\n\n[spark](/tag/spark?nodeId=board%3AAnalyticsonAzure)\n\n[!\\[Sally_Dabbah&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xNDUwOTg5LVlTZFZjYQ?image-coordinates=0%2C448%2C1152%2C1600&amp;image-dimensions=50x50)](/users/sally_dabbah/1450989) [Sally_Dabbah](/users/sally_dabbah/1450989) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined July 10, 2022\n\n[View Profile](/users/sally_dabbah/1450989)\n\n/category/azure/blog/analyticsonazure [Analytics on Azure Blog](/category/azure/blog/analyticsonazure) Follow this blog board to get notified when there's new activity"
}
