{
  "Author": "dhaneshuk",
  "FeedName": "Microsoft Tech Community",
  "Description": "## 1. Overview of Shared AKS Architecture\n\n### 1.1 Goals\n\n- Accelerate application delivery by providing a hardened shared Kubernetes platform per environment (prod / test / dev).\n- Enable safe multi-tenancy using namespace, RBAC, NetworkPolicy, quotas, and pod security.\n- Enforce consistency (standards, guardrails) while allowing team autonomy for app lifecycle.\n- Optimize cost through shared cluster capacity, rightsizing, and autoscaling.\n\n### 1.2 Pattern Summary\n\n- One AKS cluster per environment (e.g., aks-shared-prod, aks-shared-test, aks-shared-dev).\n- Multiple business units / product teams share the environment cluster via isolated namespaces.\n- Platform services (ingress, cert management, monitoring, cost, backup) run in a dedicated platform- namespace.\n- Each team gets namespaces per environment: payments-prod, orders-prod, etc.\n\n### 1.3 High-Level Architecture Diagram\n\n![]()\n\n### 1.4 Multi-Tenancy Mechanisms\n\n| Mechanism | Purpose | Enforcement Layer | | --- | --- | --- | | Namespaces | Logical isolation per team/app | Kubernetes API | | RBAC | Access control (who can do what) | Azure AD + K8s RBAC | | NetworkPolicy | East-west traffic control | CNI (Azure CNI) | | ResourceQuota & LimitRange | Prevent noisy neighbors | K8s admission | | Pod Security Standards | Baseline/Restricted enforcement | PS Admission | | Images from ACR only | Trusted supply chain | Admission / Policy |\n\n### 1.5 Why Not One Cluster For All Environments?\n\n| Aspect | Single Cluster (All Envs) | Per-Environment Clusters | | --- | --- | --- | | Blast Radius | High | Contained per env | | Change Windows | Complex coordination | Independent | | Compliance | Hard to segregate | Easier mapping | | Observability Noise | Mixed signals | Clean per env | | Scaling Decisions | Conflicting | Environment-specific |\n\nPer-environment clusters simplify lifecycle, versioning, and SLA management at the slight cost of control-plane duplication.\n\n### 1.6 Network Isolation & Azure CNI\n\n- Azure CNI assigns IPs from a VNet subnet directly to pods (no overlay) enabling IP-level visibility.\n- Use separate subnets per node pool (system vs workload vs batch) for clearer network policy scoping.\n- Leverage network policies (Calico or Azure native) to restrict cross-namespace traffic.\n- Private cluster option ensures API server accessible only via private endpoint / VNet.\n\n![]()\n\n### 1.7 Tenancy Diagram (Namespaces → Apps per BU)\n\n![]()\n\n## 2. Key Components\n\n### 2.1 Autoscaling Architecture\n\n- Cluster Autoscaler: adjusts node count (workload & batch pools) based on pending pods.\n- Horizontal Pod Autoscaler (HPA): scales replicas based on CPU, memory, or custom metrics.\n- Vertical Pod Autoscaler (VPA): recommends or applies resource request updates.\n- KEDA: event-driven autoscaling (queue length, Azure Service Bus, Kafka, etc.).\n\n![]()\n\n### 2.2 Optional Service Mesh (Istio or Ambient Mesh)\n\nUse only when:\n\n- Need mTLS between services.\n- Require fine-grained traffic shift (canary, A/B, fault injection).\n- Require zero-trust identity propagation. Otherwise keep complexity low and rely on ingress + network policies.\n\n### 2.3 Ingress & API Exposure\n\n- NGINX or Azure Application Gateway Ingress Controller.\n- TLS via cert-manager + Azure Key Vault (CSI driver for secrets if needed).\n- Central routing and WAF (if AGIC used).\n\n### 2.4 Secrets & Configuration\n\n- Prefer Azure Key Vault: reference secrets in pods via CSI driver or sync controller.\n- Use Kubernetes sealed-secrets only for GitOps edge cases.\n- External Secrets Operator can streamline mapping.\n\n### 2.5 Storage\n\n- Azure Disk: DB/data workloads needing single-node high IOPS.\n- Azure Files: shared RW across replicas.\n- Azure NetApp Files: high throughput/low latency enterprise workloads.\n- Blob Storage: backup target + object data.\n\n### 2.6 Backup & Disaster Recovery\n\n- Velero backs up cluster metadata + PV snapshots (when using supported providers).\n- Off-cluster backups stored in Blob with lifecycle management.\n- DR strategy: recreate cluster via IaC + restore Velero backups + bootstrap GitOps.\n\n### 2.7 Observability Stack\n\n- Prometheus (metrics) + exporters (node, kube-state, custom).\n- Grafana dashboards per team and shared platform board.\n- Azure Monitor Container Insights for baseline + log retention.\n- Tracing: OpenTelemetry Collector + Jaeger or Azure Monitor tracing backend.\n\n## 3. Placeholder: CI/CD Strategy (to be expanded)\n\n## 3. CI/CD Strategy\n\n### 3.1 Principles\n\n- Everything declarative (Helm charts / Kustomize) stored in Git.\n- One pipeline per application per environment stage (build once, promote with image digest).\n- GitOps for platform and cross-cutting components (ingress, monitoring, backup) via Flux or Argo CD.\n- Separation of duties: App teams manage their namespace manifests; platform team manages cluster addons.\n\n### 3.2 Recommended Flow\n\n1. Developer merges to main → CI builds & scans image → pushes to ACR with immutable tag & digest.\n2. CI updates Helm values-prod.yaml (or image tag file) in Git (infra repo) via PR.\n3. GitOps controller detects change → deploys to namespace.\n4. Post-deploy tests & smoke checks run.\n5. Promotion to higher environment uses same image digest (no rebuild).\n\n### 3.3 Pipeline Diagram\n\n![]()\n\n### 3.4 Tools\n\n| Concern | Tool | Notes | | --- | --- | --- | | Build | Azure DevOps / GitHub Actions | Container build, unit tests | | Scan (Image) | Trivy / Microsoft Defender | Fail pipeline on critical vulnerabilities | | Sign | Cosign | Image provenance signature | | Deploy | Helm + Flux | Reconciled from Git | | Secrets | Key Vault CSI / External Secrets | No secrets in Git |\n\n### 3.5 Sample GitHub Actions Snippet (Build & Push)\n\n- jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Login ACR uses: azure/docker-login@v1 with: login-server: ${{ env.ACR\\_NAME }}.azurecr.io username: ${{ secrets.ACR\\_USERNAME }} password: ${{ secrets.ACR\\_PASSWORD }} - name: Build run: docker build -t ${{ env.ACR\\_NAME }}.azurecr.io/payments:${{ github.sha }} . - name: Scan uses: aquasecurity/trivy-action@v0.13.0 with: image-ref: ${{ env.ACR\\_NAME }}.azurecr.io/payments:${{ github.sha }} severity: HIGH,CRITICAL - name: Push run: docker push ${{ env.ACR\\_NAME }}.azurecr.io/payments:${{ github.sha }}\n\n### 3.6 Helm Deployment Command (Manual)\n\n> >\n> helm upgrade --install payments charts/payments \\ --namespace payments-prod \\ --set image.repository=myacr.azurecr.io/payments \\ --set image.tag=sha256:\n> >\n\n### 3.7 GitOps Advantages\n\n- Drift detection & self-healing.\n- Auditability (all changes in PR history).\n- Immutable artifacts (image digest pinned).\n\n### 3.8 Namespace Alignment\n\n| Namespace | Repo Path | Release Strategy | | --- | --- | --- | | payments-prod | apps/payments/overlays/prod | Auto after PR merge | | orders-prod | apps/orders/overlays/prod | Manual approval | | inventory-prod | apps/inventory/overlays/prod | Auto | | platform-prod | platform/addons | Platform team only |\n\n## 4. Backup Strategy (Deep Dive)\n\n### 4.1 Objectives\n\n- Recover from accidental deletion, corruption, cluster loss.\n- Meet RPO/RTO defined per application tier (e.g., Tier-1: RPO 15m, RTO 1h).\n\n### 4.2 Velero Architecture\n\n![]()\n\n### 4.3 Backup Scope\n\n| Item | Method | Notes | | --- | --- | --- | | Namespace manifests | Velero backup | Included automatically | | Persistent Volumes (Azure Disk) | CSI snapshots | Fast point-in-time | | Azure Files | File-level backup (optional) | Consider rsync / custom | | ACR images | Not needed (immutable stored) | Use retention policies | | Secrets | Included; consider encryption | Key Vault references not stored |\n\n### 4.4 Backup Command Examples\n\n> >\n> # Create daily schedule velero schedule create daily-prod --schedule \"0 2 \\* \\* \\*\" --include-namespaces payments-prod,orders-prod,inventory-prod # On-demand backup velero backup create payments-manual-$(date +%Y%m%d) --include-namespaces payments-prod # Restore velero restore create --from-backup payments-manual-20250101\n> >\n\n### 4.5 Blob Storage Configuration\n\n- Use versioning & soft delete for container.\n- Configure lifecycle: transition >90 day old backups to Cool / Archive.\n- Private endpoint for storage account if cluster private.\n\n### 4.6 DR Runbook (Summary)\n\n1. Recreate cluster via Bicep/Terraform.\n2. Install platform addons (GitOps bootstrap).\n3. Install Velero + connect to backup bucket.\n4. Restore critical namespaces (payments → orders → inventory).\n5. Run validation scripts & synthetic tests.\n\n### 4.7 Testing Backups\n\n- Monthly restore into ephemeral test cluster.\n- Validate app startup & data integrity checksums.\n\n### 4.8 KPIs\n\n| KPI | Target | | --- | --- | | Backup success rate | > 99% | | Restore drill frequency | Monthly | | DR RTO (Tier-1) | | | DR RPO (Tier-1) | |\n\n## 5. Operational Insights\n\n### 5.1 Resource Optimization\n\n- Use VPA recommendations to refine requests bi-weekly.\n- KEDA for spiky workloads (workers / consumers) to avoid over-provisioning.\n- Batch node pool with Spot instances for cost-efficient asynchronous jobs.\n\n### 5.2 Quotas & LimitRanges\n\n| Namespace | CPU Quota | Memory Quota | Notes | | --- | --- | --- | --- | | payments-prod | 30 | 60Gi | High-traffic workload | | orders-prod | 40 | 80Gi | Larger processing window | | inventory-prod | 20 | 40Gi | Moderate update frequency |\n\n### 5.3 Noisy Neighbor Mitigation\n\n- Enforce per-deployment resource limits.\n- Use priority classes (system > platform > business apps > batch).\n- Alert on sustained throttling or eviction events.\n\n### 5.4 Operational Dashboard Metrics\n\n| Category | Metric | Source | | --- | --- | --- | | Capacity | Node utilization % | Prometheus node exporter | | Efficiency | Requested vs actual usage | kube-state-metrics | | Reliability | Pod restarts, crash loops | kube events | | Performance | API latency P95 | Ingress metrics | | Scaling | HPA decision latency | Prometheus adapter |\n\n### 5.5 SLO Examples\n\n| Service | SLO | Measurement | | --- | --- | --- | | Payments API | 99.9% availability | Successful request % over 5m windows | | Orders API | P95 | Ingress / app metrics | | Inventory Sync | Completion | Job duration metrics |\n\n### 5.6 Incident Playbook (Abbreviated)\n\n1. Detect (alert fires) → classify severity.\n2. Gather: kubectl describe, logs, metrics timeline.\n3. Mitigate: rollback image / scale resources / isolate via NetworkPolicy.\n4. Communicate: status page / stakeholder channel.\n5. Postmortem within 48h → action items tracked.\n\n## 6. Cost & Billing Strategy\n\n### 6.1 Tagging & Labeling\n\n- Azure resources: Environment=Prod, BusinessUnit=Payments, CostCenter=1234.\n- Kubernetes: Namespace labels bu=payments, env=prod used by cost tools (Kubecost / Azure Advisor).\n\n### 6.2 Cost Visibility\n\nNo diagram type detected matching given configuration for text:\n\n### 6.3 Optimization Levers\n\n| Lever | Description | Example | | --- | --- | --- | | Rightsizing | Adjust requests via VPA | Reduce CPU from 500m to 200m | | Spot | Use for batch/non-critical | BatchPool spot nodes | | Autoscaling | Scale down at night | WorkloadPool min nodes reduced | | Image Slimming | Smaller images → faster deploy | Multi-stage Docker builds | | Storage Tiering | Archive old backups | Blob lifecycle rules |\n\n### 6.4 Chargeback/Showback\n\n- Monthly export per namespace (CPU-hours, memory GB-hours, storage GB, network egress).\n- Map to internal rate card (e.g., $ per vCPU-hour).\n- Provide dashboard + monthly PDF summary.\n\n### 6.5 KPIs\n\n| KPI | Target | | --- | --- | | Unallocated capacity | | | Spot utilization | > 30% of batch workloads | | Orphan resources cleanup time | |\n\n## 7. Security & Compliance\n\n### 7.1 Layered Model\n\n| Layer | Control | Tool | | --- | --- | --- | | Identity | Azure AD RBAC → K8s RBAC | Azure AD groups | | Workload Policies | PSP Replacement / PSS | Built-in Pod Security Admission | | Network | Namespace isolation | NetworkPolicy (Calico/Azure) | | Supply Chain | Image provenance/signature | Cosign + ACR Content Trust | | Secrets | External vault storage | Azure Key Vault | | Runtime | Behavioral detection | Defender for Containers |\n\n### 7.2 RBAC Pattern\n\n- ClusterRoles for common verbs (view, deploy, ops).\n- Bind Role to Azure AD groups via AAD integration (Group mapping).\n- Example groups: aks-platform-admins, aks-payments-devs, aks-readonly.\n\n### 7.3 NetworkPolicy Example\n- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-ingress-to-payments namespace: payments-prod spec: podSelector: matchLabels: app: payments-api ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: platform-prod ports: - protocol: TCP port: 8080 policyTypes: [Ingress]\n\n### 7.4 Pod Security Standards\n\n- Set namespace labels: pod-security.kubernetes.io/enforce=restricted for prod.\n- Use baseline for dev/test to allow debugging tools.\n\n### 7.5 Image Policy\n\n- Admission controller verifies images originate from approved ACR & are signed.\n\n### 7.6 Secret Management Pattern\n- apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: payments-kv namespace: payments-prod spec: provider: azure parameters: usePodIdentity: \"false\" useVMManagedIdentity: \"true\" userAssignedIdentityID: keyvaultName: my-shared-kv objects: | array: - | secret;db-password tenantId:\n\n### 7.7 Compliance Mapping\n\n| Requirement | Control | Evidence | | --- | --- | --- | | Least Privilege | RBAC roles | GitOps repo + audit logs | | Data Protection | Encrypted disks | Azure policy compliance report | | Audit | Central log retention | Azure Monitor workspace | | Vulnerability Mgmt | Image scanning | Pipeline reports |\n\n## 8. Monitoring & Observability\n\n### 8.1 Pillars\n\n| Pillar | Tool | Output | | --- | --- | --- | | Metrics | Prometheus | Time-series dashboards | | Logs | Azure Monitor / Loki (optional) | Query & retention | | Traces | OpenTelemetry Collector | Distributed latency maps | | Events | Kubernetes API / Alertmanager | Incident triggers |\n\n### 8.2 Observability Diagram\n\n![]()\n\n### 8.3 Example PrometheusRule\n- apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: payments-alerts namespace: platform-prod spec: groups: - name: payments-availability rules: - alert: PaymentsHighErrorRate expr: rate(http\\_requests\\_total{namespace=\"payments-prod\",status=~\"5..\"}[5m]) > 5 for: 2m labels: severity: page annotations: summary: High 5xx error rate in payments API description: More than 5 errors/min for 2 minutes.\n\n### 8.4 Alerting Strategy\n\n- Page only on user-impact (availability, latency SLO breaches).\n- Ticket for capacity trend warnings.\n- Daily digest for low-priority issues.\n\n### 8.5 Dashboards\n\n- Platform Overview (cluster health, node capacity).\n- Namespace Cost & Efficiency.\n- Application Performance (latency, errors, throughput).\n\n### 8.6 Log Retention\n\n- 30 days hot, 180 days archive (Blob / ADLS).\n- PII scrubbing before long-term archival.\n\n### 8.7 Tracing Adoption Steps\n\n1. Inject OTel SDK into services.\n2. Export spans to collector via OTLP.\n3. Add trace ID to logs for correlation.\n4. Establish latency budgets per critical path.\n\n### 8.8 KPIs\n\n| KPI | Target | | --- | --- | | Alert false positives | | | Missing metrics coverage | | | Trace sampled rate (critical paths) | > 90% |\n\n## 9. Placeholder: Lab Section\n\n## 9. Hands-On Lab: Build & Operate Shared AKS\n\n> >\n> Estimated Time: ~120 minutes. Run commands from an Azure Cloud Shell or local workstation logged into Azure (az login). Replace variables as needed.\n> >\n\n### 9.1 Prerequisites\n\n> >\n> export LOCATION=eastus\n> > >\n> export RG=rg-shared-aks-prod\n> > >\n> export AKS=aks-shared-prod\n> > >\n> export ACR=acrsharedprod$RANDOM\n> > >\n> export KV=kv-shared-prod-$RANDOM\n> > >\n> export BLOBSA=stsharedprod$RANDOM\n> >\n\n### 9.2 Create Resource Group & Shared Services\n- az group create -n $RG -l $LOCATION az acr create -n $ACR -g $RG --sku Premium --location $LOCATION az keyvault create -n $KV -g $RG -l $LOCATION --enabled-for-deployment true az storage account create -n $BLOBSA -g $RG -l $LOCATION --sku Standard\\_LRS --kind StorageV2 export BLOBKEY=$(az storage account keys list -g $RG -n $BLOBSA --query [0].value -o tsv) az acr login -n $ACR\n\n### 9.3 Provision AKS Cluster (Per-Environment)\n- az aks create -g $RG -n $AKS \\ --enable-managed-identity \\ --node-count 3 \\ --node-vm-size Standard\\_D4s\\_v5 \\ --network-plugin azure \\ --enable-addons monitoring \\ --enable-oidc-issuer \\ --enable-workload-identity \\ --generate-ssh-keys az aks nodepool add -g $RG --cluster-name $AKS -n workloadpool \\ --node-count 3 --enable-cluster-autoscaler --min-count 3 --max-count 10 \\ --node-vm-size Standard\\_D8s\\_v5 az aks nodepool add -g $RG --cluster-name $AKS -n batchpool \\ --enable-cluster-autoscaler --min-count 0 --max-count 5 \\ --node-vm-size Standard\\_D4s\\_v5 --priority Spot az aks get-credentials -g $RG -n $AKS\n\n### 9.4 Create Namespaces & Quotas\n- for ns in platform-prod payments-prod orders-prod inventory-prod; do kubectl create namespace $ns; done kubectl apply -f -\n\n### 9.5 Deploy Ingress & cert-manager (Platform Namespace)\n- helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add jetstack https://charts.jetstack.io helm repo update helm upgrade --install ingress ingress-nginx/ingress-nginx \\ --namespace platform-prod \\ --set controller.replicaCount=2 \\ --set controller.resources.requests.cpu=200m \\ --set controller.resources.requests.memory=256Mi kubectl apply -n platform-prod -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.3/cert-manager.crds.yaml helm upgrade --install cert-manager jetstack/cert-manager \\ --namespace platform-prod \\ --version v1.14.3\n\n### 9.6 Sample App (Payments API) with HPA\n- kubectl apply -n payments-prod -f -\n\n### 9.7 KEDA Installation & Event-Driven Worker\n- kubectl apply -f https://github.com/kedacore/keda/releases/latest/download/keda.yml kubectl apply -n payments-prod -f -\n\n### 9.8 Monitoring Stack (Prometheus & Grafana via Helm)\n- helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm upgrade --install kube-prom prometheus-community/kube-prometheus-stack \\ --namespace platform-prod \\ --set grafana.enabled=true \\ --set prometheus.prometheusSpec.retention=15d\n\n### 9.9 Velero Backup Setup\n- velero install \\ --provider azure \\ --plugins velero/velero-plugin-for-microsoft-azure:v1.8.0 \\ --bucket backups \\ --secret-file ./credentials-velero \\ --backup-location-config resourceGroup=$RG,storageAccount=$BLOBSA \\ --use-restic velero schedule create daily --schedule \"0 1 \\* \\* \\*\" --include-namespaces payments-prod,orders-prod,inventory-prod\n\n### 9.10 Cost Visibility (Optional Kubecost)\n- helm repo add kubecost https://kubecost.github.io/cost-analyzer/ helm upgrade --install kubecost kubecost/cost-analyzer \\ --namespace platform-prod \\ --set global.prometheus.enabled=false \\ --set global.prometheus.fqdn=http://kube-prom-prometheus.platform-prod.svc\n\n### 9.11 Basic Load Test (Simulate Traffic)\n- kubectl run loader -n payments-prod --image=busybox --restart=Never -- /bin/sh -c 'for i in $(seq 1 1000); do wget -q -O- http://payments-api.payments-prod.svc.cluster.local; done'\n\n### 9.12 Validate Autoscaling\n- kubectl get hpa -n payments-prod kubectl describe hpa payments-api-hpa -n payments-prod kubectl get pods -n payments-prod -w\n\n### 9.13 Cleanup\n- az group delete -n $RG --yes --no-wait\n\n### 9.14 Lab Outcomes\n\n- Provisioned shared prod cluster with node pools.\n- Established namespaces & quotas.\n- Deployed sample app + HPA + KEDA worker.\n- Installed ingress, monitoring, cost, backup tooling.\n- Validated scaling & backup schedule.\n\n### 9.15 Next Steps\n\n- Add GitOps bootstrap (Flux) repo sync.\n- Implement NetworkPolicies per namespace.\n- Integrate image signing (Cosign) & policy enforcement.",
  "PubDate": "2025-11-10T14:45:40+00:00",
  "Link": "https://techcommunity.microsoft.com/t5/azure-infrastructure-blog/building-enterprise-grade-shared-aks-clusters-a-guide-to-multi/ba-p/4468563",
  "Tags": [],
  "OutputDir": "_community",
  "EnhancedContent": "## Building Shared AKS Clusters: A Hands-On Guide with Labs and Best Practices\n\n## 1. Overview of Shared AKS Architecture\n\n### 1.1 Goals\n\n- Accelerate application delivery by providing a hardened shared Kubernetes platform per environment (prod / test / dev).\n- Enable safe multi-tenancy using namespace, RBAC, NetworkPolicy, quotas, and pod security.\n- Enforce consistency (standards, guardrails) while allowing team autonomy for app lifecycle.\n- Optimize cost through shared cluster capacity, rightsizing, and autoscaling.\n\n### 1.2 Pattern Summary\n\n- One AKS cluster per environment (e.g., aks-shared-prod, aks-shared-test, aks-shared-dev).\n- Multiple business units / product teams share the environment cluster via isolated namespaces.\n- Platform services (ingress, cert management, monitoring, cost, backup) run in a dedicated platform-&lt;env&gt; namespace.\n- Each team gets namespaces per environment: payments-prod, orders-prod, etc.\n\n### 1.3 High-Level Architecture Diagram\n\n### 1.4 Multi-Tenancy Mechanisms\n\n| Mechanism | Purpose | Enforcement Layer | | --- | --- | --- | | Namespaces | Logical isolation per team/app | Kubernetes API | | RBAC | Access control (who can do what) | Azure AD + K8s RBAC | | NetworkPolicy | East-west traffic control | CNI (Azure CNI) | | ResourceQuota & LimitRange | Prevent noisy neighbors | K8s admission | | Pod Security Standards | Baseline/Restricted enforcement | PS Admission | | Images from ACR only | Trusted supply chain | Admission / Policy |\n\n### 1.5 Why Not One Cluster For All Environments?\n\n| Aspect | Single Cluster (All Envs) | Per-Environment Clusters | | --- | --- | --- | | Blast Radius | High | Contained per env | | Change Windows | Complex coordination | Independent | | Compliance | Hard to segregate | Easier mapping | | Observability Noise | Mixed signals | Clean per env | | Scaling Decisions | Conflicting | Environment-specific |\n\nPer-environment clusters simplify lifecycle, versioning, and SLA management at the slight cost of control-plane duplication.\n\n### 1.6 Network Isolation & Azure CNI\n\n- Azure CNI assigns IPs from a VNet subnet directly to pods (no overlay) enabling IP-level visibility.\n- Use separate subnets per node pool (system vs workload vs batch) for clearer network policy scoping.\n- Leverage network policies (Calico or Azure native) to restrict cross-namespace traffic.\n- Private cluster option ensures API server accessible only via private endpoint / VNet.\n\n### 1.7 Tenancy Diagram (Namespaces → Apps per BU)\n\n## 2. Key Components\n\n### 2.1 Autoscaling Architecture\n\n- Cluster Autoscaler: adjusts node count (workload & batch pools) based on pending pods.\n- Horizontal Pod Autoscaler (HPA): scales replicas based on CPU, memory, or custom metrics.\n- Vertical Pod Autoscaler (VPA): recommends or applies resource request updates.\n- KEDA: event-driven autoscaling (queue length, Azure Service Bus, Kafka, etc.).\n\n### 2.2 Optional Service Mesh (Istio or Ambient Mesh)\n\nUse only when:\n\n- Need mTLS between services.\n- Require fine-grained traffic shift (canary, A/B, fault injection).\n- Require zero-trust identity propagation. Otherwise keep complexity low and rely on ingress + network policies.\n\n### 2.3 Ingress & API Exposure\n\n- NGINX or Azure Application Gateway Ingress Controller.\n- TLS via cert-manager + Azure Key Vault (CSI driver for secrets if needed).\n- Central routing and WAF (if AGIC used).\n\n### 2.4 Secrets & Configuration\n\n- Prefer Azure Key Vault: reference secrets in pods via CSI driver or sync controller.\n- Use Kubernetes sealed-secrets only for GitOps edge cases.\n- External Secrets Operator can streamline mapping.\n\n### 2.5 Storage\n\n- Azure Disk: DB/data workloads needing single-node high IOPS.\n- Azure Files: shared RW across replicas.\n- Azure NetApp Files: high throughput/low latency enterprise workloads.\n- Blob Storage: backup target + object data.\n\n### 2.6 Backup & Disaster Recovery\n\n- Velero backs up cluster metadata + PV snapshots (when using supported providers).\n- Off-cluster backups stored in Blob with lifecycle management.\n- DR strategy: recreate cluster via IaC + restore Velero backups + bootstrap GitOps.\n\n### 2.7 Observability Stack\n\n- Prometheus (metrics) + exporters (node, kube-state, custom).\n- Grafana dashboards per team and shared platform board.\n- Azure Monitor Container Insights for baseline + log retention.\n- Tracing: OpenTelemetry Collector + Jaeger or Azure Monitor tracing backend.\n\n## 3. Placeholder: CI/CD Strategy (to be expanded)\n\n## 3. CI/CD Strategy\n\n### 3.1 Principles\n\n- Everything declarative (Helm charts / Kustomize) stored in Git.\n- One pipeline per application per environment stage (build once, promote with image digest).\n- GitOps for platform and cross-cutting components (ingress, monitoring, backup) via Flux or Argo CD.\n- Separation of duties: App teams manage their namespace manifests; platform team manages cluster addons.\n\n### 3.2 Recommended Flow\n\n1. Developer merges to main → CI builds & scans image → pushes to ACR with immutable tag & digest.\n2. CI updates Helm values-prod.yaml (or image tag file) in Git (infra repo) via PR.\n3. GitOps controller detects change → deploys to namespace.\n4. Post-deploy tests & smoke checks run.\n5. Promotion to higher environment uses same image digest (no rebuild).\n\n### 3.3 Pipeline Diagram\n\n### 3.4 Tools\n\n| Concern | Tool | Notes | | --- | --- | --- | | Build | Azure DevOps / GitHub Actions | Container build, unit tests | | Scan (Image) | Trivy / Microsoft Defender | Fail pipeline on critical vulnerabilities | | Sign | Cosign | Image provenance signature | | Deploy | Helm + Flux | Reconciled from Git | | Secrets | Key Vault CSI / External Secrets | No secrets in Git |\n\n### 3.5 Sample GitHub Actions Snippet (Build & Push)\n\n``` jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Login ACR uses: azure/docker-login@v1 with: login-server: ${{ env.ACR_NAME }}.azurecr.io username: ${{ secrets.ACR_USERNAME }} password: ${{ secrets.ACR_PASSWORD }} - name: Build run: docker build -t ${{ env.ACR_NAME }}.azurecr.io/payments:${{ github.sha }} . - name: Scan uses: aquasecurity/trivy-action@v0.13.0 with: image-ref: ${{ env.ACR_NAME }}.azurecr.io/payments:${{ github.sha }} severity: HIGH,CRITICAL - name: Push run: docker push ${{ env.ACR_NAME }}.azurecr.io/payments:${{ github.sha }} ```\n\n### 3.6 Helm Deployment Command (Manual)\n\n> >\n> helm upgrade --install payments charts/payments \\ --namespace payments-prod \\ --set image.repository=myacr.azurecr.io/payments \\ --set image.tag=sha256:&lt;digest&gt;\n> >\n\n### 3.7 GitOps Advantages\n\n- Drift detection & self-healing.\n- Auditability (all changes in PR history).\n- Immutable artifacts (image digest pinned).\n\n### 3.8 Namespace Alignment\n\n| Namespace | Repo Path | Release Strategy | | --- | --- | --- | | payments-prod | apps/payments/overlays/prod | Auto after PR merge | | orders-prod | apps/orders/overlays/prod | Manual approval | | inventory-prod | apps/inventory/overlays/prod | Auto | | platform-prod | platform/addons | Platform team only |\n\n## 4. Backup Strategy (Deep Dive)\n\n### 4.1 Objectives\n\n- Recover from accidental deletion, corruption, cluster loss.\n- Meet RPO/RTO defined per application tier (e.g., Tier-1: RPO 15m, RTO 1h).\n\n### 4.2 Velero Architecture\n\n### 4.3 Backup Scope\n\n| Item | Method | Notes | | --- | --- | --- | | Namespace manifests | Velero backup | Included automatically | | Persistent Volumes (Azure Disk) | CSI snapshots | Fast point-in-time | | Azure Files | File-level backup (optional) | Consider rsync / custom | | ACR images | Not needed (immutable stored) | Use retention policies | | Secrets | Included; consider encryption | Key Vault references not stored |\n\n### 4.4 Backup Command Examples\n\n> >\n> # Create daily schedule velero schedule create daily-prod --schedule \"0 2 \\* \\* \\*\" --include-namespaces payments-prod,orders-prod,inventory-prod # On-demand backup velero backup create payments-manual-$(date +%Y%m%d) --include-namespaces payments-prod # Restore velero restore create --from-backup payments-manual-20250101\n> >\n\n### 4.5 Blob Storage Configuration\n\n- Use versioning & soft delete for container.\n- Configure lifecycle: transition &gt;90 day old backups to Cool / Archive.\n- Private endpoint for storage account if cluster private.\n\n### 4.6 DR Runbook (Summary)\n\n1. Recreate cluster via Bicep/Terraform.\n2. Install platform addons (GitOps bootstrap).\n3. Install Velero + connect to backup bucket.\n4. Restore critical namespaces (payments → orders → inventory).\n5. Run validation scripts & synthetic tests.\n\n### 4.7 Testing Backups\n\n- Monthly restore into ephemeral test cluster.\n- Validate app startup & data integrity checksums.\n\n### 4.8 KPIs\n\n| KPI | Target | | --- | --- | | Backup success rate | &gt; 99% | | Restore drill frequency | Monthly | | DR RTO (Tier-1) | &lt;= 60m | | DR RPO (Tier-1) | &lt;= 15m |\n\n## 5. Operational Insights\n\n### 5.1 Resource Optimization\n\n- Use VPA recommendations to refine requests bi-weekly.\n- KEDA for spiky workloads (workers / consumers) to avoid over-provisioning.\n- Batch node pool with Spot instances for cost-efficient asynchronous jobs.\n\n### 5.2 Quotas & LimitRanges\n\n| Namespace | CPU Quota | Memory Quota | Notes | | --- | --- | --- | --- | | payments-prod | 30 | 60Gi | High-traffic workload | | orders-prod | 40 | 80Gi | Larger processing window | | inventory-prod | 20 | 40Gi | Moderate update frequency |\n\n### 5.3 Noisy Neighbor Mitigation\n\n- Enforce per-deployment resource limits.\n- Use priority classes (system &gt; platform &gt; business apps &gt; batch).\n- Alert on sustained throttling or eviction events.\n\n### 5.4 Operational Dashboard Metrics\n\n| Category | Metric | Source | | --- | --- | --- | | Capacity | Node utilization % | Prometheus node exporter | | Efficiency | Requested vs actual usage | kube-state-metrics | | Reliability | Pod restarts, crash loops | kube events | | Performance | API latency P95 | Ingress metrics | | Scaling | HPA decision latency | Prometheus adapter |\n\n### 5.5 SLO Examples\n\n| Service | SLO | Measurement | | --- | --- | --- | | Payments API | 99.9% availability | Successful request % over 5m windows | | Orders API | P95 &lt; 400ms | Ingress / app metrics | | Inventory Sync | Completion &lt; 10m | Job duration metrics |\n\n### 5.6 Incident Playbook (Abbreviated)\n\n1. Detect (alert fires) → classify severity.\n2. Gather: kubectl describe, logs, metrics timeline.\n3. Mitigate: rollback image / scale resources / isolate via NetworkPolicy.\n4. Communicate: status page / stakeholder channel.\n5. Postmortem within 48h → action items tracked.\n\n## 6. Cost & Billing Strategy\n\n### 6.1 Tagging & Labeling\n\n- Azure resources: Environment=Prod, BusinessUnit=Payments, CostCenter=1234.\n- Kubernetes: Namespace labels bu=payments, env=prod used by cost tools (Kubecost / Azure Advisor).\n\n### 6.2 Cost Visibility\n\nNo diagram type detected matching given configuration for text:\n\n### 6.3 Optimization Levers\n\n| Lever | Description | Example | | --- | --- | --- | | Rightsizing | Adjust requests via VPA | Reduce CPU from 500m to 200m | | Spot | Use for batch/non-critical | BatchPool spot nodes | | Autoscaling | Scale down at night | WorkloadPool min nodes reduced | | Image Slimming | Smaller images → faster deploy | Multi-stage Docker builds | | Storage Tiering | Archive old backups | Blob lifecycle rules |\n\n### 6.4 Chargeback/Showback\n\n- Monthly export per namespace (CPU-hours, memory GB-hours, storage GB, network egress).\n- Map to internal rate card (e.g., $ per vCPU-hour).\n- Provide dashboard + monthly PDF summary.\n\n### 6.5 KPIs\n\n| KPI | Target | | --- | --- | | Unallocated capacity | &lt; 20% | | Spot utilization | &gt; 30% of batch workloads | | Orphan resources cleanup time | &lt; 7 days |\n\n## 7. Security & Compliance\n\n### 7.1 Layered Model\n\n| Layer | Control | Tool | | --- | --- | --- | | Identity | Azure AD RBAC → K8s RBAC | Azure AD groups | | Workload Policies | PSP Replacement / PSS | Built-in Pod Security Admission | | Network | Namespace isolation | NetworkPolicy (Calico/Azure) | | Supply Chain | Image provenance/signature | Cosign + ACR Content Trust | | Secrets | External vault storage | Azure Key Vault | | Runtime | Behavioral detection | Defender for Containers |\n\n### 7.2 RBAC Pattern\n\n- ClusterRoles for common verbs (view, deploy, ops).\n- Bind Role to Azure AD groups via AAD integration (Group mapping).\n- Example groups: aks-platform-admins, aks-payments-devs, aks-readonly.\n\n### 7.3 NetworkPolicy Example\n\n``` apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-ingress-to-payments namespace: payments-prod spec: podSelector: matchLabels: app: payments-api ingress: - from: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: platform-prod ports: - protocol: TCP port: 8080 policyTypes: [Ingress] ```\n\n### 7.4 Pod Security Standards\n\n- Set namespace labels: pod-security.kubernetes.io/enforce=restricted for prod.\n- Use baseline for dev/test to allow debugging tools.\n\n### 7.5 Image Policy\n\n- Admission controller verifies images originate from approved ACR & are signed.\n\n### 7.6 Secret Management Pattern\n\n``` apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: payments-kv namespace: payments-prod spec: provider: azure parameters: usePodIdentity: \"false\" useVMManagedIdentity: \"true\" userAssignedIdentityID: <client-id> keyvaultName: my-shared-kv objects: | array: - | secret;db-password tenantId: <tenant-id> ```\n\n### 7.7 Compliance Mapping\n\n| Requirement | Control | Evidence | | --- | --- | --- | | Least Privilege | RBAC roles | GitOps repo + audit logs | | Data Protection | Encrypted disks | Azure policy compliance report | | Audit | Central log retention | Azure Monitor workspace | | Vulnerability Mgmt | Image scanning | Pipeline reports |\n\n## 8. Monitoring & Observability\n\n### 8.1 Pillars\n\n| Pillar | Tool | Output | | --- | --- | --- | | Metrics | Prometheus | Time-series dashboards | | Logs | Azure Monitor / Loki (optional) | Query & retention | | Traces | OpenTelemetry Collector | Distributed latency maps | | Events | Kubernetes API / Alertmanager | Incident triggers |\n\n### 8.2 Observability Diagram\n\n### 8.3 Example PrometheusRule\n\n``` apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: payments-alerts namespace: platform-prod spec: groups: - name: payments-availability rules: - alert: PaymentsHighErrorRate expr: rate(http_requests_total{namespace=\"payments-prod\",status=~\"5..\"}[5m]) > 5 for: 2m labels: severity: page annotations: summary: High 5xx error rate in payments API description: More than 5 errors/min for 2 minutes. ```\n\n### 8.4 Alerting Strategy\n\n- Page only on user-impact (availability, latency SLO breaches).\n- Ticket for capacity trend warnings.\n- Daily digest for low-priority issues.\n\n### 8.5 Dashboards\n\n- Platform Overview (cluster health, node capacity).\n- Namespace Cost & Efficiency.\n- Application Performance (latency, errors, throughput).\n\n### 8.6 Log Retention\n\n- 30 days hot, 180 days archive (Blob / ADLS).\n- PII scrubbing before long-term archival.\n\n### 8.7 Tracing Adoption Steps\n\n1. Inject OTel SDK into services.\n2. Export spans to collector via OTLP.\n3. Add trace ID to logs for correlation.\n4. Establish latency budgets per critical path.\n\n### 8.8 KPIs\n\n| KPI | Target | | --- | --- | | Alert false positives | &lt; 10% | | Missing metrics coverage | &lt; 5% of services | | Trace sampled rate (critical paths) | &gt; 90% |\n\n## 9. Placeholder: Lab Section\n\n## 9. Hands-On Lab: Build & Operate Shared AKS\n\n> >\n> Estimated Time: ~120 minutes. Run commands from an Azure Cloud Shell or local workstation logged into Azure (az login). Replace variables as needed.\n> >\n\n### 9.1 Prerequisites\n\n> >\n> export LOCATION=eastus\n> > >\n> export RG=rg-shared-aks-prod\n> > >\n> export AKS=aks-shared-prod\n> > >\n> export ACR=acrsharedprod$RANDOM\n> > >\n> export KV=kv-shared-prod-$RANDOM\n> > >\n> export BLOBSA=stsharedprod$RANDOM\n> >\n\n### 9.2 Create Resource Group & Shared Services\n\n``` az group create -n $RG -l $LOCATION az acr create -n $ACR -g $RG --sku Premium --location $LOCATION az keyvault create -n $KV -g $RG -l $LOCATION --enabled-for-deployment true az storage account create -n $BLOBSA -g $RG -l $LOCATION --sku Standard_LRS --kind StorageV2 export BLOBKEY=$(az storage account keys list -g $RG -n $BLOBSA --query [0].value -o tsv) az acr login -n $ACR ```\n\n### 9.3 Provision AKS Cluster (Per-Environment)\n\n``` az aks create -g $RG -n $AKS \\ --enable-managed-identity \\ --node-count 3 \\ --node-vm-size Standard_D4s_v5 \\ --network-plugin azure \\ --enable-addons monitoring \\ --enable-oidc-issuer \\ --enable-workload-identity \\ --generate-ssh-keys az aks nodepool add -g $RG --cluster-name $AKS -n workloadpool \\ --node-count 3 --enable-cluster-autoscaler --min-count 3 --max-count 10 \\ --node-vm-size Standard_D8s_v5 az aks nodepool add -g $RG --cluster-name $AKS -n batchpool \\ --enable-cluster-autoscaler --min-count 0 --max-count 5 \\ --node-vm-size Standard_D4s_v5 --priority Spot az aks get-credentials -g $RG -n $AKS ```\n\n### 9.4 Create Namespaces & Quotas\n\n``` for ns in platform-prod payments-prod orders-prod inventory-prod; do kubectl create namespace $ns; done kubectl apply -f - <<'EOF' apiVersion: v1 kind: ResourceQuota metadata: name: rq-payments namespace: payments-prod spec: hard: requests.cpu: \"30\" requests.memory: 60Gi limits.cpu: \"40\" limits.memory: 80Gi --- apiVersion: v1 kind: LimitRange metadata: name: lr-payments namespace: payments-prod spec: limits: - type: Container default: cpu: \"500m\" memory: \"512Mi\" defaultRequest: cpu: \"250m\" memory: \"256Mi\" EOF ```\n\n### 9.5 Deploy Ingress & cert-manager (Platform Namespace)\n\n``` helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo add jetstack https://charts.jetstack.io helm repo update helm upgrade --install ingress ingress-nginx/ingress-nginx \\ --namespace platform-prod \\ --set controller.replicaCount=2 \\ --set controller.resources.requests.cpu=200m \\ --set controller.resources.requests.memory=256Mi kubectl apply -n platform-prod -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.3/cert-manager.crds.yaml helm upgrade --install cert-manager jetstack/cert-manager \\ --namespace platform-prod \\ --version v1.14.3 ```\n\n### 9.6 Sample App (Payments API) with HPA\n\n``` kubectl apply -n payments-prod -f - <<'EOF' apiVersion: apps/v1 kind: Deployment metadata: name: payments-api spec: replicas: 2 selector: matchLabels: app: payments-api template: metadata: labels: app: payments-api spec: containers: - name: api image: nginx:1.25 resources: requests: cpu: 250m memory: 256Mi limits: cpu: 500m memory: 512Mi ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: payments-api spec: selector: app: payments-api ports: - port: 80 targetPort: 80 --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: payments-api-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: payments-api minReplicas: 2 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 EOF ```\n\n### 9.7 KEDA Installation & Event-Driven Worker\n\n``` kubectl apply -f https://github.com/kedacore/keda/releases/latest/download/keda.yml kubectl apply -n payments-prod -f - <<'EOF' apiVersion: apps/v1 kind: Deployment metadata: name: payments-worker spec: replicas: 1 selector: matchLabels: app: payments-worker template: metadata: labels: app: payments-worker spec: containers: - name: worker image: busybox args: [\"/bin/sh\",\"-c\",\"while true; do echo processing; sleep 30; done\"] resources: requests: cpu: 100m memory: 128Mi limits: cpu: 200m memory: 256Mi --- apiVersion: keda.sh/v1alpha1 kind: ScaledObject metadata: name: payments-worker-so spec: scaleTargetRef: name: payments-worker minReplicaCount: 1 maxReplicaCount: 10 triggers: - type: azure-servicebus metadata: queueName: payments-queue messageCount: '5' connectionFromEnv: SERVICEBUS_CONNECTION_STRING EOF ```\n\n### 9.8 Monitoring Stack (Prometheus & Grafana via Helm)\n\n``` helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo add grafana https://grafana.github.io/helm-charts helm repo update helm upgrade --install kube-prom prometheus-community/kube-prometheus-stack \\ --namespace platform-prod \\ --set grafana.enabled=true \\ --set prometheus.prometheusSpec.retention=15d ```\n\n### 9.9 Velero Backup Setup\n\n``` velero install \\ --provider azure \\ --plugins velero/velero-plugin-for-microsoft-azure:v1.8.0 \\ --bucket backups \\ --secret-file ./credentials-velero \\ --backup-location-config resourceGroup=$RG,storageAccount=$BLOBSA \\ --use-restic velero schedule create daily --schedule \"0 1 * * *\" --include-namespaces payments-prod,orders-prod,inventory-prod ```\n\n### 9.10 Cost Visibility (Optional Kubecost)\n\n``` helm repo add kubecost https://kubecost.github.io/cost-analyzer/ helm upgrade --install kubecost kubecost/cost-analyzer \\ --namespace platform-prod \\ --set global.prometheus.enabled=false \\ --set global.prometheus.fqdn=http://kube-prom-prometheus.platform-prod.svc ```\n\n### 9.11 Basic Load Test (Simulate Traffic)\n\n``` kubectl run loader -n payments-prod --image=busybox --restart=Never -- /bin/sh -c 'for i in $(seq 1 1000); do wget -q -O- http://payments-api.payments-prod.svc.cluster.local; done' ```\n\n### 9.12 Validate Autoscaling\n\n``` kubectl get hpa -n payments-prod kubectl describe hpa payments-api-hpa -n payments-prod kubectl get pods -n payments-prod -w ```\n\n### 9.13 Cleanup\n\n``` az group delete -n $RG --yes --no-wait ```\n\n### 9.14 Lab Outcomes\n\n- Provisioned shared prod cluster with node pools.\n- Established namespaces & quotas.\n- Deployed sample app + HPA + KEDA worker.\n- Installed ingress, monitoring, cost, backup tooling.\n- Validated scaling & backup schedule.\n\n### 9.15 Next Steps\n\n- Add GitOps bootstrap (Flux) repo sync.\n- Implement NetworkPolicies per namespace.\n- Integrate image signing (Cosign) & policy enforcement.\n\nUpdated Nov 10, 2025\n\nVersion 1.0\n\n[!\\[dhaneshuk&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-4.svg?image-dimensions=50x50)](/users/dhaneshuk/2708924) [dhaneshuk](/users/dhaneshuk/2708924) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined October 10, 2024\n\n[View Profile](/users/dhaneshuk/2708924)\n\n/category/azure/blog/azureinfrastructureblog [Azure Infrastructure Blog](/category/azure/blog/azureinfrastructureblog) Follow this blog board to get notified when there's new activity",
  "ProcessedDate": "2025-11-10 15:03:42",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Title": "Building Enterprise-Grade Shared AKS Clusters: A Guide to Multi-Tenant Kubernetes Architecture"
}
