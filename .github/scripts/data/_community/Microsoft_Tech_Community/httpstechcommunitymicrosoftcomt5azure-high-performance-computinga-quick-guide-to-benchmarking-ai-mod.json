{
  "Description": "by Mark Gitau (Software Engineer)\n\n### **Introduction**\n\nFor the MLPerf Inference v5.1 submission, Azure shared performance results on the new ND GB200 v6 virtual machines. A single ND GB200 v6 VM on Azure is powered by two NVIDIA Grace CPUs and four NVIDIA Blackwell B200 GPUs.\n\nThis document highlights Azure’s MLPerf Inference v5.1 results and outlines the steps to run these benchmarks on Azure. These MLPerf™ benchmark results demonstrate Azure’s commitment to providing our customers with the latest GPU offerings of the highest quality.\n\nHighlights from MLPerf Inference v5.1 benchmark results include:\n\n- Azure had the highest Llama 2 70B Offline submission with 52,000 tokens/s on a single ND GB200 v6 virtual machine. This corresponds to an 8% increase on single node performance since [our record](https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/azure%E2%80%99s-nd-gb200-v6-delivers-record-performance-for-inference-workloads/4399253) which would correspond to 937,098 tokens/s on a full NVL72 rack.\n\n- Azure results for Llama 3.1 405B are at par with the best submitters (1% difference), cloud and on-premises, with 847 tokens/s.\n\n### **How to replicate the results in Azure**\n\n##### Pre-requisites:\n\n- ND GB200 v6-series (single node): Deploy and set up a virtual machine on [Azure](https://ms.portal.azure.com/)\n\n##### Set up the environment\n\n- First, we need to export the path to the directory where we will perform the benchmarks.\n\n- For ND GB200 v6-series (single node), create a directory called mlperf in /mnt/nvme\n\n- Set mlperf scratch space:\n\n``` export MLPERF_SCRATCH_PATH=/mnt/nvme/mlperf ```\n\n- Clone the MLPerf repository inside the scratch path:\n\n``` git clone https://github.com/mlcommons/submissions_inference_v5.1 ```\n\n- Then create empty directories in your scratch space to house the data:\n\n``` mkdir $MLPERF_SCRATCH_PATH/data $MLPERF_SCRATCH_PATH/models $MLPERF_SCRATCH_PATH/preprocessed_data ```\n\n##### Download the models & datasets\n\n- Download the models inside the models directory you created in the previous step. This will take a while because the weights are large.\n- [Llama 2 70B model](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-model)\n- [Llama 3.1 405B model](https://github.com/mlcommons/inference/tree/master/language/llama3.1-405b#get-model)\n\n- Download the preprocessed datasets for both models\n- [Llama 2 70B datasets](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-dataset)\n- [Llama 3.1 405B datasets](https://github.com/mlcommons/inference/tree/master/language/llama3.1-405b#get-dataset)\n- Prepare the datasets for Llama 2 70B: [https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama2-70b/tensorrt#download-and-prepare-data](https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama2-70b/tensorrt#download-and-prepare-data)\n- prepare the datasets for Llama 3.1 405B: [https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama3.1-405b/tensorrt#download-and-prepare-data](https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama3.1-405b/tensorrt#download-and-prepare-data)\n\n##### Build & launch MLPerf container\n\n- Export the Submitter and System name:\n\n``` export SUBMITTER=Azure SYSTEM_NAME=ND_GB200_v6 ```\n\n- Enter the container by entering the closed/Azure directory and running:\n\n``` make prebuild ```\n\n- Inside the container, run\n\n``` make build ```\n\n##### Build engines & run benchmarks\n\n- Make sure you are still in the closed/Azure directory of the MLPerf repository\n\n- To build the engines for both Llama 3.1 405B and Llama 2 70B:\n\n``` make generate_engines RUN_ARGS=\"--benchmarks=llama2-70b,llama3.1-405b --scenarios=offline,server\" ```\n\n- To run the benchmarks for both Llama 3.1 405B and Llama 2 70B:\n\n``` make run_harness RUN_ARGS==\"--benchmarks=llama2-70b,llama3.1-405b --scenarios=offline,server\" ```\n\n#### MLPerf from MLCommons®\n\nMLCommons® is an open engineering consortium of AI leaders from academia, research, and industry where the mission is to “build fair and useful benchmarks” that provide unbiased evaluations of training and inference performance for hardware, software, and services—all conducted under predetermined conditions. MLPerf™ Inference benchmarks consist of compute-intensive AI workloads that simulate realistic usage of the systems, making the results very influential in technology management’s buying decisions.",
  "Author": "Mark_Gitau",
  "EnhancedContent": "by Mark Gitau (Software Engineer)\n\n### **Introduction**\n\nFor the MLPerf Inference v5.1 submission, Azure shared performance results on the new ND GB200 v6 virtual machines. A single ND GB200 v6 VM on Azure is powered by two NVIDIA Grace CPUs and four NVIDIA Blackwell B200 GPUs.\n\nThis document highlights Azure’s MLPerf Inference v5.1 results and outlines the steps to run these benchmarks on Azure. These MLPerf™ benchmark results demonstrate Azure’s commitment to providing our customers with the latest GPU offerings of the highest quality.\n\nHighlights from MLPerf Inference v5.1 benchmark results include:\n\n- Azure had the highest Llama 2 70B Offline submission with 52,000 tokens/s on a single ND GB200 v6 virtual machine. This corresponds to an 8% increase on single node performance since [our record](https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/azure%E2%80%99s-nd-gb200-v6-delivers-record-performance-for-inference-workloads/4399253) which would correspond to 937,098 tokens/s on a full NVL72 rack.\n\n- Azure results for Llama 3.1 405B are at par with the best submitters (1% difference), cloud and on-premises, with 847 tokens/s.\n\n### **How to replicate the results in Azure**\n\n##### Pre-requisites:\n\n- ND GB200 v6-series (single node): Deploy and set up a virtual machine on [Azure](https://ms.portal.azure.com/)\n\n##### Set up the environment\n\n- First, we need to export the path to the directory where we will perform the benchmarks.\n\n- For ND GB200 v6-series (single node), create a directory called mlperf in /mnt/nvme\n\n- Set mlperf scratch space:\n\n``` export MLPERF_SCRATCH_PATH=/mnt/nvme/mlperf ```\n\n- Clone the MLPerf repository inside the scratch path:\n\n``` git clone https://github.com/mlcommons/submissions_inference_v5.1 ```\n\n- Then create empty directories in your scratch space to house the data:\n\n``` mkdir $MLPERF_SCRATCH_PATH/data $MLPERF_SCRATCH_PATH/models $MLPERF_SCRATCH_PATH/preprocessed_data ```\n\n##### Download the models & datasets\n\n- Download the models inside the models directory you created in the previous step. This will take a while because the weights are large.\n- [Llama 2 70B model](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-model)\n- [Llama 3.1 405B model](https://github.com/mlcommons/inference/tree/master/language/llama3.1-405b#get-model)\n\n- Download the preprocessed datasets for both models\n- [Llama 2 70B datasets](https://github.com/mlcommons/inference/tree/master/language/llama2-70b#get-dataset)\n- [Llama 3.1 405B datasets](https://github.com/mlcommons/inference/tree/master/language/llama3.1-405b#get-dataset)\n- Prepare the datasets for Llama 2 70B: [https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama2-70b/tensorrt#download-and-prepare-data](https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama2-70b/tensorrt#download-and-prepare-data)\n- prepare the datasets for Llama 3.1 405B: [https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama3.1-405b/tensorrt#download-and-prepare-data](https://github.com/mlcommons/submissions_inference_v5.1/tree/main/closed/NVIDIA/code/llama3.1-405b/tensorrt#download-and-prepare-data)\n\n##### Build & launch MLPerf container\n\n- Export the Submitter and System name:\n\n``` export SUBMITTER=Azure SYSTEM_NAME=ND_GB200_v6 ```\n\n- Enter the container by entering the closed/Azure directory and running:\n\n``` make prebuild ```\n\n- Inside the container, run\n\n``` make build ```\n\n##### Build engines & run benchmarks\n\n- Make sure you are still in the closed/Azure directory of the MLPerf repository\n\n- To build the engines for both Llama 3.1 405B and Llama 2 70B:\n\n``` make generate_engines RUN_ARGS=\"--benchmarks=llama2-70b,llama3.1-405b --scenarios=offline,server\" ```\n\n- To run the benchmarks for both Llama 3.1 405B and Llama 2 70B:\n\n``` make run_harness RUN_ARGS==\"--benchmarks=llama2-70b,llama3.1-405b --scenarios=offline,server\" ```\n\n#### MLPerf from MLCommons®\n\nMLCommons® is an open engineering consortium of AI leaders from academia, research, and industry where the mission is to “build fair and useful benchmarks” that provide unbiased evaluations of training and inference performance for hardware, software, and services—all conducted under predetermined conditions. MLPerf™ Inference benchmarks consist of compute-intensive AI workloads that simulate realistic usage of the systems, making the results very influential in technology management’s buying decisions.\n\nUpdated Sep 08, 2025\n\nVersion 1.0\n\n[ai infrastructure](/tag/ai%20infrastructure?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[benchmarking](/tag/benchmarking?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[!\\[Mark_Gitau&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-10.svg?image-dimensions=50x50)](/users/mark_gitau/2759476) [Mark_Gitau](/users/mark_gitau/2759476) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined November 11, 2024\n\n[View Profile](/users/mark_gitau/2759476)\n\n/category/azure/blog/azurehighperformancecomputingblog [Azure High Performance Computing (HPC) Blog](/category/azure/blog/azurehighperformancecomputingblog) Follow this blog board to get notified when there's new activity",
  "Link": "https://techcommunity.microsoft.com/t5/azure-high-performance-computing/a-quick-guide-to-benchmarking-ai-models-on-azure-llama-405b-and/ba-p/4452192",
  "OutputDir": "_community",
  "PubDate": "2025-09-09T15:00:00+00:00",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Tags": [],
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "FeedName": "Microsoft Tech Community",
  "ProcessedDate": "2025-09-09 15:13:54",
  "Title": "A Quick Guide to Benchmarking AI models on Azure: Llama 405B and 70B with MLPerf Inference v5.1"
}
