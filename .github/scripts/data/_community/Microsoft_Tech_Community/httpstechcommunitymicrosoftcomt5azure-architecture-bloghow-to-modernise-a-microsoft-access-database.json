{
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "EnhancedContent": "## In this post, I document a real world modernisation engagement where a Microsoft Access solution was modernised to Node.JS, OpenAPI and SQL Server using GitHub Copilot and Visual Studio Code. Demonstrating how each layer of this solution can be migrated and modernised in to a modern tech stack with example prompts, context and copilot-instructions.md\n\nMicrosoft Access has played a significant role in enterprise environments for over three decades. Released in November 1992, its flexibility and ease of use made it a popular choice for organizations of all sizesâ€”from FTSE250 companies to startups and the public sector. The platform enables rapid development of graphical user interfaces (GUIs) paired with relational databases, allowing users to quickly create professional-looking applications.\n\nDevelopers, data architects, and power users have all leveraged Microsoft Access to address various enterprise challenges. Its integration with Microsoft Visual Basic for Applications (VBA), an object-based programming language, ensured that Access solutions often became central to business operations. Unsurprisingly, modernizing these applications is a common requirement in contemporary IT engagements as thse solutions lead to data fragmentation, lack of integration into master data systems, multiple copies of the same data replicated across each access database and so on.\n\nAt first glance, upgrading a Microsoft Access application may seem simple, given its reliance on forms, VBA code, queries, and tables. However, substantial complexity often lurks beneath this straightforward exterior. Modernization efforts must consider whether to retain the familiar user interface to reduce staff retraining, how to accurately re-implement business logic, strategies for seamless data migration, and whether to introduce an API layer for data access. These factors can significantly increase the scope and effort required to deliver a modern equivalent, especially when dealing with numerous web forms, making manual rewrites a daunting task.\n\nThis is where GitHub Copilot can have a transformative impact, dramatically reducing redevelopment time. By following a defined migration path, it is possible to deliver a modernized solution in as little as two weeks.\n\nIn this blog post, Iâ€™ll walk you through each tier of the application and give you example prompts used at each stage.\n\n## ğŸ›ï¸Architecture Breakdown: The N-Tier Approach\n\nBreaking down the application architecture reveals a classic N-Tier structure, consisting of a presentation layer, business logic layer, data access layer, and data management layer.\n\n## ğŸ’«First-Layer Migration: Migrating a Microsoft Access Database to SQL Server\n\nThe migration process began with the database layer, which is typically the most straightforward to move from Access to another relational database management system (RDBMS). In this case, SQL Server was selected to leverage the [SQL Server Migration Assistant (SSMA) for Microsoft Access](https://learn.microsoft.com/en-us/sql/ssma/sql-server-migration-assistant?view=sql-server-ver17)â€”a free tool from Microsoft that streamlines database migration to SQL Server, Azure SQL Database, or Azure SQL Database Managed Instance (SQLMI). While GitHub Copilot could generate new database schemas and insert scripts, the availability of a specialized tool made the process more efficient.\n\nUsing SSMA, the database was migrated to SQL Server with minimal effort. However, it is important to note that relationships in Microsoft Access may lack explicit names. In such cases, SSMA appends a GUID or uses one entirely to create unique foreign key names, which can result in confusing relationship names post-migration. Fortunately, GitHub Copilot can batch-rename these relationships in the generated SQL scripts, applying more meaningful naming conventions. By dropping and recreating the constraints, relationships become easier to understand and maintain.\n\nSSMA handles the bulk of the migration workload, allowing you to quickly obtain a fully functional SQL Server database containing all original data. In practice, renaming and recreating constraints often takes longer than the data migration itself.\n\n``` Prompt Used:Â # ContextÂ I want to refactor the #file:script.sql SQL script. Your task is to follow the below steps to analyse it and refactor it according to the specified rules.Â You are allowed to create / run any python scripts or terminal commands to assist in the analysis and refactoring process.Â # Analysis PhaseÂ Identify: ```\n\n1. ```\nAny warning comments ```\n2. ```\nRelations between tables ```\n3. ```\nForeign key creation ```\n4. ```\nReferences to these foreign keys in 'MS_SSMA_SOURCE' metadata ```\n\n```\n# Refactor PhaseÂ Refactor any SQL matching the following rules:Â Â  - Create a new script file with the same name as the original but with a `.refactored.sql` extensionÂ Â  - Rename any primary key constraints to follow the format PK_{table_name}_{column_name}Â Â  - Rename any foreign key constraints like [TableName]${GUID} to FK_{child_table}_{parent_table}Â Â  - Rename any indexes like [TableName]${GUID} to IDX_{table_name}_{column_name}Â Â  - Ensure any updated foreign keys are updated elsewhere in the scriptÂ Â  - Identify which warnings flagged by the migration assistant need addressedÂ # Summary PhaseÂ Create a summary file in markdown format with the following sections:Â Â  - Summary of changes madeÂ Â  - List of warnings addressedÂ Â  - List of foreign keys renamedÂ Â  - Any other relevant notes\n```\n\n## ğŸ¤–Bonus: Introduce Database Automation and Change Management\n\nAs we now had a SQL database, we needed to consider how we would roll out changes to the database and we could introduce a formal tool to cater for this within the solution which was [Liquibase](https://www.liquibase.com/).\n\n``` Prompt Used:Â # ContextÂ I want to refactor #file:db.changelog.xml. Your task is to follow the below steps to analyse it and refactor it according to the specified rules.Â You are allowed to create / run any python scripts or terminal commands to assist in the analysis and refactoring process.Â # Analysis Phase ```\n\n1. ```\nAnalyse the generated changelog to identify the structure and content. ```\n2. ```\nIdentify the tables, columns, data types, constraints, and relationships present in the database. ```\n3. ```\nIdentify any default values, indexes, and foreign keys that need to be included in the changelog. ```\n4. ```\nIdentify any vendor specific data types / fucntions that need to be converted to common Liquibase types. ```\n\n```\n# Refactor PhaseÂ DO NOT modify the original #file:db.changelog.xml file in any way. Instead, create a new changelog file called `db.changelog-1-0.xml` to store the refactored changesets. The new file should follow the structure and conventions of Liquibase changelogs.Â You can fetch https://docs.liquibase.com/concepts/data-type-handling.html to get available Liquibase types and their mappings across RDBMS implementations.\n```\n\n1. ```\nCopy the original changesets from the `db.changelog.xml` file into the new file ```\n2. ```\nRefactor the changesets according to the following rules: ```\n\n```\n- The main changelog should only include child changelogs and not directly run migration operationsÂ Â  - Child changelogs should follow the convention db.changelog-{version}.xml and start at 1-0Â Â  - Ensure data types are converted to common Liquibase data types. For example:Â Â Â Â  - `nvarchar(max)` should be converted to `TEXT`Â Â Â Â  - `datetime2` should be converted to `TIMESTAMP`Â Â Â Â  - `bit` should be converted to `BOOLEAN`Â Â  - Ensure any default values are retained but ensure that they are compatible with the liquibase data type for the column.Â Â  - Use standard SQL functions like `CURRENT_TIMESTAMP` instead of vendor-specific functions.Â Â  - Only use vendor specific data types or functions if they are necessary and cannot be converted to common Liquibase types. These must be documented in the changelog and summary.\n```\n\n1. ```\nEnsure that the original changeset IDs are preserved for traceability. ```\n2. ```\nEnsure that the author of all changesets is \"liquibase (generated)\" ```\n\n```\n# Validation Phase\n```\n\n1. ```\nValidate the new changelog file against the original #file:db.changelog.xml to ensure that all changesets are correctly refactored and that the structure is maintained. ```\n2. ```\nConfirm no additional changesets are added that were not present in the original changelog. ```\n\n```\n# Finalisation Phase\n```\n\n1. ```\nProvide a summary of the changes made in the new changelog file. ```\n2. ```\nDocument any vendor specific data types or functions that were used and why they could not be converted to common Liquibase types. ```\n3. ```\nEnsure the main changelog file (`db.changelog.xml`) is updated to include the new child changelog file (`db.changelog-1-0.xml`). ```\n\n## ğŸ¤–Bonus: Synthetic Data Generation\n\nSince the legacy system lacked synthetic data for development or testing, GitHub Copilot was used to generate fake seed data. Care was taken to ensure all generated data was clearly fictionalâ€”using placeholders like â€˜Fake Nameâ€™ and â€˜Fake Townâ€™â€”to avoid any confusion with real-world information. This step greatly improved the maintainability of the project, enabling developers to test features without handling sensitive or real data.\n\n## ğŸ’«Second-Layer Migration: OpenAPI Specifications\n\nWith data migration complete, the focus shifted to implementing an API-driven approach for data retrieval. Adopting modern standards, OpenAPI specifications were used to define new RESTful APIs for creating, reading, updating, and deleting data. Because these APIs mapped directly to underlying entities, GitHub Copilot efficiently generated the required endpoints and services in Node.js, utilizing a repository pattern. This approach not only provided robust APIs but also included comprehensive self-describing documentation, validation at the API boundary, automatic error handling, and safeguards against invalid data reaching business logic or database layers.\n\n## ğŸ’«Third-Layer Migration: Business Logic\n\nThe business logic, originally authored in VBA, was generally straightforward. GitHub Copilot translated this logic into its Node.js equivalent and created corresponding tests for each method. These tests were developed directly from the code, adding a layer of quality assurance that was absent in the original Access solution. The result was a set of domain services mirroring the functionality of their VBA predecessors, successfully completing the migration of the third layer.\n\nAt this stage, the project had a new database, a fresh API tier, and updated business logic, all conforming to the latest organizational standards. The final major component was the user interface, an area where advances in GitHub Copilotâ€™s capabilities became especially evident.\n\n## ğŸ’«Fourth Layer: User Interface\n\nThe modernization of the Access Forms user interface posed unique challenges. To minimize retraining requirements, the new system needed to retain as much of the original layout as possible, ensuring familiar placement of buttons, dropdowns, and other controls. At the same time, it was necessary to meet new accessibility standards and best practices.\n\nSome Access forms were complex, spanning multiple tabs and containing numerous controls. Manually describing each interface for redevelopment would have been time-consuming. Fortunately, newer versions of GitHub Copilot support image-based prompts, allowing screenshots of Access Forms to serve as context. Using these screenshots, Copilot generated Government Digital Service Views that closely mirrored the original application while incorporating required accessibility features, such as descriptive labels and field selectors.\n\nAlthough the automatically generated UI might not fully comply with all current accessibility standards, prompts referencing WCAG guidelines helped guide Copilotâ€™s improvements. The generated interfaces provided a strong starting point for UX engineers to further refine accessibility and user experience to meet organizational requirements.\n\n## ğŸ¤–Bonus: User Story Generation from the User Interface\n\nFor organizations seeking a specification-driven development approach, GitHub Copilot can convert screenshots and business logic into user stories following the â€œ**As a** â€¦ **I want to** â€¦ **So that** â€¦â€ format. While not flawless, this capability is invaluable for systems lacking formal requirements, giving business analysts a foundation to build upon in future iterations.\n\n## ğŸ¤–Bonus: Introducing MongoDB\n\nTowards the end of the modernization engagement, there was interest in demonstrating migration from SQL Server to MongoDB. GitHub Copilot can facilitate this migration, provided it is given adequate context. As with all NoSQL databases, the design should be based on application data access patternsâ€”typically reading and writing related data together. Copilotâ€™s ability to automate this process depends on a comprehensive understanding of the applicationâ€™s data relationships and patterns.\n\n```\n# ContextÂ The `<business_entity>` entity from the existing system needs to be added to the MongoDB schema. You have been provided with the following:Â - #file:documentation - System documentation to provide domain / business entity contextÂ - #file:db.changelog.xml - Liquibase changelog for SQL contextÂ - #file:mongo-erd.md - Contains the current Mongo schema Mermaid ERD. Create this if it does not exist.Â - #file:stories - Contains the user stories that will the system will be built aroundÂ # Analysis Phase\n```\n\n1. ```\nAnalyse the available documentation and changelog to identify the structure, relationships, and business context of the `<business_entity>`. ```\n2. ```\nIdentify: ```\n\n```\n- All relevant data fields and attributesÂ Â  - Relationships with other entitiesÂ Â  - Any specific data types, constraints, or business rules\n```\n\n1. ```\nDetermine how this entity fits into the overall MongoDB schema: ```\n\n```\n- Should it be a separate collection?Â Â  - Should it be embedded in another document?Â Â  - Should it be a reference to another collection for lookups or relationships?Â Â  - Explore the benefit of denormalization for performance and business needs\n```\n\n1. ```\nConsider the data access patterns and how this entity will be used in the application. ```\n\n```\n# MongoDB Schema DesignÂ Using the analysis, suggest how the `<business_entity>` should be represented in MongoDB:Â - The name of the MongoDB collection that will represent this entityÂ - List each field in the collection, its type, any constraints, and what it maps to in the original business contextÂ - For fields that are embedded, document the parent collection and how the fields are nested. Nested fields should follow the format `parentField->childField`.Â - For fields that are referenced, document the reference collection and how the lookup will be performed.Â - Provide any additional notes on indexing, performance considerations, or specific MongoDB features that should be usedÂ - Always use pascal case for collection names and camel case for field namesÂ # ERD CreationÂ Create or update the Mermaid ERD in `mongo-erd.md` to include the results of your analysis. The ERD should reflect:Â - The new collection or embedded document structureÂ - Any relationships with other collections/entitiesÂ - The data types, constraints, and business rules that are relevant for MongoDBÂ - Ensure the ERD is clear and follows best practices for MongoDB schema designÂ Each entity in the ERD should have the following layout:\n```\n\n1. ```\n**Entity Name**: The name of the MongoDB collection / schema ```\n2. ```\n**Fields**: A list of fields in the collection, including: ```\n\n```\n- Field Name (in camel case)Â Â Â Â  - Data Type (e.g., String, Number, Date, ObjectId)Â Â Â Â  - Constraints (e.g. indexed, unique, not null, nullable)\n```\n\nIn this example, Liquibase was used as a changelog to supply the necessary context, detailing entities, columns, data types, and relationships. Based on this, Copilot could offer architectural recommendations for new document or collection types, including whether to embed documents or use separate collections with cache references for lookup data.\n\nCopilot can also generate an entity relationship diagram (ERD), allowing for review and validation before proceeding. From there, a new data access layer can be generated, configurable to switch between SQL Server and MongoDB as needed.\n\nWhile production environments typically standardize on a single database model, this demonstration showcased the speed and flexibility with which strategic architectural components can be introduced using GitHub Copilot.\n\n## ğŸ‘¨â€ğŸ’»Conclusion\n\nThis modernization initiative demonstrated how strategic use of automation and best practices can transform legacy Microsoft Access solutions into scalable, maintainable architectures utilizing Node.js, SQL Server, MongoDB, and OpenAPI. By carefully planning each migration layerâ€”from database and API specifications to business logicâ€”the team preserved core functionality while introducing modern standards and enhanced capabilities. GitHub Copilot played a pivotal role, not only speeding up redevelopment but also improving code quality through automated documentation, test generation, and meaningful naming conventions. The result was a significant reduction in development time, with a robust, standards-compliant system delivered in just two weeks compared to an estimated six to eight months using traditional manual methods.\n\nThis project serves as a blueprint for organizations seeking to modernize their Access-based applications, highlighting the efficiency gains and quality improvements that can be achieved by leveraging AI-powered tools and well-defined migration strategies. The approach ensures future scalability, easier maintenance, and alignment with contemporary enterprise requirements.\n\nPublished Dec 08, 2025\n\nVersion 1.0\n\n[application](/tag/application?nodeId=board%3AAzureArchitectureBlog)\n\n[apps & devops](/tag/apps%20%26%20devops?nodeId=board%3AAzureArchitectureBlog)\n\n[artificial intelligence](/tag/artificial%20intelligence?nodeId=board%3AAzureArchitectureBlog)\n\n[!\\[anthkernan&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xOTU0NTkyLTQ5Mjg4MGkwOTA0MjYwMkU4NjY0MTJE?image-dimensions=50x50)](/users/anthkernan/1954592) [anthkernan](/users/anthkernan/1954592) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined July 26, 2023\n\n[View Profile](/users/anthkernan/1954592)\n\n/category/azure/blog/azurearchitectureblog [Azure Architecture Blog](/category/azure/blog/azurearchitectureblog) Follow this blog board to get notified when there's new activity",
  "OutputDir": "_community",
  "Link": "https://techcommunity.microsoft.com/t5/azure-architecture-blog/how-to-modernise-a-microsoft-access-database-forms-vba-to-node/ba-p/4473504",
  "Description": "Microsoft Access has played a significant role in enterprise environments for over three decades. Released in November 1992, its flexibility and ease of use made it a popular choice for organizations of all sizesâ€”from FTSE250 companies to startups and the public sector. The platform enables rapid development of graphical user interfaces (GUIs) paired with relational databases, allowing users to quickly create professional-looking applications.\n\nDevelopers, data architects, and power users have all leveraged Microsoft Access to address various enterprise challenges. Its integration with Microsoft Visual Basic for Applications (VBA), an object-based programming language, ensured that Access solutions often became central to business operations. Unsurprisingly, modernizing these applications is a common requirement in contemporary IT engagements as thse solutions lead to data fragmentation, lack of integration into master data systems, multiple copies of the same data replicated across each access database and so on.\n\nAt first glance, upgrading a Microsoft Access application may seem simple, given its reliance on forms, VBA code, queries, and tables. However, substantial complexity often lurks beneath this straightforward exterior. Modernization efforts must consider whether to retain the familiar user interface to reduce staff retraining, how to accurately re-implement business logic, strategies for seamless data migration, and whether to introduce an API layer for data access. These factors can significantly increase the scope and effort required to deliver a modern equivalent, especially when dealing with numerous web forms, making manual rewrites a daunting task.\n\nThis is where GitHub Copilot can have a transformative impact, dramatically reducing redevelopment time. By following a defined migration path, it is possible to deliver a modernized solution in as little as two weeks.\n\nIn this blog post, Iâ€™ll walk you through each tier of the application and give you example prompts used at each stage.\n\n## ğŸ›ï¸Architecture Breakdown: The N-Tier Approach\n\nBreaking down the application architecture reveals a classic N-Tier structure, consisting of a presentation layer, business logic layer, data access layer, and data management layer.\n\n## ğŸ’«First-Layer Migration: Migrating a Microsoft Access Database to SQL Server\n\nThe migration process began with the database layer, which is typically the most straightforward to move from Access to another relational database management system (RDBMS). In this case, SQL Server was selected to leverage the [SQL Server Migration Assistant (SSMA) for Microsoft Access](https://learn.microsoft.com/en-us/sql/ssma/sql-server-migration-assistant?view=sql-server-ver17)â€”a free tool from Microsoft that streamlines database migration to SQL Server, Azure SQL Database, or Azure SQL Database Managed Instance (SQLMI). While GitHub Copilot could generate new database schemas and insert scripts, the availability of a specialized tool made the process more efficient.\n\nUsing SSMA, the database was migrated to SQL Server with minimal effort. However, it is important to note that relationships in Microsoft Access may lack explicit names. In such cases, SSMA appends a GUID or uses one entirely to create unique foreign key names, which can result in confusing relationship names post-migration. Fortunately, GitHub Copilot can batch-rename these relationships in the generated SQL scripts, applying more meaningful naming conventions. By dropping and recreating the constraints, relationships become easier to understand and maintain.\n\nSSMA handles the bulk of the migration workload, allowing you to quickly obtain a fully functional SQL Server database containing all original data. In practice, renaming and recreating constraints often takes longer than the data migration itself.\n\n``` Prompt Used: # Context I want to refactor the #file:script.sql SQL script. Your task is to follow the below steps to analyse it and refactor it according to the specified rules. You are allowed to create / run any python scripts or terminal commands to assist in the analysis and refactoring process. # Analysis Phase Identify: ```\n\n1. ```\nAny warning comments ```\n2. ```\nRelations between tables ```\n3. ```\nForeign key creation ```\n4. ```\nReferences to these foreign keys in 'MS_SSMA_SOURCE' metadata ```\n\n```\n# Refactor Phase Refactor any SQL matching the following rules: - Create a new script file with the same name as the original but with a `.refactored.sql` extension - Rename any primary key constraints to follow the format PK_{table_name}_{column_name} - Rename any foreign key constraints like [TableName]${GUID} to FK_{child_table}_{parent_table} - Rename any indexes like [TableName]${GUID} to IDX_{table_name}_{column_name} - Ensure any updated foreign keys are updated elsewhere in the script - Identify which warnings flagged by the migration assistant need addressed # Summary Phase Create a summary file in markdown format with the following sections: - Summary of changes made - List of warnings addressed - List of foreign keys renamed - Any other relevant notes\n```\n\n## ğŸ¤–Bonus: Introduce Database Automation and Change Management\n\nAs we now had a SQL database, we needed to consider how we would roll out changes to the database and we could introduce a formal tool to cater for this within the solution which was [Liquibase](https://www.liquibase.com/).\n\n``` Prompt Used: # Context I want to refactor #file:db.changelog.xml. Your task is to follow the below steps to analyse it and refactor it according to the specified rules. You are allowed to create / run any python scripts or terminal commands to assist in the analysis and refactoring process. # Analysis Phase ```\n\n1. ```\nAnalyse the generated changelog to identify the structure and content. ```\n2. ```\nIdentify the tables, columns, data types, constraints, and relationships present in the database. ```\n3. ```\nIdentify any default values, indexes, and foreign keys that need to be included in the changelog. ```\n4. ```\nIdentify any vendor specific data types / fucntions that need to be converted to common Liquibase types. ```\n\n```\n# Refactor Phase DO NOT modify the original #file:db.changelog.xml file in any way. Instead, create a new changelog file called `db.changelog-1-0.xml` to store the refactored changesets. The new file should follow the structure and conventions of Liquibase changelogs. You can fetch https://docs.liquibase.com/concepts/data-type-handling.html to get available Liquibase types and their mappings across RDBMS implementations.\n```\n\n1. ```\nCopy the original changesets from the `db.changelog.xml` file into the new file ```\n2. ```\nRefactor the changesets according to the following rules: ```\n\n```\n- The main changelog should only include child changelogs and not directly run migration operations - Child changelogs should follow the convention db.changelog-{version}.xml and start at 1-0 - Ensure data types are converted to common Liquibase data types. For example: - `nvarchar(max)` should be converted to `TEXT` - `datetime2` should be converted to `TIMESTAMP` - `bit` should be converted to `BOOLEAN` - Ensure any default values are retained but ensure that they are compatible with the liquibase data type for the column. - Use standard SQL functions like `CURRENT_TIMESTAMP` instead of vendor-specific functions. - Only use vendor specific data types or functions if they are necessary and cannot be converted to common Liquibase types. These must be documented in the changelog and summary.\n```\n\n1. ```\nEnsure that the original changeset IDs are preserved for traceability. ```\n2. ```\nEnsure that the author of all changesets is \"liquibase (generated)\" ```\n\n```\n# Validation Phase\n```\n\n1. ```\nValidate the new changelog file against the original #file:db.changelog.xml to ensure that all changesets are correctly refactored and that the structure is maintained. ```\n2. ```\nConfirm no additional changesets are added that were not present in the original changelog. ```\n\n```\n# Finalisation Phase\n```\n\n1. ```\nProvide a summary of the changes made in the new changelog file. ```\n2. ```\nDocument any vendor specific data types or functions that were used and why they could not be converted to common Liquibase types. ```\n3. ```\nEnsure the main changelog file (`db.changelog.xml`) is updated to include the new child changelog file (`db.changelog-1-0.xml`). ```\n\n## ğŸ¤–Bonus: Synthetic Data Generation\n\nSince the legacy system lacked synthetic data for development or testing, GitHub Copilot was used to generate fake seed data. Care was taken to ensure all generated data was clearly fictionalâ€”using placeholders like â€˜Fake Nameâ€™ and â€˜Fake Townâ€™â€”to avoid any confusion with real-world information. This step greatly improved the maintainability of the project, enabling developers to test features without handling sensitive or real data.\n\n## ğŸ’«Second-Layer Migration: OpenAPI Specifications\n\nWith data migration complete, the focus shifted to implementing an API-driven approach for data retrieval. Adopting modern standards, OpenAPI specifications were used to define new RESTful APIs for creating, reading, updating, and deleting data. Because these APIs mapped directly to underlying entities, GitHub Copilot efficiently generated the required endpoints and services in Node.js, utilizing a repository pattern. This approach not only provided robust APIs but also included comprehensive self-describing documentation, validation at the API boundary, automatic error handling, and safeguards against invalid data reaching business logic or database layers.\n\n## ğŸ’«Third-Layer Migration: Business Logic\n\nThe business logic, originally authored in VBA, was generally straightforward. GitHub Copilot translated this logic into its Node.js equivalent and created corresponding tests for each method. These tests were developed directly from the code, adding a layer of quality assurance that was absent in the original Access solution. The result was a set of domain services mirroring the functionality of their VBA predecessors, successfully completing the migration of the third layer.\n\nAt this stage, the project had a new database, a fresh API tier, and updated business logic, all conforming to the latest organizational standards. The final major component was the user interface, an area where advances in GitHub Copilotâ€™s capabilities became especially evident.\n\n## ğŸ’«Fourth Layer: User Interface\n\nThe modernization of the Access Forms user interface posed unique challenges. To minimize retraining requirements, the new system needed to retain as much of the original layout as possible, ensuring familiar placement of buttons, dropdowns, and other controls. At the same time, it was necessary to meet new accessibility standards and best practices.\n\nSome Access forms were complex, spanning multiple tabs and containing numerous controls. Manually describing each interface for redevelopment would have been time-consuming. Fortunately, newer versions of GitHub Copilot support image-based prompts, allowing screenshots of Access Forms to serve as context. Using these screenshots, Copilot generated Government Digital Service Views that closely mirrored the original application while incorporating required accessibility features, such as descriptive labels and field selectors.\n\nAlthough the automatically generated UI might not fully comply with all current accessibility standards, prompts referencing WCAG guidelines helped guide Copilotâ€™s improvements. The generated interfaces provided a strong starting point for UX engineers to further refine accessibility and user experience to meet organizational requirements.\n\n## ğŸ¤–Bonus: User Story Generation from the User Interface\n\nFor organizations seeking a specification-driven development approach, GitHub Copilot can convert screenshots and business logic into user stories following the â€œ**As a** â€¦ **I want to** â€¦ **So that** â€¦â€ format. While not flawless, this capability is invaluable for systems lacking formal requirements, giving business analysts a foundation to build upon in future iterations.\n\n## ğŸ¤–Bonus: Introducing MongoDB\n\nTowards the end of the modernization engagement, there was interest in demonstrating migration from SQL Server to MongoDB. GitHub Copilot can facilitate this migration, provided it is given adequate context. As with all NoSQL databases, the design should be based on application data access patternsâ€”typically reading and writing related data together. Copilotâ€™s ability to automate this process depends on a comprehensive understanding of the applicationâ€™s data relationships and patterns.\n\n```\n# Context The `` entity from the existing system needs to be added to the MongoDB schema. You have been provided with the following: - #file:documentation - System documentation to provide domain / business entity context - #file:db.changelog.xml - Liquibase changelog for SQL context - #file:mongo-erd.md - Contains the current Mongo schema Mermaid ERD. Create this if it does not exist. - #file:stories - Contains the user stories that will the system will be built around # Analysis Phase\n```\n\n1. ```\nAnalyse the available documentation and changelog to identify the structure, relationships, and business context of the ``. ```\n2. ```\nIdentify: ```\n\n```\n- All relevant data fields and attributes - Relationships with other entities - Any specific data types, constraints, or business rules\n```\n\n1. ```\nDetermine how this entity fits into the overall MongoDB schema: ```\n\n```\n- Should it be a separate collection? - Should it be embedded in another document? - Should it be a reference to another collection for lookups or relationships? - Explore the benefit of denormalization for performance and business needs\n```\n\n1. ```\nConsider the data access patterns and how this entity will be used in the application. ```\n\n```\n# MongoDB Schema Design Using the analysis, suggest how the `` should be represented in MongoDB: - The name of the MongoDB collection that will represent this entity - List each field in the collection, its type, any constraints, and what it maps to in the original business context - For fields that are embedded, document the parent collection and how the fields are nested. Nested fields should follow the format `parentField->childField`. - For fields that are referenced, document the reference collection and how the lookup will be performed. - Provide any additional notes on indexing, performance considerations, or specific MongoDB features that should be used - Always use pascal case for collection names and camel case for field names # ERD Creation Create or update the Mermaid ERD in `mongo-erd.md` to include the results of your analysis. The ERD should reflect: - The new collection or embedded document structure - Any relationships with other collections/entities - The data types, constraints, and business rules that are relevant for MongoDB - Ensure the ERD is clear and follows best practices for MongoDB schema design Each entity in the ERD should have the following layout:\n```\n\n1. ```\n**Entity Name**: The name of the MongoDB collection / schema ```\n2. ```\n**Fields**: A list of fields in the collection, including: ```\n\n```\n- Field Name (in camel case) - Data Type (e.g., String, Number, Date, ObjectId) - Constraints (e.g. indexed, unique, not null, nullable)\n```\n\nIn this example, Liquibase was used as a changelog to supply the necessary context, detailing entities, columns, data types, and relationships. Based on this, Copilot could offer architectural recommendations for new document or collection types, including whether to embed documents or use separate collections with cache references for lookup data.\n\nCopilot can also generate an entity relationship diagram (ERD), allowing for review and validation before proceeding. From there, a new data access layer can be generated, configurable to switch between SQL Server and MongoDB as needed.\n\nWhile production environments typically standardize on a single database model, this demonstration showcased the speed and flexibility with which strategic architectural components can be introduced using GitHub Copilot.\n\n## ğŸ‘¨â€ğŸ’»Conclusion\n\nThis modernization initiative demonstrated how strategic use of automation and best practices can transform legacy Microsoft Access solutions into scalable, maintainable architectures utilizing Node.js, SQL Server, MongoDB, and OpenAPI. By carefully planning each migration layerâ€”from database and API specifications to business logicâ€”the team preserved core functionality while introducing modern standards and enhanced capabilities. GitHub Copilot played a pivotal role, not only speeding up redevelopment but also improving code quality through automated documentation, test generation, and meaningful naming conventions. The result was a significant reduction in development time, with a robust, standards-compliant system delivered in just two weeks compared to an estimated six to eight months using traditional manual methods.\n\nThis project serves as a blueprint for organizations seeking to modernize their Access-based applications, highlighting the efficiency gains and quality improvements that can be achieved by leveraging AI-powered tools and well-defined migration strategies. The approach ensures future scalability, easier maintenance, and alignment with contemporary enterprise requirements.",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Tags": [],
  "Title": "How to Modernise a Microsoft Access Database (Forms + VBA) to Node.JS, OpenAPI and SQL Server",
  "FeedName": "Microsoft Tech Community",
  "Author": "anthkernan",
  "PubDate": "2025-12-08T19:59:25+00:00",
  "ProcessedDate": "2025-12-08 20:05:10"
}
