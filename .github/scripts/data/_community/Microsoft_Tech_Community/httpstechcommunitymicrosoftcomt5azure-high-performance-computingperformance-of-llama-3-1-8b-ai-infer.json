{
  "OutputDir": "_community",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "ProcessedDate": "2025-08-26 17:11:06",
  "Link": "https://techcommunity.microsoft.com/t5/azure-high-performance-computing/performance-of-llama-3-1-8b-ai-inference-using-vllm-on-nd-h100/ba-p/4448355",
  "EnhancedContent": "# Introduction\n\nThe pace of development in large language models (LLMs) has continued to accelerate as the global AI community races toward the goal of artificial general intelligence (AGI). Today’s most advanced models boast trillions of parameters, pushing the boundaries of what machines can understand and generate. However, this scale comes at a steep cost—both in terms of training and inference—due to the immense GPU resources required to host and operate these models.\n\nYet, innovation is not limited to those with access to the largest AI supercomputers. DeepSeek have demonstrated that it is possible to build highly competitive models without relying on the latest, most expensive infrastructure. At the same time, a renewed wave of open-source collaboration is challenging the closed-source strategies of leading AI companies, offering more accessible and customizable alternatives.\n\nFor enterprise customers, the focus is shifting toward practical, cost-effective solutions. Rather than deploying trillion-parameter giants, many organizations are turning to smaller models—such as those with around 8 billion parameters—that strike a balance between accuracy and efficiency. These models are not only easier to fine-tune and deploy but also significantly reduce the cost per token, making them ideal for real-world business applications.\n\nIn this paper, we explore the capabilities of the Llama 3.1 8B model as a representative example of a modern, enterprise-ready LLM. We benchmark its inference performance on Azure’s ND-H100 v5 infrastructure using the vLLM engine and present our findings along with recommendations for enterprise deployment.\n\n# AI Inference architecture\n\nInference in transformer-based large language models (LLMs) is typically divided into two primary stages: **prefill** and **decode**. Understanding the characteristics and resource demands of each stage is essential for optimizing performance, especially when deploying models like Llama 3.1 8B in enterprise environments.\n\n**Prefill Stage: Compute-Bound Initialization**\n\nThe prefill stage is responsible for processing the input prompt. It involves tokenizing the input and performing a forward pass through the model to populate the key-value (KV) cache. This stage is **compute-intensive**, as it requires full attention computation across all input tokens. The performance bottleneck here is typically the GPU's compute throughput, especially for long prompts or large batch sizes.\n\n**Decode Stage: Memory-Bound Token Generation**\n\nOnce the KV cache is populated, the decode stage begins. This stage generates one token at a time, using the cached context to predict the next token. The decode step is **memory-bound**, as it relies heavily on fast access to the KV cache. When the model achieves a KV cache hit, it can skip re-computation of earlier tokens, significantly reducing latency and compute load. This makes cache efficiency a critical factor in real-time inference performance.\n\nFig 1. High level architecture of AI Inference, showing efficient use of KV cache can increase token throughput and reduce AI inference latency.\n\n**Overall Inference Characteristics**\n\nIn general, **AI inference is memory-bound**, particularly during the decode phase. The ability to serve multiple concurrent requests efficiently depends on how well the system can manage memory bandwidth and cache locality. As such, optimizing memory access patterns and minimizing cache misses are key to achieving high throughput and low latency.\n\n**Techniques for Optimization**\n\nTo maximize GPU utilization and token throughput while minimizing latency, several architectural strategies are employed:\n\n- **KV Cache Management**: Efficient reuse and eviction policies to maintain high cache hit rates. vLLM uses PagedAttention (which is inspired by virtual memory and paging techniques used in operating systems), to manage the KV cache using  blocks/pages. This allows vLLM to efficiently/dynamically utilize HBM memory and minimizes memory fragmentation.\n- **Batching and Scheduling**: Grouping similar requests to improve parallelism and reduce idle GPU time. VLLM has a few parameters to control batching/parallelism.\n- **MAX\\_NUM\\_SEQS:** How many input requests to process in parallel.\n- **MAX\\_NUM\\_BATCHED\\_TOKENS:** The number of tokens to process in parallel (Forward pass in Deep neural network)\n\n**Note:** Larger values may not always be optimal; you could improve token throughput at the expense of latency.\n\n- **Weight and Activation Quantization (fp8)**: Reducing the precision of AI model weights and activations can give more memory to load AI models or have a larger KV cache. Lowering the precision also allows computations to be performed on more efficient GPU (higher FLOPS) computational units.\n\n**Parallelization techniques**: *Tensor parallelism*, *Pipeline parallelism, Expert parallelism or Data parallelism* can be used to split larger models across multiple Nodes/GPUs. Tensor parallelism distributes the model across the GPUs, with each GPU handling multiple layers of the model. Pipeline parallelism involves dividing the model, where a group of nodes (or GPUs) is responsible for processing its assigned DDN layer. Expert parallelism supports Mixture of Experts (MoE) models where different expert networks can be distributed across GPU’s. Data parallelism\n\n- replicates the entire model across multiple GPU sets and processes different batches of requests in parallel\n- **Speculative Decoding**: Predicting multiple tokens ahead to reduce the number of forward passes.\n- **Prefill/Decode Decoupling**: Recent advancements, such as those implemented in **vLLM (**and **NVIDIA Dynamo)**, decouple the prefill and decode stages, allowing each to be assigned dedicated GPU or CPU resources. This separation enables better resource allocation and parallelism, especially in multi-tenant or high-throughput environments.\n\nBy leveraging these techniques, vLLM provides a highly efficient inference engine that is well-suited for serving modern LLMs like Llama 3.1 8B. This makes it a compelling choice for enterprise applications where cost, latency, and scalability are critical considerations.\n\n# Benchmark environment\n\n## Inference benchmark\n\nThe Huggingface Inference benchmarking code was used for the AI Inference benchmark. Three different popular AI inference profiles were examined.\n\n- **Chat**: Probably the most common use case, question and answer format on a wide range of topics.\n- **Classification**: Providing various documents and requesting a summary of its contents.\n- **Code generation**: Providing code and requesting code generation, e.g. create a new function.\n\n| **Profile** | **Data set** | **Input prompt** | **Output prompt** | | --- | --- | --- | --- | | **Chat** | hlarcher/inference-benchmarker/share\\_gpt\\_turns.json | N/A | min=50, max=800, variance=100 | | **Classification** | hlarcher/inference-benchmarker/classification.json | Min=8000, max=12000, variance=5000 | Min=30, max=80, variance=10 | | **Code generation** | hlarcher/inference-benchmarker/github\\_code.json | Min=3000, max=6000, variance=1000 | Min=30, max=80, variance=10 |\n\n| **Huggingface Lama 3.1 8B models used** | **Precision** | **Model Size (GiB)** | | --- | --- | --- | | **meta-llama/Llama-3.1-8B-Instruct** | FP16 | 14.9 | | **neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8** | FP8 | 8.4 | | **nvidia/Llama-3.1-8B-Instruct-FP8** | FP8 | 8.4 |\n\n| **vLLM parameters** | **Default value** | | --- | --- | | **gpu\\_memory\\_utilization** | 0.9 | | **max\\_num\\_seqs** | 1024 | | **max\\_num\\_batched\\_tokens** | 8192 | | **enable\\_chunked\\_prefill** | True | | **enable\\_prefix\\_caching** | True |\n\n| **Dynamo parameters** | **Default values** | | --- | --- | | **block-size** | 64 | | **max-model-len** | 16384 | | **kv\\_connector** | DynamoNixlConnector | | **router** | round-robin | | **remote-prefill** | True | | **conditional-disagg** | True | | **max-local-prefill-length** | 10 | | **max-prefill-queue-size** | 2 | | **max-num-batched-tokens** | 16384 | | **Environment** | Local | | **No-operation (Planner)** | True |\n\n# Results\n\nFig 2: AI Inference performance comparison between the chat, classification and code generation profiles on ND-H100-v5 (8 H100).\n\n| **Profile** | **Avg prompt throughput** | **Avg generation throughput** | **Max # Requests waiting** | **Max KV Cache usage %** | **Avg KV Cache hit rate %** | | --- | --- | --- | --- | --- | --- | | **Chat** | ~7500 | ~17600 | 0 | ~2% | ~78% | | **Classification** | ~55300 | ~2900 | 0 | ~5% | ~100% | | **Code generation** | ~130400 | ~1450 | ~<br><br><br><br>37 | ~10% | ~2% |\n\nFig 3: Show the impact of modifying MAX\\_NUM\\_BATCHED\\_TOKENS on the code-generation inference benchmark. (This parameter has a greater impact on the code generation benchmark compared to the chat/classification because of the low KV cache hit percentage.)\n\nFig 4: Code generation inference benchmark run on 1 H100 showing the performance impact of using fp8 quantization.\n\nFig 5: Code generation benchmark run on 1, 2, 4 & 8 H100. The results indicate that higher token throughput could be achieved running inference using 1 copy of model on each GPU instead of distributing model (via tensor parallel) amongst the 8 GPUs.\n\nFig 6: Impact on throughput (tokens/sec) by adjusting AI Inference configuration (vLLM) on ND-H100-v5.\n\n## Results comparing Dynamo vs traditional vLLM\n\nFig 7: Dynamo vs traditional vLLM throughput (tokens/sec) comparison on 1 ND\\_H100\\_v5 (8 GPU’s). The best traditional vLLM configuration (8 x vLLM tensor\\_parallel=1) throughput performance is compared with various Dynamo configurations (i.e different combinations of GPU’s assigned for decode and prefill).\n\n**Note:** vLLM does have an experimental disaggregated prefill capability with various connector types. I attempted to run vLLM experimental disaggregated prefill using the kv-connector = LMCacheConnectorV1 (with Nixl enabled). I got mixed results, eventually running into the following issues (and deciding to switch to Nvidia Dynamo instead).\n\n- Limited control over allocating GPU’s to decode vs prefill (used tensor parallel option, but limited by specific ratio with number of attention multi-heads).\n- Memory management problems, got OOM errors even though there was plenty of HBM memory available on GPU’s (HBM was not distributed evenly amongst GPU’s).\n\n# Analysis\n\nThe performance of the inference prefill is determined by length and number of input prompts, this phase is compute bound (effectively a Matrix-Matrix operation) and we see that the code-generation profile does best primarily because it had the largest number of input tokens. The decode phase is a memory bound operation (effectively a Vector-Matrix operation) and performance in this phase is heavily dependent on the KV cache hit percentage. The code-generation profile only had ~1.7% KV cache hit percentage (there was plenty of HBM capacity, only ~10% of available KV Cache capacity was used), which resulted in slow decode performance impacting its overall throughput and latency especially at higher QPS (the code-generation benchmark was the only one which had requests being backed-up and waiting). The classification profile did well in the decode phase primarily due to the high KV cache hit percentage (~100%), it did struggle in overall throughput due to the small length of the output tokens.\n\nAdjusting the size of MAX\\_NUM\\_BATCHED\\_TOKENS had very little impact on the chat and classification benchmarks probably because they had high KV Cache hit percentages, but it did impact the performance of the code-generation benchmark (a ~3% improvement in tokens/sec using a larger value).\n\nQuantization of AI model can free up HBM memory to allow a large model to load or improve the AI inference performance by providing more memory for KV caching, it can also improve performance by performing computations with a higher FLOPS lower precision (e.g. FP8) format. In this case there is plenty of HBM and all three profiles did not use all the available KV cache space available, so FP8 quantization does not improve KV caching efficiency. Improvements in compute performance are observed with quantization, especially for the code generation profile which had a low KV cache hit percentage. The code generation tokens/sec on 1 GPU improved by ~38%.\n\nSince the Llama 3.1 8B model can easily fit on 1 H100, you can get significantly better total throughput (tokens/sec) on ND-H100-v5 if a complete model is loaded into each separate GPU instead of splitting the model across multiple GPU’s (via tensor parallel). The chat, classification and code generation inference throughput improved 4.2, 5.2, 1.9 times respectively.\n\nNewer inferencing server architectures feature disaggregated prefill, which allows you to decouple prefill from decode and assign resources (GPU’s, CPU’s) to each type of worker (prefill or decode). This is especially suited for large reasoning models with large context windows, running on large GPU inference clusters, significant performance gains have been reported. In this case we have a modest size (8B parameters) NLP model running on a single ND\\_H100\\_v5 node (only 8 GPU’s), so we were not expecting any significant performance improvements. The traditional aggregated vLLM was much faster than the best Dynamo configuration, running this inference benchmark on llama 3 8B model on ND\\_H100\\_v5. In this case the model can fit in a single GPU and the overhead of disaggregation might outweigh any parallelism gains when one GPU can already handle both phases efficiently.\n\n# Conclusions\n\nWhen analyzing the performance of AI inference, it’s important not only to focus on the number of input and output tokens but also on the type of AI inference application, which can have a big impact on KV Cache effectiveness.\n\nSmaller AI models provide opportunities and more options to configure your environment in an optimal way to maximize token/sec throughput.\n\n# References\n\n1. [Welcome to vLLM — vLLM](https://docs.vllm.ai/en/v0.8.1/index.html)\n2. [Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)\n3. [huggingface/inference-benchmarker: Inference server benchmarking tool](https://github.com/huggingface/inference-benchmarker/)\n4. [meta-llama/llama-models: Utilities intended for use with Llama models.](https://github.com/meta-llama/llama-models)\n5. [\\[2309.06180\\] Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)\n6. [ND-H100-v5 size series - Azure Virtual Machines | Microsoft Learn](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/ndh100v5-series?tabs=sizebasic)\n7. [Hugging Face – The AI community building the future.](https://huggingface.co/)\n8. [Introducing NVIDIA Dynamo, A Low-Latency Distributed Inference Framework for Scaling Reasoning AI Models | NVIDIA Technical Blog](https://developer.nvidia.com/blog/introducing-nvidia-dynamo-a-low-latency-distributed-inference-framework-for-scaling-reasoning-ai-models/)\n\n# Appendix\n\n## Installation of hugging face inference benchmarker\n\nInstall Rust\n\n``` curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh ```\n\nBuild and install inference benchmarker\n\n``` cargo install --git https://github.com/huggingface/inference-benchmarker/ ```\n\n## Installation of vLLM\n\nInstall uv\n\n``` curl -LsSf https://astral.sh/uv/install.sh | sh ```\n\nSet-up python workspace\n\n``` uv venv --python 3.10 --seedsource .venv/bin/activate ```\n\nInstall vLLM\n\n``` uv pip install vllm --torch-backend=auto ```\n\nInstall FlashInfer\n\ngit clone [https://github.com/flashinfer-ai/flashinfer.git](https://github.com/flashinfer-ai/flashinfer.git) --recursive\n\npip install ninja\n\ncd flashinfer\n\npip install --no-build-isolation --verbose .\n\n## Start vLLM server\n\n#!/bin/bash\n\nNUM\\_GPUS=8\n\n#NUM\\_GPUS=4\n\n#NUM\\_GPUS=2\n\n#NUM\\_GPUS=1\n\nMODEL=meta-llama/Llama-3.1-8B-Instruct\n\n#MODEL=nvidia/Llama-3.1-8B-Instruct-FP8\n\n#MODEL=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n\nPORT=5000\n\nexport VLLM\\_LOGGING\\_LEVEL=INFO\n\npython -m vllm.entrypoints.openai.api\\_server --host 0.0.0.0 --port $PORT --model $MODEL --tensor-parallel-size $NUM\\_GPUS --dtype auto\n\n### Run Inference benchmark\n\nTOKENIZER\\_NAME=\"meta-llama/Llama-3.1-8B-Instruct\"\n\n#TOKENIZER\\_NAME=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\"\n\n#TOKENIZER\\_NAME=\"nvidia/Llama-3.1-8B-Instruct-FP8\"\n\ninference-benchmarker \\\n\n--tokenizer-name $TOKENIZER\\_NAME \\\n\n--url http://localhost:5000 \\\n\n--profile code-generation -n\n\n## Nvidia Dynamo installation\n\nInstall python virtual environments\n\n``` sudo apt install python3.10-venv ```\n\nCreate dynamo virtual workspace\n\n``` python3 -m venv mydynamo ```\n\nActivate virtual environment\n\n``` source /home/azureuser/mydynamo/bin/activate ```\n\nCheck out dynamo github code.\n\n``` git  clone https://github.com/ai-dynamo/dynamo.git ```\n\ncd dynamo\n\n``` git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) ```\n\nInstall dynamo prerequisites\n\n``` pip install \"ai-dynamo[all]\" ```\n\nInstall some additional python modules and packages\n\n``` pip install tensorboardXpip install pynvmlsudo apt install etcdsystemctl status nats ```\n\nRestart etcd\n\n``` systemctl restart etcd ```\n\nSet-up nats-server\n\n``` wget https://github.com/nats-io/nats-server/releases/download/v2.10.22/nats-server-v2.10.22-linux-amd64.zipunzip  nats-server-v2.10.22-linux-amd64.zipsudo mv nats-server-v2.10.22-linux-amd64/nats-server /usr/local/bin/sudo chmod +x /usr/local/bin/nats-server ```\n\nCreate/edit  /etc/systemd/system/nats.service\n\n``` [Unit]Description=NATS ServerAfter=network.target[Service]Type=simpleExecStart=/usr/local/bin/nats-server -jsRestart=alwaysRestartSec=10sLimitNOFILE=40000[Install]WantedBy=multi-user.target ```\n\n``` export NATS_SERVER=\"nats://localhost:4222\" ```\n\nStart Dynamo server/service\n\ncd $DYNAMO\\_HOME/examples/llm\n\nedit disagg.yaml file to modify parameters.\n\n``` dynamo serve graphs.disagg:Frontend -f ./configs/disagg.yaml ```\n\nSet Dynamo environmental variables\n\nexport DYNAMO\\_HOME=$(pwd)\n\ncd $DYNAMO\\_HOME/examples/llm\n\nUpdated Aug 26, 2025\n\nVersion 1.0\n\n[ai infrastructure](/tag/ai%20infrastructure?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[benchmarking](/tag/benchmarking?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[virtual machines](/tag/virtual%20machines?nodeId=board%3AAzureHighPerformanceComputingBlog)\n\n[!\\[CormacGarvey&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-8.svg?image-dimensions=50x50)](/users/cormacgarvey/364170) [CormacGarvey](/users/cormacgarvey/364170) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined June 20, 2019\n\n[View Profile](/users/cormacgarvey/364170)\n\n/category/azure/blog/azurehighperformancecomputingblog [Azure High Performance Computing (HPC) Blog](/category/azure/blog/azurehighperformancecomputingblog) Follow this blog board to get notified when there's new activity",
  "FeedName": "Microsoft Tech Community",
  "PubDate": "2025-08-26T17:02:48+00:00",
  "Title": "Performance of Llama 3.1 8B AI Inference using vLLM on ND-H100-v5",
  "Tags": [],
  "Author": "CormacGarvey",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "Description": "# Introduction\n\nThe pace of development in large language models (LLMs) has continued to accelerate as the global AI community races toward the goal of artificial general intelligence (AGI). Today’s most advanced models boast trillions of parameters, pushing the boundaries of what machines can understand and generate. However, this scale comes at a steep cost—both in terms of training and inference—due to the immense GPU resources required to host and operate these models.\n\nYet, innovation is not limited to those with access to the largest AI supercomputers. DeepSeek have demonstrated that it is possible to build highly competitive models without relying on the latest, most expensive infrastructure. At the same time, a renewed wave of open-source collaboration is challenging the closed-source strategies of leading AI companies, offering more accessible and customizable alternatives.\n\nFor enterprise customers, the focus is shifting toward practical, cost-effective solutions. Rather than deploying trillion-parameter giants, many organizations are turning to smaller models—such as those with around 8 billion parameters—that strike a balance between accuracy and efficiency. These models are not only easier to fine-tune and deploy but also significantly reduce the cost per token, making them ideal for real-world business applications.\n\nIn this paper, we explore the capabilities of the Llama 3.1 8B model as a representative example of a modern, enterprise-ready LLM. We benchmark its inference performance on Azure’s ND-H100 v5 infrastructure using the vLLM engine and present our findings along with recommendations for enterprise deployment.\n\n# AI Inference architecture\n\nInference in transformer-based large language models (LLMs) is typically divided into two primary stages: **prefill** and **decode**. Understanding the characteristics and resource demands of each stage is essential for optimizing performance, especially when deploying models like Llama 3.1 8B in enterprise environments.\n\n**Prefill Stage: Compute-Bound Initialization**\n\nThe prefill stage is responsible for processing the input prompt. It involves tokenizing the input and performing a forward pass through the model to populate the key-value (KV) cache. This stage is **compute-intensive**, as it requires full attention computation across all input tokens. The performance bottleneck here is typically the GPU's compute throughput, especially for long prompts or large batch sizes.\n\n**Decode Stage: Memory-Bound Token Generation**\n\nOnce the KV cache is populated, the decode stage begins. This stage generates one token at a time, using the cached context to predict the next token. The decode step is **memory-bound**, as it relies heavily on fast access to the KV cache. When the model achieves a KV cache hit, it can skip re-computation of earlier tokens, significantly reducing latency and compute load. This makes cache efficiency a critical factor in real-time inference performance.\n\n![]()\n\nFig 1. High level architecture of AI Inference, showing efficient use of KV cache can increase token throughput and reduce AI inference latency.\n\n**Overall Inference Characteristics**\n\nIn general, **AI inference is memory-bound**, particularly during the decode phase. The ability to serve multiple concurrent requests efficiently depends on how well the system can manage memory bandwidth and cache locality. As such, optimizing memory access patterns and minimizing cache misses are key to achieving high throughput and low latency.\n\n**Techniques for Optimization**\n\nTo maximize GPU utilization and token throughput while minimizing latency, several architectural strategies are employed:\n\n- **KV Cache Management**: Efficient reuse and eviction policies to maintain high cache hit rates. vLLM uses PagedAttention (which is inspired by virtual memory and paging techniques used in operating systems), to manage the KV cache using blocks/pages. This allows vLLM to efficiently/dynamically utilize HBM memory and minimizes memory fragmentation.\n- **Batching and Scheduling**: Grouping similar requests to improve parallelism and reduce idle GPU time. VLLM has a few parameters to control batching/parallelism.\n- **MAX\\_NUM\\_SEQS:** How many input requests to process in parallel.\n- **MAX\\_NUM\\_BATCHED\\_TOKENS:** The number of tokens to process in parallel (Forward pass in Deep neural network)\n\n**Note:** Larger values may not always be optimal; you could improve token throughput at the expense of latency.\n\n- **Weight and Activation Quantization (fp8)**: Reducing the precision of AI model weights and activations can give more memory to load AI models or have a larger KV cache. Lowering the precision also allows computations to be performed on more efficient GPU (higher FLOPS) computational units.\n\n**Parallelization techniques**: *Tensor parallelism*, *Pipeline parallelism, Expert parallelism or Data parallelism* can be used to split larger models across multiple Nodes/GPUs. Tensor parallelism distributes the model across the GPUs, with each GPU handling multiple layers of the model. Pipeline parallelism involves dividing the model, where a group of nodes (or GPUs) is responsible for processing its assigned DDN layer. Expert parallelism supports Mixture of Experts (MoE) models where different expert networks can be distributed across GPU’s. Data parallelism\n\n- replicates the entire model across multiple GPU sets and processes different batches of requests in parallel\n- **Speculative Decoding**: Predicting multiple tokens ahead to reduce the number of forward passes.\n- **Prefill/Decode Decoupling**: Recent advancements, such as those implemented in **vLLM (**and **NVIDIA Dynamo)**, decouple the prefill and decode stages, allowing each to be assigned dedicated GPU or CPU resources. This separation enables better resource allocation and parallelism, especially in multi-tenant or high-throughput environments.\n\nBy leveraging these techniques, vLLM provides a highly efficient inference engine that is well-suited for serving modern LLMs like Llama 3.1 8B. This makes it a compelling choice for enterprise applications where cost, latency, and scalability are critical considerations.\n\n# Benchmark environment\n\n## Inference benchmark\n\nThe Huggingface Inference benchmarking code was used for the AI Inference benchmark. Three different popular AI inference profiles were examined.\n\n- **Chat**: Probably the most common use case, question and answer format on a wide range of topics.\n- **Classification**: Providing various documents and requesting a summary of its contents.\n- **Code generation**: Providing code and requesting code generation, e.g. create a new function.\n\n| **Profile** | **Data set** | **Input prompt** | **Output prompt** | | --- | --- | --- | --- | | **Chat** | hlarcher/inference-benchmarker/share\\_gpt\\_turns.json | N/A | min=50, max=800, variance=100 | | **Classification** | hlarcher/inference-benchmarker/classification.json | Min=8000, max=12000, variance=5000 | Min=30, max=80, variance=10 | | **Code generation** | hlarcher/inference-benchmarker/github\\_code.json | Min=3000, max=6000, variance=1000 | Min=30, max=80, variance=10 |\n\n| **Huggingface Lama 3.1 8B models used** | **Precision** | **Model Size (GiB)** | | --- | --- | --- | | **meta-llama/Llama-3.1-8B-Instruct** | FP16 | 14.9 | | **neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8** | FP8 | 8.4 | | **nvidia/Llama-3.1-8B-Instruct-FP8** | FP8 | 8.4 |\n\n| **vLLM parameters** | **Default value** | | --- | --- | | **gpu\\_memory\\_utilization** | 0.9 | | **max\\_num\\_seqs** | 1024 | | **max\\_num\\_batched\\_tokens** | 8192 | | **enable\\_chunked\\_prefill** | True | | **enable\\_prefix\\_caching** | True |\n\n| **Dynamo parameters** | **Default values** | | --- | --- | | **block-size** | 64 | | **max-model-len** | 16384 | | **kv\\_connector** | DynamoNixlConnector | | **router** | round-robin | | **remote-prefill** | True | | **conditional-disagg** | True | | **max-local-prefill-length** | 10 | | **max-prefill-queue-size** | 2 | | **max-num-batched-tokens** | 16384 | | **Environment** | Local | | **No-operation (Planner)** | True |\n\n# Results\n\n![]()\n\nFig 2: AI Inference performance comparison between the chat, classification and code generation profiles on ND-H100-v5 (8 H100).\n\n| **Profile** | **Avg prompt throughput** | **Avg generation throughput** | **Max # Requests waiting** | **Max KV Cache usage %** | **Avg KV Cache hit rate %** | | --- | --- | --- | --- | --- | --- | | **Chat** | ~7500 | ~17600 | 0 | ~2% | ~78% | | **Classification** | ~55300 | ~2900 | 0 | ~5% | ~100% | | **Code generation** | ~130400 | ~1450 | ~<br><br><br><br>37 | ~10% | ~2% |\n\n![]()\n\nFig 3: Show the impact of modifying MAX\\_NUM\\_BATCHED\\_TOKENS on the code-generation inference benchmark. (This parameter has a greater impact on the code generation benchmark compared to the chat/classification because of the low KV cache hit percentage.)\n\n![]()\n\nFig 4: Code generation inference benchmark run on 1 H100 showing the performance impact of using fp8 quantization.\n\n![]()\n\nFig 5: Code generation benchmark run on 1, 2, 4 & 8 H100. The results indicate that higher token throughput could be achieved running inference using 1 copy of model on each GPU instead of distributing model (via tensor parallel) amongst the 8 GPUs.\n\n![]()\n\nFig 6: Impact on throughput (tokens/sec) by adjusting AI Inference configuration (vLLM) on ND-H100-v5.\n\n## Results comparing Dynamo vs traditional vLLM\n\n![]()\n\nFig 7: Dynamo vs traditional vLLM throughput (tokens/sec) comparison on 1 ND\\_H100\\_v5 (8 GPU’s). The best traditional vLLM configuration (8 x vLLM tensor\\_parallel=1) throughput performance is compared with various Dynamo configurations (i.e different combinations of GPU’s assigned for decode and prefill).\n\n**Note:** vLLM does have an experimental disaggregated prefill capability with various connector types. I attempted to run vLLM experimental disaggregated prefill using the kv-connector = LMCacheConnectorV1 (with Nixl enabled). I got mixed results, eventually running into the following issues (and deciding to switch to Nvidia Dynamo instead).\n\n- Limited control over allocating GPU’s to decode vs prefill (used tensor parallel option, but limited by specific ratio with number of attention multi-heads).\n- Memory management problems, got OOM errors even though there was plenty of HBM memory available on GPU’s (HBM was not distributed evenly amongst GPU’s).\n\n# Analysis\n\nThe performance of the inference prefill is determined by length and number of input prompts, this phase is compute bound (effectively a Matrix-Matrix operation) and we see that the code-generation profile does best primarily because it had the largest number of input tokens. The decode phase is a memory bound operation (effectively a Vector-Matrix operation) and performance in this phase is heavily dependent on the KV cache hit percentage. The code-generation profile only had ~1.7% KV cache hit percentage (there was plenty of HBM capacity, only ~10% of available KV Cache capacity was used), which resulted in slow decode performance impacting its overall throughput and latency especially at higher QPS (the code-generation benchmark was the only one which had requests being backed-up and waiting). The classification profile did well in the decode phase primarily due to the high KV cache hit percentage (~100%), it did struggle in overall throughput due to the small length of the output tokens.\n\nAdjusting the size of MAX\\_NUM\\_BATCHED\\_TOKENS had very little impact on the chat and classification benchmarks probably because they had high KV Cache hit percentages, but it did impact the performance of the code-generation benchmark (a ~3% improvement in tokens/sec using a larger value).\n\nQuantization of AI model can free up HBM memory to allow a large model to load or improve the AI inference performance by providing more memory for KV caching, it can also improve performance by performing computations with a higher FLOPS lower precision (e.g. FP8) format. In this case there is plenty of HBM and all three profiles did not use all the available KV cache space available, so FP8 quantization does not improve KV caching efficiency. Improvements in compute performance are observed with quantization, especially for the code generation profile which had a low KV cache hit percentage. The code generation tokens/sec on 1 GPU improved by ~38%.\n\nSince the Llama 3.1 8B model can easily fit on 1 H100, you can get significantly better total throughput (tokens/sec) on ND-H100-v5 if a complete model is loaded into each separate GPU instead of splitting the model across multiple GPU’s (via tensor parallel). The chat, classification and code generation inference throughput improved 4.2, 5.2, 1.9 times respectively.\n\nNewer inferencing server architectures feature disaggregated prefill, which allows you to decouple prefill from decode and assign resources (GPU’s, CPU’s) to each type of worker (prefill or decode). This is especially suited for large reasoning models with large context windows, running on large GPU inference clusters, significant performance gains have been reported. In this case we have a modest size (8B parameters) NLP model running on a single ND\\_H100\\_v5 node (only 8 GPU’s), so we were not expecting any significant performance improvements. The traditional aggregated vLLM was much faster than the best Dynamo configuration, running this inference benchmark on llama 3 8B model on ND\\_H100\\_v5. In this case the model can fit in a single GPU and the overhead of disaggregation might outweigh any parallelism gains when one GPU can already handle both phases efficiently.\n\n# Conclusions\n\nWhen analyzing the performance of AI inference, it’s important not only to focus on the number of input and output tokens but also on the type of AI inference application, which can have a big impact on KV Cache effectiveness.\n\nSmaller AI models provide opportunities and more options to configure your environment in an optimal way to maximize token/sec throughput.\n\n# References\n\n1. [Welcome to vLLM — vLLM](https://docs.vllm.ai/en/v0.8.1/index.html)\n2. [Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/)\n3. [huggingface/inference-benchmarker: Inference server benchmarking tool](https://github.com/huggingface/inference-benchmarker/)\n4. [meta-llama/llama-models: Utilities intended for use with Llama models.](https://github.com/meta-llama/llama-models)\n5. [\\[2309.06180\\] Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)\n6. [ND-H100-v5 size series - Azure Virtual Machines | Microsoft Learn](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/ndh100v5-series?tabs=sizebasic)\n7. [Hugging Face – The AI community building the future.](https://huggingface.co/)\n8. [Introducing NVIDIA Dynamo, A Low-Latency Distributed Inference Framework for Scaling Reasoning AI Models | NVIDIA Technical Blog](https://developer.nvidia.com/blog/introducing-nvidia-dynamo-a-low-latency-distributed-inference-framework-for-scaling-reasoning-ai-models/)\n\n# Appendix\n\n## Installation of hugging face inference benchmarker\n\nInstall Rust\n\n``` curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh ```\n\nBuild and install inference benchmarker\n\n``` cargo install --git https://github.com/huggingface/inference-benchmarker/ ```\n\n## Installation of vLLM\n\nInstall uv\n\n``` curl -LsSf https://astral.sh/uv/install.sh | sh ```\n\nSet-up python workspace\n\n``` uv venv --python 3.10 --seedsource .venv/bin/activate ```\n\nInstall vLLM\n\n``` uv pip install vllm --torch-backend=auto ```\n\nInstall FlashInfer\n\ngit clone [https://github.com/flashinfer-ai/flashinfer.git](https://github.com/flashinfer-ai/flashinfer.git) --recursive\n\npip install ninja\n\ncd flashinfer\n\npip install --no-build-isolation --verbose .\n\n## Start vLLM server\n\n#!/bin/bash\n\nNUM\\_GPUS=8\n\n#NUM\\_GPUS=4\n\n#NUM\\_GPUS=2\n\n#NUM\\_GPUS=1\n\nMODEL=meta-llama/Llama-3.1-8B-Instruct\n\n#MODEL=nvidia/Llama-3.1-8B-Instruct-FP8\n\n#MODEL=neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\n\nPORT=5000\n\nexport VLLM\\_LOGGING\\_LEVEL=INFO\n\npython -m vllm.entrypoints.openai.api\\_server --host 0.0.0.0 --port $PORT --model $MODEL --tensor-parallel-size $NUM\\_GPUS --dtype auto\n\n### Run Inference benchmark\n\nTOKENIZER\\_NAME=\"meta-llama/Llama-3.1-8B-Instruct\"\n\n#TOKENIZER\\_NAME=\"neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8\"\n\n#TOKENIZER\\_NAME=\"nvidia/Llama-3.1-8B-Instruct-FP8\"\n\ninference-benchmarker \\\n\n--tokenizer-name $TOKENIZER\\_NAME \\\n\n--url http://localhost:5000 \\\n\n--profile code-generation -n\n\n## Nvidia Dynamo installation\n\nInstall python virtual environments\n\n``` sudo apt install python3.10-venv ```\n\nCreate dynamo virtual workspace\n\n``` python3 -m venv mydynamo ```\n\nActivate virtual environment\n\n``` source /home/azureuser/mydynamo/bin/activate ```\n\nCheck out dynamo github code.\n\n``` git clone https://github.com/ai-dynamo/dynamo.git ```\n\ncd dynamo\n\n``` git checkout $(git describe --tags $(git rev-list --tags --max-count=1)) ```\n\nInstall dynamo prerequisites\n\n``` pip install \"ai-dynamo[all]\" ```\n\nInstall some additional python modules and packages\n\n``` pip install tensorboardXpip install pynvmlsudo apt install etcdsystemctl status nats ```\n\nRestart etcd\n\n``` systemctl restart etcd ```\n\nSet-up nats-server\n\n``` wget https://github.com/nats-io/nats-server/releases/download/v2.10.22/nats-server-v2.10.22-linux-amd64.zipunzip nats-server-v2.10.22-linux-amd64.zipsudo mv nats-server-v2.10.22-linux-amd64/nats-server /usr/local/bin/sudo chmod +x /usr/local/bin/nats-server ```\n\nCreate/edit /etc/systemd/system/nats.service\n\n``` [Unit]Description=NATS ServerAfter=network.target[Service]Type=simpleExecStart=/usr/local/bin/nats-server -jsRestart=alwaysRestartSec=10sLimitNOFILE=40000[Install]WantedBy=multi-user.target ```\n\n``` export NATS_SERVER=\"nats://localhost:4222\" ```\n\nStart Dynamo server/service\n\ncd $DYNAMO\\_HOME/examples/llm\n\nedit disagg.yaml file to modify parameters.\n\n``` dynamo serve graphs.disagg:Frontend -f ./configs/disagg.yaml ```\n\nSet Dynamo environmental variables\n\nexport DYNAMO\\_HOME=$(pwd)\n\ncd $DYNAMO\\_HOME/examples/llm"
}
