{
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Tags": [],
  "OutputDir": "_community",
  "ProcessedDate": "2025-08-18 07:20:44",
  "FeedName": "Microsoft Tech Community",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Community",
  "Link": "https://techcommunity.microsoft.com/t5/educator-developer-blog/how-to-use-custom-models-with-foundry-local-a-beginner-s-guide/ba-p/4428857",
  "Title": "How to Use Custom Models with Foundry Local: A Beginner's Guide",
  "Description": "**What is Foundry Local?**\n\nFoundry Local is a user-friendly tool that helps you run small AI language models directly on your Windows or Mac computer. Think of it as a way to have your own personal ChatGPT running locally on your machine, without needing an internet connection or sending your data to external servers.\n\nCurrently, Foundry Local works great with several popular model families:\n\n- **Phi models** (Microsoft's small but powerful models)\n- **Qwen models** (Alibaba's multilingual models)\n- **DeepSeek models** (Efficient reasoning models)\n\n![]()\n\nIn this tutorial, we'll learn how to set up the Qwen3-0.6B model step by step. Don't worry if you're new to AI - we'll explain everything along the way!\n\n**Why Do We Need to Convert AI Models?**\n\nWhen you download AI models from websites like Hugging Face (think of it as GitHub for AI models), they come in a format called PyTorch. While PyTorch is great for training models, it's not the best format for running them on your personal computer.\n\nTo make these models work efficiently on your laptop or desktop, we need to:\n\n1. **Convert the format** - Change it to something your computer can run faster\n2. **Make it smaller** - Compress the model so it uses less memory and storage\n\nWe'll use two main formats for this conversion:\n\n**GGUF vs ONNX: Which Format Should You Choose?**\n\nThink of these as different \"languages\" that your computer can understand. Since we're working with small language models (like Qwen3-0.6B), let's see which format works best for your needs:\n\n**GGUF (GPT-Generated Unified Format)**\n\n**Best for:** Basic computers, simple setups, or if you want maximum simplicity\n\n**Advantages:**\n\n- üíæ **Super memory-efficient** - Uses much less RAM through smart compression\n- üìÅ **One file, that's it** - Everything you need is in a single file (no complicated folders)\n- üíª **CPU-friendly** - Works great on older computers or laptops without powerful graphics cards\n- üõ†Ô∏è **Simple tools** - Easy to use with popular tools like llama.cpp\n- ‚ö° **Quick to set up** - Less configuration needed\n\n**Disadvantages:**\n\n- üö´ **CPU only** - Can't take advantage of your graphics card for speed\n- üìè **Size matters** - For small models, the format adds relatively more \"overhead\"\n- üîí **Limited flexibility** - Only works with certain types of AI models (transformer-based)\n\n**ONNX (Open Neural Network Exchange)**\n\n**Best for:** Modern computers, when you want the best performance, or professional use\n\n**Advantages:**\n\n- üîÑ **Works with everything** - Compatible with many different AI model types and architectures\n- üöÄ **Hardware acceleration** - Can use your graphics card (GPU) or special AI chips (NPU) for much faster performance\n- üè≠ **Professional-grade** - Used by companies in production environments\n- üîß **Flexible conversion** - Easy to convert from almost any AI training framework\n- üì± **Mobile-ready** - Great support for running on phones and tablets\n- ‚öôÔ∏è **Smart optimization** - ONNX Runtime automatically makes your model run faster\n\n**Disadvantages:**\n\n- üì¶ **More complex** - Multiple files and folders to manage\n- üíæ **Larger file sizes** - Takes up more storage space\n- üõ†Ô∏è **More setup** - Requires a bit more configuration to get running\n\n**Our Recommendation for Beginners**\n\nFor this tutorial, **we'll use ONNX** because:\n\n- ‚úÖ It gives you the best performance on most modern computers\n- ‚úÖ You can upgrade to GPU acceleration later if you want\n- ‚úÖ It's the industry standard that you'll encounter in most AI projects\n- ‚úÖ Foundry Local works excellently with ONNX models\n\n**Meet Microsoft Olive: Your Model Conversion Helper**\n\nMicrosoft Olive is like a smart assistant that helps convert AI models for you. Instead of doing all the technical work manually, Olive automates the process and makes sure everything works correctly.\n\nHere's what makes Olive special:\n\n- **Works with any computer setup** - Whether you have a basic laptop or a gaming PC with a powerful graphics card\n- **Does the work for you** - No need to learn complex conversion commands\n- **Multiple compression options** - Can make your model smaller in different ways (INT4, INT8, FP16 - don't worry about these terms for now!)\n- **Plays well with others** - Works seamlessly with other AI tools you might use\n\n**Let's Convert Your Model: Step-by-Step Guide**\n\n![]()\n\nDon't worry if you've never done this before - we'll go through each step carefully!\n\n**Step 1: Install the Tools We Need**\n\nFirst, we need to install some software tools. Think of this like downloading apps on your phone - each tool has a specific job to help us convert the model.\n\n**Open your terminal** (Command Prompt on Windows, Terminal on Mac) and run these commands one by one:\n\n- # This updates the main AI library to the latest version\n\npip install transformers -U\n\n# This installs Microsoft Olive (our conversion helper)\n\npip install git+https://github.com/microsoft/Olive.git\n\n# This downloads and installs additional AI tools\n\ngit clone https://github.com/microsoft/onnxruntime-genai\n\ncd onnxruntime-genai && python build.py --config Release\n\npip install {Your build release path}/onnxruntime\\_genai-0.9.0.dev0-cp311-cp311-linux\\_x86\\_64.whl\n\n**üìù** **Important note:** You'll also need CMake version 3.31 or newer. If you don't have it, you can download it from [cmake.org](https://cmake.org/download/).\n\n**Step 2: The Easy Way - One Command Conversion**\n\nOnce everything is installed, converting your model is surprisingly simple! Just run this command (but replace {Your Qwen3-0.6B Path} with the actual location where you downloaded the model):\n- olive auto-opt \\\n\n--model\\_name\\_or\\_path {Your Qwen3-0.6B Path} \\\n\n--device cpu \\\n\n--provider CPUExecutionProvider \\\n\n--use\\_model\\_builder \\\n\n--precision int4 \\\n\n--output\\_path models/Qwen3-0.6B/onnx \\\n\n--log\\_level 1\n\n**What does this command do?**\n\n- --device cpu means we're optimizing for your computer's processor\n- --precision int4 makes the model smaller (about 75% size reduction!)\n- --output\\_path tells Olive where to save the converted model\n\n**üí°** **Tip:** If you have a powerful graphics card, you can change cpu to cuda for potentially better performance.\n\n**Step 3: The Advanced Way - Using a Configuration File**\n\nFor those who want more control, you can create a configuration file. This is like creating a recipe that tells Olive exactly how you want your model converted.\n\nCreate a new file called conversion\\_config.json and add this content:\n- {\n\n\"input\\_model\": {\n\n\"type\": \"HfModel\",\n\n\"model\\_path\": \"Qwen/Qwen3-0.6B\",\n\n\"task\": \"text-generation\"\n\n},\n\n\"systems\": {\n\n\"local\\_system\": {\n\n\"type\": \"LocalSystem\",\n\n\"accelerators\": [\n\n{\n\n\"execution\\_providers\": [\n\n\"CPUExecutionProvider\"\n\n]\n\n}\n\n]\n\n}\n\n},\n\n\"passes\": {\n\n\"builder\": {\n\n\"type\": \"ModelBuilder\",\n\n\"config\": {\n\n\"precision\": \"int4\"\n\n}\n\n}\n\n},\n\n\"host\": \"local\\_system\",\n\n\"target\": \"local\\_system\",\n\n\"cache\\_dir\": \"cache\",\n\n\"output\\_dir\": \"model/output/Qwen3-0.6B-ONNX\"\n\n}\n\nThen run this command:\n- olive run --config ./conversion\\_config.json\n\n**üîê** **Before you start:** If this is your first time downloading models from Hugging Face, you'll need to log in first:\n- huggingface-cli login\n\nThis will ask for your Hugging Face token (you can get one free from their website).\n\n**Setting Up Your Converted Model in Foundry Local**\n\nGreat! Now that you have a converted model, let's get it running in Foundry Local. This is like installing a new app on your computer.\n\n**What You'll Need**\n\n- ‚úÖ Foundry Local installed on your computer\n- ‚úÖ Your freshly converted ONNX model from the previous steps\n- ‚úÖ A few minutes to set everything up\n\n**Getting Started**\n\nFirst, let's navigate to where Foundry Local stores its models:\n\nfoundry cache cd ./models/\n\nThis command takes you to the \"models folder\" - think of it as the app store for your AI models.\n\n**Step 1: Create a Chat Template**\n\nAI models need to know how to format conversations. It's like teaching them the \"grammar\" of chatting. Create a new file called inference\\_model.json with this content:\n- {\n\n\"Name\": \"Qwen3-0.6b-cpu\",\n\n\"PromptTemplate\": {\n\n\"system\": \"system\\n{Content}\",\n\n\"user\": \"user\\n/think{Content}\",\n\n\"assistant\": \"assistant\\n{Content}\",\n\n\"prompt\": \"user\\n/think{Content}\\nassistant\"\n\n}\n\n}\n\n**ü§î** **What's this \"think\" thing?** Qwen models have a special feature where they can \"think out loud\" before giving you an answer. It's like showing their work in math class! This often leads to better, more thoughtful responses. If you don't want this feature, just remove /think from the templates above.\n\n**Step 2: Organize Your Files**\n\nCreate a neat folder structure for your model. This helps Foundry Local find everything easily:\n\n*# Create a folder for your model*\n- mkdir -p ./models/qwen/Qwen3-0.6B\n\n*# Copy your converted files here*\n\n*# (You'll need to move your ONNX files and the inference\\_model.json to this folder)*\n\n**üìÅ** **Why this structure?**\n\n- qwen = the company that made the model\n- Qwen3-0.6B = the specific model name\n\n**Step 3: Check If Everything Worked**\n\nLet's verify that Foundry Local can see your new model:\n- foundry cache ls\n\nYou should see Qwen3-0.6b-cpu in the list. If you don't see it, double-check that your files are in the right place.\n\n![]()\n\n**Step 4: Take It for a Test Drive!**\n\nThe moment of truth - let's start chatting with your model:\n- foundry model run Qwen3-0.6b-cpu\n\nIf everything worked correctly, you should see your model starting up, and you can begin asking it questions!\n\n![]()\n\n**Troubleshooting: When Things Don't Go as Planned**\n\nDon't worry if you run into issues - this is totally normal! Here are the most common problems and how to fix them:\n\n**Problem: \"Model not found\" error**\n\n**What happened:** Foundry Local can't find your model files **How to fix it:**\n\n1. Double-check that your files are in the right folder: ./models/qwen/Qwen3-0.6B/\n2. Make sure the inference\\_model.json file is in the same folder as your ONNX files\n3. Check that the model name in the JSON file matches what you're trying to run\n\n**Problem: Model starts but gives weird responses**\n\n**What happened:** The chat template might not be set up correctly **How to fix it:**\n\n1. Check your inference\\_model.json file for typos\n2. Make sure the special characters (, ) are exactly as shown\n3. Try removing the /think part if you're getting strange outputs\n\n**Problem: Model runs very slowly**\n\n**What happened:** Your computer might be working harder than it needs to **How to fix it:**\n\n1. Close other programs to free up memory\n2. If you have a good graphics card, try the GPU version instead of CPU\n3. Consider using a smaller model if performance is still poor\n\n**Problem: Installation commands fail**\n\n**What happened:** Something went wrong during setup **How to fix it:**\n\n1. Make sure you have Python installed (version 3.8 or newer)\n2. Try running the commands one at a time instead of all at once\n3. Check your internet connection - some downloads are quite large\n\n**Congratulations! You Did It!** **üéâ**\n\nYou've successfully:\n\n- ‚úÖ Learned the difference between model formats\n- ‚úÖ Converted a PyTorch model to ONNX format\n- ‚úÖ Set up your own local AI assistant\n- ‚úÖ Got it running on your personal computer\n\n*Have questions or run into issues? [The AI Discord](https://aka.ms/ai/discord) is very helpful - don't hesitate to ask for help on forums or in the [Foundry Local repo](https://github.com/microsoft/foundry-Local).*",
  "EnhancedContent": "**What is Foundry Local?**\n\nFoundry Local is a user-friendly tool that helps you run small AI language models directly on your Windows or Mac computer. Think of it as a way to have your own personal ChatGPT running locally on your machine, without needing an internet connection or sending your data to external servers.\n\nCurrently, Foundry Local works great with several popular model families:\n\n- **Phi models**¬†(Microsoft's small but powerful models)\n- **Qwen models**¬†(Alibaba's multilingual models)\n- **DeepSeek models** (Efficient reasoning models)\n\nIn this tutorial, we'll learn how to set up the Qwen3-0.6B model step by step. Don't worry if you're new to AI - we'll explain everything along the way!\n\n**Why Do We Need to Convert AI Models?**\n\nWhen you download AI models from websites like Hugging Face (think of it as GitHub for AI models), they come in a format called PyTorch. While PyTorch is great for training models, it's not the best format for running them on your personal computer.\n\nTo make these models work efficiently on your laptop or desktop, we need to:\n\n1. **Convert the format**¬†- Change it to something your computer can run faster\n2. **Make it smaller**¬†- Compress the model so it uses less memory and storage\n\nWe'll use two main formats for this conversion:\n\n**GGUF vs ONNX: Which Format Should You Choose?**\n\nThink of these as different \"languages\" that your computer can understand. Since we're working with small language models (like Qwen3-0.6B), let's see which format works best for your needs:\n\n**GGUF (GPT-Generated Unified Format)**\n\n**Best for:**¬†Basic computers, simple setups, or if you want maximum simplicity\n\n**Advantages:**\n\n- üíæ¬†**Super memory-efficient**¬†- Uses much less RAM through smart compression\n- üìÅ¬†**One file, that's it**¬†- Everything you need is in a single file (no complicated folders)\n- üíª¬†**CPU-friendly**¬†- Works great on older computers or laptops without powerful graphics cards\n- üõ†Ô∏è¬†**Simple tools**¬†- Easy to use with popular tools like llama.cpp\n- ‚ö°¬†**Quick to set up**¬†- Less configuration needed\n\n**Disadvantages:**\n\n- üö´¬†**CPU only**¬†- Can't take advantage of your graphics card for speed\n- üìè¬†**Size matters**¬†- For small models, the format adds relatively more \"overhead\"\n- üîí¬†**Limited flexibility**¬†- Only works with certain types of AI models (transformer-based)\n\n**ONNX (Open Neural Network Exchange)**\n\n**Best for:**¬†Modern computers, when you want the best performance, or professional use\n\n**Advantages:**\n\n- üîÑ¬†**Works with everything**¬†- Compatible with many different AI model types and architectures\n- üöÄ¬†**Hardware acceleration**¬†- Can use your graphics card (GPU) or special AI chips (NPU) for much faster performance\n- üè≠¬†**Professional-grade**¬†- Used by companies in production environments\n- üîß¬†**Flexible conversion**¬†- Easy to convert from almost any AI training framework\n- üì±¬†**Mobile-ready**¬†- Great support for running on phones and tablets\n- ‚öôÔ∏è¬†**Smart optimization**¬†- ONNX Runtime automatically makes your model run faster\n\n**Disadvantages:**\n\n- üì¶¬†**More complex**¬†- Multiple files and folders to manage\n- üíæ¬†**Larger file sizes**¬†- Takes up more storage space\n- üõ†Ô∏è¬†**More setup**¬†- Requires a bit more configuration to get running\n\n**Our Recommendation for Beginners**\n\nFor this tutorial,¬†**we'll use ONNX**¬†because:\n\n- ‚úÖ It gives you the best performance on most modern computers\n- ‚úÖ You can upgrade to GPU acceleration later if you want\n- ‚úÖ It's the industry standard that you'll encounter in most AI projects\n- ‚úÖ Foundry Local works excellently with ONNX models\n\n**Meet Microsoft Olive: Your Model Conversion Helper**\n\nMicrosoft Olive is like a smart assistant that helps convert AI models for you. Instead of doing all the technical work manually, Olive automates the process and makes sure everything works correctly.\n\nHere's what makes Olive special:\n\n- **Works with any computer setup**¬†- Whether you have a basic laptop or a gaming PC with a powerful graphics card\n- **Does the work for you**¬†- No need to learn complex conversion commands\n- **Multiple compression options**¬†- Can make your model smaller in different ways (INT4, INT8, FP16 - don't worry about these terms for now!)\n- **Plays well with others**¬†- Works seamlessly with other AI tools you might use\n\n**Let's Convert Your Model: Step-by-Step Guide**\n\nDon't worry if you've never done this before - we'll go through each step carefully!\n\n**Step 1: Install the Tools We Need**\n\nFirst, we need to install some software tools. Think of this like downloading apps on your phone - each tool has a specific job to help us convert the model.\n\n**Open your terminal**¬†(Command Prompt on Windows, Terminal on Mac) and run these commands one by one:\n\n```\n# This updates the main AI library to the latest version\n\npip install transformers -U\n\n# This installs Microsoft Olive (our conversion helper)\n\npip install git+https://github.com/microsoft/Olive.git\n\n# This downloads and installs additional AI tools\n\ngit clone https://github.com/microsoft/onnxruntime-genai\n\ncd onnxruntime-genai && python build.py --config Release\n\npip install {Your build release path}/onnxruntime_genai-0.9.0.dev0-cp311-cp311-linux_x86_64.whl ```\n\n**üìù** **Important note:**¬†You'll also need CMake version 3.31 or newer. If you don't have it, you can download it from¬†[cmake.org](https://cmake.org/download/).\n\n**Step 2: The Easy Way - One Command Conversion**\n\nOnce everything is installed, converting your model is surprisingly simple! Just run this command (but replace¬†{Your Qwen3-0.6B Path}¬†with the actual location where you downloaded the model):\n\n``` olive auto-opt \\\n\n--model_name_or_path {Your Qwen3-0.6B Path} \\\n\n--device cpu \\\n\n--provider CPUExecutionProvider \\\n\n--use_model_builder \\\n\n--precision int4 \\\n\n--output_path models/Qwen3-0.6B/onnx \\\n\n--log_level 1 ```\n\n**What does this command do?**\n\n- --device cpu¬†means we're optimizing for your computer's processor\n- --precision int4¬†makes the model smaller (about 75% size reduction!)\n- --output\\_path¬†tells Olive where to save the converted model\n\n**üí°** **Tip:**¬†If you have a powerful graphics card, you can change¬†cpu¬†to¬†cuda¬†for potentially better performance.\n\n**Step 3: The Advanced Way - Using a Configuration File**\n\nFor those who want more control, you can create a configuration file. This is like creating a recipe that tells Olive exactly how you want your model converted.\n\nCreate a new file called¬†conversion\\_config.json¬†and add this content:\n\n``` {\n\n\"input_model\": {\n\n\"type\": \"HfModel\",\n\n\"model_path\": \"Qwen/Qwen3-0.6B\",\n\n\"task\": \"text-generation\"\n\n},\n\n\"systems\": {\n\n\"local_system\": {\n\n\"type\": \"LocalSystem\",\n\n\"accelerators\": [\n\n{\n\n\"execution_providers\": [\n\n\"CPUExecutionProvider\"\n\n]\n\n}\n\n]\n\n}\n\n},\n\n\"passes\": {\n\n\"builder\": {\n\n\"type\": \"ModelBuilder\",\n\n\"config\": {\n\n\"precision\": \"int4\"\n\n}\n\n}\n\n},\n\n\"host\": \"local_system\",\n\n\"target\": \"local_system\",\n\n\"cache_dir\": \"cache\",\n\n\"output_dir\": \"model/output/Qwen3-0.6B-ONNX\"\n\n} ```\n\nThen run this command:\n\n``` olive run --config ./conversion_config.json ```\n\n**üîê** **Before you start:**¬†If this is your first time downloading models from Hugging Face, you'll need to log in first:\n\n``` huggingface-cli login ```\n\nThis will ask for your Hugging Face token (you can get one free from their website).\n\n**Setting Up Your Converted Model in Foundry Local**\n\nGreat! Now that you have a converted model, let's get it running in Foundry Local. This is like installing a new app on your computer.\n\n**What You'll Need**\n\n- ‚úÖ Foundry Local installed on your computer\n- ‚úÖ Your freshly converted ONNX model from the previous steps\n- ‚úÖ A few minutes to set everything up\n\n**Getting Started**\n\nFirst, let's navigate to where Foundry Local stores its models:\n\nfoundry cache cd ./models/\n\nThis command takes you to the \"models folder\" - think of it as the app store for your AI models.\n\n**Step 1: Create a Chat Template**\n\nAI models need to know how to format conversations. It's like teaching them the \"grammar\" of chatting. Create a new file called¬†inference\\_model.json¬†with this content:\n\n``` {\n\n\"Name\": \"Qwen3-0.6b-cpu\",\n\n\"PromptTemplate\": {\n\n\"system\": \"<|im_start|>system\\n{Content}<|im_end|>\",\n\n\"user\": \"<|im_start|>user\\n/think{Content}<|im_end|>\",\n\n\"assistant\": \"<|im_start|>assistant\\n{Content}<|im_end|>\",\n\n\"prompt\": \"<|im_start|>user\\n/think{Content}<|im_end|>\\n<|im_start|>assistant\"\n\n}\n\n} ```\n\n**ü§î** **What's this \"think\" thing?**¬†Qwen models have a special feature where they can \"think out loud\" before giving you an answer. It's like showing their work in math class! This often leads to better, more thoughtful responses. If you don't want this feature, just remove¬†/think¬†from the templates above.\n\n**Step 2: Organize Your Files**\n\nCreate a neat folder structure for your model. This helps Foundry Local find everything easily:\n\n*# Create a folder for your model*\n\n``` mkdir -p ./models/qwen/Qwen3-0.6B ```\n\n*# Copy your converted files here*\n\n*# (You'll need to move your ONNX files and the inference\\_model.json to this folder)*\n\n**üìÅ** **Why this structure?**\n\n- qwen¬†= the company that made the model\n- Qwen3-0.6B¬†= the specific model name\n\n**Step 3: Check If Everything Worked**\n\nLet's verify that Foundry Local can see your new model:\n\n``` foundry cache ls ```\n\nYou should see¬†Qwen3-0.6b-cpu¬†in the list. If you don't see it, double-check that your files are in the right place.\n\n**Step 4: Take It for a Test Drive!**\n\nThe moment of truth - let's start chatting with your model:\n\n``` foundry model run Qwen3-0.6b-cpu ```\n\nIf everything worked correctly, you should see your model starting up, and you can begin asking it questions!\n\n**Troubleshooting: When Things Don't Go as Planned**\n\nDon't worry if you run into issues - this is totally normal! Here are the most common problems and how to fix them:\n\n**Problem: \"Model not found\" error**\n\n**What happened:**¬†Foundry Local can't find your model files¬†**How to fix it:**\n\n1. Double-check that your files are in the right folder:¬†./models/qwen/Qwen3-0.6B/\n2. Make sure the¬†inference\\_model.json¬†file is in the same folder as your ONNX files\n3. Check that the model name in the JSON file matches what you're trying to run\n\n**Problem: Model starts but gives weird responses**\n\n**What happened:**¬†The chat template might not be set up correctly¬†**How to fix it:**\n\n1. Check your¬†inference\\_model.json¬†file for typos\n2. Make sure the special characters (&lt;|im\\_start|&gt;,¬†&lt;|im\\_end|&gt;) are exactly as shown\n3. Try removing the¬†/think¬†part if you're getting strange outputs\n\n**Problem: Model runs very slowly**\n\n**What happened:**¬†Your computer might be working harder than it needs to¬†**How to fix it:**\n\n1. Close other programs to free up memory\n2. If you have a good graphics card, try the GPU version instead of CPU\n3. Consider using a smaller model if performance is still poor\n\n**Problem: Installation commands fail**\n\n**What happened:**¬†Something went wrong during setup¬†**How to fix it:**\n\n1. Make sure you have Python installed (version 3.8 or newer)\n2. Try running the commands one at a time instead of all at once\n3. Check your internet connection - some downloads are quite large\n\n**Congratulations! You Did It!** **üéâ**\n\nYou've successfully:\n\n- ‚úÖ Learned the difference between model formats\n- ‚úÖ Converted a PyTorch model to ONNX format\n- ‚úÖ Set up your own local AI assistant\n- ‚úÖ Got it running on your personal computer\n\n*Have questions or run into issues? [The AI Discord](https://aka.ms/ai/discord) is very helpful - don't hesitate to ask for help on forums or in the [Foundry Local repo](https://github.com/microsoft/foundry-Local).*\n\nUpdated Aug 14, 2025\n\nVersion 1.0\n\n[!\\[kinfey&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0xMTU4ODcwLTU0ODQxMWlERTQ5OEYxMkNFQTBBQzcw?image-dimensions=50x50)](/users/kinfey/1158870) [kinfey](/users/kinfey/1158870) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined September 17, 2021\n\n[View Profile](/users/kinfey/1158870)\n\n/category/educationsector/blog/educatordeveloperblog [Educator Developer Blog](/category/educationsector/blog/educatordeveloperblog) Follow this blog board to get notified when there's new activity",
  "PubDate": "2025-08-18T07:00:00+00:00",
  "Author": "kinfey"
}
