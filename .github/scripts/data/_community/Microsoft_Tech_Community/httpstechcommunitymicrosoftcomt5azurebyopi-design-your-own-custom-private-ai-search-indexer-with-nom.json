{
  "Link": "https://techcommunity.microsoft.com/t5/azure/byopi-design-your-own-custom-private-ai-search-indexer-with-no/m-p/4464205#M22283",
  "EnhancedContent": "#### **Executive Summary**\n\n*Building a fully private search indexing solution using Azure Data Factory (ADF) to sync SQL Server data from private VM to Azure AI Search is achievable but comes with notable complexities and limitations.*\n\n*This blog shares my journey, discoveries, and honest assessment of the BYOPI (Build Your Own Private Indexer) architecture.*\n\nArchitectural flow:\n\n####\n\n#### **Table of Contents**\n\n1. ##### *Overall Setup*\n2. ##### *How ADF works in this approach with Azure AI Search*\n3. ##### *Challenges - discovered*\n4. ##### *Pros and Cons: An Honest Assessment*\n5. ##### *Conclusion and Recommendations*\n\n####\n\n#### **1. Overall Setup:**\n\n###### *Phase 1*: Resource Group & Network Setup : create resource group and vNET (virtual network) in any region of your choice\n\n*Phase 2*: Deploy SQL Server VM:\n\n*Phase 3*: Create Azure Services - ADF (Azure Data Factory), Azure AI Search¬† and AKV (Azure Key Vault) service from portal or from your choice of deployment.\n\n*Phase 4:* Create Private Endpoints for all the services in their dedicated subnets:\n\n*Phase 5*: Configure SQL Server on VM : connect to VM via bastion and setup database, tables & SP:\n\nSample metadata used as below:\n\n``` CREATE DATABASE BYOPI_DB; GO\n\nUSE BYOPI_DB; GO\n\nCREATE TABLE Products ( ProductId INT IDENTITY(1,1) PRIMARY KEY, ProductName NVARCHAR(200) NOT NULL, Description NVARCHAR(MAX), Category NVARCHAR(100), Price DECIMAL(10,2), InStock BIT DEFAULT 1, Tags NVARCHAR(500), IsDeleted BIT DEFAULT 0, CreatedDate DATETIME DEFAULT GETDATE(), ModifiedDate DATETIME DEFAULT GETDATE() );\n\nCREATE TABLE WatermarkTable ( TableName NVARCHAR(100) PRIMARY KEY, WatermarkValue DATETIME );\n\nINSERT INTO WatermarkTable VALUES ('Products', '2024-01-01');\n\nCREATE PROCEDURE sp_update_watermark @TableName NVARCHAR(100), @NewWatermark DATETIME AS BEGIN UPDATE WatermarkTable SET WatermarkValue = @NewWatermark WHERE TableName = @TableName; END;\n\nINSERT INTO Products (ProductName, Description, Category, Price, Tags) VALUES ('Laptop Pro', 'High-end laptop', 'Electronics', 1299.99, 'laptop,computer'), ('Office Desk', 'Adjustable desk', 'Furniture', 599.99, 'desk,office'), ('Wireless Mouse', 'Bluetooth mouse', 'Electronics', 29.99, 'mouse,wireless'); ```\n\n*Phase 6*: Install Self-Hosted Integration Runtime\n\n###### Create SHIR in ADF:\n\n1. Go to ADF resource in Azure Portal\n2. Click \"Open Azure Data Factory Studio\"\n- Note: You need to access from a VM in the same VNet or via VPN since ADF is private\n3. In ADF Studio, click Manage (toolbox icon)\n4. Select Integration runtimes ‚Üí \"+ New\"\n5. Select \"Azure, Self-Hosted\" ‚Üí \"Self-Hosted\"\n6. Name: SHIR-BYOPI or of your choice\n7. Click \"Create\"\n8. Copy Key1 (save it)\n\n###### Install SHIR on VM\n\n1. In the VM (via Bastion):\n2. Open browser, go to: https://www.microsoft.com/download/details.aspx?id=39717\n3. Download and install Integration Runtime\n4. During setup:\n- Launch Configuration Manager\n- Paste the Key1 from Step 14\n- Click \"Register\"\n- Wait for \"Connected\" status\n\n*Phase 7*: Create Search Index through below powershell script and saving it as search\\_index.ps1\n\n``` $searchService = \"search-byopi\" $apiKey = \"YOUR-ADMIN-KEY\"\n\n$headers = @{ 'api-key' = $apiKey 'Content-Type' = 'application/json' }\n\n$index = @{ name = \"products-index\" fields = @( @{name=\"id\"; type=\"Edm.String\"; key=$true} @{name=\"productName\"; type=\"Edm.String\"; searchable=$true} @{name=\"description\"; type=\"Edm.String\"; searchable=$true} @{name=\"category\"; type=\"Edm.String\"; filterable=$true; facetable=$true} @{name=\"price\"; type=\"Edm.Double\"; filterable=$true} @{name=\"inStock\"; type=\"Edm.Boolean\"; filterable=$true} @{name=\"tags\"; type=\"Collection(Edm.String)\"; searchable=$true} ) } | ConvertTo-Json -Depth 10\n\nInvoke-RestMethod ` -Uri \"https://$searchService.search.windows.net/indexes/products-index?api-version=2020-06-30\" ` -Method PUT ` -Headers $headers ` -Body $index ```\n\n*Phase 8*: Configure AKV & ADF Components - Link AKV and ADF for secrets\n\nCreate Key Vault Secrets\n\n1. Navigate to kv-byopi (created AKV resource) in Portal\n2. Go to \"Access policies\"\n3. Click \"+ Create\"\n4. Select permissions: Get, List for secrets\n5. Select principal: adf-byopi-private\n6. Create\n7. Go to \"Secrets\" ‚Üí \"+ Generate/Import\":\n- Name: sql-password, Value: &lt;&gt;\n- Name: search-api-key, Value: Your search key\n\n###### Create Linked Services in ADF\n\nAccess ADF Studio from the VM (since it's private):\n\n1. Key Vault Linked Service:\n\n1. Manage ‚Üí Linked services ‚Üí \"+ New\"\n2. Search \"Azure Key Vault\"\n3. Configure:\n- Name: LS\\_KeyVault\n- Azure Key Vault: kv-byopi\n- Integration runtime: AutoResolveIntegrationRuntime\n4. Test connection ‚Üí Create\n\n1. SQL Server Linked Service:\n\n1. \"+ New\" ‚Üí \"SQL Server\"\n2. Configure:\n- Name: LS\\_SqlServer\n- Connect via: SHIR-BYOPI\n- Server name: localhost\n- Database: BYOPI\\_DB\n- Authentication: SQL Authentication\n- User: sqladmin\n- Password: Select from Key Vault ‚Üí LS\\_KeyVault ‚Üí sql-password\n3. Test ‚Üí Create\n\n1. Azure Search Linked Service:\n\n1. \"+ New\" ‚Üí \"Azure Search\"\n2. Configure:\n- Name: LS\\_AzureSearch\n- URL: https://search-byopi.search.windows.net\n- Connect via: SHIR-BYOPI¬† - Important - use SHIR\n- API Key: From Key Vault ‚Üí LS\\_KeyVault ‚Üí search-api-key\n3. Test ‚Üí Create\n\n*Phase 9*: Create ADF Datasets and PipelineCreate Datasets\n\nSQL Products Dataset:\n\n1. Author ‚Üí Datasets ‚Üí \"+\" ‚Üí \"New dataset\"\n2. Select \"SQL Server\" ‚Üí Continue\n3. Select \"Table\" ‚Üí Continue\n4. Properties:\n- Name: DS\\_SQL\\_Products\n- Linked service: LS\\_SqlServer\n- Table: Select Products\n5. click OK\n\n1. Watermark Dataset:\n\n1. Repeat with:\n- Name: DS\\_SQL\\_Watermark\n- Table: WatermarkTable\n\n1. Search Dataset:\n\n1. \"+\" ‚Üí \"Azure Search\"\n2. Properties:\n- Name: DS\\_Search\\_Index\n- Linked service: LS\\_AzureSearch\n- Index name: products-index\n\n###### Create Pipeline\n\n1. Author ‚Üí Pipelines ‚Üí \"+\" ‚Üí \"Pipeline\"\n2. Name: PL\\_BYOPI\\_Private\n3. From Activities ‚Üí General, drag \"Lookup\" activity\n4. Configure Lookup 1:\n- Name: LookupOldWatermark\n- Settings:\n- Source dataset: DS\\_SQL\\_Watermark\n- Query: below\n\nsql\n\nSELECT WatermarkValue FROM WatermarkTable WHERE TableName='Products'\n\n- \\*\\*First row only\\*\\*: ‚úì\n\n1. Add another Lookup:\n\n- Name: LookupNewWatermark\n- Query: below\n\nsql\n\nSELECT MAX(ModifiedDate) as NewWatermark FROM Products\n\n1. Add Copy Data activity:\n- Name: CopyToSearchIndex\n- Source:\n- Dataset: DS\\_SQL\\_Products\n- Query:\n\nsql\n\nSELECT CAST(ProductId AS NVARCHAR(50)) as id, ProductName as productName, Description as description, Category as category, Price as price, InStock as inStock, Tags as tags, CASE WHEN IsDeleted = 1 THEN 'delete' ELSE 'upload' END as [@search.action] FROM Products WHERE ModifiedDate &gt; '@{activity('LookupOldWatermark').output.firstRow.WatermarkValue}' AND ModifiedDate &lt;= '@{activity('LookupNewWatermark').output.firstRow.NewWatermark}'\n\n- Sink:\n- Dataset: DS\\_Search\\_Index\n- Write behavior: Merge\n- Batch size: 1000\n\n1. Add Stored Procedure activity:\n- Name: UpdateWatermark\n- SQL Account: LS\\_SqlServer\n- Stored procedure: sp\\_update\\_watermark\n- Parameters:\n- TableName: Products\n- NewWatermark: @{activity('LookupNewWatermark').output.firstRow.NewWatermark}\n2. Connect activities with success conditions\n\n###### *Phase 10*: Test and Schedule\n\n###### Test Pipeline\n\n1. ###### Click \"Debug\" in pipeline\n2. ###### Monitor in Output panel\n3. ###### Check for green checkmarks\n\n###### Create Trigger\n\n1. ###### In pipeline, click \"Add trigger\" ‚Üí \"New/Edit\"\n2. ###### Click \"+ New\"\n3. ###### Configure:\n\n- ###### Name: TR\\_Hourly\n- ###### Type: Schedule\n- ###### Recurrence: Every 1 Hour\n4. ###### OK ‚Üí Publish All\n\n###### Monitor\n\n1. ###### Go to Monitor tab\n2. ###### View Pipeline runs\n3. ###### Check Trigger runs\n\nYour pipeline should look like this:\n\n*Phase 11*: Validation & Testing\n\n###### Verify Private Connectivity\n\n###### From the VM, run PowerShell:\n\n```\n# Test DNS resolution (should return private IPs)\nnslookup adf-byopi-private.datafactory.azure.net\n# Should show private IP like : 10.0.2.x\n\nnslookup search-byopi.search.windows.net\n# Should show private IP like : 10.0.2.x\n\nnslookup kv-byopi.vault.azure.net\n# Should show private IP like : 10.0.2.x\n\n# Test Search\n$headers = @{ 'api-key' = 'YOUR-KEY' } Invoke-RestMethod -Uri \"https://search-byopi.search.windows.net/indexes/products-index/docs?`$count=true&api-version=2020-06-30\" -Headers $headers ```\n\nTest Data Sync (adding few records) and verify in search index:\n\n``` -- Add test record INSERT INTO Products (ProductName, Description, Category, Price, Tags) VALUES ('Test Product Private', 'Testing private pipeline', 'Test', 199.99, 'test,private'); -- Trigger pipeline manually or wait for schedule -- Then verify in Search index ```\n\n#### **2. How ADF works in this approach with Azure AI search:**\n\nAzure AI Search uses a REST API for indexing or called as uploading. When ADF sink uploads data to AI Search, it's actually making HTTP POST requests: for example -\n\nPOST https://search-byopi.search.windows.net/indexes/products-index/docs/index?api-version=2020-06-30 Content-Type: application/json api-key: YOUR-ADMIN-KEY\n\n{ \"value\": [ { \"@search.action\": \"upload\", \"id\": \"1\", \"productName\": \"Laptop\", \"price\": 999.99 }, { \"@search.action\": \"delete\", \"id\": \"2\" } ] }\n\nDelete action used here is soft delete and not hard delete.\n\npipeline query:\n\nSELECT CAST(ProductId AS NVARCHAR(50)) as id, ¬†-- Renamed to match index field ProductName as productName, ¬† ¬† ¬† ¬† ¬† ¬† ¬†-- Renamed to match index field Description as description, Category as category, Price as price, InStock as inStock, Tags as tags, CASE WHEN IsDeleted = 1 THEN 'delete' ELSE 'upload' END as [@search.action] ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬†-- Special field with @ prefix FROM Products WHERE ModifiedDate &gt; '2024-01-01' ```\n\nReturns this resultset: ``` id ¬†| productName ¬† ¬†| description ¬† ¬† ¬†| category ¬† ¬†| price ¬†| inStock | tags ¬† ¬† ¬† ¬† ¬† | @search.action ----|----------------|------------------|-------------|--------|---------|----------------|--------------- 1 ¬† | Laptop Pro ¬† ¬† | High-end laptop ¬†| Electronics | 1299 ¬† | 1 ¬† ¬† ¬† | laptop,computer| upload 2 ¬† | Office Chair ¬† | Ergonomic chair ¬†| Furniture ¬† | 399 ¬† ¬†| 1 ¬† ¬† ¬† | chair,office ¬† | upload 3 ¬† | Deleted Item ¬† | Old product ¬† ¬† ¬†| Archive ¬† ¬† | 0 ¬† ¬† ¬†| 0 ¬† ¬† ¬† | old ¬† ¬† ¬† ¬† ¬† ¬†| delete\n\n##### The @search.action Field - The Magic Control\n\nThis special field tells Azure AI Search what to do with each document:\n\n| @search.action | What It Does | When to Use | What Happens If Document... | | --- | --- | --- | --- | | upload | Insert OR Update | Most common - upsert operation | Exists: Updates it&lt;br&gt;Doesn't exist: Creates it | | merge | Update only | When you know it exists | Exists: Updates specified fields&lt;br&gt;Doesn't exist: ERROR | | mergeOrUpload | Update OR Insert | Safe update | Exists: Updates fields&lt;br&gt;Doesn't exist: Creates it | | delete | Remove from index | To remove documents | Exists: Deletes it&lt;br&gt;Doesn't exist: Ignores (no error) |\n\nADF automatically converts SQL results to JSON format required by Azure Search:\n\n``` { \"value\": [ { \"@search.action\": \"upload\", \"id\": \"1\", \"productName\": \"Laptop Pro\", \"description\": \"High-end laptop\", \"category\": \"Electronics\", \"price\": 1299.00, \"inStock\": true, \"tags\": \"laptop,computer\" }, { \"@search.action\": \"upload\", \"id\": \"2\", \"productName\": \"Office Chair\", \"description\": \"Ergonomic chair\", \"category\": \"Furniture\", \"price\": 399.00, \"inStock\": true, \"tags\": \"chair,office\" }, { \"@search.action\": \"delete\", \"id\": \"3\" // For delete, only ID is needed } ] } ```\n\nADF doesn't send all records at once. It batches them based on writeBatchSize and each batch is a separate HTTP POST to Azure Search\n\nHow ADF will detect new changes and run batches:\n\nWatermark will be updated after each successful ADF run to detect new changes¬† as below:\n\nHandling different scenarios:\n\n*Scenario 1: No Changes Between Runs:*\n\nRun at 10:00 AM:\n- Old Watermark: 09:45:00\n- New Watermark: 10:00:00\n- Query: WHERE ModifiedDate &gt; '09:45' AND &lt;= '10:00'\n- Result: 0 rows\n- Action: Still update watermark to 10:00\n- Why: Prevents reprocessing if changes come later\n\n*Scenario 2: Bulk Insert Happens:*\n\nSomeone inserts 5000 records at 10:05 AM\n\nRun at 10:15 AM:\n- Old Watermark: 10:00:00\n- New Watermark: 10:15:00\n- Query: WHERE ModifiedDate &gt; '10:00' AND &lt;= '10:15'\n- Result: 5000 rows\n- Action: Process all 5000, update watermark to 10:15\n\n*Scenario 3: Pipeline Fails*\n\nRun at 10:30 AM:\n- Old Watermark: 10:15:00 (unchanged from last success)\n- Pipeline fails during Copy activity\n- Watermark NOT updated (still 10:15:00)\n\nNext Run at 10:45 AM:\n- Old Watermark: 10:15:00 (still the last successful)\n- New Watermark: 10:45:00\n- Query: WHERE ModifiedDate &gt; '10:15' AND &lt;= '10:45'\n- Result: Gets ALL changes from 10:15 to 10:45 (30 minutes of data)\n- No data loss!\n\nNote:\n\nThere is still room for improvement by refining this logic to handle more advanced scenarios. However, I have not examined the logic in depth, as the goal here is to review how the overall setup functions, identify its limitations, and compare it with the indexing solutions available in AI Search.\n\n#### **3. Challenges - disovered:**\n\nWhen I tried to set out to build a private search indexer for SQL Server data residing on an Azure VM with no public IP, the solution seemed straightforward: use Azure Data Factory to orchestrate the data movement to Azure AI Search.\n\nThe materials made it sound simple. The reality? It's possible, but the devil is in the details.\n\nWhat We Needed: ‚úÖ SQL Server on private VM (no public IP) ‚úÖ Azure AI Search with private endpoint ‚úÖ No data over public internet ‚úÖ Support for full CRUD operations ‚úÖ Near real-time synchronization ‚úÖ No-code/low-code solution\n\nReality Check: ‚ö†Ô∏è DELETE operations not natively supported in ADF sink ‚ö†Ô∏è Complex networking requirements ‚ö†Ô∏è Higher costs than expected ‚ö†Ô∏è Significant setup complexity ‚úÖ But it IS possible with workarounds\n\n###### Components Required\n\n- Azure VM: ~$150/month (D4s\\_v3)\n- Self-Hosted Integration Runtime: Free (runs on VM)\n- Private Endpoints: ~$30/month (approx 3 endpoints)\n- Azure Data Factory: ~$15-60/month (depends on frequency)\n- Azure AI Search: ~$75/month (Basic tier)\n- Total: ~$270-315/month\\*\\*\n\nThe DELETE Challenge:\n\nDespite Azure AI Search REST API fully supporting delete operations via @search.action, ADF's native Azure Search sink does NOT support delete operations. This isn't clearly documented and catches many architects off guard.\n\n-- This SQL query with delete action SELECT ProductId as id, CASE WHEN IsDeleted = 1 THEN 'delete' ELSE 'upload' END as [@search.action] FROM Products\n\n-- Will NOT delete documents in Azure Search when using Copy activity -- The @search.action = 'delete' is ignored by ADF sink!\n\nNevertheless, there is a workaround using the Web Activity approach or by calling the REST API from the ADF side to perform the delete operation.\n\n``` { \"name\": \"DeleteViaREST\", \"type\": \"Web\", \"typeProperties\": { \"url\": \"https://search.windows.net/indexes/index/docs/index\", \"method\": \"POST\", \"body\": { \"value\": [ {\"@search.action\": \"delete\", \"id\": \"123\"} ] } } } ```\n\n###### Development Challenges\n\n1. No Direct Portal Access: With ADF private, you need:\n- Jump box in the same VNet\n- VPN connection\n- Bastion for access\n2. Testing Complexity:\n- Can't use Postman from local machine\n- Need to test from within VNet\n- Debugging requires multiple tools\n\n#### **4. Pros and Cons: An Honest Assessment:**\n\n###### Pros:\n\n1. Security: Complete network isolation\n2. Compliance: Meets strict requirements\n3. No-Code: Mostly configuration-based\n4. Scalability: Can handle large datasets\n5. Monitoring: Built-in ADF monitoring\n6. Managed Service: Microsoft handles updates\n\n###### Cons:\n\n1. DELETE Complexity: Not natively supported\n2. Cost: Higher than expected\n3. Setup Complexity: Many moving parts\n4. Debugging: Difficult with private endpoints\n\nHidden Gotchas:\n\n- SHIR requires Windows VM (Linux in preview)\n- Private endpoint DNS propagation delays\n- ADF Studio timeout with private endpoints\n- SHIR auto-update can break pipelines\n\n#### **5. Conclusion and Recommendations:**\n\n###### When to Use BYOPI:\n\n‚úÖ **** Good Fit:\n\n- Strict security requirements\n- Needs indexing from an un-supported scenarios for example SQL server residing on private VM\n- Budget &gt; $500/month\n- Team familiar with Azure networking\n- Read-heavy workloads\n\n‚ùå Poor Fit:\n\n- Simple search requirements\n- Budget conscious\n- Need real-time updates\n- Heavy DELETE operations\n- Small team without Azure expertise\n\nBYOPI works, but it's more complex and expensive than initially expected. The lack of native DELETE support in ADF sink is a significant limitation that requires workarounds.\n\n###### Key Takeaways\n\n1. It works but requires significant effort\n2. DELETE (hard) operations need workarounds\n3. Costs will be higher than expected\n4. Complexity is substantial for a \"no-code\" solution\n5. Alternative solutions might be better for many scenarios\n\n#### Disclaimer:\n\nThe sample scripts provided in this article are provided AS IS without warranty of any kind. The author is not responsible for any issues, damages, or problems that may arise from using these scripts. Users should thoroughly test any implementation in their environment before deploying to production. Azure services and APIs may change over time, which could affect the functionality of the provided scripts. Always refer to the latest Azure documentation for the most up-to-date information.\n\nThanks for reading this blog! I hope you've found this approach of creating own private indexing solution for Azure AI Search (BYOPI) useful üòÄ",
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "FeedName": "Microsoft Tech Community",
  "OutputDir": "_community",
  "Description": "#### **Executive Summary**\n\n*Building a fully private search indexing solution using Azure Data Factory (ADF) to sync SQL Server data from private VM to Azure AI Search is achievable but comes with notable complexities and limitations.*\n\n*This blog shares my journey, discoveries, and honest assessment of the BYOPI (Build Your Own Private Indexer) architecture.*\n\nArchitectural flow:\n\n![]()\n\n####\n\n#### **Table of Contents**\n\n1. ##### *Overall Setup*\n2. ##### *How ADF works in this approach with Azure AI Search*\n3. ##### *Challenges - discovered*\n4. ##### *Pros and Cons: An Honest Assessment*\n5. ##### *Conclusion and Recommendations*\n\n####\n\n#### **1. Overall Setup:**\n\n###### *Phase 1*: Resource Group & Network Setup : create resource group and vNET (virtual network) in any region of your choice\n\n*Phase 2*: Deploy SQL Server VM:\n\n![]()\n\n*Phase 3*: Create Azure Services - ADF (Azure Data Factory), Azure AI Search and AKV (Azure Key Vault) service from portal or from your choice of deployment.\n\n*Phase 4:* Create Private Endpoints for all the services in their dedicated subnets:\n\n![]()\n\n*Phase 5*: Configure SQL Server on VM : connect to VM via bastion and setup database, tables & SP:\n\nSample metadata used as below:\n\n- CREATE DATABASE BYOPI\\_DB;\nGO\n\nUSE BYOPI\\_DB; GO\n\nCREATE TABLE Products ( ProductId INT IDENTITY(1,1) PRIMARY KEY, ProductName NVARCHAR(200) NOT NULL, Description NVARCHAR(MAX), Category NVARCHAR(100), Price DECIMAL(10,2), InStock BIT DEFAULT 1, Tags NVARCHAR(500), IsDeleted BIT DEFAULT 0, CreatedDate DATETIME DEFAULT GETDATE(), ModifiedDate DATETIME DEFAULT GETDATE() );\n\nCREATE TABLE WatermarkTable ( TableName NVARCHAR(100) PRIMARY KEY, WatermarkValue DATETIME );\n\nINSERT INTO WatermarkTable VALUES ('Products', '2024-01-01');\n\nCREATE PROCEDURE sp\\_update\\_watermark @TableName NVARCHAR(100), @NewWatermark DATETIME AS BEGIN UPDATE WatermarkTable SET WatermarkValue = @NewWatermark WHERE TableName = @TableName; END;\n\nINSERT INTO Products (ProductName, Description, Category, Price, Tags) VALUES ('Laptop Pro', 'High-end laptop', 'Electronics', 1299.99, 'laptop,computer'), ('Office Desk', 'Adjustable desk', 'Furniture', 599.99, 'desk,office'), ('Wireless Mouse', 'Bluetooth mouse', 'Electronics', 29.99, 'mouse,wireless');\n\n![]()\n\n![]()\n\n*Phase 6*: Install Self-Hosted Integration Runtime\n\n###### Create SHIR in ADF:\n\n1. Go to ADF resource in Azure Portal\n2. Click \"Open Azure Data Factory Studio\"\n- Note: You need to access from a VM in the same VNet or via VPN since ADF is private\n3. In ADF Studio, click Manage (toolbox icon)\n4. Select Integration runtimes ‚Üí \"+ New\"\n5. Select \"Azure, Self-Hosted\" ‚Üí \"Self-Hosted\"\n6. Name: SHIR-BYOPI or of your choice\n7. Click \"Create\"\n8. Copy Key1 (save it)\n\n###### Install SHIR on VM\n\n1. In the VM (via Bastion):\n2. Open browser, go to: https://www.microsoft.com/download/details.aspx?id=39717\n3. Download and install Integration Runtime\n4. During setup:\n- Launch Configuration Manager\n- Paste the Key1 from Step 14\n- Click \"Register\"\n- Wait for \"Connected\" status\n\n![]()\n\n![]()\n\n![]()\n\n![]()\n\n![]()\n\n![]()\n\n![]()\n\n*Phase 7*: Create Search Index through below powershell script and saving it as search\\_index.ps1\n- $searchService = \"search-byopi\"\n$apiKey = \"YOUR-ADMIN-KEY\"\n\n$headers = @{ 'api-key' = $apiKey 'Content-Type' = 'application/json' }\n\n$index = @{ name = \"products-index\" fields = @( @{name=\"id\"; type=\"Edm.String\"; key=$true} @{name=\"productName\"; type=\"Edm.String\"; searchable=$true} @{name=\"description\"; type=\"Edm.String\"; searchable=$true} @{name=\"category\"; type=\"Edm.String\"; filterable=$true; facetable=$true} @{name=\"price\"; type=\"Edm.Double\"; filterable=$true} @{name=\"inStock\"; type=\"Edm.Boolean\"; filterable=$true} @{name=\"tags\"; type=\"Collection(Edm.String)\"; searchable=$true} ) } | ConvertTo-Json -Depth 10\n\nInvoke-RestMethod ` -Uri \"https://$searchService.search.windows.net/indexes/products-index?api-version=2020-06-30\" ` -Method PUT ` -Headers $headers ` -Body $index\n\n![]()\n\n*Phase 8*: Configure AKV & ADF Components - Link AKV and ADF for secrets\n\nCreate Key Vault Secrets\n\n1. Navigate to kv-byopi (created AKV resource) in Portal\n2. Go to \"Access policies\"\n3. Click \"+ Create\"\n4. Select permissions: Get, List for secrets\n5. Select principal: adf-byopi-private\n6. Create\n7. Go to \"Secrets\" ‚Üí \"+ Generate/Import\":\n- Name: sql-password, Value:\n- Name: search-api-key, Value: Your search key\n\n###### Create Linked Services in ADF\n\nAccess ADF Studio from the VM (since it's private):\n\n1. Key Vault Linked Service:\n\n1. Manage ‚Üí Linked services ‚Üí \"+ New\"\n2. Search \"Azure Key Vault\"\n3. Configure:\n- Name: LS\\_KeyVault\n- Azure Key Vault: kv-byopi\n- Integration runtime: AutoResolveIntegrationRuntime\n4. Test connection ‚Üí Create\n\n1. SQL Server Linked Service:\n\n1. \"+ New\" ‚Üí \"SQL Server\"\n2. Configure:\n- Name: LS\\_SqlServer\n- Connect via: SHIR-BYOPI\n- Server name: localhost\n- Database: BYOPI\\_DB\n- Authentication: SQL Authentication\n- User: sqladmin\n- Password: Select from Key Vault ‚Üí LS\\_KeyVault ‚Üí sql-password\n3. Test ‚Üí Create\n\n1. Azure Search Linked Service:\n\n1. \"+ New\" ‚Üí \"Azure Search\"\n2. Configure:\n- Name: LS\\_AzureSearch\n- URL: https://search-byopi.search.windows.net\n- Connect via: SHIR-BYOPI - Important - use SHIR\n- API Key: From Key Vault ‚Üí LS\\_KeyVault ‚Üí search-api-key\n3. Test ‚Üí Create\n\n![]()\n\n![]()\n\n*Phase 9*: Create ADF Datasets and PipelineCreate Datasets\n\nSQL Products Dataset:\n\n1. Author ‚Üí Datasets ‚Üí \"+\" ‚Üí \"New dataset\"\n2. Select \"SQL Server\" ‚Üí Continue\n3. Select \"Table\" ‚Üí Continue\n4. Properties:\n- Name: DS\\_SQL\\_Products\n- Linked service: LS\\_SqlServer\n- Table: Select Products\n5. click OK\n\n1. Watermark Dataset:\n\n1. Repeat with:\n- Name: DS\\_SQL\\_Watermark\n- Table: WatermarkTable\n\n1. Search Dataset:\n\n1. \"+\" ‚Üí \"Azure Search\"\n2. Properties:\n- Name: DS\\_Search\\_Index\n- Linked service: LS\\_AzureSearch\n- Index name: products-index\n\n###### Create Pipeline\n\n1. Author ‚Üí Pipelines ‚Üí \"+\" ‚Üí \"Pipeline\"\n2. Name: PL\\_BYOPI\\_Private\n3. From Activities ‚Üí General, drag \"Lookup\" activity\n4. Configure Lookup 1:\n- Name: LookupOldWatermark\n- Settings:\n- Source dataset: DS\\_SQL\\_Watermark\n- Query: below\n\nsql\n\nSELECT WatermarkValue FROM WatermarkTable WHERE TableName='Products'\n\n- \\*\\*First row only\\*\\*: ‚úì\n\n1. Add another Lookup:\n\n- Name: LookupNewWatermark\n- Query: below\n\nsql\n\nSELECT MAX(ModifiedDate) as NewWatermark FROM Products\n\n1. Add Copy Data activity:\n- Name: CopyToSearchIndex\n- Source:\n- Dataset: DS\\_SQL\\_Products\n- Query:\n\nsql\n\nSELECT CAST(ProductId AS NVARCHAR(50)) as id, ProductName as productName, Description as description, Category as category, Price as price, InStock as inStock, Tags as tags, CASE WHEN IsDeleted = 1 THEN 'delete' ELSE 'upload' END as [@search.action] FROM Products WHERE ModifiedDate > '@{activity('LookupOldWatermark').output.firstRow.WatermarkValue}' AND ModifiedDate\n\n- Sink:\n- Dataset: DS\\_Search\\_Index\n- Write behavior: Merge\n- Batch size: 1000\n\n1. Add Stored Procedure activity:\n- Name: UpdateWatermark\n- SQL Account: LS\\_SqlServer\n- Stored procedure: sp\\_update\\_watermark\n- Parameters:\n- TableName: Products\n- NewWatermark: @{activity('LookupNewWatermark').output.firstRow.NewWatermark}\n2. Connect activities with success conditions\n\n###### *Phase 10*: Test and Schedule\n\n###### Test Pipeline\n\n1. ###### Click \"Debug\" in pipeline\n2. ###### Monitor in Output panel\n3. ###### Check for green checkmarks\n\n###### Create Trigger\n\n1. ###### In pipeline, click \"Add trigger\" ‚Üí \"New/Edit\"\n2. ###### Click \"+ New\"\n3. ###### Configure:\n\n- ###### Name: TR\\_Hourly\n- ###### Type: Schedule\n- ###### Recurrence: Every 1 Hour\n4. ###### OK ‚Üí Publish All\n\n###### Monitor\n\n1. ###### Go to Monitor tab\n2. ###### View Pipeline runs\n3. ###### Check Trigger runs\n\n![]()\n\n![]()\n\nYour pipeline should look like this:\n\n![]()\n\n*Phase 11*: Validation & Testing\n\n###### Verify Private Connectivity\n\n###### From the VM, run PowerShell:\n- # Test DNS resolution (should return private IPs)\nnslookup adf-byopi-private.datafactory.azure.net\n# Should show private IP like : 10.0.2.x\n\nnslookup search-byopi.search.windows.net\n# Should show private IP like : 10.0.2.x\n\nnslookup kv-byopi.vault.azure.net\n# Should show private IP like : 10.0.2.x\n\n# Test Search\n$headers = @{ 'api-key' = 'YOUR-KEY' } Invoke-RestMethod -Uri \"https://search-byopi.search.windows.net/indexes/products-index/docs?`$count=true&api-version=2020-06-30\" -Headers $headers\n\nTest Data Sync (adding few records) and verify in search index:\n- -- Add test record INSERT INTO Products (ProductName, Description, Category, Price, Tags) VALUES ('Test Product Private', 'Testing private pipeline', 'Test', 199.99, 'test,private'); -- Trigger pipeline manually or wait for schedule -- Then verify in Search index\n\n#### **2. How ADF works in this approach with Azure AI search:**\n\nAzure AI Search uses a REST API for indexing or called as uploading. When ADF sink uploads data to AI Search, it's actually making HTTP POST requests: for example -\n\nPOST https://search-byopi.search.windows.net/indexes/products-index/docs/index?api-version=2020-06-30 Content-Type: application/json api-key: YOUR-ADMIN-KEY\n\n{ \"value\": [ { \"@search.action\": \"upload\", \"id\": \"1\", \"productName\": \"Laptop\", \"price\": 999.99 }, { \"@search.action\": \"delete\", \"id\": \"2\" } ] }\n\nDelete action used here is soft delete and not hard delete.\n\npipeline query:\n\nSELECT CAST(ProductId AS NVARCHAR(50)) as id, -- Renamed to match index field ProductName as productName, -- Renamed to match index field Description as description, Category as category, Price as price, InStock as inStock, Tags as tags, CASE WHEN IsDeleted = 1 THEN 'delete' ELSE 'upload' END as [@search.action] -- Special field with @ prefix FROM Products WHERE ModifiedDate > '2024-01-01' ```\n\nReturns this resultset: ``` id | productName | description | category | price | inStock | tags | @search.action ----|----------------|------------------|-------------|--------|---------|----------------|--------------- 1 | Laptop Pro | High-end laptop | Electronics | 1299 | 1 | laptop,computer| upload 2 | Office Chair | Ergonomic chair | Furniture | 399 | 1 | chair,office | upload 3 | Deleted Item | Old product | Archive | 0 | 0 | old | delete\n\n##### The @search.action Field - The Magic Control\n\nThis special field tells Azure AI Search what to do with each document:\n\n| @search.action | What It Does | When to Use | What Happens If Document... | | --- | --- | --- | --- | | upload | Insert OR Update | Most common - upsert operation | Exists: Updates it<br>Doesn't exist: Creates it | | merge | Update only | When you know it exists | Exists: Updates specified fields<br>Doesn't exist: ERROR | | mergeOrUpload | Update OR Insert | Safe update | Exists: Updates fields<br>Doesn't exist: Creates it | | delete | Remove from index | To remove documents | Exists: Deletes it<br>Doesn't exist: Ignores (no error) |\n\nADF automatically converts SQL results to JSON format required by Azure Search:\n- {\n\"value\": [ { \"@search.action\": \"upload\", \"id\": \"1\", \"productName\": \"Laptop Pro\", \"description\": \"High-end laptop\", \"category\": \"Electronics\", \"price\": 1299.00, \"inStock\": true, \"tags\": \"laptop,computer\" }, { \"@search.action\": \"upload\", \"id\": \"2\", \"productName\": \"Office Chair\", \"description\": \"Ergonomic chair\", \"category\": \"Furniture\", \"price\": 399.00, \"inStock\": true, \"tags\": \"chair,office\" }, { \"@search.action\": \"delete\", \"id\": \"3\" // For delete, only ID is needed } ] }\n\nADF doesn't send all records at once. It batches them based on writeBatchSize and each batch is a separate HTTP POST to Azure Search\n\nHow ADF will detect new changes and run batches:\n\nWatermark will be updated after each successful ADF run to detect new changes as below:\n\n![]()\n\nHandling different scenarios:\n\n*Scenario 1: No Changes Between Runs:*\n\nRun at 10:00 AM:\n- Old Watermark: 09:45:00\n- New Watermark: 10:00:00\n- Query: WHERE ModifiedDate > '09:45' AND - Result: 0 rows\n- Action: Still update watermark to 10:00\n- Why: Prevents reprocessing if changes come later\n\n*Scenario 2: Bulk Insert Happens:*\n\nSomeone inserts 5000 records at 10:05 AM\n\nRun at 10:15 AM:\n- Old Watermark: 10:00:00\n- New Watermark: 10:15:00\n- Query: WHERE ModifiedDate > '10:00' AND - Result: 5000 rows\n- Action: Process all 5000, update watermark to 10:15\n\n*Scenario 3: Pipeline Fails*\n\nRun at 10:30 AM:\n- Old Watermark: 10:15:00 (unchanged from last success)\n- Pipeline fails during Copy activity\n- Watermark NOT updated (still 10:15:00)\n\nNext Run at 10:45 AM:\n- Old Watermark: 10:15:00 (still the last successful)\n- New Watermark: 10:45:00\n- Query: WHERE ModifiedDate > '10:15' AND - Result: Gets ALL changes from 10:15 to 10:45 (30 minutes of data)\n- No data loss!\n\nNote:\n\nThere is still room for improvement by refining this logic to handle more advanced scenarios. However, I have not examined the logic in depth, as the goal here is to review how the overall setup functions, identify its limitations, and compare it with the indexing solutions available in AI Search.\n\n#### **3. Challenges - disovered:**\n\nWhen I tried to set out to build a private search indexer for SQL Server data residing on an Azure VM with no public IP, the solution seemed straightforward: use Azure Data Factory to orchestrate the data movement to Azure AI Search.\n\nThe materials made it sound simple. The reality? It's possible, but the devil is in the details.\n\nWhat We Needed: ‚úÖ SQL Server on private VM (no public IP) ‚úÖ Azure AI Search with private endpoint ‚úÖ No data over public internet ‚úÖ Support for full CRUD operations ‚úÖ Near real-time synchronization ‚úÖ No-code/low-code solution\n\nReality Check: ‚ö†Ô∏è DELETE operations not natively supported in ADF sink ‚ö†Ô∏è Complex networking requirements ‚ö†Ô∏è Higher costs than expected ‚ö†Ô∏è Significant setup complexity ‚úÖ But it IS possible with workarounds\n\n###### Components Required\n\n- Azure VM: ~$150/month (D4s\\_v3)\n- Self-Hosted Integration Runtime: Free (runs on VM)\n- Private Endpoints: ~$30/month (approx 3 endpoints)\n- Azure Data Factory: ~$15-60/month (depends on frequency)\n- Azure AI Search: ~$75/month (Basic tier)\n- Total: ~$270-315/month\\*\\*\n\nThe DELETE Challenge:\n\nDespite Azure AI Search REST API fully supporting delete operations via @search.action, ADF's native Azure Search sink does NOT support delete operations. This isn't clearly documented and catches many architects off guard.\n\n-- This SQL query with delete action SELECT ProductId as id, CASE WHEN IsDeleted = 1 THEN 'delete' ELSE 'upload' END as [@search.action] FROM Products\n\n-- Will NOT delete documents in Azure Search when using Copy activity -- The @search.action = 'delete' is ignored by ADF sink!\n\nNevertheless, there is a workaround using the Web Activity approach or by calling the REST API from the ADF side to perform the delete operation.\n- {\n\"name\": \"DeleteViaREST\", \"type\": \"Web\", \"typeProperties\": { \"url\": \"https://search.windows.net/indexes/index/docs/index\", \"method\": \"POST\", \"body\": { \"value\": [ {\"@search.action\": \"delete\", \"id\": \"123\"} ] } } }\n\n###### Development Challenges\n\n1. No Direct Portal Access: With ADF private, you need:\n- Jump box in the same VNet\n- VPN connection\n- Bastion for access\n2. Testing Complexity:\n- Can't use Postman from local machine\n- Need to test from within VNet\n- Debugging requires multiple tools\n\n#### **4. Pros and Cons: An Honest Assessment:**\n\n###### Pros:\n\n1. Security: Complete network isolation\n2. Compliance: Meets strict requirements\n3. No-Code: Mostly configuration-based\n4. Scalability: Can handle large datasets\n5. Monitoring: Built-in ADF monitoring\n6. Managed Service: Microsoft handles updates\n\n###### Cons:\n\n1. DELETE Complexity: Not natively supported\n2. Cost: Higher than expected\n3. Setup Complexity: Many moving parts\n4. Debugging: Difficult with private endpoints\n\nHidden Gotchas:\n\n- SHIR requires Windows VM (Linux in preview)\n- Private endpoint DNS propagation delays\n- ADF Studio timeout with private endpoints\n- SHIR auto-update can break pipelines\n\n#### **5. Conclusion and Recommendations:**\n\n###### When to Use BYOPI:\n\n‚úÖ **** Good Fit:\n\n- Strict security requirements\n- Needs indexing from an un-supported scenarios for example SQL server residing on private VM\n- Budget > $500/month\n- Team familiar with Azure networking\n- Read-heavy workloads\n\n‚ùå Poor Fit:\n\n- Simple search requirements\n- Budget conscious\n- Need real-time updates\n- Heavy DELETE operations\n- Small team without Azure expertise\n\nBYOPI works, but it's more complex and expensive than initially expected. The lack of native DELETE support in ADF sink is a significant limitation that requires workarounds.\n\n###### Key Takeaways\n\n1. It works but requires significant effort\n2. DELETE (hard) operations need workarounds\n3. Costs will be higher than expected\n4. Complexity is substantial for a \"no-code\" solution\n5. Alternative solutions might be better for many scenarios\n\n#### Disclaimer:\n\nThe sample scripts provided in this article are provided AS IS without warranty of any kind. The author is not responsible for any issues, damages, or problems that may arise from using these scripts. Users should thoroughly test any implementation in their environment before deploying to production. Azure services and APIs may change over time, which could affect the functionality of the provided scripts. Always refer to the latest Azure documentation for the most up-to-date information.\n\nThanks for reading this blog! I hope you've found this approach of creating own private indexing solution for Azure AI Search (BYOPI) useful üòÄ",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Tags": [],
  "Author": "ani_ms_emea",
  "Title": "BYOPI - Design your own custom private AI Search indexer with no code ADF (SQLServer on VM example)",
  "PubDate": "2025-10-25T15:57:45+00:00",
  "ProcessedDate": "2025-10-25 16:05:15"
}
