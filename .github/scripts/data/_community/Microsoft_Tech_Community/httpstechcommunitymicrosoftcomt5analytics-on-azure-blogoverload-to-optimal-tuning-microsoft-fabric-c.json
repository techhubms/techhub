{
  "PubDate": "2025-10-28T16:48:43+00:00",
  "OutputDir": "_community",
  "Author": "Rafia_Aqil",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "Title": "Overload to Optimal: Tuning Microsoft Fabric Capacity",
  "Link": "https://techcommunity.microsoft.com/t5/analytics-on-azure-blog/overload-to-optimal-tuning-microsoft-fabric-capacity/ba-p/4464639",
  "ProcessedDate": "2025-10-28 17:06:47",
  "EnhancedContent": "Co-Authored by: Daya Ram, Sr. Cloud Solutions Architect\n\nOptimizing Microsoft Fabric capacity is both a performance and cost exercise. By diagnosing workloads, tuning cluster and Spark settings, and applying data best practices, teams can reduce run times, avoid throttling, and lower total cost of ownership—without compromising SLAs. Use Fabric’s built-in observability (Monitoring Hub, Capacity Metrics, Spark UI) to identify hot spots and then apply cluster- and data-level remediations. For capacity planning and sizing guidance, see [Plan your capacity size.](https://learn.microsoft.com/en-us/fabric/enterprise/plan-capacity)\n\n#### **Options to Diagnose Capacity Issues**\n\n##### **1) Monitoring Hub — Start with the Story of the Run**\n\n**[What to use it for:](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-monitoring-overview)******Browse Spark activity across applications (notebooks, Spark Job Definitions, and pipelines). Quickly surface long‑running or anomalous runs; view read/write bytes, idle time, core allocation, and utilization.\n\n**[How to use it](https://learn.microsoft.com/en-us/fabric/data-engineering/browse-spark-applications-monitoring-hub)**\n\n- - From the Fabric portal, open Monitoring (Monitor Hub).\n\n- - Select a Notebook or Spark Job Definition to run and choose Historical Runs.\n\n- - Inspect the Run Duration chart; click on a run to see read/write bytes, idle time, core allocation, overall utilization, and other Spark metrics.\n\n**What to look for**\n\n- - Use the [guide: application detail monitoring](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-detail-monitoring)to review and monitor your application.\n\n##### **2) Capacity Metrics App — Measure the Whole Environment**\n\n**[What to use it for:](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app)******Review capacity-wide utilization and system events (overloads, queueing); compare utilization across time windows and identify sustained peaks.\n\n**[How to use it](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-install?tabs=1st)**\n\n- - Open the Microsoft Fabric Capacity Metrics app for your capacity.\n\n- - Review the Compute page (ribbon charts, utilization trends) and the System events tab to see overload or throttling windows.\n\n- - Use the Timepoint page to drill into a 30‑second interval and see which operations consumed the most compute.\n\n**[What to look for](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-compute-page)**\n\n- - Use the [Troubleshooting guide:](https://learn.microsoft.com/en-us/fabric/enterprise/capacity-planning-troubleshoot-consumption) Monitor and identify capacity usage to pinpoint top CU‑consuming items.\n\n##### **3) Spark UI — Diagnose at Deeper Level**\n\n**Why it matters:** [Spark UI](https://learn.microsoft.com/en-us/fabric/data-engineering/apache-spark-history-server#open-the-spark-web-ui-from-progress-indicator-notebook) exposes skew, shuffle, memory pressure, and long stages. Use it after Monitoring Hub/Capacity Metrics to pinpoint the problematic job.\n\n**[Key tabs to inspect](https://spark.apache.org/docs/latest/web-ui.html)**\n\n- - Stages: uneven task durations (data skew), heavy shuffle read/write, large input/output volumes.\n\n- - Executors: storage memory, task time (GC), shuffle metrics. High GC or frequent spills indicate memory tuning is needed.\n\n- - Storage: which RDDs/cached tables occupy memory; any disk spill.\n\n- - Jobs: long‑running jobs and gaps in the timeline (driver compilation, non‑Spark code, driver overload).\n\n**What to look for**\n\nSet via [environment Spark propertie](https://learn.microsoft.com/en-us/fabric/data-engineering/create-and-use-environment)s or session config.\n\n- - Data skew, Memory usage, High/Low Shuffles: Adjust Apache Spark settings: i.e. spark.ms.autotune.enabled,  spark.task.cpus and spark.sql.shuffle.partitions.\n\n#### **Section 2: Remediation and Optimization Suggestions**\n\n##### **A) Cluster & Workspace Settings**\n\n##### **[Runtime & Native Execution Engine (NEE)](https://learn.microsoft.com/en-us/fabric/data-engineering/native-execution-engine-overview?tabs=sparksql)**\n\n- - Use Fabric Runtime 1.3 (Spark 3.5, Delta 3.2) and enable the Native Execution Engine to boost performance; enable at the environment level under Spark compute → Acceleration.\n\n##### **Starter Pools vs. Custom Pools**\n\n- - [Starter Pool:](https://learn.microsoft.com/en-us/fabric/data-engineering/configure-starter-pools) prehydrated, medium‑size pools; fast session starts, good for dev/quick runs.\n\n- - [Custom Pools](https://learn.microsoft.com/en-us/fabric/data-engineering/create-custom-spark-pools): size nodes, enable autoscale, dynamic executors. Create via workspace Spark Settings (requires capacity admin to enable workspace customization).\n\n##### **High Concurrency Session Sharing**\n\n- - Enable [High Concurrency](https://learn.microsoft.com/en-us/fabric/data-engineering/high-concurrency-overview) to share Spark Sessions across notebooks (and pipelines) to reduce session startup latency and cost; use session tags in pipelines to group notebooks.\n\n##### **[Autotune for Spark](https://learn.microsoft.com/en-us/fabric/data-engineering/autotune?tabs=sparksql)**\n\nEnable Autotune (spark.ms.autotune.enabled = true) to auto‑adjust per‑query:\n\n- - spark.sql.shuffle.partitions\n\n- - Spark.sql.autoBroadcastJoinThreshold\n\n- - spark.sql.files.maxPartitionBytes.\n\nAutotune is disabled by default and is in preview; enable per environment or session.\n\n##### **B) Data‑level best practices**\n\nMicrosoft Fabric offers several approaches to maintain optimal file sizes in Delta tables, review documentation here: [Table Compaction - Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/data-engineering/table-compaction?tabs=sparksql#compaction-methods).\n\n**[Intelligent Cache](https://learn.microsoft.com/en-us/fabric/data-engineering/intelligent-cache)**\n\n- - Enabled by default (Runtime 1.1/1.2) for Spark pools: caches frequently read files at node level for Delta/Parquet/CSV; improves subsequent read performance and TCO.\n\n**[OPTIMIZE & Z‑Order](https://learn.microsoft.com/en-us/fabric/data-engineering/table-compaction?tabs=sparksql#optimize-with-z-order)**\n\n- - Run OPTIMIZE regularly to rewrite files and improve file layout.\n\n**[V‑Order](https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql)**\n\n- - V‑Order (disabled by default in new workspaces) can accelerate reads for read‑heavy workloads; enable via spark.sql.parquet.vorder.default = true.\n\n**Vacuum**\n\n- - Run VACUUM to remove unreferenced files (stale data); default retention is 7 days; align retention across OneLake to control storage costs and maintain time travel.\n\n#### **Collaboration & Next Steps**\n\n##### **Engage Data Engineering Team to Define an Optimization Playbook**\n\n- - Start with reviewing [capacity sizing guidance,](https://learn.microsoft.com/en-us/fabric/enterprise/plan-capacity) cluster‑level optimizations (runtime/NEE, pools, concurrency, Autotune) and then target data improvements (Z‑order, compaction, caching, query refactors).\n\n- - Triage: [Monitor Hub](https://learn.microsoft.com/en-us/fabric/admin/monitoring-hub) → [Capacity Metrics](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-compute-page) → [Spark UI](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-detail-monitoring)to map workloads and identify high‑impact jobs, and workloads causing throttling.\n\n- - Schedule: [Operationalize maintenance:](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance) OPTIMIZE (full or selective) during off‑peak windows; enable Auto Compaction for micro‑batch/streaming writes; add VACUUM to your cadence with agreed retention.\n- Add regular [code review sessions](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-best-practices-basics#udf-best-practices) to ensure consistent performance patterns.\n\n- - Fix: [Adjust pool sizing](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-job-concurrency-and-queueing) or concurrency; [enable Autotune;](https://learn.microsoft.com/en-us/fabric/data-engineering/autotune?tabs=sparksql) tune shuffle partitions; refactor problematic queries; [re‑run compaction](https://learn.microsoft.com/en-us/fabric/data-engineering/table-compaction?tabs=sparksql).\n\n- - [Verify](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-detail-monitoring): Re‑run the job and change, i.e. reduced run time, lower shuffle, improved utilization.\n\nUpdated Oct 28, 2025\n\nVersion 1.0\n\n[analytics](/tag/analytics?nodeId=board%3AAnalyticsonAzure)\n\n[azure](/tag/azure?nodeId=board%3AAnalyticsonAzure)\n\n[microsoft fabric](/tag/microsoft%20fabric?nodeId=board%3AAnalyticsonAzure)\n\n[spark](/tag/spark?nodeId=board%3AAnalyticsonAzure)\n\n[!\\[Rafia_Aqil&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/dS0zMDcyNDQwLUZEQmYzMQ?image-coordinates=60%2C75%2C544%2C559&amp;image-dimensions=50x50)](/users/rafia_aqil/3072440) [Rafia_Aqil](/users/rafia_aqil/3072440) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined June 13, 2025\n\n[View Profile](/users/rafia_aqil/3072440)\n\n/category/azure/blog/analyticsonazure [Analytics on Azure Blog](/category/azure/blog/analyticsonazure) Follow this blog board to get notified when there's new activity",
  "FeedName": "Microsoft Tech Community",
  "Description": "Co-Authored by: Daya Ram, Sr. Cloud Solutions Architect\n\nOptimizing Microsoft Fabric capacity is both a performance and cost exercise. By diagnosing workloads, tuning cluster and Spark settings, and applying data best practices, teams can reduce run times, avoid throttling, and lower total cost of ownership—without compromising SLAs. Use Fabric’s built-in observability (Monitoring Hub, Capacity Metrics, Spark UI) to identify hot spots and then apply cluster- and data-level remediations. For capacity planning and sizing guidance, see [Plan your capacity size.](https://learn.microsoft.com/en-us/fabric/enterprise/plan-capacity)\n\n#### **Options to Diagnose Capacity Issues**\n\n##### **1) Monitoring Hub — Start with the Story of the Run**\n\n![]()\n\n**[What to use it for:](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-monitoring-overview)** **** Browse Spark activity across applications (notebooks, Spark Job Definitions, and pipelines). Quickly surface long‑running or anomalous runs; view read/write bytes, idle time, core allocation, and utilization.\n\n**[How to use it](https://learn.microsoft.com/en-us/fabric/data-engineering/browse-spark-applications-monitoring-hub)**\n\n- - From the Fabric portal, open Monitoring (Monitor Hub).\n\n- - Select a Notebook or Spark Job Definition to run and choose Historical Runs.\n\n- - Inspect the Run Duration chart; click on a run to see read/write bytes, idle time, core allocation, overall utilization, and other Spark metrics.\n\n**What to look for**\n\n- - Use the [guide: application detail monitoring](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-detail-monitoring)to review and monitor your application.\n\n##### **2) Capacity Metrics App — Measure the Whole Environment**\n\n![]()\n\n**[What to use it for:](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app)** **** Review capacity-wide utilization and system events (overloads, queueing); compare utilization across time windows and identify sustained peaks.\n\n**[How to use it](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-install?tabs=1st)**\n\n- - Open the Microsoft Fabric Capacity Metrics app for your capacity.\n\n- - Review the Compute page (ribbon charts, utilization trends) and the System events tab to see overload or throttling windows.\n\n- - Use the Timepoint page to drill into a 30‑second interval and see which operations consumed the most compute.\n\n**[What to look for](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-compute-page)**\n\n- - Use the [Troubleshooting guide:](https://learn.microsoft.com/en-us/fabric/enterprise/capacity-planning-troubleshoot-consumption) Monitor and identify capacity usage to pinpoint top CU‑consuming items.\n\n##### **3) Spark UI — Diagnose at Deeper Level**\n\n![]()\n\n**Why it matters:** [Spark UI](https://learn.microsoft.com/en-us/fabric/data-engineering/apache-spark-history-server#open-the-spark-web-ui-from-progress-indicator-notebook) exposes skew, shuffle, memory pressure, and long stages. Use it after Monitoring Hub/Capacity Metrics to pinpoint the problematic job.\n\n**[Key tabs to inspect](https://spark.apache.org/docs/latest/web-ui.html)**\n\n- - Stages: uneven task durations (data skew), heavy shuffle read/write, large input/output volumes.\n\n- - Executors: storage memory, task time (GC), shuffle metrics. High GC or frequent spills indicate memory tuning is needed.\n\n- - Storage: which RDDs/cached tables occupy memory; any disk spill.\n\n- - Jobs: long‑running jobs and gaps in the timeline (driver compilation, non‑Spark code, driver overload).\n\n**What to look for**\n\n![]()\n\nSet via [environment Spark propertie](https://learn.microsoft.com/en-us/fabric/data-engineering/create-and-use-environment)s or session config.\n\n- - Data skew, Memory usage, High/Low Shuffles: Adjust Apache Spark settings: i.e. spark.ms.autotune.enabled, spark.task.cpus and spark.sql.shuffle.partitions.\n\n#### **Section 2: Remediation and Optimization Suggestions**\n\n##### **A) Cluster & Workspace Settings**\n\n##### **[Runtime & Native Execution Engine (NEE)](https://learn.microsoft.com/en-us/fabric/data-engineering/native-execution-engine-overview?tabs=sparksql)**\n\n![]()\n- - Use Fabric Runtime 1.3 (Spark 3.5, Delta 3.2) and enable the Native Execution Engine to boost performance; enable at the environment level under Spark compute → Acceleration.\n\n##### **Starter Pools vs. Custom Pools**\n\n![]()\n- - [Starter Pool:](https://learn.microsoft.com/en-us/fabric/data-engineering/configure-starter-pools) prehydrated, medium‑size pools; fast session starts, good for dev/quick runs.\n\n- - [Custom Pools](https://learn.microsoft.com/en-us/fabric/data-engineering/create-custom-spark-pools): size nodes, enable autoscale, dynamic executors. Create via workspace Spark Settings (requires capacity admin to enable workspace customization).\n\n##### **High Concurrency Session Sharing**\n\n![]()\n- - Enable [High Concurrency](https://learn.microsoft.com/en-us/fabric/data-engineering/high-concurrency-overview) to share Spark Sessions across notebooks (and pipelines) to reduce session startup latency and cost; use session tags in pipelines to group notebooks.\n\n##### **[Autotune for Spark](https://learn.microsoft.com/en-us/fabric/data-engineering/autotune?tabs=sparksql)**\n\n![]()\n\nEnable Autotune (spark.ms.autotune.enabled = true) to auto‑adjust per‑query:\n\n- - spark.sql.shuffle.partitions\n\n- - Spark.sql.autoBroadcastJoinThreshold\n\n- - spark.sql.files.maxPartitionBytes.\n\n![]()\n\nAutotune is disabled by default and is in preview; enable per environment or session.\n\n##### **B) Data‑level best practices**\n\nMicrosoft Fabric offers several approaches to maintain optimal file sizes in Delta tables, review documentation here: [Table Compaction - Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/data-engineering/table-compaction?tabs=sparksql#compaction-methods).\n\n**[Intelligent Cache](https://learn.microsoft.com/en-us/fabric/data-engineering/intelligent-cache)**\n\n- - Enabled by default (Runtime 1.1/1.2) for Spark pools: caches frequently read files at node level for Delta/Parquet/CSV; improves subsequent read performance and TCO.\n\n**[OPTIMIZE & Z‑Order](https://learn.microsoft.com/en-us/fabric/data-engineering/table-compaction?tabs=sparksql#optimize-with-z-order)**\n\n- - Run OPTIMIZE regularly to rewrite files and improve file layout.\n\n**[V‑Order](https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql)**\n\n- - V‑Order (disabled by default in new workspaces) can accelerate reads for read‑heavy workloads; enable via spark.sql.parquet.vorder.default = true.\n\n**Vacuum**\n\n- - Run VACUUM to remove unreferenced files (stale data); default retention is 7 days; align retention across OneLake to control storage costs and maintain time travel.\n\n#### **Collaboration & Next Steps**\n\n##### **Engage Data Engineering Team to Define an Optimization Playbook**\n\n- - Start with reviewing [capacity sizing guidance,](https://learn.microsoft.com/en-us/fabric/enterprise/plan-capacity) cluster‑level optimizations (runtime/NEE, pools, concurrency, Autotune) and then target data improvements (Z‑order, compaction, caching, query refactors).\n\n- - Triage: [Monitor Hub](https://learn.microsoft.com/en-us/fabric/admin/monitoring-hub) → [Capacity Metrics](https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app-compute-page) → [Spark UI](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-detail-monitoring)to map workloads and identify high‑impact jobs, and workloads causing throttling.\n\n- - Schedule: [Operationalize maintenance:](https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance) OPTIMIZE (full or selective) during off‑peak windows; enable Auto Compaction for micro‑batch/streaming writes; add VACUUM to your cadence with agreed retention.\n- Add regular [code review sessions](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-best-practices-basics#udf-best-practices) to ensure consistent performance patterns.\n\n- - Fix: [Adjust pool sizing](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-job-concurrency-and-queueing) or concurrency; [enable Autotune;](https://learn.microsoft.com/en-us/fabric/data-engineering/autotune?tabs=sparksql) tune shuffle partitions; refactor problematic queries; [re‑run compaction](https://learn.microsoft.com/en-us/fabric/data-engineering/table-compaction?tabs=sparksql).\n\n- - [Verify](https://learn.microsoft.com/en-us/fabric/data-engineering/spark-detail-monitoring): Re‑run the job and change, i.e. reduced run time, lower shuffle, improved utilization.",
  "Tags": [],
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure"
}
