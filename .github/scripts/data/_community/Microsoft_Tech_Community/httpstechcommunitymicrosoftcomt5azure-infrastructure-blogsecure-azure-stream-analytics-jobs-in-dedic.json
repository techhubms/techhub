{
  "FeedUrl": "https://techcommunity.microsoft.com/t5/s/gxcuf89792/rss/Category?category.id=Azure",
  "FeedLevelAuthor": "rss.livelink.threads-in-node",
  "PubDate": "2025-11-17T10:31:44+00:00",
  "Title": "Secure Azure Stream Analytics Jobs in Dedicated Clusters Using Managed PE for Blob Input/Output",
  "Author": "PratibhaShenoy",
  "Tags": [],
  "EnhancedContent": "## This article provides a hands-on guide for IT administrators to configure an Azure Stream Analytics Job running in a Dedicated Stream Analytics Cluster with Managed Private Endpoints for secure Blob Storage connectivity. It uses Managed Identity for authentication, ensures private networking, and includes a sample job for testing with step-by-step instructions and best practices.\nThis approach helps organizations meet zero-trust and compliance requirements by eliminating public network exposure.\n\n# Introduction\n\nModern data pipelines demand security and compliance. Azure Stream Analytics supports **Dedicated Clusters** and **Managed Private Endpoints**, enabling jobs to run in isolated environments and connect privately to Azure resources without exposing them to the public internet.\n\n**Key benefits:**\n\n- **Private connectivity** via Azure Private Link.\n- **Managed Identity** for secure, keyless authentication.\n- **Dedicated clusters** for predictable performance and isolation.\n\n# Architecture Overview\n\n# Prerequisites\n\n- Azure subscription.\n- **Blob Storage** with Public Network Access disabled and two containers configured for Input and Output.\n- **User Assigned Managed Identity** created.\n\n# Implementation Steps\n\n## 1. Assign Managed Identity\n\n1. Navigate to **Storage Account → Access Control (IAM)**.\n2. Add role assignment:\n- **Role**: Storage Blob Data Contributor\n- **Scope**: Storage Account\n- **Assign to**: Your User Assigned Managed Identity.\n\n**Note:** In this example, the role assignment is applied at the **Storage Account level**, which grants access to all containers within the account. If needed, you can scope the assignment to an individual container for more granular control.\n\n## 2. Configure Stream Analytics Job\n\n1. In the **Azure portal,** go to **Stream Analytics Jobs** and create a new job if one does not already exist:\n\n- **Name:** e.g., stream-job\n- **Hosting environment:** Cloud\n- **Streaming units:** 1\n2. **Managed Identity:** Enable Managed Identity and assign the User Assigned Managed Identity to the job.\n\n## 3. Configure Stream Analytics Cluster\n\nThe Stream Analytics Cluster resource provides dedicated compute for running multiple jobs securely and at scale.\n\n1. In **Azure Portal**, go to **Stream Analytics Clusters**.\n2. Select your cluster or create a new one:\n- **Name**: e.g., stream-cluster\n- **Streaming unit details**: 12\n- **Location**: Same as your Blob Storage and Stream Analytics Job\n\n## 4. Add Stream Analytics Job to Cluster\n\nOnce the Stream Analytics Cluster resource is provisioned, from **Stream Analytics Cluster → Settings → Stream Analytics Jobs → add the Stream Analytics Job.**\n\n## 5. Add Managed Private Endpoint\n\nFrom **Stream Analytics Cluster → Settings → Managed Private Endpoints → add a new Managed Private Endpoint.**\n\n- - Select **Blob Storage** as the target resource.\n- Approve the Private Endpoint connection on the target resource.\n- Ensure Managed Private Endpoint setup is completed.\n\n## 6. Configure Blob Input\n\n- **Input alias**: InputStream\n- **Container**: input-container\n- **Event serialization**: JSON\n- **Encoding**: UTF-8\n\n## 7. Configure Blob Output\n\n- **Output alias**: BlobOutput\n- **Container**: output-container\n- **Path pattern**: output/{date}/{time}\n- **Serialization**: JSON\n\n## 8. Prepare Sample Input\n\n- **Create** *sample-input.json*:\n\n``` [ { \"DeviceId\": \"sensor-001\", \"Temperature\": 28.5, \"Humidity\": 65, \"EventEnqueuedUtcTime\": \"2025-10-30T10:00:00Z\" }, { \"DeviceId\": \"sensor-002\", \"Temperature\": 30.2, \"Humidity\": 60, \"EventEnqueuedUtcTime\": \"2025-10-30T10:01:00Z\" }, { \"DeviceId\": \"sensor-001\", \"Temperature\": 29.0, \"Humidity\": 64, \"EventEnqueuedUtcTime\": \"2025-10-30T10:02:00Z\" }, { \"DeviceId\": \"sensor-003\", \"Temperature\": 27.8, \"Humidity\": 70, \"EventEnqueuedUtcTime\": \"2025-10-30T10:03:00Z\" } ] ```\n\n## 9. Define Query\n\n- Use **Test Query** in the portal for quick validation.\n\n``` SELECT DeviceId, AVG(Temperature) AS AvgTemperature, COUNT(*) AS ReadingCount, System.Timestamp AS WindowEndTime INTO BlobOutput FROM InputStream TIMESTAMP BY EventEnqueuedUtcTime GROUP BY DeviceId, TumblingWindow(minute, 5) ```\n\n## 10. Start and Validate\n\n- Start the job.\n- **Upload the Sample data to Input Blob:** input-container/sample-input.json\n- Monitor the job:\n- **Input Events**\n- **Output Events**\n- Check output files in output-container.\n\n# Troubleshooting\n\n- **Input Events = 0** → Verify path pattern and folder structure.\n- **Role assignment** → Ensure required role assignment is configured at Storage Account level/individual Container level.\n- **Private Connectivity issues** → Ensure Managed Private Endpoint configuration is complete and verify that the 'Test connection' for both Input and Output succeeds.\n\n# Automation with Terraform\n\nHere’s a snippet to automate key steps:\n\n```\n# Create a Resource Group\nresource \"azurerm_resource_group\" \"example\" { name = \"asa-rg\" location = \"Central US\" }\n\n# Create a Stream Analytics Cluster\nresource \"azurerm_stream_analytics_cluster\" \"example\" { name = \"asa-cluster1\" resource_group_name = azurerm_resource_group.example.name location = azurerm_resource_group.example.location streaming_capacity = 36 }\n\n# Create a User Assigned Managed Identity\nresource \"azurerm_user_assigned_identity\" \"example\" { location = azurerm_resource_group.example.location name = \"asa-job-identity\" resource_group_name = azurerm_resource_group.example.name }\n\n# Assign Role to Managed Identity to access Storage Account\nresource \"azurerm_role_assignment\" \"example\" { scope = azurerm_storage_account.example.id role_definition_name = \"Storage Blob Data Contributor\" principal_id = azurerm_user_assigned_identity.example.principal_id }\n\n# Create a Stream Analytics Job using User Assigned Managed Identity\nresource \"azurerm_stream_analytics_job\" \"example\" { name = \"asa-job1\" resource_group_name = azurerm_resource_group.example.name location = azurerm_resource_group.example.location compatibility_level = \"1.2\" data_locale = \"en-GB\" events_late_arrival_max_delay_in_seconds = 60 events_out_of_order_max_delay_in_seconds = 50 events_out_of_order_policy = \"Adjust\" output_error_policy = \"Drop\" streaming_units = 3 sku_name = \"Standard\" stream_analytics_cluster_id = azurerm_stream_analytics_cluster.example.id type = \"Cloud\" identity { type = \"UserAssigned\" identity_ids = [azurerm_user_assigned_identity.example.id] } content_storage_policy = \"SystemAccount\" transformation_query = <<QUERY SELECT DeviceId, AVG(Temperature) AS AvgTemperature, COUNT(*) AS ReadingCount, System.Timestamp AS WindowEndTime INTO BlobOutput FROM InputStream TIMESTAMP BY EventEnqueuedUtcTime GROUP BY DeviceId, TumblingWindow(minute, 5) QUERY\n\ndepends_on = [azurerm_role_assignment.example] }\n\n# Create a Managed Private Endpoint for Stream Analytics Job to access Storage Account\nresource \"azurerm_stream_analytics_managed_private_endpoint\" \"example\" { resource_group_name = azurerm_resource_group.example.name stream_analytics_cluster_name = azurerm_stream_analytics_cluster.example.name name = \"asa-mpe-storage\" target_resource_id = azurerm_storage_account.example.id subresource_name = \"blob\" }\n\n# Configure Input Blob and Output Blob for Stream Analytics Job using MSI authentication mode\nresource \"azurerm_stream_analytics_stream_input_blob\" \"example\" { name = \"InputStream\" resource_group_name = azurerm_resource_group.example.name stream_analytics_job_name = azurerm_stream_analytics_job.example.name storage_account_name = azurerm_storage_account.example.name storage_account_key = azurerm_storage_account.example.primary_access_key storage_container_name = azurerm_storage_container.input.name path_pattern = \"\" date_format = \"yyyy-MM-dd\" time_format = \"HH\" authentication_mode = \"Msi\" serialization { type = \"Json\" encoding = \"UTF8\" } }\n\nresource \"azurerm_stream_analytics_output_blob\" \"example\" { name = \"BlobOutput\" resource_group_name = azurerm_resource_group.example.name stream_analytics_job_name = azurerm_stream_analytics_job.example.name date_format = \"yyyy-MM-dd\" time_format = \"HH\"\n\nstorage_account_name = azurerm_storage_account.example.name storage_account_key = azurerm_storage_account.example.primary_access_key storage_container_name = azurerm_storage_container.output.name path_pattern = \"output/{date}/{time}\" authentication_mode = \"Msi\" serialization { type = \"Json\" encoding = \"UTF8\" format = \"LineSeparated\" } }\n\n# Create a Stream Analytics Job Schedule\nresource \"azurerm_stream_analytics_job_schedule\" \"example\" { stream_analytics_job_id = azurerm_stream_analytics_job.example.id start_mode = \"JobStartTime\"\n\ndepends_on = [azurerm_stream_analytics_stream_input_blob.example, azurerm_stream_analytics_output_blob.example] }\n\n```\n\n# Disclaimer\n\nThe information provided in this article is for educational and informational purposes only. While the steps and configurations described are based on best practices for Azure Stream Analytics, they should be validated in your own environment before implementation. Microsoft services and features may change over time; always refer to the official https://learn.microsoft.com/azure/ for the latest guidance. The author assumes no responsibility for any issues arising from the use of this content in production environments.\n\n# References\n\n- https://learn.microsoft.com/en-gb/azure/stream-analytics/\n- https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/stream\\_analytics\\_cluster\n\nUpdated Nov 17, 2025\n\nVersion 1.0\n\n[!\\[PratibhaShenoy&#x27;s avatar\\](https://techcommunity.microsoft.com/t5/s/gxcuf89792/m_assets/avatars/default/avatar-7.svg?image-dimensions=50x50)](/users/pratibhashenoy/2460949) [PratibhaShenoy](/users/pratibhashenoy/2460949) ![Icon for Microsoft rank](https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/cmstNC05WEo0blc?image-dimensions=100x16&amp;constrain-image=true)Microsoft\n\nJoined May 08, 2024\n\n[View Profile](/users/pratibhashenoy/2460949)\n\n/category/azure/blog/azureinfrastructureblog [Azure Infrastructure Blog](/category/azure/blog/azureinfrastructureblog) Follow this blog board to get notified when there's new activity",
  "ProcessedDate": "2025-11-17 11:04:26",
  "Description": "# Introduction\n\nModern data pipelines demand security and compliance. Azure Stream Analytics supports **Dedicated Clusters** and **Managed Private Endpoints**, enabling jobs to run in isolated environments and connect privately to Azure resources without exposing them to the public internet.\n\n**Key benefits:**\n\n- **Private connectivity** via Azure Private Link.\n- **Managed Identity** for secure, keyless authentication.\n- **Dedicated clusters** for predictable performance and isolation.\n\n# Architecture Overview\n\n![]()\n\n# Prerequisites\n\n- Azure subscription.\n- **Blob Storage** with Public Network Access disabled and two containers configured for Input and Output.\n- **User Assigned Managed Identity** created.\n\n# Implementation Steps\n\n## 1. Assign Managed Identity\n\n1. Navigate to **Storage Account → Access Control (IAM)**.\n2. Add role assignment:\n- **Role**: Storage Blob Data Contributor\n- **Scope**: Storage Account\n- **Assign to**: Your User Assigned Managed Identity.\n\n**Note:** In this example, the role assignment is applied at the **Storage Account level**, which grants access to all containers within the account. If needed, you can scope the assignment to an individual container for more granular control.\n\n![]()\n\n## 2. Configure Stream Analytics Job\n\n1. In the **Azure portal,** go to **Stream Analytics Jobs** and create a new job if one does not already exist:\n\n- **Name:** e.g., stream-job\n- **Hosting environment:** Cloud\n- **Streaming units:** 1\n2. **Managed Identity:** Enable Managed Identity and assign the User Assigned Managed Identity to the job.\n\n![]()\n\n## 3. Configure Stream Analytics Cluster\n\nThe Stream Analytics Cluster resource provides dedicated compute for running multiple jobs securely and at scale.\n\n1. In **Azure Portal**, go to **Stream Analytics Clusters**.\n2. Select your cluster or create a new one:\n- **Name**: e.g., stream-cluster\n- **Streaming unit details**: 12\n- **Location**: Same as your Blob Storage and Stream Analytics Job\n\n![]()\n\n## 4. Add Stream Analytics Job to Cluster\n\nOnce the Stream Analytics Cluster resource is provisioned, from **Stream Analytics Cluster → Settings → Stream Analytics Jobs → add the Stream Analytics Job.**\n\n![]()\n\n## 5. Add Managed Private Endpoint\n\nFrom **Stream Analytics Cluster → Settings → Managed Private Endpoints → add a new Managed Private Endpoint.**\n\n- - Select **Blob Storage** as the target resource.\n- Approve the Private Endpoint connection on the target resource.\n- Ensure Managed Private Endpoint setup is completed.\n\n![]()\n\n## 6. Configure Blob Input\n\n- **Input alias**: InputStream\n- **Container**: input-container\n- **Event serialization**: JSON\n- **Encoding**: UTF-8\n\n![]()\n\n## 7. Configure Blob Output\n\n- **Output alias**: BlobOutput\n- **Container**: output-container\n- **Path pattern**: output/{date}/{time}\n- **Serialization**: JSON\n\n![]()\n\n## 8. Prepare Sample Input\n\n- **Create** *sample-input.json*:\n\n- [\n{ \"DeviceId\": \"sensor-001\", \"Temperature\": 28.5, \"Humidity\": 65, \"EventEnqueuedUtcTime\": \"2025-10-30T10:00:00Z\" }, { \"DeviceId\": \"sensor-002\", \"Temperature\": 30.2, \"Humidity\": 60, \"EventEnqueuedUtcTime\": \"2025-10-30T10:01:00Z\" }, { \"DeviceId\": \"sensor-001\", \"Temperature\": 29.0, \"Humidity\": 64, \"EventEnqueuedUtcTime\": \"2025-10-30T10:02:00Z\" }, { \"DeviceId\": \"sensor-003\", \"Temperature\": 27.8, \"Humidity\": 70, \"EventEnqueuedUtcTime\": \"2025-10-30T10:03:00Z\" } ]\n\n## 9. Define Query\n\n- Use **Test Query** in the portal for quick validation.\n- SELECT\nDeviceId, AVG(Temperature) AS AvgTemperature, COUNT(\\*) AS ReadingCount, System.Timestamp AS WindowEndTime INTO BlobOutput FROM InputStream TIMESTAMP BY EventEnqueuedUtcTime GROUP BY DeviceId, TumblingWindow(minute, 5)\n\n## 10. Start and Validate\n\n- Start the job.\n- **Upload the Sample data to Input Blob:** input-container/sample-input.json\n- Monitor the job:\n- **Input Events**\n- **Output Events**\n- Check output files in output-container.\n\n![]()\n\n# Troubleshooting\n\n- **Input Events = 0** → Verify path pattern and folder structure.\n- **Role assignment** → Ensure required role assignment is configured at Storage Account level/individual Container level.\n- **Private Connectivity issues** → Ensure Managed Private Endpoint configuration is complete and verify that the 'Test connection' for both Input and Output succeeds.\n\n![]()\n\n# Automation with Terraform\n\nHere’s a snippet to automate key steps:\n- # Create a Resource Group\nresource \"azurerm\\_resource\\_group\" \"example\" { name = \"asa-rg\" location = \"Central US\" }\n\n# Create a Stream Analytics Cluster\nresource \"azurerm\\_stream\\_analytics\\_cluster\" \"example\" { name = \"asa-cluster1\" resource\\_group\\_name = azurerm\\_resource\\_group.example.name location = azurerm\\_resource\\_group.example.location streaming\\_capacity = 36 }\n\n# Create a User Assigned Managed Identity\nresource \"azurerm\\_user\\_assigned\\_identity\" \"example\" { location = azurerm\\_resource\\_group.example.location name = \"asa-job-identity\" resource\\_group\\_name = azurerm\\_resource\\_group.example.name }\n\n# Assign Role to Managed Identity to access Storage Account\nresource \"azurerm\\_role\\_assignment\" \"example\" { scope = azurerm\\_storage\\_account.example.id role\\_definition\\_name = \"Storage Blob Data Contributor\" principal\\_id = azurerm\\_user\\_assigned\\_identity.example.principal\\_id }\n\n# Create a Stream Analytics Job using User Assigned Managed Identity\nresource \"azurerm\\_stream\\_analytics\\_job\" \"example\" { name = \"asa-job1\" resource\\_group\\_name = azurerm\\_resource\\_group.example.name location = azurerm\\_resource\\_group.example.location compatibility\\_level = \"1.2\" data\\_locale = \"en-GB\" events\\_late\\_arrival\\_max\\_delay\\_in\\_seconds = 60 events\\_out\\_of\\_order\\_max\\_delay\\_in\\_seconds = 50 events\\_out\\_of\\_order\\_policy = \"Adjust\" output\\_error\\_policy = \"Drop\" streaming\\_units = 3 sku\\_name = \"Standard\" stream\\_analytics\\_cluster\\_id = azurerm\\_stream\\_analytics\\_cluster.example.id type = \"Cloud\" identity { type = \"UserAssigned\" identity\\_ids = [azurerm\\_user\\_assigned\\_identity.example.id] } content\\_storage\\_policy = \"SystemAccount\" transformation\\_query =\n\n# Disclaimer\n\nThe information provided in this article is for educational and informational purposes only. While the steps and configurations described are based on best practices for Azure Stream Analytics, they should be validated in your own environment before implementation. Microsoft services and features may change over time; always refer to the official https://learn.microsoft.com/azure/ for the latest guidance. The author assumes no responsibility for any issues arising from the use of this content in production environments.\n\n# References\n\n- https://learn.microsoft.com/en-gb/azure/stream-analytics/\n- https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/stream\\_analytics\\_cluster",
  "FeedName": "Microsoft Tech Community",
  "OutputDir": "_community",
  "Link": "https://techcommunity.microsoft.com/t5/azure-infrastructure-blog/secure-azure-stream-analytics-jobs-in-dedicated-clusters-using/ba-p/4470385"
}
