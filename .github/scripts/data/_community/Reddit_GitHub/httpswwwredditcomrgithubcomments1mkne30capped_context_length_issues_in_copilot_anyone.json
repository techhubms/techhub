{
  "FeedUrl": "https://www.reddit.com/r/github/.rss",
  "Link": "https://www.reddit.com/r/github/comments/1mkne30/capped_context_length_issues_in_copilot_anyone/",
  "Tags": [
    "github"
  ],
  "OutputDir": "_community",
  "FeedName": "Reddit GitHub",
  "ProcessedDate": "2025-08-08 16:18:53",
  "Title": "Capped Context Length Issues in Copilot - Anyone Else Experiencing This?",
  "FeedLevelAuthor": "Github: social coding",
  "EnhancedContent": "I've been testing various models in Copilot and noticed they're all capping out at around 128k context length (Found this out with some debugging), even though some models like GPT-5 are supposed to handle 400k. This is causing conversations to get summarized way too early and breaking continuity. Same observation with Sonnet-4, gemini-2.5-pro, gpt-4.1.\n\nHas anyone else run into this? Is this a known limitation right now, or am I missing something in the settings?\n\nReally hoping this gets bumped up to the full supported lengths soon — would make such a difference for longer conversations and complex tasks. Also wasting our Premium requests as part of shorter agent context lengths.\n\nScreenshots attached to which tells what is the actual context length of the model.\n\nAnyone from Copilot team noticing this, Plz restore to full context length.\n\nI think these are deliberately capped as per their pricing/ plan design.\n\nThen in that case they should be really transparent about it. I recently switched to copilot from cursor which, and they show in the chat window itself that how much of token is supported by the Model.",
  "Author": "EfficientApartment52",
  "PubDate": "2025-08-08T06:02:15+00:00",
  "Description": "| [!\\[Capped Context Length Issues in Copilot - Anyone Else Experiencing This?\\](https://b.thumbs.redditmedia.com/5_hkgysySffsOgk9xlXcC2I1SWLEhXKVLiafH_9wrzs.jpg \"Capped Context Length Issues in Copilot - Anyone Else Experiencing This?\")](https://www.reddit.com/r/github/comments/1mkne30/capped_context_length_issues_in_copilot_anyone/) | I've been testing various models in Copilot and noticed they're all capping out at around 128k context length (Found this out with some debugging), even though some models like GPT-5 are supposed to handle 400k. This is causing conversations to get summarized way too early and breaking continuity.<br> Same observation with Sonnet-4, gemini-2.5-pro, gpt-4.1.<br><br> <br>Has anyone else run into this? Is this a known limitation right now, or am I missing something in the settings?<br><br> <br>Really hoping this gets bumped up to the full supported lengths soon — would make such a difference for longer conversations and complex tasks. Also wasting our Premium requests as part of shorter agent context lengths.<br><br> <br>Screenshots attached to which tells what is the actual context length of the model.<br><br> <br>Anyone from Copilot team noticing this, Plz restore to full context length.<br>"
}
