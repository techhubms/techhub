---
layout: "post"
title: ".NET AI Community Standup: High-Performance AI Inference on a Budget"
description: "This community standup delves into accelerating AI inference using .NET technologies, focusing on making machine learning workloads, traditionally developed in Python, more efficient and cost-effective with .NET. The session discusses leveraging Hugging Face models, ONNX Runtime, new APIs in .NET 9 and 10, and the design of flexible AI libraries for production-ready AI. Demonstrations, GPU utilization comparisons, and actionable recommendations are provided for developers aiming to deploy fast, budget-conscious AI solutions on Microsoft platforms."
author: "dotnet"
excerpt_separator: <!--excerpt_end-->
canonical_url: "https://www.youtube.com/watch?v=ptdNWGj8CN8"
viewing_mode: "internal"
feed_name: "DotNet YouTube"
feed_url: "https://www.youtube.com/feeds/videos.xml?channel_id=UCvtT19MZW8dq5Wwfu6B0oxw"
date: 2025-08-06 20:46:02 +00:00
permalink: "/2025-08-06-NET-AI-Community-Standup-High-Performance-AI-Inference-on-a-Budget.html"
categories: ["AI", "Coding", "ML"]
tags: [".NET", ".NET 10", ".NET 9", "AI", "AI Inference", "AI Library Design", "AI Performance", "AIML", "Coding", "Cost Optimization", "Demo", "Developer", "Developer Community", "Developer Tools", "Dotnetdeveloper", "GPU Utilization", "Hugging Face Models", "Machine Learning", "MachineLearning", "ML", "Onnx", "ONNX Runtime", "Production AI", "Python Integration", "Software Developer", "Videos"]
tags_normalized: ["dotnet", "dotnet 10", "dotnet 9", "ai", "ai inference", "ai library design", "ai performance", "aiml", "coding", "cost optimization", "demo", "developer", "developer community", "developer tools", "dotnetdeveloper", "gpu utilization", "hugging face models", "machine learning", "machinelearning", "ml", "onnx", "onnx runtime", "production ai", "python integration", "software developer", "videos"]
---

Hosted by the dotnet team, this session features Bruno Capuano and Tal Wald as they demonstrate strategies for achieving fast, cost-effective AI inference using .NET, ONNX Runtime, and modern APIs.<!--excerpt_end-->

{% youtube ptdNWGj8CN8 %}

# .NET AI Community Standup: Blazing-Fast AI Inference on a Budget

**Presented by:** Bruno Capuano and Tal Wald  
**Hosted by:** dotnet team

## Overview

This session explores how developers can process over 20,000 sentences per second with minimal expenses by leveraging .NET for AI inference. The presenters demonstrate how AI workloads, typically developed in Python, can be accelerated and optimized using Microsoft's .NET ecosystem.

Highlights include:

- Leveraging Hugging Face models in .NET workflows
- Utilizing ONNX Runtime for high-performance inference
- Making use of the latest .NET 9 and 10 APIs
- Designing flexible AI libraries to avoid tight coupling with specific engines or hardware

## Key Topics Covered

### Migrating from Python to .NET for AI

- Advantages of migrating AI/ML workloads to .NET
- Compatibility with popular Python-based workflows

### Hugging Face Model Integration

- Using pre-trained models from Hugging Face repositories
- Adapting models for .NET-based inference

### High-Performance Inference with ONNX Runtime

- Introduction to ONNX and ONNX Runtime
- Benchmarks: achieving over 20,000 sentences/sec
- Strategies for cost-effective deployment

### .NET 9 & 10 AI APIs

- Overview of new APIs supporting AI workloads
- Increased flexibility and abstraction for developers
- Decoupling inference logic from underlying hardware

### AI Library Architecture

- Building libraries that interface with multiple inference backends
- Best practices for extensibility and performance

### Demos and Performance Comparisons

- Real-world demonstrations of .NET-based AI inference
- GPU versus CPU utilization metrics
- Cost and performance trade-offs

### From Research to Production

- Tips for transitioning ML research prototypes to scalable, production-ready solutions
- Managing operational costs

## Useful Links

- [Community Links Collection](https://learn.microsoft.com/en-us/collections/yjwzhet31ez28w)
- [Bruno Capuano on LinkedIn](https://www.linkedin.com/in/elbruno/)
- [Tal Wald on LinkedIn](https://www.linkedin.com/in/tal-wald/)

## Conclusion

This standup provides actionable insights for .NET developers aiming to bring AI workloads to production efficiently. By combining ONNX Runtime, .NETâ€™s new APIs, and proper architectural design, high performance and low cost are achievable without sacrificing flexibility.
