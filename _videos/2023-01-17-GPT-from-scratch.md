---
layout: "post"
title: "Let's build GPT from scratch, in code, spelled out."
author: "Andrej Karpathy"
canonical_url: "https://www.youtube.com/watch?reload=9&v=kCc8FmEb1nY"
excerpt_separator: <!--excerpt_end-->
categories: ["ML"]
permalink: "/2023-01-17-GPT-from-scratch.html"
viewing_mode: "internal"
tags: ["GPT", "Language Models", "ML", "Neural Networks", "PyTorch", "Videos"]
tags_normalized: ["gpt", "language models", "ml", "neural networks", "pytorch", "videos"]
---

We build a Generatively Pretrained Transformer (GPT), following the paper "Attention is All You Need" and OpenAI's GPT-2 / GPT-3.<!--excerpt_end-->

{% youtube kCc8FmEb1nY %}

We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!). I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch.nn, which we take for granted in this video.
