{
  "FeedUrl": "https://blog.fabric.microsoft.com/en-us/blog/feed/",
  "OutputDir": "_news",
  "FeedName": "Microsoft Fabric Blog",
  "Description": "As data becomes more immediate, the gap between when an event occurs and when you must use its insights is getting smaller. Fabric Eventstreams enables users to ingest, transform, extract insights and route streaming data where it’s needed. In response to the growing demand for SQL-based transformations, we have introduced a new SQL operator feature …\n\n[Continue reading “Fabric Eventstream SQL Operator: Your tool kit to Real-Time data processing in Fabric Real-Time Intelligence”](https://blog.fabric.microsoft.com/en-us/blog/fabric-eventstream-sql-operator-your-tool-kit-to-real-time-data-processing-in-fabric-real-time-intelligence/)",
  "FeedLevelAuthor": "Microsoft Fabric Blog",
  "Author": "Microsoft Fabric Blog",
  "Link": "https://blog.fabric.microsoft.com/en-US/blog/fabric-eventstream-sql-operator-your-tool-kit-to-real-time-data-processing-in-fabric-real-time-intelligence/",
  "ProcessedDate": "2025-12-18 17:05:00",
  "PubDate": "2025-12-18T09:59:11+00:00",
  "Title": "Fabric Eventstream SQL Operator: Your tool kit to Real-Time data processing in Fabric Real-Time Intelligence",
  "Tags": [],
  "EnhancedContent": "As data becomes more immediate, the gap between when an event occurs and when you must use its insights is getting smaller. Fabric Eventstreams enables users to ingest, transform, extract insights and route streaming data where it’s needed. In response to the growing demand for SQL-based transformations, we have introduced a new **SQL operator** feature — enabling you to apply SQL logic directly to your live data streams.\n\n## Why the SQL Operator Matters\n\nThe SQL Operator brings the power of SQL to real-time data transformation. It enables low-code, real-time processing using familiar SQL syntax, complementing built-in no-code operators, and running on the proven **Azure** **Stream Analytics** runtime.\n\nImplementing logic upstream minimizes post-processing latency and ensures that downstream pipelines remain streamlined. The SQL operator enables the development of complex data processing logic within Eventstream, facilitating the creation of advanced transformation rules at an early stage using the familiar SQL language. For a step-by-step guild on how to add SQL operator to your Eventstream topology, please refer to [From Clicks to Code: SQL Operator under Fabric Eventstream (Preview)](https://blog.fabric.microsoft.com/blog/from-clicks-to-code-sql-operator-now-in-public-preview-under-fabric-eventstream/).\n\nIn this blog post, we will explore examples of SQL-based analysis and transformation within Eventstream using some sample scenarios.\n\nWith a rich collection of built-in functions, the Eventstream SQL operator simplifies most everyday data transformation and processing needs.\n\nFor details on all supported SQL functions, refer to the [Built-in Functions](https://learn.microsoft.com/stream-analytics-query/built-in-functions-azure-stream-analytics) documentation.\n\nThe following table gives an overview of the different data transformation functions provided by the Eventstream SQL operator.\n\n| **Function Category** | **Description** | | --- | --- | | [Aggregate Functions](https://learn.microsoft.com/stream-analytics-query/aggregate-functions-azure-stream-analytics) | Operate on a collection of values but return a single, summarizing value. | | [Analytic Functions](https://learn.microsoft.com/stream-analytics-query/analytic-functions-azure-stream-analytics) | Return a value based on defined constraints. | | [Array Functions](https://learn.microsoft.com/stream-analytics-query/array-functions-stream-analytics) | Returns information from an array. | | [GeoSpatial Functions](https://learn.microsoft.com/en-us/stream-analytics-query/geospatial-functions) | Perform specialized GeoSpatial functions. | | [Input Metadata Functions](https://learn.microsoft.com/stream-analytics-query/input-metadata-functions) | Query the metadata of property in the data input. | | [Record Functions](https://learn.microsoft.com/stream-analytics-query/record-functions-azure-stream-analytics) | Returns record properties or values. | | [Windowing Functions](https://learn.microsoft.com/stream-analytics-query/windowing-azure-stream-analytics) | Perform operations on events within a time window. | | [Scalar Functions](https://learn.microsoft.com/stream-analytics-query/built-in-functions-azure-stream-analytics#BKMK_ScalarFunctions) | Operate on a single value and then return a single value. Scalar functions can be used wherever an expression is valid. |\n\n## Scenario\n\nTo illustrate how SQL operators support real-time data processing in Eventstream, let’s consider a practical example. Imagine an e-commerce platform where every time an order is placed, an event is generated and sent to Eventstream through a custom endpoint for real-time order analytics.\n\n| **Field** | **Type** | **Example** | | --- | --- | --- | | orderId | String | 98211 | | customerId | String | 2040 | | orderAmount | Float | 129.99 | | City | String | “Seattle” | | timestamp | Datetime | 2025-11-06T22:34:55Z |\n\nObjectives: In this scenario, we’ll demonstrate how to use the Eventstream SQL operator to accomplish these real-time data processing tasks.\n\n1. Calculate *per-minute sales totals per city* and report high spikes in order of value.\n2. Detect Bot Attacks in Real Time. You want to identify possible **bot-like order bursts** — situations where a single customer places more than 10 orders within 2 minutes.\n3. Detect outlier orders — say, any order that’s more than 1.5 times the city’s 5-minute rolling average.\n\n![A diagram of a software development process AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/a-diagram-of-a-software-development-process-ai-ge.png)\n\nFigure 1: Eventstream ingests order-created events from various regions and processes the data in real time to deliver target outcomes.\n\nCreate an Eventstream and configure real-time ingestion with a custom endpoint.\n\nNext, in edit mode after default stream, Eventstream suggest you to either transform or add destinations. From this drop-down menu, select the SQL Code option to add data processing logic using SQL expressions. Notice that Eventstream also provides a variety of built-in no code transformations for building processing rules. You can also build powerful transformation logic using no code options. In this blog post, we will focus on the SQL-based data transformation capabilities.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/word-image-32407-2.png)\n\nFigure 2: Eventstream edit topology showing a custom endpoint as source, default stream, and SQL code option for event transformation.\n\nSelect SQL node in the topology and click edit query to enter the code editor experience.\n\n[![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/i3bmp.bmp)](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/i3bmp.bmp)\n\nFigure 3: Illustrates a SQL node included in the topology, featuring an edit query button on the right panel that opens the query editor view.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/a-screenshot-of-a-computer-ai-generated-content-m-1.png)\n\nFigure 4: Query editor view of SQL operator\n\nInside the query editor we can analyze the incoming data, test SQL data transformation expressions and build transformation logic to complete the objectives that we have defined above. Use familiar T-SQL projections and filter expressions to analyze the incoming stream.\n\nLet’s review our objectives to understand what data processing logic we need to define.\n\n### 1. Calculate per-minute sales totals per city and report high spikes in order value.\n\nThe aim is to compute per-minute sales totals for each city and identify unusual spikes in order values. Sales totals should include the total number of orders, the total order amount, and the average order amount. To achieve this, we will add the SQL operator node to Eventstream and implement aggregation logic that calculates the required sales metrics for every city. Next, configure an Eventhouse destination named “citySalesAgg” to send these aggregated results for visualization in Power BI and Real-Time Dashboard.\n\n``` SELECT ```\n\n``` System.Timestamp AS WindowEnd,city,COUNT(orderId) AS OrderCount,SUM(orderAmount) AS TotalRevenue,AVG(orderAmount) AS AvgOrderValueINTOcitySalesAggFROM[ECommerceExample-stream]GROUP BY city,TumblingWindow(minute, 1) ```\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/a-screenshot-of-a-computer-ai-generated-content-m-2.png)\n\nFigure 5: Test results for the query in example1 are displayed in the SQL query editor, allowing for analysis and editing as necessary.\n\nUnder test results, notice that there are bigger precision values for AvgOrderValue and TotalRevenue. Let’s modify the query and round off these values.\n\n![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/a-screenshot-of-a-computer-ai-generated-content-m-3.png)\n\nThis seems fine, but what happens if there are repeated events? Let’s analyze this example. ![A screenshot of a computer AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/a-screenshot-of-a-computer-ai-generated-content-m-4.png)\n\nThe query added the same event 4 times to calculate total revenue.\n\nLet’s modify the query to fix this problem.\n\n``` WITH Temp AS ( SELECT orderAmount,orderID,city FROM [ECommerceExample-stream] GROUP BY orderAmount,orderID,city, System.Timestamp())SELECT System.Timestamp AS WindowEnd, city, COUNT(orderId) AS OrderCount, ROUND(SUM(orderAmount),2) AS TotalRevenue, ROUND(AVG(orderAmount),2) AS AvgOrderValueINTO citySalesAggFROM TempGROUP BY city, TumblingWindow(minute, 1) ```\n\n``` When the first statement is executed, the duplicate records are combined into one as the fields in the group by clause are all the same. Therefore, it removes the duplicates. ```\n\nSQL editor enables you to develop and test the processing logic incrementally with ease and lets to analyze the input data with test results for you to handle such cases while designing the processing logic.\n\n### 2. Detecting Bot Attacks in Real-time.\n\nThe next objective is to detect potential **bot-like order bursts**, defined as instances where an individual customer submits **more than 10 orders within a two-minute window**.\n\nTo address this, we will implement a sliding window aggregate query that quantifies the number of orders placed by each customer within short time frames.\n\nIn real time-streaming scenarios, performing operations on the data contained in temporal windows is a common pattern.\n\nNext in our SQL editor, we will use this SQL query to highlight suspicious customer behavior and configure a curated stream (derived stream) containing only these flagged activities. Specifically, we will choose the Derived Stream output destination and assign it the name “Suspicious orders.”\n\nLet’s use the code snippet and select the Test query button to analyze the result. After validating the result, select Save to save and exit the code editor.\n\n``` SELECT System.Timestamp AS WindowEnd, customerId, COUNT(orderId) AS OrdersPerCustomerINTO SuspiciousOrdersFROM [ECommerceExample-stream]GROUP BY customerId, SlidingWindow(minute, 2) -- 2-minute windowHAVING COUNT(orderId) >=10 ```\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/word-image-32407-9.png)\n\nFigure 6: Test results for the query in example2 are displayed in the SQL query editor, allowing for analysis and editing as necessary.\n\n### 3.Detecting outlier orders\n\nFor instance, any order that’s more than 1.5 times the city’s 5-minute rolling average. Let’s add another SQL operator and use the sql query from the code snippet. This query continuously compares new orders against their city’s rolling average — flagging anomalies in near-real time. Add Activator destination to this sql node to set alerts when we detect these anomalies.\n\n``` WITH CityAverages AS ( SELECT System.Timestamp AS windowEnd, city, AVG(orderAmount) AS RollingAvg FROM [ECommerceExample-stream] TIMESTAMP BY eventTime GROUP BY city, SlidingWindow(minute, 5))SELECT o.orderId, o.city, o.orderAmount, a.RollingAvg, a.windowEnd, CASE WHEN o.orderAmount > a.RollingAvg * 1.5 THEN 'Anomaly' ELSE 'Normal' END AS OrderStatusINTO ActivatorFROM [ECommerceExample-stream] o TIMESTAMP BY eventTimeJOIN CityAverages aON o.city = a.cityAND DATEDIFF(second, o, a) BETWEEN 0 AND 60 ```\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/word-image-32407-10.png)\n\nFigure 7: Test results for the query in example3 are displayed in the SQL query editor, allowing for analysis and editing as necessary.\n\nThe query used in this scenario is a multiset query where we are calculating a rolling (sliding) **average order amount per city** using a sliding window. Comparing each incoming order to that average. Then flagging orders that meet anomaly criteria using CASE statements. Then we **join** the incoming event (orderStream o) with the current rolling averages (CityAverages a) **by city**.\n\nAs shown in the figure above, under the test result section, our query is successfully detecting the anomalies in the order of events.\n\n![](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/LastFigure-1.bmp)\n\nWe have now achieved all three objectives using the power of SQL operator available withing Fabric Eventstream. We can now click Publish on the top right corner, as shown in the figure, to deploy this real-time data pipeline and process live traffic.\n\n![A diagram of a software AI-generated content may be incorrect.](//dataplatformblogwebfd-d3h9cbawf0h8ecgf.b01.azurefd.net/wp-content/uploads/2025/12/a-diagram-of-a-software-ai-generated-content-may.png)\n\nFigure 8: Eventstream receives data via a custom endpoint, processes it using an SQL operator, and then writes the results to various destinations.\n\n## Summary\n\nAs demonstrated in these scenarios, adding SQL node and centralizing all business logic using SQL syntax provides significant advantages:\n\n1. Advanced real-time data processing logic can be implemented within Eventstream, consolidating all logic in one SQL node. This greatly enhances the development experience by incrementally building complex logic and testing the result.\n2. Issues can be debugged more efficiently, and outputs are validated with ease. As requirements change frequently, extending the logic, testing results, and adjusting data destinations in the pipeline becomes straightforward.\n\nWith the introduction of robust SQL support in Eventstream, data engineers and analysts are now able to move far beyond basic ingestion, filtering, or type of conversion. Eventstream is evolving into a comprehensive stream analytics platform, enabling advanced real-time solutions to be deployed directly within Fabric.\n\nUsers can aggregate, join, enrich, and analyze streaming data utilizing expressive SQL queries, which facilitates powerful capabilities such as anomaly detection, real-time metrics, and automated business actions. This marks a new era of possibilities for developing intelligent and responsive real-time data pipelines with Eventstream—delivering both flexibility and productivity through SQL. Give the SQL operator in Eventstream a try today and let us know what you think.\n\n## Get help with SQL in Fabric Eventstream\n\nTo learn more about language support please refer our [MS learn](https://learn.microsoft.com/en-us/stream-analytics-query/stream-analytics-query-language-reference) page.\n\nAzure Stream Analytics has consistently employed the same SQL language semantics for many years. Users benefit from extensive community support, with a variety of solutions and patterns developed over time by stream analytics practitioners. The same knowledge base is now available for the Fabric Eventstream users to use and design real-time data processing pipelines in Fabric Real-Time Intelligence.\n\n## Need help or want to suggest an improvement?\n\nReach out to us on Real-Time Intelligence Forum: [Get Help with Real-Time Intelligence – Microsoft Fabric Community](https://community.fabric.microsoft.com/t5/Get-Help-with-Real-Time/ct-p/da_gethelp)\n\nRequest or upvote a suggestion on Fabric Ideas RTI: [Fabric Ideas – Microsoft Fabric Community](https://aka.ms/Fabric.Kusto.Ideas)"
}
