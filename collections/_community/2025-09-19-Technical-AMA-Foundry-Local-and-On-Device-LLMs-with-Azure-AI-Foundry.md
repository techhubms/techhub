---
external_url: https://techcommunity.microsoft.com/t5/microsoft-developer-community/join-us-for-a-technical-deep-dive-and-q-a-on-foundry-local-llms/ba-p/4455060
title: 'Technical AMA: Foundry Local and On-Device LLMs with Azure AI Foundry'
author: Lee_Stott
viewing_mode: external
feed_name: Microsoft Tech Community
date: 2025-09-19 07:00:00 +00:00
tags:
- AI Frameworks
- AI Workflow
- API Integration
- Azure AI Foundry
- CLI Tools
- Cloud Transition
- Cost Efficiency
- Edge AI
- Foundry Local
- Large Language Models
- LLM
- Local Development
- Microsoft
- Model Customization
- On Device Inference
- Private AI
- Prompt Engineering
- SDK
section_names:
- ai
- azure
---
Lee_Stott invites the community and AI developers to a hands-on AMA about Foundry Local—Microsoft’s edge AI toolkit for running and customizing LLMs on local hardware, with seamless integration to Azure AI Foundry.<!--excerpt_end-->

# Technical AMA: Foundry Local and On-Device LLMs with Azure AI Foundry

## Overview

Join Lee_Stott and the Foundry Local team for an Ask Me Anything session delving into local AI workflows and on-device inference. The event is scheduled for September 29th, 2025, and is tailored for practitioners who want to boost performance, privacy, and flexibility in AI development while leveraging the Microsoft Azure AI ecosystem.

[Register for the Azure AI Foundry Discord Community Event](https://discord.gg/azureaifoundry?event=1418186096830972056)

## What You'll Learn

- **Foundry Local Tooling**: A suite of SDKs, CLI tools, and APIs for building and evaluating LLM (Large Language Model) applications directly on your own hardware.
- **On-Device Inference**: Run pre-set or custom AI models locally, reducing cloud dependency, recurring costs, and keeping data secure on-premises.
- **Seamless Cloud Integration**: Quickly scale or migrate local models and pipelines to Azure AI Foundry for broader cloud capabilities.
- **Model Customization**: Tailor models to fit unique requirements directly in your own environment before productionizing.
- **Use Cases**: Ideal for edge deployments, environments with limited connectivity, privacy/regulatory constraints, and rapid AI prototyping.

## Event Highlights

- Deep dive demo of Foundry Local CLI and SDK workflows
- Step-by-step examples of local inference, model selection, and prompt engineering
- Best practices to move projects between local/dev and Azure cloud
- Q&A with core Microsoft technical staff, including Maanav Dalal (Product Manager, AI Frameworks)

## Key Features of Foundry Local

- **Performance**: Real-time, low-latency inference on your hardware
- **Privacy**: Keep sensitive data and model artifacts on local infrastructure
- **Cost Savings**: Avoid recurring cloud inference costs by utilizing your own compute
- **Integration**: SDK, REST API, and CLI for embedding into existing AI and app ecosystems

## Why Attend?

- Get actionable knowledge from Microsoft’s engineering team
- Network with other AI developers and practitioners
- Learn practical tips for deploying, customizing, and migrating LLM workloads at the edge and in the cloud

## Speaker

- **Maanav Dalal** – Product Manager, Foundry Local (AI Frameworks team, Microsoft)  
  [LinkedIn Profile](https://www.linkedin.com/in/maanavdalal/)  

## More Resources

- [Foundry Local documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/)
- Tag examples: #FoundryLocal #AzureAIFoundry #EdgeAI #LLM #AIWorkflow

**Published by Lee_Stott via Microsoft Developer Community Blog, Sep 19, 2025**

---

For anyone interested in robust, private, and flexible AI development on Microsoft platforms, this session is a hands-on entry point and networking opportunity.

This post appeared first on "Microsoft Tech Community". [Read the entire article here](https://techcommunity.microsoft.com/t5/microsoft-developer-community/join-us-for-a-technical-deep-dive-and-q-a-on-foundry-local-llms/ba-p/4455060)
